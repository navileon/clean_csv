Build and Deploy Secure AI Applications with AIShield and Amazon SageMaker

Adversarial machine learning (AML) attacks, also known as "artificial intelligence attacks" (AI attacks), involve deliberate attempts to manipulate or compromise machine learning models or even make it reveal sensitive information. AML attacks impact one or more of the three main tenets of security–confidentiality, availability, and integrity. A recent Gartner survey indicates that two in five organizations have experienced artificial intelligence (AI) security breaches, underscoring the urgency to address this issue. In this post, we will explore how AIShield's seamless integration within the Amazon SageMaker environment alleviates AI security concerns by mitigating risks before and after deployment, enabling customers to develop and deploy AI applications with confidence. Bosch Global Software Technologies (BGSW) is an AWS Select Tier Services Partner and AWS Marketplace Seller that is using machine learning solutions to improve its manufacturing, healthcare, and Internet of Things (IoT) solutions. Risks and Consequences of AI Attacks Adversarial machine learning attacks lead to various forms of organizational harm and loss—for example, financial, reputational, safety, intellectual property, people's information, and proprietary data. In healthcare, banking, automotive, telecom, public sector, and other industries, AI adoption suffers due to security risks from emerging attacks which relate to safety, nonconformance to AI principles, regulatory violations, and software security. AI attacks typically exploit vulnerabilities within data payloads. The traditional defense mechanisms like layer 3/4 network firewalls and layer 7 application firewalls often fall short in providing comprehensive protection against such threats. Specifically, the concept of zero trust architectures, while valuable in theory, can be a complex task to implement efficiently due to the persistent threat of adversarial attacks. AI Security Challenges The challenge of securing AI is complicated due to the multifaceted technology landscape, shortage of skilled staff, and unavailability of essential components of AI security in open source. Developing an enterprise solution from scratch can be costly, and responsible AI adoption and uncertainty about regulatory compliance provides further challenges. The cumulative impact includes reduced revenue due to delayed time to market and increased costs, making the return on investment (ROI) in AI less attractive for organizations. Organizations seek holistic AI security solutions that enhance their security posture, improve developer experience, seamlessly integrate into existing systems, ensure compliance, and offer resilience throughout the AI/ML development lifecycle. In the DevOps context, this means:

Developers require a straightforward solution that can scan AI/ML models, identify vulnerabilities, and automatically remediate them during the development phase. Deployers and operators, including security teams, need tools such as endpoint detection and response (EDR) specific to AI workloads. They need to rely on solutions capable of detecting and responding to emerging AI attacks to prevent incidents and reduce the mean time to detect (MTTD) and mean time to resolve (MTTR). Managers need visibility into the security posture of the AI/ML models they deploy to ensure better governance, compliance, and model risk management at an organizational level.

Enhancing Security of AI Workloads with AIShield AIShield is a comprehensive AI security product for organizations running AI workloads on Amazon SageMaker, helping them protect against AML attacks. By leveraging the capabilities of both AIShield and Amazon SageMaker, organizations can maximize the value of their AI/ML efforts. Let's explore how AIShield can be used by organizations to overcome AI security challenges in the AI/ML lifecycle.

Figure 1 – Intervention points of AI security in the ML development workflow. There are two main intervention points for AIShield in the ML development workflow:

Model evaluation: After model validation and prior to the deployment, AI models undergo evaluation. AIShield offers an API-based AI security vulnerability assessment, which can be used in the development workflow by developers to assess the security posture of their AI application.

Figure 2 – Outcomes of API-based AI security vulnerability assessment for AI/ML developers.

Model operationalization: The threat-informed defense model can be deployed within the model deployment environment for real-time detection of any attack vector. The defense model is trained to bifurcate the effective attack vectors from the original data.

Figure 3 – Defense model deployment architecture – parallel deployment. AIShield dashboards provide visualization to the managers on the overall security posture, analytics on the outcomes, and AI security licenses usages which can help lead to better model governance practices.

Figure 4 – AI security posture dashboard for organization leaders. AIShield in SageMaker Studio ML Development Workflow AIShield offers seamless integration through AWS Marketplace and provides easy access to its cloud-based solutions. AIShield seamlessly integrates with various AWS machine learning environments, including Amazon SageMaker Studio Notebooks and Amazon SageMaker Notebook Instances. SageMaker Studio Notebooks are highlighted in this post for their native features, including Amazon SageMaker Experiments and Amazon SageMaker Model Registry. These features offer improved organization, tracking, and management of models, providing valuable capabilities for users leveraging SageMaker Studio Notebooks. Consider a scenario where a developer is utilizing SageMaker Studio to build a machine learning model for a specific use case, such as digital recognition. In this context, let's see how AIShield can be integrated into this pipeline quickly.

Figure 5 – Reference architecture on AWS. This integration process is divided into two parts—developer flow and deployment flow—ensuring a comprehensive end-to-end integration of AIShield API with SageMaker Studio Notebook. Part 1: Developer Flow The developer flow focuses on the activities conducted by developers during the integration process (see Figure 2). 1.1 – Set Up SageMaker Studio When integrating AIShield into a machine learning pipeline like SageMaker Studio, the initial step involves installing the AIShield PyPI package. This installation facilitates rapid experimentation during model development. On the other hand, AIShield APIs provide scalable solutions that are better suited for MLOps pipelines, enabling seamless integration and deployment of AIShield functionalities.

!pip install aishield # install the aishield pypi package import aishield as ais # import the aishield packageHTML

1.2 – Model Development and Iterative Refinement As with any model development pipeline, the developer goes through the process of defining, training, and refining the model until it reaches the desired performance level. Following this, the model is typically stored in an Amazon Simple Storage Service (Amazon S3) bucket. 1.3 – API Shield API Integration After subscribing to AIShield, the AIShield API can be initialized by providing the necessary parameters (URL and organization ID) received during the subscription process. This initialization step allows developers to access and utilize the functionalities provided by the AIShield API.

""" Description: AIShield API URL and subscription keys """ baseurl = " " url = baseurl + "/api/ais/v1.5" org_id = " " # fill in Org_Id provided in welcome email

""" Description: Initialize the AIShield API """ client = ais.AIShieldApi(api_url=url, org_id=org_id)HTML

To leverage AIShield's model vulnerability analysis, the necessary data, labels, and model file are provided to AIShield, securely stored using pre-signed URLs utilizing Amazon S3 storage. AIShield adopts a low data and no model approach, typically utilizing a representative sample of 2-5% of the dataset to generate test reports and assess model performance. Furthermore, the model-under-test can be hosted as an API, with traffic routed through AWS PrivateLink to ensure a secure and private connection. For a quick demonstration, the model can be submitted to AIShield for analysis, involving the registration of the model within the AIShield system.

""" Description: Perform model registration and upload the input artifacts like sample Data, Label and Model """ status, job_details = client.register_model(task_type=task_type, analysis_type=analysis_type) model_id = job_details.model_id print('Model id: {} \nInput artifacts will be uploaded as:\n data_upload_uri: {}\n label_upload_uri: {}' '\n model_upload_uri: {}'.format(model_id, job_details.data_upload_uri, job_details.label_upload_uri, job_details.model_upload_uri))

upload_status = client.upload_input_artifacts(job_details=job_details, data_path=data_path, label_path=label_path, model_path=model_path) print('Upload status: {}'.format(', '.join(upload_status)))HTML

AIShield supports diverse task types, including image and tabular classification, time series forecasting, and offers analysis for various attack types like extraction and evasion. Users have the flexibility to choose specific tasks and desired attack analysis options within AIShield, ensuring customization and adaptability to meet their specific requirements.

""" Description: Define the task and analysis type """ task_type = ais.get_type("task", "image_classification") analysis_type = ais.get_type("analysis", "extraction")HTML

After initiating the vulnerability analysis in AIShield, once the analysis is completed the output artifacts such as reports, defense models, and attack samples can be uploaded to an S3 bucket.

""" Description: Run vulnerability analysis """ my_status, job_details = client.vuln_analysis(model_id=model_id, vuln_config=vuln_config) my_job_id = job_details.job_id print('status: {} \nJob_id: {} '.format(my_status, my_job_id)) print('Click on the URL to view Vulnerability Dashboard (GUI): {}'.format(job_details.job_monitor_uri)) """ Description: Fetch Job status using Job ID """ my_status = client.job_status (job_id = my_job_id) print('job status ', my_status) """ Description: Download the Vulnerability Report """ if my_status == "success": """ 1. Get aishield output artifacts & optionally upload to s3 bucket 2. Parse the reports & log the values to experimentation """ parse_reports(job_id=my_job_id, postfix_str=num_attack_queries, run_name=run_name)HTML

1.4 – SageMaker Experiments To further streamline the workflow and keep track of the AIShield artifacts, the S3 locations of these artifacts can be pushed to SageMaker Experiments. This integration ensures the AIShield output artifacts are organized, tracked, and readily available for analysis and further experimentation within the SageMaker environment.

# upload the defense model to S3 bucket def_model_converted_path = convert_to_tf_protobuf(os.path.join(OUTPUT_PATH, 'Defense_artifact')) prefix = "defense_model_{}".format(postfix_str) def_mdl_loc = sess.upload_data(path=def_model_converted_path, bucket=bucket_name, key_prefix=prefix) print("defense report saved at:\n", def_mdl_loc)

# log saved artifacts path with load_run(experiment_name=experiment_name, run_name=run_name) as run: run.log_artifact(name="AIShield_Vulnerability_Report", value=vul_report_loc, is_output=True) run.log_artifact(name="AIShield_Defense_Report", value=def_report_loc, is_output=True) run.log_artifact(name="AIShield_Defense_Model", value=def_mdl_loc, is_output=True)HTML

The process of analyzing model vulnerability and generating corresponding defenses using AIShield can be orchestrated for multiple runs, allowing for flexibility in adjusting parameters such as the number of attack queries and the attack type. This enables comprehensive testing and evaluation of the model's robustness under various scenarios, enhancing its overall security.

To improve tracking and analytics, the metrics and artifacts generated from multiple runs can be logged to SageMaker Experiments and can be analyzed through charts. This enables comprehensive record-keeping and facilitates in-depth analysis of the results, enhancing the overall monitoring and evaluation process. This information aids in making informed decisions for selecting the specific run and, consequently, the defense model to deploy in the subsequent process. As the number of attack queries increase, the efficacy of vulnerability detection and defense model accuracy increase (see Figure 6).

Figure 6 – For the three runs: (1) Number of attack queries; (2) Original model accuracy;(3) Vulnerability analysis scores; (4) Defense model accuracy. Following that, the model card along with its relevant configurations can be saved and pushed to an S3 bucket. This ensures the model card is properly stored and easily accessible for future reference and documentation purposes.

# Create model card my_card = ModelCard( name=model_card_name, status=ModelCardStatusEnum.DRAFT, model_overview=model_overview, training_details=training_details, intended_uses=intended_uses, sagemaker_session=sess, ) my_card.create()HTML

Part 2: Deployment Flow The deployment flow focuses on the activities involved in deploying the integrated solution (the defense model and original model are shown in Figure 3), making it available for use by end-users. This typically includes the following steps. 2.1 – Model Packaging Once developers are satisfied with their main application model's and defense model's performance as in the developer workflow, both can be downloaded from respective S3 buckets for deployment.

# Define the s3 location for original & defense model to be deployed (both models are created in the developer flow) orig_model_loc = 's3:// ' defense_model_loc = 's3:// ' orig_model_key = urlparse(orig_model_loc).path.strip("/") defense_model_key = urlparse(defense_model_loc).path.strip("/")

# Download both the models from s3 bucket s3.download_file(bucket, orig_model_key, "orig_model.tar.gz") s3.download_file(bucket, defense_model_key, "def_model.tar.gz")HTML

2.2 – Deployment and Monitoring Both the original and defense models can be deployed to the target environment, leveraging SageMaker deployment capabilities. Deploy the main application model as usual.

# Create a Sagemaker Tensorflow object for the original model orig_model = TensorFlowModel(model_data=orig_model_loc, role=role, framework_version="2.3.1")

# Deploy the original model predictor = orig_model.deploy(initial_instance_count=1, instance_type="ml.c4.large") print('predictor endpoint: ', predictor.endpoint)HTML

Next, deploy the defense model the same way as the original model.

# Create a Sagemaker Tensorflow object for the defense model defense_model = TensorFlowModel(model_data=defense_model_loc, role=role, framework_version="2.3.1")

# Deploy the defense model defense_predictor = defense_model.deploy(initial_instance_count=1, instance_type="ml.c4.large") print('Defense predictor endpoint: ', defense_predictor.endpoint)HTML

Figure 7 – Defense model deployment architecture. 2.3 – End-User Interaction The API abstraction can be done for parallel deployment of the defense model alongside the original model. The decision block can take remedial actions, such as blocking the user or randomizing the response from the main application model when the defense model identifies a malicious payload. Telemetry data can be sent to a Security Information and Event Management (SIEM) system like Splunk for further analysis and monitoring.

Figure 8 – Splunk SIEM reporting dashboard with AIShield integration. Use Case: Secure a Breast Cancer Screening AI Algorithm A Germany-based healthcare startup is using patented machine learning algorithms for accurate detection of breast cancer. To protect this core intellectual property from model extraction, evasion, and poisoning, the company needed a comprehensive security solution. AIShield's AI security product was leveraged by the company to conduct a risk vulnerability assessment of their AI model and improve its security posture. Key benefits of AIShield include:

Secures the core IP of ML algorithms for breast cancer detection. Protects against any manipulation with model evasion attacks and ensure patient safety. Helps adhere to medical cybersecurity guidelines and AI-based Software as a Medical Device (SaMD) cybersecurity requirements.

Use Case: Reduce Operational Losses from Credit Card Transaction Fraud A prominent UK bank, handling over 30% of the nation's £300 billion debit and credit card transactions, experienced nearly £70 million in external fraud losses in 2021 despite having a fraud detection model. This represented about 70% of total losses, marking it as the highest operational risk. The bank's CTO—along with AI/ML, risk and cybersecurity teams—aim to strengthen the current fraud detection model to decrease these losses and increase profit margins. The AIShield team and the bank's cybersecurity and AI teams conducted a joint proof-of-concept using the AIShield SaaS API. A fraud detection ML model was hosted on-premises and API access via AWS PrivateLink was provided to consume the product API. AIShield demonstrated automatic triggering of a vulnerability assessment as part of the model development workflow, with a reference implementation of SageMaker. In under six hours, the AIShield product generated a vulnerability assessment report, including explanations and defensive measures. These artifacts were delivered securely through signed URL mechanisms in S3. With its transparent and objective assessment of residual risk, the solution has the potential to significantly improve the bank's ability to reduce residual operational risk by up to 15%. The risk management team is now able to make quick and secure releases of fraud detection models in just eight hours, 11x faster and with a 30x productivity boost compared to the previous process. Solution Benefits AIShield offers a free trial for users to explore its capabilities and supports on-premises deployment with sales support. Technically, AIShield reduces vulnerability detection and remediation time for AI models from months to hours and provides reference implementations on GitHub. The benefits to businesses include an augmented return on investment (ROI), decrease in critical vulnerabilities, and estimated cost savings ranging from 40-60% of the security expenditure in AI projects. These advantages ensure a rapid realization of value through brand protection and regulatory compliance.

Vulnerability analysis for a wide variety of attacks across models, frameworks, and data Frequent updates to attack library Low data/no model approach

One-click application through PyPi package Detailed reference implementations Metrics pushed for every experiment

AWS PrivateLink enabled Updates to the model card/governance dashboards Detailed documentation and how-to videos

Model security by design allowing for scaled AI security initiatives Defense is lightweight and can be deployed on target – cloud/edge

Ease to integrate APIs into MLOps pipelines Containerized defense model available in multiple formats (ONNX, .h5) Pre-configured connectors to SIEM solutions like Splunk and Sentinel

Published apps in Splunk app base and Sentinel marketplace Pre-built workbooks and playbooks available Telemetry is OCSF-compliant Easy and smooth billing

Conclusion This post demonstrated the importance of AI security for organizations and how the AIShield security product can be integrated into Amazon SageMaker ML pipelines. The solution empowers organizations to leverage the combined capabilities of AIShield and SageMaker ML workflows, maximizing the value derived from their AI/ML initiatives with confidence and security assurance. AIShield has received notable accolades for its technology, including the CES Innovation Award 2023, IoT World Congress Award 2023: Best Cybersecurity Solution, and recognition from Gartner in its Market Guide for AI Trust, Risk and Security Management. Learn more about AIShield in AWS Marketplace. The sample code; software libraries; command line tools; proofs of concept; templates; or other related technology (including any of the foregoing that are provided by our personnel) is provided to you as AWS Content under the AWS Customer Agreement, or the relevant written agreement between you and AWS (whichever applies). You should not use this AWS Content in your production accounts, or on production or other critical data. You are responsible for testing, securing, and optimizing the AWS Content, such as sample code, as appropriate for production grade use based on your specific quality control practices and standards. Deploying AWS Content may incur AWS charges for creating or using AWS chargeable resources, such as running Amazon EC2 instances or using Amazon S3 storage. .

Bosch – AWS Partner Spotlight Bosch Global Software Technologies (BGSW) is an AWS Partner that's using machine learning solutions to improve its manufacturing, healthcare, and IoT solutions. Contact Bosch | Partner Overview | AWS Marketplace

TAGS: AIShield, AWS Partner Guest Post, AWS Partner References, AWS Partner Success Stories, Bosch

Multi-account infrastructure provisioning with AWS Control Tower and AWS Proton

Introduction The majority of the enterprise customers tend to establish centralize control and well-architected organization-wide policies when it comes to distribution of cloud resources in multiple teams. These teams are primarily divided into three categories: IT operations, Enterprise Security, and Application (App)-development. While delivery of business value from application standpoint falls under the purview of the App-development teams, the IT operations teams' control the cloud resource provisioning and security teams ensures the delivery and coordination between these teams happens at scale. Within AWS, AWS Control Tower offers the easiest way to set up and govern a secure, multi-account environment. It establishes a landing zone based on best-practices blueprints, and it enables governance using guardrails you can choose from a pre-packaged list. The landing zone is a well-architected, multi-account baseline that follows AWS best practices. First, it's good to know that AWS Control Tower shares a lot of terminology with the AWS Organizations service, including the terms organization and organizational unit (OU). While Organization refers to an entity that you create to consolidate your AWS accounts for administration as a single unit, Organization unit acts as a container for Accounts within a root and provides necessary hierarchy. Often there is a need for securing infrastructure in a consistent and compliant fashion, which requires a decoupling of the infrastructure management from the business application delivery that results in undifferentiated heavy lifting by teams. The pattern discussed in this post addresses this challenge by using a combination of AWS Control Tower to set up a well-architected, multi-account environment and AWS Proton to simplify multi-account continuous integration and continuous delivery (CI/CD) Deployments for application deployment and management. Solution overview Multi-account deployment using AWS Proton AWS Proton service is a two-pronged automation framework. As a platform team administrator, you create environment infrastructure as code template that defines shared infrastructure used by multiple applications and service templates that define deployment tooling for serverless and/or container-based applications. As an application developer, AWS Proton enables you to select desired service from the available service templates to automate your application deployments. For platform teams to improve visibility and efficiency at scale, AWS Proton offers a capability called Environment Account Connections. Environment account connections help platform teams to establish secure bi-directional connections between a single management account and multiple development team accounts, also referred to as environment accounts and is described in the following figure.

Walkthrough The solution described in this post assumes that you are using AWS Control Tower to create OUs and accounts. To know more on how to create an OU in the AWS Control Tower from AWS Management Console, refer to the AWS Documentation here. Prerequisites We used an AWS Cloud9 instance to run this tutorial and if you want to create a Cloud9 instance in your account, refer to the AWS Documentation here. If you are not using AWS Cloud9 you need to install the latest version of the AWS CLI. Step 1: Create service control policy for OUs For this solution, we created two OUs to demonstrate a typical customer use case. The first one is the Management OU for the AWS Proton Management Account, where platform teams can maintain the AWS Proton environment templates and service templates. The second one is the Development OU, which is used to create the AWS Proton Environment Account, where developer teams host their business applications. Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your OUs. Here, we have created a simple SCP AWS-Proton-Blocker and associated that with our Development OU. This SCP prevents any developer environment account from using AWS Proton to provision any resources while ensuring platform teams have full control on Infrastructure provisioning.

{ "Version": "2012-10-17", "Statement": [ { "Sid": "Statement1", "Effect": "Deny", "Action": [ "proton:*" ], "Resource": [ "*" ] } ] } Apache Configuration

Note: This policy can be scoped i.e., by providing fine-grained access control relevant for a particular team as per the needs of your organization. To learn more about how to create SCP, refer to AWS documentation here. To learn more about how to attach SCP to OU, refer to AWS documentation here. Step 2: Testing the SCP

Log in to the AWS Account created under Development OU from the login link provided to you by AWS at the time of creating AWS Account. You'll find log in details in the welcome email. Alternatively, you can also find the account login URL on the AWS Service Catalog dashboard under Provisioned Product. Search for AWS Proton from the AWS Management Console. Create an Environment template in AWS Proton as mentioned here. The Environment template creation fails due to the explicit deny at the OU level with the following error.

User: [EMAIL] is not authorized to perform proton:GetTemplateSyncConfig on resource arn:aws:proton:us-east-1:[aws-account-number]:environment-template/example-environment with an explicit deny in a service control policy Apache Configuration

By using the SCP at the OU level, we ensure that the Platform Engineering team can enforce the organizational guardrails between the service teams and departments within an organization. Step 3: Create environment template in the AWS Proton management account To run AWS Command Line Interface (AWS CLI) commands in the provisioned AWS accounts, we created AWS Cloud9 workspaces and attached an AWS Identity and Access Management (AWS IAM) role with AdministratorAccess policy to the AWS Cloud9 instances in accounts created under Management and Development OUs. Please follow these instructions to attach the AWS IAM role to an Amazon Elastic Compute Cloud (Amazon EC2) instances. For this post, we forked this GitHub repository and created a AWS CodeStar connection. Please find the steps to Set up an AWS CodeStar connection in the AWS Documentation. AWS Proton uses a source connection to trigger template updates or new application deploys whenever a new change is introduced. For the scope of this post, the connection is managed through the AWS CodeStar connections using GitHub as a provider. To read more on Service Sync configuration, please refer to this documentation.

Create a new environment template (multi-svc-env) based on the templates located under multi-svc-env/v1 and published it to a major version. This structure holds the AWS CloudFormation template that we'll use to provision the environment's infrastructure.

aws proton create-environment-template \ --region ${AWS_REGION} \ --name "multi-svc-env" \ --display-name "Multi Service Environment" \ --description "Environment with VPC and public subnets" Apache Configuration

Next, create a template sync configuration to register new environment template versions automatically.

REPOSITORY_ARN=$(aws proton list-repositories | \ jq -r '.repositories[] | select( .name | endswith("aws-proton-workshop-code")) | .arn'); REPOSITORY_NAME=$(echo $REPOSITORY_ARN | cut -d':' -f7); REPOSITORY_PROVIDER=$(echo $REPOSITORY_ARN | cut -d':' -f6 | tr a-z A-Z); aws proton create-template-sync-config \ --region ${AWS_REGION} \ --repository-name $REPOSITORY_NAME \ --repository-provider ${REPOSITORY_PROVIDER#"REPOSITORY/"} \ --branch main \ --subdirectory "aws-managed/multi-svc-env" \ --template-name "multi-svc-env" \ --template-type "ENVIRONMENT" Apache Configuration

To publish it, we need to run the following command:

aws proton update-environment-template-version \ --region ${AWS_REGION} \ --template-name "multi-svc-env" \ --major-version "1" \ --minor-version "0" \ --status "PUBLISHED" Apache Configuration

Step 4: Create environment account connection from management account in AWS Proton AWS Proton alleviates complicated cross-account policies by using a secure environment account connection feature. With environment account connections, platform engineers can give AWS Proton permissions to provision infrastructure in other accounts. To create and provision an environment from AWS Proton management account, log into the AWS Account created by AWS Control Tower under the Management OU. You'll find log in details in the welcome email accordingly.

Create a scoped AWS IAM role in the AWS Proton management account

aws cloudformation deploy \ --template-file proton-account-connection-roles.yaml \ --stack-name AWSProtonWorkshop-AccountConnectionRoles \ --parameter-overrides "EnvironmentAccountId=${SECONDARY_ENV_ACCOUNT_ID}" \ --capabilities "CAPABILITY_IAM" "CAPABILITY_NAMED_IAM" Bash

Now log in to the AWS Account and navigate to the AWS Proton's Console created under the Development OU, or copy and paste the output of AWSProtonWorkshop-AccountConnectionRoles stack in your browser to login to the Development AWS Account. Choose Environment Account Connections from the navigation pane. Then select Request to Connect in Sent requests to connect to a management account. (For the Management account ID use the Proton Management Account).

Switch back or log in again to the management account and you should have a new environment account connection request now in the AWS Proton Console. Go ahead and accept it!

Step 5: Create environment in Developer Team account After defining the environment template, service template in the AWS Proton Management account and establishing the Environment Account Connections to the development account, we're ready to provision the environment in the development account using AWS Proton. First, we define the specification, which is a YAML-formatted string that provides inputs defined in the environment template bundle schema file. A sample can be found here.

SPEC=$(cat <<-EOF proton: EnvironmentSpec spec: vpc_cidr: 172.16.0.0/16 dns_hostname: [name-of-your-app].dev.local EOF ); Apache Configuration

[[name-of-your-app] is a place-holder, replace that with the name of your app] The create-environment command creates the environment with the infrastructure needed for the App-Development teams to deploy micro-services. The template used below, deploys an Amazon Virtual Private Cloud (VPC) to secure networking boundary, an Amazon ECS cluster with optional configuration inputs like Amazon EC2 capacity, Container Insights, and Amazon ECS Executive logging for the application deployment.

MAJOR_VERSION=$(aws proton list-environment-template-versions \ --template-name "multi-svc-env" --region=${AWS_REGION} | \ jq -r ' .templateVersions[] | select( .status=="PUBLISHED") | .majorVersion' | tail -1 \ ); \ ACCOUNT_CONNECTION_ID=$(aws proton list-environment-account-connections \ --requested-by "MANAGEMENT_ACCOUNT" --statuses "CONNECTED" --region=${AWS_REGION} | \ jq -r ' .environmentAccountConnections[] | select( .environmentName | startswith("multi-svc")) | .id' \ ); \ aws proton create-environment \ --region ${AWS_REGION} \ --name "multi-svc-${AWS_ACCOUNT_ID}" \ --template-name "multi-svc-env" \ --template-major-version "$MAJOR_VERSION" \ --environment-account-connection-id "$ACCOUNT_CONNECTION_ID" \ --spec "$SPEC" Apache Configuration

In the Developer account (i.e., environment account), you can view and access the provisioned infrastructure resources. You can now use this environment to deploy application as AWS Proton Service. Please review the documentation to learn more about the service deployment in an AWS Proton environment.

Cleaning up You continue to incur cost until deleting the infrastructure that you created for this post. Use the below instructions to delete AWS resources in the blog. Delete the environment:

aws proton delete-environment --name multi-svc-${AWS_ACCOUNT_ID} # Wait for above to complete before proceeding next steps. It takes ~3 minutes. Apache Configuration

Once you have deleted your AWS Proton resources, please follow AWS Documentation here to delete your AWS Control Tower resources. Conclusion In this post, we showed you how customers improved the security posture of multi-account infrastructure deployments by using AWS Control Tower to provision and organize AWS accounts that adhere to best-practices. We used AWS Proton to amplify platform engineering impacts and improve developer productivity. This established workflow allowed Platform teams to have a unified view of all the AWS Proton managed resources from management account using the AWS Proton dashboard. To get started with AWS Proton, head over to our sample repository for examples and our documentation for a deeper dive into its full functionality.

SSG vs SSR in Next.js Web Applications: Choosing the Right Rendering Approach

Next.js, a popular React framework, has changed the way developers build modern web applications. It offers powerful features, such as Server-Side Rendering (SSR) and Static Site Generation (SSG), which optimize your application's performance and user experience. In this blog post, we'll explore the key differences between SSG and SSR, their advantages, when to choose one over the other, and how to deploy either approach using AWS Amplify. Amplify is a complete solution that lets frontend web and mobile developers easily build, ship, and host full-stack applications on AWS. What is Static Site Generation (SSG)? Static Site Generation, as the name suggests, generates static HTML files during the build time. Every time you build an app, a bunch of pages are created. These files are then served to users, which means the server doesn't need to do any extra work when a user visits the website. This approach is ideal for websites with content that doesn't change frequently, like blogs or documentation sites. Advantages of SSG

Speed: Pre-rendered HTML files are served directly, resulting in faster load times. Scalability: Static files can be easily served by Content Delivery Networks (CDNs), improving your application's ability to handle more global traffic around the world.

What is Server-Side Rendering (SSR)? Server-Side Rendering involves generating the HTML for each page on the server when a user requests it. With SSR, there is a server that pre-renders the page – it's like a template that you plug variables into and the server handles all of the rendering. This happens at request time, so when the user requests the page, they get the server side rendered page. This rendering happens all on the server side and never runs in the browser. So, unlike SSG where the page is already rendered in the server waiting to be served to the client, SSR renders the page on the server upon receiving a request. SSR is ideal for websites with dynamic or personalized content that changes frequently, like e-commerce websites or social media platforms. Advantages of SSR

Consistent user experience: Users see the latest content as it's generated on-the-fly, ensuring they always have up-to-date information. Personalization: SSR allows you to serve unique content based on user preferences or other dynamic data.

Is your content updated frequently? If your content doesn't change often, SSG is the better choice for improved performance and scalability. For dynamic content, SSR ensures users see the most up-to-date information. Does your website provide an interactive user experience? SSR websites provide an interactive user experience, whereas SSG websites are largely static sites with little to no dynamic content, unless your site is combined with CSR or SSR. Do you want to incur the rendering cost at build-time or run-time? Choose SSG if you want to incur the rendering cost at build-time and SSR for run-time. Do you have an SEO requirement? The primary difference between SSR and SSG for SEO is only the server response time.

Consider your Business Requirements While SSG provides excellent SEO benefits, SSR offers better support for dynamic content and personalization that may be crucial for search rankings.

User Experience: Consider whether your users need real-time data or personalized content. If so, SSR is the better option. Otherwise, SSG offers a faster experience with lower server requirements. Latest Data: SSR pages will always display the latest data because they are generated on the server with every request from the user. With every request, you can always be sure that you have the most recent/up-to-date information.

SSG vs SSR in Next.js: Choosing the Right Rendering Approach In this article, we will discuss the two rendering approaches in Next.js: SSG and SSR. We will also provide an example codebase that will help you determine the best rendering approach for your application. Next.js supports the ability to generate an "n" number of dynamic generated routes. In our example codebase, we fetch 100 of the top Hacker News posts in the last few days and render those IDs onto the home page. When clicked, each of those pages reveal the details of the post. Each of those pages will also have the route like "pages/[hackerNewsArticleId]". You can see how that template page is available on the pages folder with the [slug] designation. Now, you may be asking how do we define this list of paths to be statically generated? We will use two NextJS functions: getStaticProps and getStaticPaths. getStaticProps will pre-render the page at build time, fetch from the Hacker News API the list of posts information, and pass that information into the props of our component as well as getStaticPaths. Our second function, getStaticPaths, is the key to dynamic routing. It will receive the list of IDs from getStaticProps and create a list of path objects like this: export async function getStaticPaths() { return { paths: [{ params: { id: '1' } }, { params: { id: '2' } }], fallback: false, // can also be true or 'blocking' } }JavaScript Next.JS will now ingest this list of paths and generate a new page for each of those params. We have also now allowed the primary component ingest the Hacker News IDs to render for routing to the individual page. If you go to our dynamically rendered component page, you will see that we again call the getStaticProps method to call the Hacker News details endpoint to fetch the post's title, votes, etc. Notice that this is a snapshot of the blog post at the time of compilation. SSG Approach We will go over deploying a SSG Next.js app from this Github repository to Amplify. To start off, click below to deploy this project in your own environment. Here is more information about deploying and hosting with Amplify.

On the Deploy App page, go ahead and select Create new role, leaving all of the defaults for your Amplify backend role.

Once you've created the role, refresh the existing roles back on the Deploy App page and select the role you just created. Select Save and Deploy. Deploying your app will take 2-3 minutes. Click Continue when the box appears. Then, you can monitor as your Amplify app goes into the provision, build, and deploy phases.

Once it has finished deploying, you'll be able to visit the website hosted under the Domain.

When you select your domain, you'll be navigated to your Amplify app which will display your SSG Next.js app.

SSR Approach We will look how we can deploy a NextJS web application from GitHub directly to Amplify. Surprise! We actually already deployed a SSR page when we deployed to Amplify. The amazing thing about Next.JS is that you can have some pages be generated at build time, while other pages can still render dynamic data by leveraging SSR. To use Server-side Rendering for a particular page, we export an async function called getServerSideProps. This function will be called by the server on every request. Now, on your hosted link, navigate to the /ssr route (ie. Now, you should see a page like this:

Common Gotchas Next.js requires a different paradigm shift away from the very popular frontend frameworks leveraging the libraries of Create React App (CRA) and React Router. CRA is a starter kit that provides you with a boilerplate project for creating a React application. CRA uses client-side rendering (CSR), which means that your application is rendered in the browser, and typically it is best practice to break up your network requests based on user's interactions. In comparison to SSG, you now have to load all the data all at once. By including these Common Gotcha section, we hope you can learn from our mistakes and save some time and frustration.

Instead of passing along all of the build and data fetching time to the customer, you might be stuck hours waiting for your site to build because you have a large scale static website. Every time, you double the number of static pages, you are doubling the build time duration.

If you have an e-commerce website with 10,000 items, each taking about 1 second to fetch/build, you are looking at nearly 3 hours of building and deployment. In this example, you could go the incremental static regeneration pathway, where you can create and update pages after you've built your primary website bundle. This allows you to make changes without rebuilding the entire website.

In modern web development, it is often recommended that users make API calls only enough to fill the user's viewport and potentially prefetch data for subsequent user actions. However, with SSG, you will need to call all the APIs necessary to populate the entire webpage and all potential user's actions.

If you have a frequent update to a page on your website, for example, a Blog page that has multiple sections; SSG might cause you to rebuild the entire site very frequently for changes that are contained in a single page. The pages are generated before the requests are made, as a result, SSG pages don't have direct access to requests like HTTP headers and query parameters at build time. If you want to access these, you have to write custom middleware in addition to SSG.

During the generation process, there are external factors that can ultimately influence page loading speed like fetching data from external APIs. If the APIs are slow to respond, page generation loading times will be slower. Also, because SSR generates each page per request, this made it slower compared to SSG. To enable caching with SSR pages and improve the loading times (speed), you will need to add an additional HTTP header , Cache-Control, when calling "getServerSideProps". See the docs here. Learn more about caching.

SSR requires server to process and generate the front-end on each request, which is less scalable and slower; provided the website have complex pages and a lot of end clients to process. SSR dynamic HTML cannot be cached by static CDNs (Content Delivery Network, e.g. CloudFront), which would lead to longer trips to and from the server, and a slower time between requesting a page and the first byte loaded by a server (TTFB: Time to First Byte).

Although Google and Bing announced they leverage the same Chromium based crawlers with the latest rendering engine, Javascript still complicates the search engine's ability to parse your page and potentially lower your SEO rank. Search engines must download your Javascript files, and potentially not wait for the entire bundle to load before moving onto to other parts of your site. At the end of the day, there is a lot of uncertainty at guessing how Google or Bing ranks your page, and it potentially makes sense to make the crawler's life easier.

Conclusion Both SSG and SSR offer unique advantages for different types of Next.js applications. By understanding your project's content frequency, SEO requirements, user experience, and development complexity, you can make an informed decision about whether to use SSG or SSR in your Next.js project. Ultimately, the choice depends on your specific use case and the trade-offs you're willing to make between performance, scalability, and dynamic content. To get started, try your hand at building a Next.js 13 web app with user authentication using Amplify Hosting.

Alexa Perlov Alexa Perlov is a Solutions Architect at Amazon Web Services, based in New York City. She supports Media and Entertainment strategic accounts in utilizing AWS to achieve their business goals. She leads technical trainings and develops prototypes, highlighting a wide range of cloud domains.

Michael Tran Michael Tran is a Solutions Architect with Prototyping Acceleration team at Amazon Web Services. He provides technical guidance and helps customers innovate by showing the art of the possible on AWS. He specializes in building prototypes in the AI/ML space. You can contact him @Mike_Trann on Twitter.

Visualize hierarchical data using the hierarchy layout for pivot tables in Amazon QuickSight

All your trusted BI capabilities of Amazon QuickSight now come with powerful new AI features that integrate chat agents, deep research, and automation in one seamless experience with Amazon Quick Suite! Learn more »

Are you finding it cumbersome to scroll horizontally in pivot tables to see all your row fields? Are you dealing with data that has a hierarchical structure, like cost center or product hierarchies? If you are, then there's exciting news for you! Amazon QuickSight now offers a new layout option called hierarchy layout for pivot tables. This feature allows you to organize your data in a hierarchical outline format, displaying each level indented under the level above it. In this post, we delve into the advantages of using the hierarchy layout and explore its various configuration options. Hierarchy layout for pivot tables A new layout option known as hierarchy layout has been introduced in pivot tables. Currently, each field in the Rows field well adds a new column to the pivot table, which can be cumbersome for customers with numerous row fields because they have to scroll horizontally to view all fields. The hierarchy layout addresses this challenge by presenting all row fields in a single column in a hierarchical format. This format uses indentations to differentiate items from different fields, resulting in a more readable table. Additionally, this layout is a more condensed display, allowing for more room to display numeric data. In the words of one of our customers, Best Western, "With the recent addition of the hierarchy layout to the pivot tables visuals, the Best Western Hotels team can modify one of our most used dashboards. This dashboard allows the hoteliers to select the content of the first four columns and to omit up to three columns. This feature will allow the hoteliers to view more information without having to scroll right and left. The benefit to everyone is that more information is communicated more quickly."

Choosing between the hierarchy and tabular layout in pivot tables depends on the specific dashboarding and reporting needs. The hierarchy layout is ideal for displaying vast amounts of data while conserving space, making it an excellent choice for presenting hierarchical data in a clear and easy-to-read format. One of the main benefits of the hierarchy layout is that you don't need to display individual row field headings. On the other hand, if you prefer a more traditional table format, the tabular layout is an excellent option. This layout presents each row field in its own column, making it perfect for presenting detailed and granular data, such as survey responses with multiple answer options. Field configurations for Hierarchy layout Hierarchy layout offers different field configurations. For example, the following screenshot shows the hierarchy layout with values in rows.

The following screenshot shows the hierarchy layout with values in columns.

In the following sections, we look at the different features and functionality of hierarchy layouts, along with the process of configuring them. Convert to hierarchy layout All pivot tables created after launch will start with hierarchy layout as the new default experience without affecting existing pivot table visuals. You can toggle to hierarchy layout by navigating to Pivot options on the Format visual menu and selecting Hierarchy.

Sorting Similar to tabular layout, you can sort row, column, and value fields using the refreshed menu options. You can view singular field menu options for column and value fields as compared to combined row field menu options for all row fields together, allowing you to choose your field to sort by and order in which you would like to sort. The following screenshot illustrates sorting using the combined row header menu.

Expand or collapse To reveal detailed information in pivot tables, you have the option to expand row and column fields either through the menu or by choosing the plus sign. For more information, refer to Expanding and collapsing pivot table clusters. Although the plus and minus signs are ideal for expanding or collapsing individual nodes, the context menu offers a range of navigation options, including expanding or collapsing all nodes and expanding or collapsing up to a specific field level. As shown in the following screenshot, you can expand or collapse using the row header menu (choose the row header to display the menu).

You can also expand or collapse a single node using the plus and minus signs or use the menu option to collapse the node or expand all the nested nodes.

As shown in the following screenshot, you can also expand or collapse individual columns.

Row header styling The header styling options between the tabular and hierarchy layout remain largely unchanged except for the row header, which has new options like the ability to hide or rename the row label to something meaningful and apply the same row header styling to row labels.

Subtotals Styling We have enhanced the subtotal options in pivot tables with a new control called Apply styling to. This allows authors to choose their preferred styling for subtotals, including value only (new option), entire row (header and value), and header only, providing greater flexibility. It's worth noting that this option is also available for tabular layouts, ensuring a seamless transition between different layouts.

Export When exporting to Excel, there will be differences in the output between the hierarchy layout and tabular layout in pivot tables. The exported Excel file will reflect the specific layout chosen, providing accurate representation of the data based on the selected layout. The following screenshot shows an example output in hierarchy layout.

Other updates The pivot table update also includes redesigned context menus, and it now displays renamed columns in the sorting menu. Redesigned context menus As part of the update, we have redesigned the context menu options for pivot tables, replacing the super menu with a more condensed layout. This change ensures a unified experience for both authors and readers. Although both authors and readers can access field-level configurations through the context menu, authors have the additional capability to access further configurations directly from the field wells. The context menu for row, column, and value fields has been revamped to provide a consistent experience across both layouts. The only distinction is the inclusion of a combined menu option specifically for rows in the hierarchy layout. The following screenshots compare the old and new experience.

Display renamed column names in the sorting menu Previously, when renaming a column using the format pane, only the header display name was updated, causing confusion because the sort menu still showed the original column name. However, this issue has been addressed. Renamed columns now reflect the updated name in both the column headers and sort menus. For instance, if we rename Gross Margin to GM%, the new name will be consistently displayed in both the column header and sorting menu, as demonstrated in the following screenshot.

Conclusion This post explored the hierarchy layout feature available in pivot tables, highlighting its various functions such as interaction, styling, and export capabilities. The hierarchy layout can help condense multiple row levels into a more simplified and easily readable format, offering a practical solution for working with complex data. Start using hierarchy layout to enable hierarchical navigation and provide a drill-down experience on pivot tables today! If you have any questions or feedback, please leave a comment. For additional discussions and help getting answers to your questions, check out the QuickSight Community.

About the author Bhupinder Chadha is a senior product manager for Amazon QuickSight focused on visualization and front end experiences. He is passionate about BI, data visualization and low-code/no-code experiences. Prior to QuickSight he was the lead product manager for Inforiver, responsible for building a enterprise BI product from ground up. Bhupinder started his career in presales, followed by a small gig in consulting and then PM for xViz, an add on visualization product. Jose Banos is a Sr. WW Go-To-Market QuickSight Specialist focused on creating QuickSight awareness worldwide. He has over 17 years of experience in fields like Telco, cyber-security and data analytics. Jose started his career as presales, moving into business development and customer success working with customers in 4 continents and living in 3 different countries. He leads go-to-market plans across marketing, sales, and customer success and he enjoy helping customers to increase their revenue and reduce cost using QuickSight and other AWS services.

Realize Faster Time to Value with IBM's Modern Data Accelerators on AWS

Use cases for data have changed and expanded from the data warehouse days, as digital transformation has unleashed new and existing uses for data services. Many organizations still are managing expensive single use case data environments, however, and IBM's Modern Data Accelerators on AWS can help build modern implementation of a data fabric architecture, enabling customers to realize faster time to value. Customers tend to maintain the following data environments:

Data warehouses for batch business intelligence and reporting. Data lakes for data science predictive model testing and development. Digital databases for digital interactions.

These single-use environments tend to have significant duplication in data sourcing from both internal and external sources, leading to poor data quality, high maintenance costs, and inflexible data environments. In this post, we will discuss how IBM's Modern Data Accelerators can help build modern implementation of a data fabric architecture, which standardizes data integration across the enterprise. IBM Consulting is an AWS Premier Tier Services Partner and is recognized as a Global Systems Integrator (GSI) for many competencies including Data and Analytics Consulting, which positions IBM to help customers who use AWS to harness the power of innovation and drive their business transformation. Challenges with Legacy Data Environments Generally, legacy data environments are inter-connected with event-driven inbound and outbound digital channels, using the predictive models which are built in the data science sandboxes. However, real-time visualization requires a higher level of integration of an organization's data use cases. This challenge can be solved by building data environments into multi use-case data platforms that provide:

Common data provisioning data lakes: Cost effective, cloud-based stores to source real-time and batch data from internal and external sources. Integrated conform layer: Enterprise data conformed for specific domains such as customer, product, and transaction. Consumption layer: For infomarts, data science sandboxes, digital data stores, and operational data use cases.

A multi-use case data platform with common data provisioning that's instantiated on AWS generally provides a lower cost and more flexible data environment with high data quality. Modern Data Accelerators Offering by IBM Consulting IBM Consulting has built an integrated set of assets called Modern Data Accelerators that boost time-to-value and reduce delivery time as well as cost. Modern Data Accelerators allow enterprises to build out the multi-use case data platforms, and have been developed as an end-to-end set of assets that can quickly instantiate data capabilities for a data cloud implementation on AWS. These assets have been deployed in multiple geographies, and across industries.

Figure 1 – Modern Data Accelerators. It's important to note these assets are not just software. They are not extract, transform, load (ETL) tools, databases, big data stores, or other data management software. This is a combination of all of these which enable the usage of the data management services faster. There are essentially two options to accelerate data management services:

Perform traditional analysis, design, and coding. Install and configure assets, and then extend them for specific requirements.

Solution Overview The Modern Data Accelerators are known data management processes that have been codified to drive business value faster. These assets drive the lifecycle of a data environment from development to maintenance and ingestion, through analytics. Modern Data Architecture Components The following components provide a brief overview of IBM's Modern Data Accelerators.

Workload analysis and modernization automation: Creates persona-based, detailed inventories of data ecosystems and their dependencies. Provides automated translation of legacy data- processing code, and automated testing, which is essential for creating actionable migration roadmaps. Real-time and batch intelligent integration engine: A Kafka-based, real-time and batch engine that creates data pipelines for managing the ingestion, organization, and publication of data. These engines include custom connections, unified batch and stream capabilities, and ingestion APIs. The data onboarding process learns and adapts using machine learning models. Digital integration for intelligent workflows: This is a data provisioning asset for intelligent workflows that's designed for enterprise and unstructured data. Data mesh console: Provides an operational interface for data mesh implementations, which helps manage the lifecycle of data products and their dependencies. The data mesh console is designed to integrate and leverage data catalogs and data marketplaces, while also providing data product observability metrics. AI-driven cognitive classifier: The artificial intelligence-driven cognitive classifier automates the classification and organization of data against enterprise canonical models and provides real-time insight into data quality. Automated data tagging provides classification for the application of security policies across the data platform. Once trained on an organization's data, the tool can be integrated in the intelligent integration engine to classify, auto-map data, and send notifications in both real-time and batch mode. Lightweight master data management (MDM): Provides multi-domain entity matching capabilities that identifies duplicates or suspected duplicate entities across large datasets, using both probabilistic and deterministic matching logic. It's built leveraging both graph database and Elasticsearch capabilities.

Real-time and batch monitoring: Data quality across all components in the data fabric environment is maintained using customizable, open-source Grafana dashboards and portlets. Using modern technologies such as Airflow, the solution is able to provide a high degree of automated data pipeline and structure observability. Data marketplace: This is the central provisioning point for data consumption in the modern data platform on AWS. Data is persisted in original and curated forms with purpose-fit storage, allowing for publication and subscription of data. Data science marketplace: Contains a set of cloud-ready data science models that can be accessed and stored in the model marketplace, hosted in the data platform.

As stated earlier, each of these assets help accelerate time to value of data cloud solutions on AWS. See Figure 5 to understand how all of these components come together to build the Modern Data Accelerators.

Figure 3 – Various components of Modern Data Accelerators. Customer Success Story The following case study is a good representation of how the Modern Data Accelerators have helped accelerate customers' time to value. IBM Consulting was engaged to assist a large healthcare organization to transform its use of its analytic data in more productive and proactive ways while reducing the overall cost. The organization had a large legacy relational database that was primarily used for reporting purposes. The first step was to analyze the existing database workload with IBM's Workload Analysis tool to determine the types of data and personas using that data. With the objective to change the way business users consume data, IBM Consulting ingested data from hundreds of sources using IBM's real-time and batch intelligent integration engine into a new Amazon Simple Storage Service (Amazon S3)-driven data lake, where IBM envisioned 14 first-of-a-kind predictive models to define "member health profile" in 12 weeks. IBM used the data science marketplace to store the predictive models that were easy to reuse in subsequent models. The early success provided the momentum to right-size the end-state data platform, curate 23 new datasets into production in three months, and leverage real-time and batch monitoring capabilities to bring the new, single AWS Cloud platform into production within six months. Conclusion IBM's Modern Data Accelerators on AWS is a proven solution to reduce delivery time and cost for customers. It mitigates the risk of maintaining legacy data platforms while improving time to value. The Modern Data Accelerators help achieve the following objectives in a data fabric, data mesh, or modern data stack implementation:

Manage: Singular and integrated means of management of data at rest, in motion, and integration of the data. Govern: Map all enterprise assets to a single canonical model and data catalog. Secure: Information classification serves as the single means of defining security policies and entitlements.

To learn more about the solution, check out IBM's data platforms page. You can also refer this AWS blog post to explore modernizing data platforms, accelerating innovation, and unlocking business value with data mesh on AWS. .

IBM – AWS Partner Spotlight IBM Consulting is an AWS Premier Tier Services Partner and MSP that offers comprehensive service capabilities addressing both business and technology challenges that clients face today. Contact IBM | Partner Overview | Case Studies

TAGS: AWS Competency Partners, AWS MSP Partner Program, AWS Partner Guest Post, AWS Partner References, AWS Partner Success Stories, AWS Premier Tier Services Partners, AWS Public Sector Partners, AWS Service Delivery Partners, AWS Solution Provider Partners, AWS Well-Architected Partners, IBM, Managed Service Provider

Japan Manned Space Systems uses AWS Snowcone to automate and optimize data delivery from space to Earth

Pictured: AWS Snowcone SSD onboard the International Space Station (ISS). Image credit: NASA. Approximately 400 kilometers above the Earth's surface, Japan Manned Space Systems Corporation (JAMSS) used an AWS Snowcone SSD onboard the International Space Station (ISS) to quickly and autonomously transmit large volumes of data from space back on the ground into Amazon Web Services (AWS). The AWS Snowcone, which arrived on the ISS in July 2022 as part of the first Axiom Space Mission (Ax-1), is available to power on-orbit data processing experiments for researchers, students, and scientific organizations. Since its founding in 1990, Tokyo-based JAMSS has been entrusted with the operation of the Japanese Experiment Module (JEM), known as "Kibo," onboard ISS. JAMSS technologies can help ISS crew members plan, execute, and control space experiments in the Kibo module, as well as training for astronauts and flight controllers. Leveraging this unique and extensive experience, JAMSS seeks to become a global leader in crewed space expedition solutions for next-generation commercial space missions. Kibo means "hope" in Japanese, representing the infinite possibilities of science and technology to become a unifying hope for humanity. Similarly, JAMSS strives to bridge Earth and outer space in order to contribute to humanity and society, integrating the latest technologies to build new space solutions. As part of this initiative, JAMSS developed the JAMSS Automated File Dump System (JAFDS) solution that leverages the edge computing capabilities of the Snowcone to autonomously downlink, validate, and retransmit large files from the ISS to the ground. On April 21 2023 (UTC), JAMSS, in collaboration with AWS and Axiom Space, successfully demonstrated software that will help scientists transmit large amounts of data from space and help scientists on Earth analyze the information more quickly. Using AWS edge compute for space station data transmission solutions The ISS JEM, Kibo, houses research facilities in which astronauts conduct various experiments focused on space medicine, biology, Earth observations, material production, biotechnology, and communications research. During peak times, over 1 terabyte (TB) per week of data is downlinked from Kibo to Japan via the US National Aeronautics and Space Administration (NASA) Tracking and Data Relay Satellite (TDRS). TDRS is a network of geosynchronous (GEO) satellites that provide communication relay links for many missions such as the Hubble Space Telescope and the ISS. One of JAMSS's missions is to provide communication solutions for next-generation, low Earth orbit (LEO) commercial space station missions. In these future applications, space station operators may need to procure and distribute bandwidth to multiple users, or enable multiple satellite communication providers to service a single space station and contract bandwidth to individual end users. In such cases, space stations may be servicing discrete data transmissions for multiple end users, and each user will need to manage and optimize their data transmission within an allocated schedule and bandwidth. By leveraging cloud flexibility and edge computing with AWS, JAMSS can allocate communication per user for a determined time period according to a predefined delivery schedule and bandwidth. It also can specify discrete data delivery destinations per user when there are multiple communications. Solving the challenges of space data transmission without manual intervention Space communication is rarely constant. For example, during data downlink from space, packet loss may occur depending on communication conditions and network design, resulting in data acquisition delays. Furthermore, data retransmission may take multiple contact windows, which results in wasted communications opportunities and additional delays in data acquisition. In some cases, data downlink connection loss scenarios requires JAMSS to deploy operation and control personnel in on-ground mission operations centers to manually perform loss confirmation and retransmission requests and to manage these systems around the clock. To help customers avoid data acquisition delays and added cost associated with data downlink loss, JAMSS innovated the JAFDS solution. JAFDS consists of two software components: an onboard software component that is deployed to an Amazon Elastic Compute Cloud (Amazon EC2) instance in the AWS Snowcone in space, and a ground software component that runs on a virtual machine instance on the ground. Files desired to be transmitted are defined on the onboard software with a downlink schedule and selected bandwidth. When the scheduled communication window begins, the onboard software starts sending files to the ground software. The ground software automatically identifies data loss events, scheduling and retransmitting only lost file sections. Hash value matching is also confirmed to validate the integrity of the files. This reduces the number of connection windows needed to validate and reschedule failed transmissions, leading to faster data delivery and improved efficiency through autonomous operation. Seamlessly reproduce on-orbit data structures on the ground Large-capacity experimental data and video files are stored on an orbital file server and then downlinked in batches. JAFDS automatically compresses, encrypts, transmits, and re-creates the files from the on-orbit Amazon Elastic Block Store (Amazon EBS) volume to the ground software location while maintaining directory structure and file names. The ground software can be flexibly deployed on either on-premises virtual machines to support customized mission control centers, or on Amazon EC2 instances as part of a cloud-based mission operations center deployment on AWS. This allows access to files and experimental data as if they were on the ISS, and allows on-ground researchers and users to continue analysis activities and distribute data seamlessly. Furthermore, in future commercial space stations where multiple customer workloads may be hosted in the same station, JAFDS can preserve the isolation and integrity of file storage systems that may be accessed by many different end users.

Figure 1. JAFDS consists of two software components: an onboard software component that is deployed to an Amazon EC2 instance in the AWS Snowcone in space, and a ground software component that runs on a virtual machine instance on the ground. Ground operators define the files to be transmitted with a downlink schedule and desired bandwidth. Experimental data from the Snowcone is transmitted by the JAFDS onboard software according to the defined schedule through the TDRS network to the ground. The JAFDS ground software automatically identifies data loss events, scheduling and retransmitting only lost file sections. This reduces the number of connection windows, leading to faster data delivery and improved efficiency through autonomous operation. Technical demonstration on the AWS Snowcone aboard the ISS In the orbital demonstration carried out on April 21, 2023 (UTC), the JAFDS software was deployed and executed on an Amazon EC2 instance in the AWS Snowcone installed on the ISS. JAMSS conducted a test in which 1 gigabyte (GB) files were created for three different users and transmitted sequentially to ground. The technical demonstration successfully reproduced all the files to a virtual machine on the ground over a defined communication bandwidth (5 Mbps) and delivery schedule. The JAFDS software also successfully demonstrated autonomous downlink operation by automatically detecting file losses and then retrieving and retransmitting the previously lost data. The JAFDS solution is positioned to serve the data transmission needs of future commercial LEO use cases by leveraging AWS Snowcone's space-capable edge compute capabilities. JAMSS plans to further develop and demonstrate edge processing solutions on AWS for space applications to accelerate further innovation in the space data ecosystem. Learn more about AWS for aerospace and satellite Organizations of all sizes across all industries are transforming and delivering on their aerospace and satellite missions every day using AWS. Learn more about the cloud for aerospace and satellite solutions so you can start your own AWS Cloud journey today. Get inspired. Watch our AWS in Space story. Read more about AWS for aerospace and satellite:

Amazon and AWS to reimagine space station operations and logistics for Orbital Reef How Satellogic and AWS are harnessing the power of space and cloud AWS selects 13 startups for the 2023 AWS Space Accelerator What we learned at Amazon re:MARS 2022 for the public sector Managing the world's natural resources with earth observation AWS joins the Digital IF Interoperability (DIFI) Consortium How Natural Resources Canada migrated petabytes of geospatial data to the cloud

Subscribe to the AWS Public Sector Blog newsletter to get the latest in AWS tools, solutions, and innovations from the public sector delivered to your inbox, or contact us. Please take a few minutes to share insights regarding your experience with the AWS Public Sector Blog in this survey, and we'll use feedback from the survey to create more content aligned with the preferences of our readers.

TAGS: aerospace and satellite, Asia Pacific, AWS Snowcone, Axiom Space, Japan, NASA, public sector, satellite, satellites

Emma Higashikawa Emma Higashikawa is a solutions architect on the Aerospace and Satellite (A&S) team at Amazon Web Services (AWS). Emma helps space industry customers achieve their missions and solve technical challenges by working together to design and implement end-to-end solutions leveraging AWS.

How to validate authentication with self-signed certificates in Amazon CloudWatch Synthetics

In today's digital landscape, ensuring optimal application performance is crucial, and Amazon CloudWatch Synthetics enables proactive testing of web applications and APIs. If you are utilizing self-signed certificates and seeking to enhance your monitoring capabilities, this blog post will guide you step-by-step on how to modify the source code of your canary to support self-signed certificates for your endpoints, and the best part is that this method won't require you to change your authentication process. We will demonstrate how to utilize the Amazon CloudWatch Synthetics canary with the heartbeat blueprint in order to monitor an application that relies on a self-signed certificate, which are certificates issued and signed by a non-recognized and trusted third-party Certificate Authority (CA). Furthermore, we recommend you check out our previous blog posts, which covers other methods to monitor APIs. The blog post titled Multi-step API monitoring using Amazon CloudWatch Synthetics provides detailed steps for creating an Amazon CloudWatch Synthetics script to execute multi-step API verification, ensuring the functionality and stability of your APIs. Additionally, our second blog post How to validate authentication using Amazon CloudWatch Synthetics – Part 2 offers step-by-step guidance on updating a synthetic blueprint to monitor applications that requires both the server and the client to present their digital certificates for a mutual authentication (mTLS). These resources will complement your understanding and enable you to build a comprehensive monitoring strategy. Solution Overview In this solution, we will demonstrate the steps to create a Heartbeat monitor canary based on an existing blueprint and insert a self-signed certificate within the provided blueprint, allowing the expansion of capabilities provided by the blueprints. Creating the canary CloudWatch Synthetics lets you utilize blueprint scripts that are ready to be consumed. However, we must utilize the editor in the console to add the extra code snippets in order to authenticate with a certificate. To simulate how CloudWatch Synthetics handles authentication, we will use the self-signed.badssl.com website. You can also use your own HTTP endpoint to simulate the same output. The first call will return a failure response, as the certificate has not yet been added. However, the error will be fixed in the next steps. Steps to create a Canary based on Heartbeat blueprint script: 1. Open the Synthetics menu of the CloudWatch console. 2. Choose Create Canary. 3. Choose Heartbeat monitoring from the blueprints list. 4. Under Name, enter a name for your canary – for example, https-selfsigned-test. 5. Enter the URL under the Application or endpoint URL. 6. Please leave all other settings as default, the IAM role with required permissions will be created for you and also a bucket with name cw-syn-results-accountID-region. In case the bucket already exists in the account, you can search for the name cw-syn-results or create your own, more details at Creating a canary. 7. Scroll to the bottom and select Create canary. When the canary is created, it will be displayed in the Canaries list, as shown in Figure 1. For information about utilizing the heartbeat monitor canary blueprint, see in the Heartbeat monitoring Amazon CloudWatch User Guide.

Figure 1: Canaries page of the CloudWatch console Checking reports The canary reports show every step and result of the calls. In this case, the canary returned the ERR_CERT_AUTHORITY_INVALID error as shown in Figure 1. This error is expected, as the endpoint uses a certificate that is not issued by a trusted CA and the canary cannot verify the authenticity of the self-signed certificate against a trusted authority.

Figure 2: http-steps-test report failing with the error ERR_CERT_AUTHORITY_INVALID Downloading the certificate details To address the authority issue with self-signed certificates, we can download the necessary certificate from the badssl website. This downloaded certificate will serve the purpose of establishing trust, and it will be used by the canary to authenticate API requests made to the self-signed.badssl.com endpoint. The following steps demonstrate how to achieve this using AWS CloudShell using CLI commands, however, these certificates can also be downloaded manually from the badssl website by using a browser. Note: We do not recommend self-signed certificates unless you are certain you trust the source. Downloading the public certificate from badssl.com For this procedure, we will use AWS CloudShell, a browser-based shell that makes it easy to securely manage, explore, and interact with your AWS resources. We recommend CloudShell to run the scripts below. However, you can use your own command line for the same output. Follow the steps below to download the .pem file from self-signed.badssl.com 1. Open the CloudShell console. 2. Wait for the environment to be created. 3. Copy and paste the script below in the bash console created by the AWS CloudShell. # Install openssl if you don't have that installed # Ubuntu users -> sudo apt-get install openssl sudo yum install openssl -y

# Download the self-signed.badssl.com certificate openssl s_client -showcerts -connect self-signed.badssl.com:443 -servername self-signed.badssl.com 1 ? "\\n" : ""), $0} END {print ""}' > self-signed.pem

# Get the certificate content to be pasted on the canary code cat self-signed.pemBash 4. Choose Paste.

Figure 3: CloudShell popup to paste multi-line text 5. The script above will generate a text with the certificate, so copy the content beginning at —–BEGIN CERTIFICATE—– and ending at —–END CERTIFICATE—– which will be used in the next steps. Editing the canary script to add the certificate: To import the certificate, you need to access the created canary and add a variable with the previously copied certificate output. To edit the code, follow these steps: 1. Open the Synthetics menu of the CloudWatch console. 2. Choose the canary created above – for example, https-selfsigned-test. 3. Choose Actions and then choose Edit. 4. Using the Script Editor box, import the 'https' package at the top of the script. This packages will let the script adjust protocol settings to feed in the certificate. const https = require('https');JavaScript 5. Insert this block of code below prior to where you attempt to use the 'page.goto()' function to access your endpoint. For the blueprint, this is within the loadBlueprint async function between the 'const sanitizedUrl' line and the 'const response = await page.goto...' line: await page.setRequestInterception(true);

page.on("request", (interceptedRequest) => { const options = { method: interceptedRequest.method(), headers: interceptedRequest.headers(), body: interceptedRequest.postData(), ca: cert, cert: cert, }; const request = https .request(interceptedRequest.url(), options, function (response) { response.on("data", function (data) { interceptedRequest.respond({ status: response.statusCode, contentType: response.headers["content-type"], headers: response.headers, body: data, }); }); }) .on("error", function (err) { console.error("Unable to call %s", options.uri, err); return interceptedRequest.abort("connectionrefused"); }); request.end(); }); JavaScript Note: This code will attempt to intercept the request being made from your canary to your endpoint. From there, it should modify the request options to include your uploaded certificate as the trusted certificate authority. 6. Add the certificate details by replacing the from the code you just pasted above. const cert = ' 'JavaScript 7. Choose Save. Although hard coding certificates is not considered a best practice, we opted for that approach in this blog post to streamline the steps. Additionally, as the certificate is public, the risk is lower. To follow best practices and reduce risks associated when deploying in your environment, you can use AWS Secrets Manager to store the secrets in a secure way as explained on How to validate authentication using Amazon CloudWatch Synthetics – Part 2. After modifying the code, save and wait for the canary to run again. Next, canary run should be PASSED with the steps tab showing the request status as PASSED.

Figure 4: https-selfsigned-test showing the status as Passed For users who are already familiar with how to use Synthetics, an advanced option is available that enables the bundling of a canary script with the certificate into a zip file. This approach is explained in the documentation called Writing a Canary Script for Node.JS or Python, providing comprehensive guidance for seamless implementation. Clean Up It is important to note that deleting a canary is irreversible and all associated data and settings will be lost. Make sure you have made all necessary backups or extracted all relevant data before proceeding with the deletion. To delete a CloudWatch Synthetics canary, follow these steps: 1. Open the Synthetics menu of the CloudWatch console. 2. Choose the canary created above – for example, https-selfsigned-test. 3. Choose Actions and then choose Stop. 4. Wait for the canary to change the state to Stopped. 5. Choose Actions again and then choose Delete. 6. Choose the resources you want to delete, for example the Role and Role policy used by the canary. 7. Choose Delete to delete the resources created by the canary. 8. Enter the phrase Delete into the field to confirm that you want to delete the resources created during this workshop. 9. Chose Confirm. The console will delete the selected resources and display Canary successfully deleted.

Figure 5: Synthetics page showing that the canary was deleted Conclusion In this post, we explored how to utilize self-signed certificates with CloudWatch Synthetics, expanding the capabilities of blueprints to connect to APIs, even if the certificate is not signed by a recognized and trusted third-party CA. Now, armed with these insights, you can confidently implement self-signed certificates in your own projects. To learn more about how to use this feature and all other capabilities, read the CloudWatch Synthetics documentation. Also, the AWS Command Line Interface (CLI) documentation for Synthetics can be found here. About the authors

Mario Jorge Salheb Leitao Mario Leitao is a Senior Cloud Architect with the AWS Managed Services (AMS) team based in Sydney. He provides technical guidance, designs and builds cost-effective and efficient solutions for customers ensuring their success on AWS. Prior to working as a Cloud Architect, he worked as a Network Development Engineer in Dublin, Ireland.

Matheus Canela In his role as Solutions Architect at Amazon Web Services, Matheus advises digital-native companies in the transformation of their technology platforms, helping all levels of engineers to achieve their goals by following best practices. Before he joined AWS, Matheus was a Senior DevOps Consultant at CMD Solutions, which is a Premier Consulting Partner based in Sydney. Matheus has also worked as a developer and a security specialist. Because helping the community is in his DNA, Matheus organizes .NET meetups and helps the IT.BR community, supporting qualified engineers from Brazil to migrate to Australia.

Edgar Yu Edgar is a Software Development Engineer on the CloudWatch Synthetics team at AWS. His current focus revolves around building and improving software to empower customers to gather more performance data and achieve better monitoring over their web applications. Outside of AWS, he enjoys food.

Matheus Canela O. Faraco In his role as Solutions Architect at Amazon Web Services, Matheus advises digital native companies in the transformation of their technology platforms, helping all levels of engineers to achieve their goals by following the best practices. Before he joined AWS, Matheus was a Senior DevOps Consultant at CMD Solutions, which is a Premier Consulting Partner based in Sydney. Matheus has also worked as a developer and a security specialist. Because helping the community is in his DNA, Matheus organizes .NET meetups and helps the IT.BR community, supporting qualified engineers from Brazil to migrate to Australia.

Use metric name filters in Amazon CloudWatch Metric Streams for finer grained control

Amazon CloudWatch Metric Streams now supports filtering by metric name. This blog will show you how to gain finer control over the metrics you want to stream using Include and Exclude metric name filters in Metric Streams. Amazon CloudWatch Metric Streams simplifies the way you collect and analyze metric data by providing continuous, near-real-time streams of Amazon CloudWatch metrics to a destination of your choice. You can use metric streams to send metrics to AWS Partner Solutions, including Datadog, Dynatrace, New Relic, Splunk, and Sumo Logic. Alternatively, you can send metrics to your data lake built on AWS, such as to Amazon Simple Storage Service (Amazon S3). You can create a continuous, scalable stream that includes the most up-to-date CloudWatch metrics to power dashboards, alarms, and other tools that rely on accurate and timely metric data. Traditionally, customers and AWS Partners relied on polling CloudWatch APIs to retrieve metrics that power all sorts of monitoring, alerting, and cost management tools. Since the introduction of Metric Streams in 2021, customers had an alternative way to create low-latency scalable streams of metrics. At launch, Metric Streams included the ability to filter metrics at a namespace level; for example, to include or exclude all metrics from a specific namespace. However, some customers wanted the ability to filter at a more granular level. That's where Metric Name Filtering in Metric Streams comes into play, addressing the need for more precise filtering capabilities. Customers want the flexibility to be fast and specific based on their use case and needs when streaming their metrics. To address the customer need of being more specific on what metrics to stream, CloudWatch Metrics Streams added support for filtering by metric name in May 2023 (Amazon CloudWatch Metric Streams adds support for filtering by metric name). This additional filtering capability provides specific metric name selection in addition to the existing option to filter metrics by namespace. You can now select what metrics to include or exclude for each streamed namespace. Now Metric Streams gives customers both things: The low latency and reduced operational overhead already offered by Metric Streams, plus the additional specific control to choose what specific metric names to stream. As Metric Streams pricing is based on the quantity of Metric Stream updates being egressed, metric name filtering is also a great mechanism for finer cost control. For more information, see Amazon CloudWatch pricing. Metric Streams Filtering by Metric Name: Include Vs Exclude The filtering by metric name feature of Metric Streams allows you to choose a list of metrics to include or exclude from the stream for each metric namespace. The "Include" and "Exclude" are mutually exclusive, you can't have both in a single stream. However, both types of filters provide valuable additional control over your streamed metrics depending on your use case. The "Include" filters have the benefit of allowing you to exactly choose what metrics to stream. However, it requires you to maintain an updated list of metric names you need, and you will not automatically receive any new metric added by AWS services unless you manually update your list of included metric names. In contrast, the "Exclude" filters have the benefit of maintaining a likely shorter list of metrics you don't want and also the fact that you will automatically receive any new metrics added to your streamed namespaces with no further action. Additionally, Metric Streams allows you to create metric name filters on metrics that may not exist yet in your AWS account. For example, you may know your application will produce metrics from the "AWS/EC2" namespace, but that application might not have been deployed yet in that account. If that is the case, those metrics will not exist yet in your CloudWatch account. The drop-down selectors in the Metric Streams console will only show metrics that currently exist in CloudWatch. However, the CloudWatch Metric Streams console allows you to free type whatever combination of namespace and metric names you might need for your filters, allowing you to create filters on metrics that may not exist. Likewise, you can also create metric name filters on non-existing metrics via AWS CloudFormation. The same applies to custom metrics and namespaces. This functionality is further explained in the following walkthrough. Walkthrough using the AWS Management Console In this section, we will demonstrate how you can gain finer control over metrics you want to stream using include and exclude metric name filters using console. Steps to use "Include" metric name filter while creating Metric Stream: 1. Open the Amazon CloudWatch console. 2. In the navigation pane, choose Metrics Streams. Then choose Create metric stream. 3. Under Select namespaces you wish to stream section, choose Selected namespaces and then select the namespaces to include to include only a few metric namespaces in the metric stream. 4. Under the Select metrics for the metric stream, choose Include metrics by metric name. Specify each metric to include by selecting its namespace and then its metric name. As mentioned above, the console will only show metrics that currently exist in CloudWatch. For that reason, some or all of the metrics shown in the below screenshot may not show in your console, depending on what resources you have deployed in your account at the moment. However, you can still add the filters shown in this example by free typing the namespace and metric names in their respective drop-down menus. After you select one metric, you can repeat this step to include another metric from the same namespace or a different one. If you don't select any metric name for one of your included namespaces, then all metrics from that namespace will be included in the metric stream.

5. Under Configuration section, select Quick S3 setup option. 6. Keep the rest of the configuration as default and then choose Create metric stream button. Your stream with the desired filters will be created in a few seconds. Steps to use "Exclude" metric name filters while creating a Metric Stream Now you are going to create a completely new stream, but this time using "Exclude" metric name filters. These steps are independent from the previous section about "Include" filters. 1. Open the Amazon CloudWatch console. 2. In the navigation pane, choose Metrics Streams. Then choose Create metric stream. 3. To demonstrate the "Exclude" use case, we will exclude multiple metrics across different namespaces. For that, select "All namespaces" under the Select namespaces you wish to stream section. 4. In the "Select metrics for the metric stream" section, choose the option to "Exclude metrics by metric name". Specify each metric to exclude by selecting its namespace and then its metric name. Similar to the "Include" example before, some or all of the metrics shown in the screenshot below for this particular example may not show in your console, depending on the existence of those metrics in your CloudWatch account. However, you can still add the filters shown in this example by free typing the namespace and metric names in their respective drop-down menus. After you select one metric, you can repeat this step to exclude another metric from the same namespace or a different one. You can check Figure 2 for further reference.

Figure 2: Stream creation using exclude metric name filters from a specific namespace

5. Under Configuration section select Quick S3 setup option. 6. Keep the rest of the configuration as default and then choose Create metric stream button. Your stream with the desired filters will be created in a few seconds. Now we will explore another use case for "Exclude" filters where we exclude a subset of namespaces entirely and then specify certain metric names to exclude from the remaining namespaces. The steps to configure this remain the same as explained above. The only additional configuration you need to add is to expand the optional menu "Exclude metric namespaces" and select the namespaces to be excluded entirely as shown in Figure 3. In Figure 3 below, we demonstrate the exclusion of two namespaces entirely (AWS/Usage and AWS/Billing), resulting in no metrics being streamed from these namespaces. Additionally, we can exclude specific metrics from the other namespace by selecting "Exclude metric by metric name" in the optional menu "Select metrics for the metric stream".

Figure 3: Example showing how to exclude namespaces fully and specific metrics from the remaining namespaces

Viewing metrics stream details and metric data being streamed to Amazon S3 destination You can open the newly created metric streams by selecting the specific metric stream in the CloudWatch Metric Streams console, and then choose View details button as shown below:

You can check the implemented metric name filters by scrolling down on the same page under "Selected metrics" section, as shown in the screenshot below.

Figure 5: Implemented metric name filters in the metric stream detail page

CloudWatch Metric Streams will create the following resources on your behalf when you create the stream using the console:

An Amazon S3 bucket: Destination for Kinesis Firehose delivery stream Amazon S3 write role: Role to allow a firehose write an Amazon S3 bucket Firehose write role: Role to allow a metric stream put metrics into a firehose Kinesis Firehose: Delivery stream to which metrics would be pushed

You can view the associated resources created in the metric stream detail page, as shown in the screenshot below.

Figure 6: Resources created by Metric Streams in the metric stream detail page

Finally, on the same page, you can explore the Amazon S3 bucket in order to view the metric data being streamed using the link under "Destination details". It may take few minutes for metric data to be streamed to Amazon S3 bucket.

Figure 7: Amazon S3 bucket destination link in the metric streams detail page

You will be redirected to the Amazon S3 bucket console page where you can see the metric data being stored as shown below.

Figure 8: Metric Data stored in the Amazon S3 bucket

Walkthrough using AWS CloudFormation You can also configure the include or exclude filters while creating Metric Streams using AWS CloudFormation. Steps to create CloudFormation stacks The following step shows you how to create a metric stream with either "Exclude" or "Include" metric name filters programmatically, using AWS CloudFormation. 1. Download the below CloudFormation templates and save them to your local machine. Choose one depending on your use case: Include or Exclude filters. a. MetricStreams_ExcludeFilter_CFN.yaml – Use this CloudFormation template to create a metric stream with "Exclude" metric name filters. b. MetricStreams_IncludeFilter_CFN.yaml – Use this CloudFormation template to create a metric stream with "Include" metric name filters. 2. Navigate to the AWS CloudFormation service in the AWS Management Console. From the Stacks list, select Create Stack > With new resources (standard) to get started with stack creation.

3. Select Template is ready and select Upload a template file. Choose Choose file and upload the template "MetricStreams_ExcludeFilter_CFN.yaml" you downloaded in step one. Once successfully uploaded, choose Next.

4. Name the CloudFormation stack as "exclude-metric-name-filter-demo" and choose Next.

5. Use default settings for the Configure Stack Options and choose Next. 6. Review the stack options and select the "I acknowledge that AWS CloudFormation might create IAM resources" check box and then choose Submit. Your CloudFormation stack status should now be showing CREATE_IN_PROGRESS indicating that the resources defined within the stack are being provisioned. 7. When the CloudFormation stack has finished creating resources (this will take few minutes), the status will change to CREATE_COMPLETE.

You can follow the above same steps in order to create another stack by using "MetricStreams_IncludeFilter_CFN.yaml" template downloaded in step one in order to create metric stream with include metric name filter resources. You can name this second stack as "include-metric-name-filter-demo" as shown below.

8. Let's verify the creation of the desired resources. Open the Amazon CloudWatch console. In the navigation pane, choose Streams under the Metrics tab. You can then open the newly created metric stream by selecting the metric stream and then choose "View details" button as shown below.

Figure 14: Viewing details of a newly created metric stream using AWS CloudFormation

Figure 15: Viewing details of a newly created metric stream

Cleaning up the resources created In order to clean up the resources created by Metric Streams using the AWS Management console walkthrough steps, please follow the steps below. 1. Open the Amazon CloudWatch console. 2. In the navigation pane, choose Metrics Streams. Then choose the metric stream that you want delete and choose "Delete" button as shown below.

Figure 17: Deleting metric stream resource created using AWS Management Console

Note: The resources mentioned in the above screenshots are not deleted when you delete the metric stream using console. You need to explicitly delete them in their associated consoles. In order to clean up the resources created by metric stream using the AWS CloudFormation walkthrough, please follow the steps below. There are couple of prerequisites that you need to complete before deleting the resources created by AWS CloudFormation stacks. Prerequisite 1 Navigate to the Amazon CloudWatch console and select the metric stream created using AWS CloudFormation. Choose "Stop" to stop streaming any new metric updates to Amazon S3 bucket. Prerequisite 2 Once you have stopped streaming metrics to Amazon S3 bucket you need to empty it. Please follow the below steps to locate your Amazon S3 bucket and then empty it:

Navigate to the AWS CloudFormation console. From the Stacks list, select stack that you want to delete and go to "Outputs" section. Copy the Amazon S3 bucket name.

Navigate to the Amazon S3 console, locate the Amazon S3 bucket name that you copied in the previous step and then choose Empty. Once the bucket is emptied move on to the next steps.

Steps to delete AWS CloudFormation stacks 1. In the AWS CloudFormation console, select the stack named "exclude-metric-name-filter-demo". 2. In the top right corner, choose Delete.

3. In the pop-up window, select Delete. 4. Wait for the stack to reach the DELETE_COMPLETE status. You need to periodically select Refresh to see the latest stack status. Similarly, by following the steps above, you can also delete the second stack created with name "include-metric-name-filter-demo". Conclusion CloudWatch Metric Streams now offers enhanced filtering capabilities through metric name filtering, providing finer grained control over the metrics you stream. Whether you choose to stream all your metrics, or include or exclude some metrics by namespace or metric name, Metric Streams helps provide low-latency streaming of metrics to your chosen destination, while reducing operational overhead. Start leveraging the capabilities of metric name filtering in CloudWatch Metric Streams today and unlock the full potential of your telemetry data and observability workflows. About the authors

Félix Mezo Gómez Félix Mezo is a Product Manager in the Amazon CloudWatch team at Amazon Web Services.

Jay Joshi Jay is a Cloud Support Engineer at Amazon Web Services, specialized in Amazon CloudWatch and Amazon Route 53. He is also passionate about assisting customers in enhancing their systems using AWS Monitoring and Observability services.

Graham Coleman Graham Coleman is a Senior Product Manager at Amazon Web Services. He has over 20 years' experience, and is passionate about Monitoring and Observability.

Félix Mezo Gómez Félix Mezo Gómez is a Product Manager for CloudWatch at Amazon Web Services. He is passionate about all things observability, helping customers get the most out of their metrics, logs and traces.

Graham Coleman Graham Coleman is a Senior Product Manager at Amazon Web Services. He has over 20 years experience, and is passionate about Monitoring and Observability.

Jay Joshi Jay Joshi is a Monitoring and Observability Specialist at Amazon Web Services (AWS), where he helps customers build resilient applications through innovative observability solutions and best practices. When not solving complex observability challenges, he enjoys watching anime and spending time with his family. LinkedIn: /jay-p-joshi

Did you know that women hold only 8% of cloud roles across European companies? And that the number of female tech founders in Europe remains under 15%? It's hard to believe that this is the situation in 2023. Fortunately, initiatives exist to drive positive change towards gender equity in tech. Amazon Web Services (AWS) re/Start is a re-skilling program for people who are new to technology. It supports individuals to gain knowledge, learn new skills, and ultimately launch a career in tech. The program is deliberate about increasing diversity in the tech sector. As part of AWS' commitment to training 29 million people globally by 2025, AWS re/Start is creating thousands of new builders every day. But the global skills shortage and the lack of diversity in tech will not be solved by one vendor, nor one hiring organization alone. It will take a movement across multiple countries, organizations and initiatives to make a significant change. AWS re/Start already works with a network of collaborating organizations on a global scale. So, when the opportunity arose to bring together a consortium of organizations on a similar mission, we seized the opportunity. Enter the Femme Forward project Femme Forward is a European Union (EU)-funded project aimed at empowering women with the skills and confidence to launch tech careers or form new start-ups. It is delivered by a consortium of experienced social value training providers and companies committed to changing the gender balance in tech, and in increasing the number of female founders. Co-funded by the EU, the project will run from January 2023 until December 2024. By providing inspiring role models, practical skills, and a robust support network, Femme Forward aspires to ignite a new wave of female tech professionals and entrepreneurs. With mentoring opportunities available in seven European languages, this program strives to make tech accessible and to positively impact over 500 women. This is the first big step in reshaping the gender balance in tech and fostering a thriving community of empowered women in the digital age. Simplon, an AWS re/Start collaborating organization, will officially lead the project, managing the various design and skills training delivery organizations including Engim, Factoria F5, Tech & Teach, Top-IX, Big Blue Data Academy, Cyprus Computer Society, BeCode, Simplon Romania, Simplon.co, Le TechSpace, and Hochschule Hannover University. AWS re/Start will provide the majority of the content, exam vouchers, and learning platforms for the 'Tech Careers' stream. Tech & Teach will provide the content for the 'Tech Founders' stream, focusing on content such as securing investment, writing a business plan, and pitching. All delivery organization members will then deliver the content in their home countries of Italy, Belgium, Germany, Romania, France, Greece, Spain, and Cyprus. As part of their commitment to addressing DE&I issues, AWS customers, Randstad and Fujitsu, have also offered their services free of charge. Randstad used its vast knowledge and experience to successfully create an online knowledge repository of useful resources for building CVs and interviewing to supplement the Tech Careers stream. Fujitsu will deliver soft-skill sessions, including communication, critical thinking, and emotional intelligence techniques, to help participants to become confident, thoughtful, and articulate professionals. I agree with Farah Verbruggen, vice president of EMEA, Randstad, who also values this push for women into tech. She says that "Randstad was keen to actively engage in the Femme Forward project. Working in the staffing industry, we see the lack of diversity and the overall tech skills shortage as a significant challenge for our customers across Europe." Get involved If you would like to be a part of the program as a mentor, or you're a female founder who could deliver a session in our inspirational workshops, please register via the Femme Forward website. If you're an aspiring tech founder, or looking for a career change into tech, we welcome you to join as many sessions as your schedule allows. Please review the upcoming events and register your attendance. Later in 2024, our final consortium member, Digital Europe, will share the knowledge and insights gained from the program, using its network and expertise to disseminate the information. We'll share more updates as the project progresses.

Consumer packaged goods (CPG) companies are facing unique challenges in today's hypercompetitive marketplace. Whether those challenges are slow market growth, increased competition, shifting consumer behaviors, commodities shortages, or inflationary pressures, they must push ahead to stay afloat. Together, Amazon Web Services (AWS) and AWS Partners enable CPG companies such as Heineken, Dole Foods, and Philips to accelerate growth, innovation, and digitally transform the CPG industry. AWS for CPG: How AWS Partners are helping CPGs accelerate innovation To accelerate revenue growth, AWS and AWS Partners enable CPGs to reinvent product development, manufacturing, supply chain, marketing, unified commerce, and digital transformation with unmatched compute, artificial intelligence (AI), machine learning (ML), and analytics capabilities. In our latest ebook, "Accelerate innovation leveraging AI/ML in Consumer Packaged Goods with AWS Partners" we explore:

How AWS and its partners help CPG companies draw key business insights from petabytes of data using unmatched compute capabilities, AI and ML to predict opportunities and risks in time to act. How AWS and its partners empower CPG companies to drive digital transformation across the organization to reduce costs while innovating key enterprise applications How to find best-in-class AWS Partner solutions for CPG. How AWS Solutions Consulting provides vetted solutions to common business and technical problems—deployed with help from AWS Partners. How companies like Danone, Britvic, and Stanley Black & Decker are using AWS Partner solutions, services, and guidance to accelerate innovation and digital transformation in the CPG industry.

The AWS Partner Network (APN) The AWS Partner Network (APN) is an integral part of our commitment to provide customers of all sizes, and in any industry around the world, a distinguishable network of partners across 150+ countries leveraging reliable and secure cloud services. AWS Partners are uniquely qualified to help customers accelerate their journey to the cloud, to innovate, and to take full advantage of the breadth and depth of AWS. Partners choose to work with AWS because we are dedicated to providing all our partners with opportunities to grow their business and help them deliver on our shared obsession to meet customer needs. Why work with an AWS Partner? Whether it's time to modernize your business, expand your solutions, or drive your business into a new, innovative territory, an AWS Partner can be a game-changer. Our robust AWS Partner community features over 100,000 partners from more than 150 countries. This vibrant, diverse network of partners can help you deliver greater value for your business, increase agility, and lower costs. On top of this, AWS Partners offer a powerful set of qualifications that are hard to find anywhere else, such as: Innovation – AWS Partners help you keep pace with cutting edge technologies, providing innovative, scalable and cost-effective solutions and capabilities for your organization. Realize faster time to value with products and professional services from thousands of partners in AWS Marketplace. Expertise – AWS Partners are strategic experts and experienced builders that help address your specific business needs. Unlock greater business value with AWS Specialization Partners that have deep technical knowledge and proven customer success. Global Reach – Wherever you want your business to go, there is an AWS partner ready to help. Choose from a global community of trusted AWS Partners across software, hardware, and services. Between increased competition, pandemic-induced commodity shortages, and shifting consumer behaviors, CPG companies have seen it all. But that doesn't mean these challenges need to derail your track to success. No matter where you are in your journey, AWS and AWS Partners are here to help move you forward, giving you access to innovative, global companies and solutions that accelerate innovation and revenue growth. Ready to accelerate innovation in CPG? Download our ebook "Accelerate innovation leveraging AI/ML in Consumer Packaged Goods with AWS Partners" to learn more. Check out more AWS Partners or contact an AWS Representative to know how we can help accelerate your business.

TAGS: AI/ML, aws partner, CPG, Manufacturing, marketing, Product Development, Supply Chain, Unified Commerce

Kevin E. McCurdy Kevin E. McCurdy is global CPG segment lead at APN for AWS, responsible for identifying and engaging relationships with strategic ISV and SI Partners. Previously, he served as VP of demand signal management at E2open; was cofounder and VP of strategic accounts for Orchestro, which was later acquired by E2open; and was also cofounder and VP of business development and services at Mercari Technologies. McCurdy has over 25 years of experience in supply chain management, category management, and demand signal management, working with global CPG companies and retailers, including Coca-Cola, General Mills, Kellogg's, PepsiCo, Unilever, and Kraft-Heinz. He holds a BSc in business logistics and international business from Penn State.

Kate Wiley Kate Wiley is the Head of Retail Industry Marketing at AWS. Prior to AWS, Kate held several marketing roles for retailers such as Dick's Sporting Goods, Reebok, Drybar, and Jenny Craig. As the Head of Retail Industry Marketing, Kate is responsible for supporting and educating retailers on how to use the cloud to build closer consumer relationships with their brand, optimize operations, and accelerate their digital transformation with AWS.

How SeatGeek uses AWS Serverless to control authorization, authentication, and rate-limiting in a multi-tenant SaaS application

SeatGeek is a ticketing platform for web and mobile users, offering ticket purchase and reselling for sports games, concerts, and theatrical productions. In 2022, SeatGeek had an average of 47 million daily tickets available, and their mobile app was downloaded 33+ million times. Historically, SeatGeek used multiple identity and access tools internally. Applications were individually managing authorization, leading to increased overhead and a need for more standardization. SeatGeek sought to simplify the API provided to customers and partners by abstracting and standardizing the authorization layer. They were also looking to introduce centralized API rate-limiting to prevent noisy neighbor problems in their multi-tenant SaaS application. In this blog, we will take you through SeatGeek's journey and explore the solution architecture they've implemented. As of the publication of this post, many B2B customers have adopted this solution to query terabytes of business data. Building multi-tenant SaaS environments Multi-tenant SaaS environments allow highly performant and cost-efficient applications by sharing underlying resources across tenants. While this is a benefit, it is important to implement cross-tenant isolation practices to adhere to security, compliance, and performance objectives. With that, each tenant should only be able to access their authorized resources. Another consideration is the noisy neighbor problem that occurs when one of the tenants monopolizes excessive shared capacity, causing performance issues for other tenants. Authentication, authorization, and rate-limiting are critical components of a secure and resilient multi-tenant environment. Without these mechanisms in place, there is a risk of unauthorized access, resource-hogging, and denial-of-service attacks, which can compromise the security and stability of the system. Validating access early in the workflow can help eliminate the need for individual applications to implement similar heavy-lifting validation techniques. SeatGeek had several criteria for addressing these concerns:

They wanted to use their existing Auth0 instance. SeatGeek did not want to introduce any additional infrastructure management overhead; plus, they preferred to use serverless services to "stitch" managed components together (with minimal effort) to implement their business requirements. They wanted this solution to scale as seamlessly as possible with demand and adoption increases; concurrently, SeatGeek did not want to pay for idle or over-provisioned resources.

Exploring the solution The SeatGeek team used a combination of Amazon Web Services (AWS) serverless services to address the aforementioned criteria and achieve the desired business outcome. Amazon API Gateway was used to serve APIs at the entry point to SeatGeek's cloud environment. API Gateway allowed SeatGeek to use a custom AWS Lambda authorizer for integration with Auth0 and defining throttling configurations for their tenants. Since all the services used in the solution are fully serverless, they do not require infrastructure management, are scaled up and down automatically on-demand, and provide pay-as-you-go pricing. SeatGeek created a set of tiered usage plans in API Gateway (bronze, silver, and gold) to introduce rate-limiting. Each usage plan had a pre-defined request-per-second rate limit configuration. A unique API key was created by API Gateway for each tenant. Amazon DynamoDB was used to store the association of existing tenant IDs (managed by Auth0) to API keys (managed by API Gateway). This allowed us to keep API key management transparent to SeatGeek's tenants. Each new tenant goes through an onboarding workflow. This is an automated process managed with Terraform. During new tenant onboarding, SeatGeek creates a new tenant ID in Auth0, a new API key in API Gateway, and stores association between them in DynamoDB. Each API key is also associated with one of the usage plans. Once onboarding completes, the new tenant can start invoking SeatGeek APIs (Figure 1).

Tenant authenticates with Auth0 using machine-to-machine authorization. Auth0 returns a JSON web token representing tenant authentication success. The token includes claims required for downstream authorization, such as tenant ID, expiration date, scopes, and signature. Tenant sends a request to the SeatGeak API. The request includes the token obtained in Step 1 and application-specific parameters, for example, retrieving the last 12 months of booking data. API Gateway extracts the token and passes it to Lambda authorizer. Lambda authorizer retrieves the token validation keys from Auth0. The keys are cached in the authorizer, so this happens only once for each authorizer launch environment. This allows token validation locally without calling Auth0 each time, reducing latency and preventing an excessive number of requests to Auth0. Lambda authorizer performs token validation, checking tokens' structure, expiration date, signature, audience, and subject. In case validation succeeds, Lambda authorizer extracts the tenant ID from the token. Lambda authorizer uses tenant ID extracted in Step 5 to retrieve the associated API key from DynamoDB and return it back to API Gateway. The API Gateway uses API key to check if the client making this particular request is above the rate-limit threshold, based on the usage plan associated with API key. If the rate limit is exceeded, HTTP 429 ("Too Many Requests") is returned to the client. Otherwise, the request will be forwarded to the backend for further processing. Optionally, the backend can perform additional application-specific token validations.

Architecture benefits The architecture implemented by SeatGeek provides several benefits:

Centralized authorization: Using Auth0 with API Gateway and Lambda authorizer allows for standardization the API authentication and removes the burden of individual applications having to implement authorization. Multiple levels of caching: Each Lambda authorizer launch environment caches token validation keys in memory to validate tokens locally. This reduces token validation time and helps to avoid excessive traffic to Auth0. In addition, API Gateway can be configured with up to 5 minutes of caching for Lambda authorizer response, so the same token will not be revalidated in that timespan. This reduces overall cost and load on Lambda authorizer and DynamoDB. Noisy neighbor prevention: Usage plans and rate limits prevent any particular tenant from monopolizing the shared resources and causing a negative performance impact for other tenants. Simple management and reduced total cost of ownership: Using AWS serverless services removed the infrastructure maintenance overhead and allowed SeatGeek to deliver business value faster. It also ensured they didn't pay for over-provisioned capacity, and their environment could scale up and down automatically and on demand.

Conclusion In this blog, we explored how SeatGeek used AWS serverless services, such as API Gateway, Lambda, and DynamoDB, to integrate with external identity provider Auth0, and implemented per-tenant rate limits with multi-tiered usage plans. Using AWS serverless services allowed SeatGeek to avoid undifferentiated heavy-lifting of infrastructure management and accelerate efforts to build a solution addressing business requirements.

Umesh Kalaspurkar Umesh is a Sr Solutions Architect at AWS, and brings more than 20 years of experience in design and delivery of Digital Innovation and Transformation projects, across enterprises and startups. He is motivated by helping customers identify and overcome challenges. Outside of work, Umesh enjoys being a father, skiing, and traveling.

Anderson Parra Ander Parra is Staff Software Engineer at SeatGeek and has over 10 years of industry experience in software engineering. He has worked with many different types of systems with diverse constraints and challenges, mainly based on JVM (Java and Scala). He is also experienced in high-scale distributed systems and functional programming. Outside work, Ander enjoys being a father, coffee, and traveling.

Anton Aleksandrov Anton is a Principal Solutions Architect for AWS Serverless and Event-Driven architectures. Having over two decades of hands-on engineering and architecture experience, Anton works with major ISV and SaaS customers to design highly scalable, innovative, and secure cloud solutions.

João Mikos João Mikos has over 20 years of experience working with development and infrastructure in distributed systems for companies sprawling four continents across various business verticals. Previously, he led infrastructure teams for highly regulated fintech companies and a Solutions Architecture team at AWS. Now a Director of Engineering at SeatGeek, he leads the Developer Acceleration team, responsible for enhancing the experience of customer and partner engineers developing with SeatGeek and the teams developing within SeatGeek.

Samit Kumbhani Samit Kumbhani is a Sr. Solutions Architect at AWS based out of New York City area. He has 18+ years of experience in building applications and focuses on Analytics, Business Intelligence, and Databases. He enjoys working with customers to understand their challenges and solve them by creating innovative solutions using AWS services. Outside of work, Samit loves playing cricket, traveling, and spending time with his family and friends.

How to use AWS Verified Access logs to write and troubleshoot access policies

On June 19, 2023, AWS Verified Access introduced improved logging functionality; Verified Access now logs more extensive user context information received from the trust providers. This improved logging feature simplifies administration and troubleshooting of application access policies while adhering to zero-trust principles. In this blog post, we will show you how to manage the Verified Access logging configuration and how to use Verified Access logs to write and troubleshoot access policies faster. We provide an example showing the user context information that was logged before and after the improved logging functionality and how you can use that information to transform a high-level policy into a fine-grained policy. Overview of AWS Verified Access AWS Verified Access helps enterprises to provide secure access to their corporate applications without using a virtual private network (VPN). Using Verified Access, you can configure fine-grained access policies to help limit application access only to users who meet the specified security requirements (for example, user identity and device security status). These policies are written in Cedar, a new policy language developed and open-sourced by AWS. Verified Access validates each request based on access policies that you set. You can use user context—such as user, group, and device risk score—from your existing third-party identity and device security services to define access policies. In addition, Verified Access provides you an option to log every access attempt to help you respond quickly to security incidents and audit requests. These logs also contain user context sent from your identity and device security services and can help you to match the expected outcomes with the actual outcomes of your policies. To capture these logs, you need to enable logging from the Verified Access console.

Figure 1: Overview of AWS Verified Access architecture showing Verified Access connected to an application

After a Verified Access administrator attaches a trust provider to a Verified Access instance, they can write policies using the user context information from the trust provider. This user context information is custom to an organization, and you need to gather it from different sources when writing or troubleshooting policies that require more extensive user context. Now, with the improved logging functionality, the Verified Access logs record more extensive user context information from the trust providers. This eliminates the need to gather information from different sources. With the detailed context available in the logs, you have more information to help validate and troubleshoot your policies. Let's walk through an example of how this detailed context can help you improve your Verified Access policies. For this example, we set up a Verified Access instance using AWS IAM Identity Center (successor to AWS Single Sign-on) and CrowdStrike as trust providers. To learn more about how to set up a Verified Access instance, see Getting started with Verified Access. To learn how to integrate Verified Access with CrowdStrike, see Integrating AWS Verified Access with device trust providers. Then we wrote the following simple policy, where users are allowed only if their email matches the corporate domain.

Before improved logging, Verified Access logged basic information only, as shown in the following example log.

"identity": { "authorizations": [ { "decision": "Allow", "policy": { "name": "inline" } } ], "idp": { "name": "user", "uid": "vatp-09bc4cbce2EXAMPLE" }, "user": { "email_addr": [EMAIL] "name": "Test User Display", "uid": [EMAIL] "uuid": "00u6wj48lbxTAEXAMPLE" } }Plain text

Modify an existing Verified Access instance To improve the preceding policy and make it more granular, you can include checks for various user and device details. For example, you can check if the user belongs to a particular group, has a verified email, should be logging in from a device with an OS that has an assessment score greater than 50, and has an overall device score greater than 15. Modify the Verified Access instance logging configuration You can modify the instance logging configuration of an existing Verified Access instance by using either the AWS Management Console or AWS Command Line Interface (AWS CLI).

Open the Verified Access console and select Verified Access instances. Select the instance that you want to modify, and then, on the Verified Access instance logging configuration tab, select Modify Verified Access instance logging configuration.

Under Update log version, select ocsf-1.0.0-rc.2, turn on Include trust context, and select where the logs should be delivered.

After you've completed the preceding steps, Verified Access will start logging more extensive user context information from the trust providers for every request that Verified Access receives. This context information can have sensitive information. To learn more about how to protect this sensitive information, see Protect Sensitive Data with Amazon CloudWatch Logs. The following example log shows information received from the IAM Identity Center identity provider (IdP) and the device provider CrowdStrike.

"data": { "context": { "crowdstrike": { "assessment": { "overall": 21, "os": 53, "sensor_config": 4, "version": "3.6.1" }, "cid": "7545bXXXXXXXXXXXXXXX93cf01a19b", "exp": 1692046783, "iat": 1690837183, "jwk_url": " ", "platform": "Windows 11", "serial_number": "ec2dXXXXb-XXXX-XXXX-XXXX-XXXXXX059f05", "sub": "99c185e69XXXXXXXXXX4c34XXXXXX65a", "typ": "crowdstrike-zta+jwt" }, "sso": { "user": { "user_id": "24a80468-XXXX-XXXX-XXXX-6db32c9f68fc", "user_name": "XXXX", "email": { "address": [EMAIL] "verified": false } }, "groups": { "04c8d4d8-e0a1-XXXX-383543e07f11": { "group_name": "XXXX" } } }, "http_request": { "hostname": "sales.example.com", "http_method": "GET", "x_forwarded_for": "52.XX.XX.XXXX", "port": 80, "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0", "client_ip": "52.XX.XX.XXXX" } } }Plain text

The following example log shows the user context information received from the OpenID Connect (OIDC) trust provider Okta. You can see the difference in the information provided by the two different trust providers: IAM Identity Center and Okta.

"data": { "context": { "http_request": { "hostname": "sales.example.com", "http_method": "GET", "x_forwarded_for": "99.X.XX.XXX", "port": 80, "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15", "client_ip": "99.X.XX.XXX" }, "okta": { "sub": "00uXXXXXXXJNbWyRI5d7", "name": "XXXXXX", "locale": "en_US", "preferred_username": [EMAIL] "given_name": "XXXX", "family_name": "XXXX", "zoneinfo": "America/Los_Angeles", "groups": [ "Everyone", "Sales", "Finance", "HR" ], "exp": 1690835175, "iss": " " } } }Plain text

The following is a sample policy written using the information received from the trust providers.

permit(principal,action,resource) when { context.sso.groups has " " && context.sso.user.email.address like [EMAIL] && context.sso.user.email.verified == true && context has "crowdstrike" && context.crowdstrike.assessment.os > 50 && context.crowdstrike.assessment.overall > 15 };Plain text

This policy only grants access to users who belong to a particular group, have a verified email address, and have a corporate email domain. Also, users can only access the application from a device with an OS that has an assessment score greater than 50, and has an overall device score greater than 15. Conclusion In this post, you learned how to manage Verified Access logging configuration from the Verified Access console and how to use improved logging information to write AWS Verified Access policies. To get started with Verified Access, see the Amazon VPC console.

If you have feedback about this post, submit comments in the Comments section below. If you have questions about this post, contact AWS Support. Want more AWS Security news? Follow us on Twitter.

Ankush Goyal Ankush is an Enterprise Support Lead in AWS Enterprise Support who helps Enterprise Support customers streamline their cloud operations on AWS. He enjoys working with customers to help them design, implement, and support cloud infrastructure. He is a results-driven IT professional with over 18 years of experience.

Anbu Kumar Krishnamurthy Anbu is a Technical Account Manager who specializes in helping clients integrate their business processes with the AWS Cloud to achieve operational excellence and efficient resource utilization. Anbu helps customers design and implement solutions, troubleshoot issues, and optimize their AWS environments. He works with customers to architect solutions aimed at achieving their desired business outcomes.

TAGS: Amazon Verified Access, Amazon Web Services, AWS, Security Blog, Zero Trust

Introduction Reality capture (RC) is the process of digitizing real-world objects or environments. As this process can be complex, this post provides context to the current state of associated technologies. RC has vast implications in many industries, including Automotive, Aerospace, Oil and Gas, Engineering, Construction, and Real Estate. There is a fundamental shift taking place across these industries related to accurately capturing and digitizing the real world and objects within it. This technology has been improved over the last several years. What was previously a very expensive and time-consuming process that led to potentially inaccurate results, is now faster, less expensive, and more accurate. With the initial value of RC now being validated in enterprise, an emphasis on enablement is critical to usher in the next wave of adoption. In order to uncover this technology's full potential, it is necessary to decrease barriers to entry and implementation. This post will cover several aspects of digitization, including methods, an overview of digital twins, and a case study to show a real-world example with an AWS customer. You will also hear from the Reality Capture Network (RCN), an online education center that focuses on digitization. Matthew Byrd, Founder of RCN, will dive into trends he sees and use cases of RC around the world. Benefits and Challenges The benefits of RC have been validated from enterprise companies around the world. Common themes among these use-cases include surveying conditions, operational improvements, safety analysis, security analysis, and core business process advancements. The benefits can be substantial and so can the challenges. To realize these benefits, a company must understand the associated key Spatial Computing topics and challenges. These associated challenges include the 3D capture process (Digitization Methods), 3D model manipulation, and final asset implementation. These challenges will be discussed throughout this post. Digitization Methods In this post I will discuss three methods for digitizing the real world: LiDAR based scanning, Photogrammetry, and Neural Radiance Fields (NeRFs). LiDAR Scanning LiDAR based solutions send out light rays from a scanning device and calculate the time it takes to return. This is referred to as a Time of Flight (ToF) calculation. This method creates a point cloud of the real world and objects that the rays come into contact with. Point clouds can vary greatly in density depending on the settings selected and device limitations. A denser point cloud will result in a proportionally higher resolution end product. These point clouds can then be converted into 3D meshes that can be consumed by industry standard 3D software. Solutions like Autodesk's ReCap can help with this conversion.

LiDAR scanners can be expensive and in some cases need a technician to operate. There have been great advancements in decreasing these barriers, and now there are also simpler scanners that are very easy to use. Handheld LiDAR scanners and newer high end smart phone models with LiDAR built in, can allow users to capture environments and objects quickly and easily. Photogrammetry Photogrammetry is a fundamentally different process from LiDAR based solutions. This process takes a set of images of an object or area as an input, and uses an algorithm to build a 3D model recreation from those images. To keep processing times reasonable, typically these image sets range roughly from 40 to 250 photographs, but can use more or less. These input images can also be extracted from video, which makes the capture process very fast, under two minutes in some cases. This makes it easy for anyone to quickly capture an environment or object. The more images included, the longer it takes for processing and recreation. Depending on the quality of the input images, this can result in a 20-30% higher resolution asset.

Left: Screenshot of the photogrammetry process. Each white triangle above the building is one input photograph camera location. Right: Example of one of the input photographs. Thomas Jefferson Memorial source data from cyark.org, license CC BY-NC-SA.

Common uses of photogrammetry include cultural heritage (art, museum relics), media and entertainment (human face and body scanning for CGI character creation), gaming (hyper-realistic game asset creation), and site surveying (capturing buildings and areas of land, commonly with use of drones). Neural Radiance Field (NeRF) NeRF technology is the most infant of the three techniques. NeRF uses a process that is similar to photogrammetry in that it uses photographs or extracted video frames for 3D reconstruction. A NeRF is a unique asset type which can only be viewed in specialized software. The result can be difficult to convert into a clean 3D mesh to then be used with other standard 3D modeling software and tools. NeRF technology, including Nvidia's Instant NeRF is improving rapidly, however early results can have substantial artifacts in their raw output. These artifacts can make the resulting 3D asset difficult to work with and less desirable for certain use cases. However, it is important to note that artifacts are also typical of raw LiDAR and photogrammetry-based scans. With LiDAR and photogrammetry having been around longer, the tools available to refine, convert and integrate these models into other software are more mature as well. For this reason, NeRF based assets have not been used heavily within enterprise use cases yet. This is sure to change as the technology and tools become more mature.

Neural Radiance Field (NeRF) Scan of a street in New York City. Using Nvidia's Instant-ngp. Notice the blurry artifacts in the resulting output.

LiDAR, photogrammetry, and NeRF models can all be very accurate and high-resolution solutions, all aimed at different use cases. With LiDAR and photogrammetry having the ability to result in millimeter accuracy, and NeRF in its infancy, these are different tools for different jobs. As this post is focused on enterprise use cases, I will not expand further on NeRF technology and will instead dive deeper into LiDAR and photogrammetry. 3D Model Optimization 3D model resolution is important to acknowledge for the following reasons. Digital Twins are only as valuable as the information that can be gleaned from them. If the information is not high enough resolution, it may not be adequate to make necessary business decisions. If the resolution is too high, the application can become slow and difficult to use, or completely unusable. Thus, it is important to note that higher resolution does not always mean a more valuable resulting asset or digital twin. In order to understand how high of a resolution scan you want, you need to understand the use case for which you are scanning, and how end users will be interacting with the 3D model. High resolution scans can take substantial processing power and time to create, and the resulting scan can end up being a very large files size on the scale of gigabytes to terabytes of data. These can be expensive to process and store, and can be difficult to work with and manipulate downstream. One of the largest challenges by industries in 3D content creation is delivering high-resolution content that is also lightweight. The term "lightweight" in computer graphics is commonly used to describe a 3D asset that does not take a lot of computational processing power to visualize. Alternatively, "heavy" 3D models are complex and take a substantial amount of processing power to visualize and manipulate.

Image of two 3D model spheres rendered with lighting and shadows

Image of the same 3D model spheres. The sphere on the right is 16x heavier than the sphere on the left with hardly a noticeable difference when rendered

Having lightweight 3D assets is beneficial for several reasons. They are easier to manipulate and work with, more efficient to render and visualize, cost less to store in data storage, and are proportionally faster to send and receive between applications. Using 3D assets that have been optimized allows 3D experiences to run more smoothly and allows the creators to add additional complexity to the experience elsewhere. These lighter weight assets free up compute resources for other functionality or processing. It can be difficult to determine the right digitization method for a specific use-case. Important questions to ask include: Are there reflective or shiny surfaces? Is the lighting of the environment changing frequently? Does the solution need to be portable? How large of an area do you need scanned? Will the render happen on the cloud/server or on a mobile device? Is the target object or area indoors or outdoors? How important is the texture resolution? How quickly do you need to scan it? What is your budget? How do you want to use the resulting 3D asset? Is there already an existing dataset? These questions are necessary to pick the right tool for the job and validate that the resulting assets will meet business objectives downstream and bring value. One powerful way to leverage these resulting assets is within a Digital Twin. Digital Twins A Digital Twin is an interactive application that combines 3D models of existing physical facilities and an overlay of additional data to provide insights and enable efficient decision making. Common Digital Twin environments include factories, industrial facilities, and manufacturing plants. Digital Twins have proven to help with building operations, increase production output, and improve equipment performance. Regardless of which method is used to digitize an object or environment, the result is typically a simplified standard 3D model format. A standard model format is critical because it is typically necessary to bring that model into other 3D software. For any of the previously mentioned industries, Automotive, Aerospace, Real Estate, each have their own industry specific 3D software. These standard model formats can also be brought into other non-industry specific tools such as web applications, Realtime engines, or Digital Twin environments. The purpose of a Digital Twin is to give users a comprehensive understanding of the asset(s) in subject. The overlaying of additional information on top of the 3D model is critical for exactly that reason. Digital Twins allow organizations to bring together critical pieces of information to a single environment in order to gain a comprehensive view, obtain actionable insights and reach business objectives. If Information is not aggregated from multiple sources, for example a single static 3D model asset, it is not a digital twin. A Digital Twin is a living asset. A Digital Twin solution, AWS IoT TwinMaker allows developers to easily build Digital Twins of real-world environments. The "Scene" is a key feature of AWS IoT TwinMaker. The scene is a 3D canvas that allows you to import one or multiple 3D models. These models are organized into a hierarchy panel on the left side that allows you to control visibility of objects and overlay live data points. Grafana is a key integration to AWS IoT TwinMaker. Grafana is a third-party interactive data visualization application that integrates nicely with AWS IoT TwinMaker. This integration allows you to quickly create charts, graphs, and dashboard layouts leveraging your existing AWS IoT data. It also allows developers to bring in their full AWS IoT TwinMaker 3D scene and pull data from the data streams within those scenes. This allows for a robust and flexible Digital Twin dashboard solution.

AWS IoT TwinMaker 3D viewer with associated live data stream in a Grafana dashboard

Another way to create a similar robust Digital Twin dashboard that can be distributed on the web is by way of AWS IoT Application Kit, or leveraging AWS Partners like Prevu3D. IoT Application Kit is an open-source UI component library which allows developers to create web applications that integrate with AWS IoT TwinMaker quickly and securely. IoT Application Kit gives developers the flexibility of the web while also allowing the security, scalability, and reliability of AWS IoT TwinMaker and AWS IoT infrastructure under the hood. KCI – Case Study KCI is a global professional services firm that focuses on land development, preservation and infrastructure. KCI needs accurate surveys of existing and potential work sites to facilitate their projects. With a focus on infrastructural work, these sites can range in scale. KCI needed to create a pipeline to obtain high resolution 3D models of specific geographic areas. The areas in question ranged from 1 acre to 20 acres in size. At this scale, typical LiDAR based scanning methods from ground level do not scale well. Ground level LiDAR scans have limits on area sizes. Depending on desired scan resolution, to cover an area larger than an acre, multiple scans must be captured and then combined. This splicing of multiple LiDAR scans together is called the "registration process". This registration process can be difficult to automate, especially at a large scale, and can become heavy and difficult to use. Another common issue with LiDAR scanning from the ground level over large areas is the perspective of the scans. Depending on the use-case of the resulting scan, with obstacles and interference (trees, cars, hills, and buildings) the resulting assets can be incomplete and unusable. If images are taken from the sky, by plane, satellite or drone, it can lead to a more complete recreation of the subject area.

A scalable pipeline was built that allows KCI to process images using photogrammetry and receive back a resulting 3D model of the subject area. Benefits of this pipeline include not having to use local computer resources, as engineers need their local machines for other tasks. It also gives the ability to scale up and down resources on demand to never over or under utilize infrastructure, and the data is backed up on the cloud. This pipeline is built to be flexible. An exciting feature added to this pipeline because of this flexibility, was the integration of Amazon Rekognition. This gave KCI the ability to use machine learning to remove any unwanted aspects of the images sent into the photogrammetry process. For instance, KCI did not want cars in the resulting 3D model. The team was able to use Rekognition to find all cars within the input image set and remove them as a pre-processing step. This allowed a clean resulting 3D scan model without any cars.

Left: Original input image. Right: Illustration of Amazon Rekognition finding all cars to cull from the image.

These resulting 3D model assets can be used throughout the lifecycle of the project for different use cases. These use cases include topographic analysis, feasibility studies, excavation volume takeoffs, project planning, coordination, and visualization. This can lead to improved process efficiency and decreased potential for costly mistakes. With projects at this scale, this impact can be substantial. KCI recorded a reduction of processing time from 2 days down to roughly an hour. When it comes to digitization best practices and understanding industry sentiment, experts in the field can help provide guidance and context. Reality Capture Network Reality Capture Network (RCN) is a podcast, online education center, and annual conference that focuses on sharing knowledge and spreading awareness on new technologies and innovation in the built environment. Matthew Byrd, Founder and Host of RCN shares his thoughts and lessons learned below: "The demand for the digitization of existing assets is experiencing exponential growth. This trend encompasses various industries, from global oil and gas companies seeking to create digital twins of their facilities for simulations, training, capital improvement projects, airports searching for more efficient asset management, security planning, and facility maintenance programs. The use-cases of the technology are vast, and I believe [the world] is just beginning to witness the onset of a significant upward growth curve. It is an opportune time to explore these technologies." Matthew expands, "There are several reasons behind the growing adoption of Reality Capture. Firstly, [the world] is witnessing advancements in both hardware and software capabilities. Simultaneously, the cost of the technology is decreasing, making it better, faster, and more affordable. Additionally, an increasing number of companies are discovering innovative ways to leverage this data, offering advanced analytic tools, drafting and modeling capabilities, and cloud-hosted online viewers. These developments enable firms across diverse industries to utilize the data in ways that were previously impossible." Conclusion As digitization technologies evolve and improve, the benefits continue to grow, and the impact of these technologies has been proven. As the tools become easier to use and can handle more data, they will continue to become more valuable to their associated industries. Decreasing the barriers for users to consume and interact with these assets will greatly increase adoption across all associated industries. Understanding the different key technologies in the space and how they can be leveraged is important to creating an effective Digital Twin or 3D asset strategy. Picking the right tool for the job to obtain actionable data is as important as knowing how to leverage that data downstream. If the end product does not provide actionable insights, then it was likely not a good investment. There is a growing number of companies exploring within this digitization space. With hardware and software solutions improving quickly and companies investing more deeply, adoption will follow. AWS is working hard to enable these workloads and help usher the next wave of Spatial Computing. —– To learn more about digitization and spatial computing at AWS, check out these other blog posts: · Building a Digital Twin with Photogrammetry and AWS IoT TwinMaker · Photogrammetry reconstruction with 3Dflow on AWS · LiDAR 3D Point Cloud Labeling with Velodyne LiDAR Sensor · Epic Games launches RealityScan app on AWS · HxDR: Transforming geospatial data in the cloud with AWS and Hexagon

Adam Chernick Adam is a Worldwide Sr. Solutions Architect dedicated to Amazon GameLift Streams. He has focused his career on real-time 3D, generative AI and adjacent emerging technology.

Sharing data on Amazon FSx for OpenZFS across Linux and Windows clients

Many organizations need a high-performance shared file system that they can access simultaneously from Linux and Windows, despite different permission models across the platforms. For example, a media and entertainment enterprise may render workloads mutually on Linux and Windows clients. These customers may use mechanisms like "User Mapping" to make sure that their Windows clients can mount file systems shared via NFS and avoid file access conflicts. Amazon FSx for OpenZFS is a fully managed AWS service providing scalable OpenZFS file systems for high-performance applications. It features data management capabilities like snapshots and encryption, supports over 1 million IOPS and 21 GB/s throughput, and is ideal for big data, DevOps, and research workloads. To bridge the gap between different operating systems and their unique permission models, organizations often use solutions like "User Mapping" for seamless file sharing via NFS. In this post, I show how to use Network File System (NFS) protocol version 3 and different authentication methods available to concurrently share data stored on FSx for OpenZFS data to your Linux and Windows clients. That enables cross-platform data accessibility, enhanced security, and efficient data management, leading to improved productivity and resource optimization. Background on NFS and Amazon FSx for OpenZFS NFS is a native protocol on Linux and you can mount FSx for OpenZFS on Linux by using the standard Linux mount command and the domain name system (DNS) name associated with the volume. You can access data on your FSx for OpenZFS file system by using NFS protocol versions (v3, v4.0, v4.1 and v4.2). Linux clients natively support NFS client versions 3 and 4.x and let you mount the file system using the standard Linux mount command. Windows clients support NFS version 2 and 3 and require installation of the NFS client. You utilize NFS version 3 when connecting Linux and Windows clients concurrently to FSx for OpenZFS, since that is the NFS version supported by both platforms. The AWS post "New – Amazon FSx for OpenZFS" provides information on setting up an FSx for OpenZFS file system. Solution walkthrough The following is a summary of the steps to implement this solution:

Install and Configure the NFS client for Windows: this is required to mount FSx for OpenZFS on Windows clients using the NFS protocol. Mount the file system: you must mount the FSx for OpenZFS file system on both Windows and Linux clients in order to access the data. Select and configure the identity mapping and authentication method: you will choose between User Name Mapping server, Active Directory (AD) integration, or anonymous authentication to map and authenticate users to access the files on FSx for OpenZFS.

1. Install and configure the NFS client on Windows You will need to install and configure NFS client on Windows. Following are the steps to install the NFS client on Windows. This can be accomplished either via GUI or Windows powershell. The following is an example using the GUI on Windows Server 2019 (this process also works in Windows Server 2022): Installation

Open the Server Manager on the Windows server. On the dashboard, select Quick Start, then Add Roles and Features, then Next in the dialog box. For Installation Type, select Role-based or feature-based installation and select Next. In the Server Selection screen, select your server name and select Next. On Server Roles, expand File and Storage Services. On File and Storage Services, select Storage Services and select Next. That will bring you to the final step. On the Features dialog, select Client for NFS and select Install.

Figure 1: Installing the NFS client on Windows Server Configuration After installing the NFS client on Windows, you will need to configure it. Use the NFS client on Windows Server GUI to customize client settings, default file permissions, and security model. The default settings (shown in the following) work for most environments. However, you must consider if the default file permissions address your security posture or match with the defaults on your Linux distribution.

Figure 2: NFS client on Windows Server – default file permissions 2. Mount the file system Access the data by mounting the file system on the Linux and Windows clients.

To mount the file system, you need its DNS name. Using the FSx console, navigate to the Network & security tab of your FSx for OpenZFS file system and copy its DNS name. In the following screenshot, I show the DNS name of one of my FSx for OpenZFS file systems:

Figure 3: Capturing the DNS name of the file system 2. Next, mount the file system on the Linux server using the file system DNS name and the following mount command (a basic mount command): mount -t nfs -o vers=3 fs-04fcff18e33270111.fsx.us-west-2.amazonaws.com:/fsx/sync_vol /fsxsync

Figure 4: Mounting the file system on Linux 3. Finally, also mount the file system on the Windows server using the following command: mount \\fs-04fcff18e33270111.fsx.us-west-2.amazonaws.com\fsx\sync_vol\ Z:

Figure 5: Mounting the file system on Windows Refer to the FSx for OpenZFS documentation for steps to optimize file system performance. 3. Select and configure the identity mapping and NFS authentication method Linux and Windows use different account and security systems. Linux represents users with user identifiers (UIDs) and group identifiers (GIDs). Windows represent users and groups with a unique security identifier (SID). User mapping is the process of converting UIDs and GIDs from Linux to SIDs from Windows and vice versa. User mapping provides a clean default permission set for Windows users to transparently access, modify, and create files. After installing and configuring the Services for NFS as described in the "Background on NFS and Amazon FSx for OpenZFS" section, you must select and configure the appropriate identity mapping and authentication method. You can use User Name Mapping server, Active Directory (AD) integration, or anonymous authentication. AUTH_SYS or AUTH_UNIX: using UID and GID identifiers for account mapping AUTH_SYS or AUTH_UNIX account mapping is the process of matching the Linux UIDs and GIDs to corresponding Windows user and group SIDs. The Microsoft Client for NFS supports two methods for account mapping: either use the %windir%\system32\etc\passwd method for standalone servers or AD for servers joined to a domain. 3.1 Standalone servers: the /etc/passwd method The /etc/passwd method creates a one-to-one mapping of a Linux UIDs and GIDs to a Windows user and group SID. 1. First, provide a User Name Mapping server using the NFS Client for Windows. In our example, the mapping server is the server containing the password file.

Figure 6: NFS client on Windows Server – User Name Mapping server 2. Next, place a password (passwd) file on the Windows path %SYSTEMROOT%\system32\drivers\etc. Each Windows user/SID on the file is matched to a Linux user based on the UID and GID in the file. 3. Restart the NFS client on Windows after configuring the /etc/passwd file and mapping server. You can use the powershell commands nfsadmin client stop and nfsadmin client start for that. Make sure that the file system is unmounted on Windows before you restart the NFS client.

Figure 7: Restarting the NFS client The following is an example of user mapping using the /etc/passwd file placed on the Windows client at %SYSTEMROOT%\system32\drivers\etc.

Figure 8: Windows passwd file example Each line has the following fields separated by a colon: Windows username, Linux UID, Linux GID, description, Windows home directory. In this example, I created a user called mary on both Linux and Windows. After logging onto the Windows server and mounting the file system, I can confirm that mary's UID and GID (UID 1002 and GID 1007) are in effect by issuing the mount command (as follows). The NFS client mapped the UID and GID using the password file I provided.

Figure 9: mount point effective UID and GID 4. Next, create a file on Linux and check it on Windows. Using the account mary on Linux, I create the text file called mary-file-linux.txt on my FSx for OpenZFS file system mounted on /sync_vol. In the following, you can see the ownership, group membership, and permissions on the file mary-file-linux.txt.

Figure 10: file permissions – Linux 5. Now, check that file from the Windows side. Logged in as mary on Windows, I open the drive where I mapped the FSx for OpenZFS file system. I can see that the file is accessible and preserved the same ownership, group membership, and permissions from Linux. In the red boxes in Figure 11 you can see the file permissions, user ID, and group ID assigned by Windows.

Figure 11: file permissions – Windows file properties 6. Next, create a file on Windows and check it on Linux. Using the account mary on Windows, I create a text file and save it on the Z: drive (the FSx for OpenZFS share).

Figure 12: sample text file saved on Z: From Windows, I can see that the NFS client for windows assigned permissions using the Linux standard: Owner, Group, Other, and R, W, X. The permissions assigned use the default file permissions set in the NFS Client for Windows (Figure 3). In addition, it assigned the UID and GID from the passwd file found on the Windows side.

Figure 13: File permissions – Windows file properties 7. Next, check the same file on the FSx for OpenZFS Linux mountpoint. The content of the file can be seen and confirmed that the permissions and ownership shown on the Windows side match the Linux side.

Figure 14: File permissions – Linux Furthermore, note that the user names do not have to necessarily match from Linux to Windows and vice versa. For example, in the following password file on the Windows side, I am mapping the Windows user jeff to a UID 1004. UID 1004 in my example is a Linux user called phill. Windows uses the UID for the mapping, not the Linux user name (phill in this case).

Figure 15: User name mapping – jeff to UID 1004 (phill) 3.2 Servers joined to an AD 1. For servers joined to an AD, you must select the identity mapping source in the NFS client to use your AD domain ("example.com" in my case).

Figure 16: NFS client on Windows Server – AD domain name 2. Next, you must update the user common name (CN) attributes in the AD Organizational Unit (OU). Use the ADSI editor to update the "gidNumber" and "uidNumber" on the Windows user to match the Linux UID and GID of the corresponding Linux user. Follow these steps: a. Type "adsi" in the Windows search bar and open the editor in the AD domain server.

Figure 17: ADSI editor (AD editor) b. Navigate to the Users subtree in the ADSI editor, selecting the desired user in the OU from your domain. Right-click on the desired user, and select properties.

Figure 18: ADSI editor – changing the CN of a user In my example, I update the CN attributes gidNumber and uidNumber of the Windows user brian to match the UID and GID of the Linux user brian with UID 1013 and GID 1005. c. First, I change the uidNumber attribute to 1013.

Figure 19: ADSI editor – changing the uidNumber attribute d. Next, I change the gidNumber to 1005.

Figure 20: ADSI editor – changing the gidNumber attribute 3. I repeat the process for every user I want to map from Linux to Windows, and then I restart the NFS client when done. I use the powershell commands nfsadmin client stop and nfsadmin client start for the NFS client restart. Make sure to unmount the file system on Windows before restarting the NFS client. 4. Next, I create a file on Linux logged as user brian and check its ownership, group membership, and permissions.

Figure 21: Linux file details 5. Finally, to confirm, I check the file on Windows logged as user brian from the example AD domain. I can confirm that the Linux ownership, group membership, and permissions match the ones from the user brian, on Linux, as expected.

Figure 22: validating the user mapping using AD 3.3 AUTH_NONE: anonymous authentication You can use anonymous authentication to map the user ID and group ID of a Linux user to your Windows client. Anonymous authentication is a generic way to provide read/write access to the file system. But note that it does not provide a strict mechanism to arbitrate write access to files. Furthermore, you cannot setup permissions on a user/group level. Therefore, it is not the recommended method. It should be considered only in situations in which security is not a concern. You must add the following registry keys and then reboot to setup anonymous authentication. New-ItemProperty HKLM:\SOFTWARE\Microsoft\ClientForNFS\CurrentVersion\Default -Name AnonymousUID -Value -PropertyType "DWord" New-ItemProperty HKLM:\SOFTWARE\Microsoft\ClientForNFS\CurrentVersion\Default -Name AnonymousGID -Value -PropertyType "DWord" After adding the keywords and rebooting, you can mount the file system on Windows using the anonymous option (-o anon) as follows. Note that the UID and GID assigned -2, which means the anonymous access is in use.

Figure 23: anonymous authentication Cleaning up If you followed along and would like to remove resources used in this solution to avoid incurring any unwanted future charges, you can unmount the file systems and delete the FSx for OpenZFS file system by following the FSx for OpenZFS user guide. Conclusion In this post, I highlighted the method of using the NFS client on Windows Server to access data on the FSx for OpenZFS file system, enabling data sharing across Linux and Windows clients. I explored the process of mounting the file system, securing it through authentication, and tuning for performance. The main takeaway is the potential to use the NFS protocol for shared file storage using FSx for OpenZFS across both Linux and Windows platforms. The benefits include cross-platform data access, heightened security, and effective data management, which enhance productivity and optimize resources. By adopting this strategy, you not only harness the power of FSx for OpenZFS but also effectively bridge the gap between different operating systems. Next, for a deeper understanding of the FSx for OpenZFS service, you can delve into the references section. References

FSx for OpenZFS – OpenZFS User Guide Amazon FSx for OpenZFS performance Nfsadmin Utility

Virgil Ennes Virgil Ennes is a Specialty Sr. Solutions Architect at AWS. Virgil enjoys helping customers take advantage of the agility, costs savings, innovation, and global reach that AWS provides. He is mostly focused on Storage, AI, Blockchain, Analytics, IoT, and Cloud Economics. In his spare time, Virgil enjoys spending time with his family and friends, and also watching his favorite football club (GALO).

AWS End User Messaging 10DLC Campaign Types and Quotas for SMS

The following 10DLC Campaigns, or, Use Cases outlined in Table 2 are currently supported by AWS End User Messaging. As part of the process to register for sending SMS to US based phone numbers you must select at least one Campaign that will be associated with the 10DLC number you procure. If you require more than one Use Case then you will need more than one 10DLC, or you can select the standard mixed use case which supports lower volumes of messages. Throughput is determined based on the company vetting score of the registered sender of the message and what is being sent, not on the amount of numbers associated with the Campaign. For a breakdown of vetting scores and quotas see below: Throughput and Volume Quotas for 10DLC Vetted Companies *Note that by default each number associated to a 10DLC campaign supports 1 MPS. In order to increase your numbers to match what your campaign qualifies for by carriers you will be required to submit a MPS increase request. Table 1

Use Case Type Vetting Score Message Parts per Second (MPS) (AT&T Limits) Maximum daily messages (T-Mobile & Sprint)

Standard 10DLC Campaign Use Cases Select the campaign that most closely aligns with your use case(s). Table 2

Account Notifications Status notifications about an account that the recipient is a part of or owns

Delivery Notifications Notifications about the status of a delivery of a product or service

Higher Education Messaging Messaging originating from colleges, universities, or other post-secondary education institutions

Low Volume Small throughput, any combination of use-cases. Examples include: test, demo accounts

Marketing Messaging Promotional content related to sales or other offers

Mixed Use Cases Covers multiple use cases such as Account Notifications and Delivery Notifications. Mixed campaigns have lower throughput than dedicated ones

Polling and Voting – Not for Political Use Delivering messages containing customer surveys or other voting related actions. Not for political use

Public Service Announcements (PSA) Messages intended to raise awareness of a particular topic

Security Alerts Notifications related to a compromised software or hardware system that requires recipients to take an action

Two Factor Authentication(2FA) or One-Time Password(OTP) Authentication, account verifications, or one-time passcode

Special Use Cases Currently End User Messaging supports only the following special use cases. These may require different registration processes and/or fees than the Standard Use Cases above

Charity / 501(c)(3) Nonprofit Communications from a registered company classified as a 501(c)(3). Does not include religious organizations

Tyler Holmes Tyler is a Senior Specialist Solutions Architect. He has a wealth of experience in the communications space as a consultant, an SA, a practitioner, and leader at all levels from Startup to Fortune 500. He has spent over 14 years in sales, marketing, and service operations, working for agencies, consulting firms, and brands, building teams and increasing revenue.

Build production-ready generative AI applications for enterprise search using Haystack pipelines and Amazon SageMaker JumpStart with LLMs

This blog post is co-written with Tuana Çelik from deepset. Enterprise search is a critical component of organizational efficiency through document digitization and knowledge management. Enterprise search covers storing documents such as digital files, indexing the documents for search, and providing relevant results based on user queries. With the advent of large language models (LLMs), we can implement conversational experiences in providing the results to users. However, we need to ensure that the LLMs limit the responses to company data, thereby mitigating model hallucinations. In this post, we showcase how to build an end-to-end generative AI application for enterprise search with Retrieval Augmented Generation (RAG) by using Haystack pipelines and the Falcon-40b-instruct model from Amazon SageMaker JumpStart and Amazon OpenSearch Service. The source code for the sample showcased in this post is available in the GitHub repository Solution overview To restrict the generative AI application responses to company data only, we need to use a technique called Retrieval Augmented Generation (RAG). An application using the RAG approach retrieves information most relevant to the user's request from the enterprise knowledge base or content, bundles it as context along with the user's request as a prompt, and then sends it to the LLM to get a response. LLMs have limitations around the maximum word count for the input prompts, so choosing the right passages among thousands or millions of documents in the enterprise has a direct impact on the LLM's accuracy. The RAG technique has become increasingly important in enterprise search. In this post, we show a workflow that takes advantage of SageMaker JumpStart to deploy a Falcon-40b-instruct model and uses Haystack to design and run a retrieval augmented question answering pipeline. The final retrieval augmentation workflow covers the following high-level steps:

The user query is used for a retriever component, which does a vector search, to retrieve the most relevant context from our database. This context is embedded into a prompt that is designed to instruct an LLM to generate an answer only from the provided context. The LLM generates a response to the original query by only considering the context embedded into the prompt it received.

SageMaker JumpStart SageMaker JumpStart serves as a model hub encapsulating a broad array of deep learning models for text, vision, audio, and embedding use cases. With over 500 models, its model hub comprises both public and proprietary models from AWS's partners such as AI21, Stability AI, Cohere, and LightOn. It also hosts foundation models solely developed by Amazon, such as AlexaTM. Some of the models offer capabilities for you to fine-tune them with your own data. SageMaker JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning (ML) with SageMaker. Haystack Haystack is an open-source framework by deepset that allows developers to orchestrate LLM applications made up of different components like models, vector DBs, file converters, and countless other modules. Haystack provides pipelines and Agents, two powerful structures for designing LLM applications for various use cases including search, question answering, and conversational AI. With a big focus on state-of-the art retrieval methods and solid evaluation metrics, it provides you with everything you need to ship a reliable, trustworthy application. You can serialize pipelines to YAML files, expose them via a REST API, and scale them flexibly with your workloads, making it easy to move your application from a prototype stage to production. Amazon OpenSearch OpenSearch Service is a fully managed service that makes it simple to deploy, scale, and operate OpenSearch in the AWS Cloud. OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, security monitoring, and observability applications, licensed under the Apache 2.0 license. In recent years, ML techniques have become increasingly popular to enhance search. Among them are the use of embedding models, a type of model that can encode a large body of data into an n-dimensional space where each entity is encoded into a vector, a data point in that space, and organized such that similar entities are closer together. A vector database provides efficient vector similarity search by providing specialized indexes like k-NN indexes. With the vector database capabilities of OpenSearch Service, you can implement semantic search, RAG with LLMs, recommendation engines, and search rich media. In this post, we use RAG to enable us to complement generative LLMs with an external knowledge base that is typically built using a vector database hydrated with vector-encoded knowledge articles. Application overview The following diagram depicts the structure of the final application.

In this application, we use the Haystack Indexing Pipeline to manage uploaded documents and index documents and the Haystack Query Pipeline to perform knowledge retrieval from indexed documents. The Haystack Indexing Pipeline includes the following high-level steps:

We use OpenSearch as our DocumentStore and a Haystack indexing pipeline to preprocess and index our files to OpenSearch. Haystack FileConverters and PreProcessor allow you to clean and prepare your raw files to be in a shape and format that your natural language processing (NLP) pipeline and language model of choice can deal with. The indexing pipeline we've used here also uses sentence-transformers/all-MiniLM-L12-v2 to create embeddings for each document, which we use for efficient retrieval. The Haystack Query Pipeline includes the following high-level steps:

We send a query to the RAG pipeline. An EmbeddingRetriever component acts as a filter that retrieves the most relevant top_k documents from our indexed documents in OpenSearch. We use our choice of embedding model to embed both the query and the documents (at indexing) to achieve this. The retrieved documents are embedded into our prompt to the Falcon-40b-instruct model. The LLM returns with a response that is based on the retrieved documents.

For model deployment, we use SageMaker JumpStart, which simplifies deploying models through a simple push of a button. Although we've used and tested Falcon-40b-instruct for this example, you may use any Hugging Face model available on SageMaker. The final solution is available on the haystack-sagemaker repository and uses the OpenSearch website and documentation (for OpenSearch 2.7) as our example data to perform retrieval augmented question answering on. Prerequisites The first thing to do before we can use any AWS services is to make sure we have signed up for and created an AWS account. Then you should create an administrative user and group. For instructions on both steps, refer to Set Up Amazon SageMaker Prerequisites. To be able to use the Haystack, you'll have to install the farm-haystack package with the required dependencies. To accomplish this, use the requirements.txt file in the GitHub repository by running pip install requirements.txt. Index documents to OpenSearch Haystack offers a number of connectors to databases, which are called DocumentStores. For this RAG workflow, we use the OpenSearchDocumentStore. The example repository includes an indexing pipeline and AWS CloudFormation template to set up an OpenSearchDocumentStore with documents crawled from the OpenSearch website and documentation pages. Often, to get an NLP application working for production use cases, we end up having to think about data preparation and cleaning. This is covered with Haystack indexing pipelines, which allows you to design your own data preparation steps, which ultimately write your documents to the database of your choice. An indexing pipeline may also include a step to create embeddings for your documents. This is highly important for the retrieval step. In our example, we use sentence-transformers/all-MiniLM-L12-v2 as our embedding model. This model is used to create embeddings for all our indexed documents, but also the user's query at query time. To index documents into the OpenSearchDocumentStore, we provide two options with detailed instructions in the README of the example repository. Here, we walk through the steps for indexing to an OpenSearch service deployed on AWS. Start an OpenSearch service Use the provided CloudFormation template to set up an OpenSearch service on AWS. By running the following command, you'll have an empty OpenSearch service. You can then either choose to index the example data we've provided or use your own data, which you can clean and preprocess using the Haystack Indexing Pipeline. Note that this creates an instance that is open to the internet, which is not recommended for production use.

```bash aws cloudformation create-stack --stack-name HaystackOpensearch --template-body file://cloudformation/opensearch-index.yaml --parameters ParameterKey=InstanceType,ParameterValue=r5.large.search ParameterKey=InstanceCount,ParameterValue=3 ParameterKey=OSPassword,ParameterValue=Password123! ```Bash

Allow approximately 30 minutes for the stack launch to complete. You can check its progress on the AWS CloudFormation console by navigating to the Stacks page and looking for the stack named HaystackOpensearch. Index documents into OpenSearch Now that we have a running OpenSearch service, we can use the OpenSearchDocumentStore class to connect to it and write our documents to it. To get the hostname for OpenSearch, run the following command:

```bash aws cloudformation describe-stacks --stack-name HaystackOpensearch --query "Stacks[0].Outputs[?OutputKey=='OpenSearchEndpoint'].OutputValue" --output text ```Bash

```bash export OPENSEARCH_HOST='your_opensearch_host' export OPENSEARCH_PORT=443 export OPENSEARCH_USERNAME=admin export OPENSEARCH_PASSWORD=Password123! ```Bash

Then, you can use the opensearch_indexing_pipeline.py script to preprocess and index the provided demo data. If you would like to use your own data, modify the indexing pipeline in opensearch_indexing_pipeline.py to include the FileConverter and PreProcessor setup steps you require. Implement the retrieval augmented question answering pipeline Now that we have indexed data in OpenSearch, we can perform question answering on these documents. For this RAG pipeline, we use the Falcon-40b-instruct model that we've deployed on SageMaker JumpStart. You also have the option of deploying the model programmatically from a Jupyter notebook. For instructions, refer to the GitHub repo.

Deploy your model on SageMaker JumpStart, and take note of the endpoint name.

This will start a command line utility that waits for a user's question. For example, let's ask "How can I install the OpenSearch cli?"

This result is achieved because we have defined our prompt in the Haystack PromptTemplate to be the following:

```python question_answering = PromptTemplate(prompt="Given the context please answer the question. If the answer is not contained within the context below, say 'I don't know'.\n" "Context: {join(documents)};\n Question: {query};\n Answer: ", output_parser=AnswerParser(reference_pattern=r"Document\[(\d+)\]")) ```Python

Further customizations You can make additional customizations to different elements in the solution, such as the following:

The data – We've provided the OpenSearch documentation and website data as example data. Remember to modify the opensearch_indexing_pipeline.py script to fit your needs if you chose to use your own data. The model – In this example, we've used the Falcon-40b-instruct model. You are free to deploy and use any other Hugging Face model on SageMaker. Note that changing a model will likely mean you should adapt your prompt to something it's designed to handle. The prompt – For this post, we created our own PromptTemplate that instructs the model to answer questions based on the provided context and answer "I don't know" if the context doesn't include relevant information. You may change this prompt to experiment with different prompts with Falcon-40b-instruct. You can also simply pull some of our prompts from the PromptHub. The embedding model – For the retrieval step, we use a lightweight embedding model: sentence-transformers/all-MiniLM-L12-v2. However, you may also change this to your needs. Remember to modify the expected embedding dimensions in your DocumentStore accordingly. The number of retrieved documents – You may also choose to play around with the number of documents you ask the EmbeddingRetriever to retrieve for each query. In our setup, this is set to top_k=5. You may experiment with changing this figure to see if providing more context improves the accuracy of your results.

Production readiness The proposed solution in this post can accelerate the time to value of the project development process. You can build a project that is easy to scale with the security and privacy environment on the AWS Cloud. For security and privacy, OpenSearch Service provides data protection with identity and access management and cross-service confused proxy prevention. You may employ fine-grained user access control so that the user can only access the data they are authorized to access. Additionally, SageMaker provides configurable security settings for access control, data protection, and logging and monitoring. You can protect your data at rest and in transit with AWS Key Management Service (AWS KMS) keys. You can also track the log of SageMaker model deployment or endpoint access using Amazon CloudWatch. For more information, refer to Monitor Amazon SageMaker with Amazon CloudWatch. For the high scalability on OpenSearch Service, you may adjust it by sizing your OpenSearch Service domains and employing operational best practices. You can also take advantage of auto scaling your SageMaker endpoint—you can automatically scale SageMaker models to adjust the endpoint both when the traffic is increased or the resources are not being used. Clean up To save costs, delete all the resources you deployed as part of this post. If you launched the CloudFormation stack, you can delete it via the AWS CloudFormation console. Similarly, you can delete any SageMaker endpoints you may have created via the SageMaker console. Conclusion In this post, we showcased how to build an end-to-end generative AI application for enterprise search with RAG by using Haystack pipelines and the Falcon-40b-instruct model from SageMaker JumpStart and OpenSearch Service. The RAG approach is critical in enterprise search because it ensures that the responses generated are in-domain and therefore mitigating hallucinations. By using Haystack pipelines, we are able to orchestrate LLM applications made up of different components like models and vector databases. SageMaker JumpStart provides us with a one-click solution for deploying LLMs, and we used OpenSearch Service as the vector database for our indexed data. You can start experimenting and building RAG proofs of concept for your enterprise generative AI applications, using the steps outlined in this post and the source code available in the GitHub repository.

About the Authors Tuana Celik is the Lead Developer Advocate at deepset, where she focuses on the open-source community for Haystack. She leads the developer relations function and regularly speaks at events about NLP and creates learning materials for the community. Roy Allela is a Senior AI/ML Specialist Solutions Architect at AWS based in Munich, Germany. Roy helps AWS customers—from small startups to large enterprises—train and deploy large language models efficiently on AWS. Roy is passionate about computational optimization problems and improving the performance of AI workloads. Mia Chang is an ML Specialist Solutions Architect for Amazon Web Services. She works with customers in EMEA and shares best practices for running AI/ML workloads on the cloud with her background in applied mathematics, computer science, and AI/ML. She focuses on NLP-specific workloads, and shares her experience as a conference speaker and a book author. In her free time, she enjoys hiking, board games, and brewing coffee. Inaam Syed is a Startup Solutions Architect at AWS, with a strong focus on assisting B2B and SaaS startups in scaling and achieving growth. He possesses a deep passion for serverless architectures and AI/ML. In his leisure time, Inaam enjoys quality moments with his family and indulges in his love for biking and badminton. David Tippett is the Senior Developer Advocate working on open-source OpenSearch at AWS. His work involves all areas of OpenSearch from search and relevance to observability and security analytics.

This post is the first in a series dedicated to the art and science of practical data mesh implementation (for an overview of data mesh, read the original whitepaper The data mesh shift). The series attempts to bridge the gap between the tenets of data mesh and its real-life implementation by deep-diving into the functional and non-functional capabilities essential to a working operating model, laying out the decisions that need to be made for each capability, and describing the key business and technical processes required to implement them. Taken together, the posts in this series lay out some possible operating models for data mesh within an organization.

Kudzu Kudzu—or kuzu (クズ)—is native to Japan and southeast China. First introduced to the southeastern United States in 1876 as a promising solution for erosion control, it now represents a cautionary tale about unintended consequences, as Kudzu's speed of growth outcompetes everything from native grasses to tree systems by growing over and shading them from the sunlight they need to photosynthesize—eventually leading to species extinction and loss of biodiversity. The story of Kudzu offers a powerful analogy to the dangers and consequences of implementing data mesh architectures without fully understanding or appreciating how they are intended to be used. When the "Kudzu" of unmanaged pseudo-data products (methods of sharing data that masquerade as data products while failing to fulfill the myriad obligations associated with them) has overwhelmed the local ecosystem of true data products, eradication is costly and prone to failure, and can represent significant wasted effort and resources, as well as lost time. Desert While Kudzu was taking over the south in the 1930s, desertification caused by extensive deforestation was overwhelming the Midwest, with large tracts of land becoming barren and residents forced to leave and find other places to make a living. In the same way, overly restrictive data governance practices that either prevent data products from taking root at all, or pare them back too aggressively (deforestation), can over time create "data deserts" that drive both the producers and consumers of data within an organization to look elsewhere for their data needs. At the same time, unstructured approaches to data mesh management that don't have a vision for what types of products should exist and how to ensure they are developed are at high risk of creating the same effect through simple neglect. This is due to a common misconception about data mesh as a data strategy, which is that it is effectively self-organizing—meaning that once presented with the opportunity, data owners within the organization will spring to the responsibilities and obligations associated with publishing high-quality data products. In reality, the work of a data producer is often thankless, and without clear incentive strategies, organizations may end up with data deserts that create more data governance issues as producers and consumers go elsewhere to seek out the data they need to perform work. Bonsai Bonsai (盆栽) is an art form originating from an ancient Chinese tradition called penjing (盆景), and later shaped by the minimalist teachings of Zen Buddhism into the practice we know and recognize today. The patient practice of Bonsai offers useful analogies to the concepts and processes required to avoid the chaos of Kudzu as well as the specter of organizational data deserts. Bonsai artists carefully observe the naturally occurring buds that are produced by the tree and encourage those that add to the overall aesthetics of the tree, while pruning those that don't work well with their neighbors. The same ideas apply equally well to data products within a data mesh—by encouraging the growth and adoption of those data products that add value to our data mesh, and continuously pruning those that do not, we maximize the value and sustainability of our data mesh implementations. In a similar vein, Bonsai artists must balance their vision for the shape of the tree with a respect for the natural characteristics and innate structure of the species they have chosen to work with—to ignore the biology of the tree would be disastrous to the longevity of the tree, as well as to the quality of the art itself. In the same way, organizations seeking to implement successful data mesh strategies must respect the nature and structure (legal, political, commercial, technology) of their organizations in their implementation. Of the key capabilities proposed for the implementation of a sustainable data mesh operating model, the one that is most relevant to the problems we've described—and explore later in this post—is data product portfolio management. Overview of data product portfolio management Data mesh architectures are, by their nature, ideal for implementation within federated organizations, with decentralized ownership of data and clear legal, regulatory, or commercial boundaries between entities or lines of business. The same organizational characteristics that make data mesh architectures valuable, however, also put them at risk of turning into one of the twin nightmares of Kudzu or data deserts. To define the shape and nature of an organizational data mesh, a number of key questions need to be answered, including but not limited to:

What are the key data domains within the organization? What are the key data products within these domains needed to solve current business problems? How do we iterate on this discovery process to add value while we are mapping our domains? Who are the consumers in our organization, and what logical, regulatory, physical, or commercial boundaries might separate them from producers and their data products? How do we encourage the development and maintenance of key data products in a decentralized organization? How do we monitor data products against their SLAs, and ensure alerting and escalation on failure so that the organization is protected from bad data? How do we enable those we see as being autonomous producers and consumers with the right skills, the right tools, and the right mindset to actually want to (and be able to) take more ownership of independently publishing data as a product and consuming it responsibly? What is the lifecycle of a data product? When do new data products get created, and who is allowed to create them? When are data products deprecated, and who is accountable for the consequences to their consumers? How do we define "risk" and "value" in the context of data products, and how can we measure this? Whose responsibility is it to justify the existence of a given data product?

To answer questions such as these and plan accordingly, organizations must implement data product portfolio management (DPPM). DPPM does not exist in a vacuum—by its nature, DPPM is closely related to and interdependent with enterprise architecture practices like business capability management and project portfolio management. DPPM itself may therefore also be considered, in part, an enterprise architecture practice. As an enterprise architecture practice, DPPM is responsible for its implementation, which should reside within a function whose remit is appropriately global and cross-functional. This may be within the CDO office for those organizations that have a CDO or equivalent central data function, or the enterprise architecture team in organizations that do not. Goals of DPPM

Protect value – DPPM protects the value of the organizational data strategy by developing, implementing, and enforcing frameworks to measure the contribution of data products to organizational goals in objective terms. Examples may include associated revenue, savings, or reductions in operational losses. Earlier in their lifecycle, data products may be measured by alternative metrics, including adoption (number of consumers) and level of activity (releases, interaction with consumers, and so on). In the pursuit of this goal, the DPPM capability is accountable for engaging with the business to continuously priorities where data as a product can add value and align delivery priority accordingly. Strategies for measuring value and prioritizing data products are explored later in this post. Manage risk – All data products introduce risk to the organization—risk of wasted money and effort through non-adoption, risk of operational loss associated with improper use, and risk of failure on the part of the data product to meet requirements on availability, completeness, or quality. These risks are exacerbated in the case of proliferation of low-quality or unsupervised data products. DPPM seeks to understand and measure these risks on an individual and aggregated basis. This is a particularly challenging goal because what constitutes risk associated with the existence of a particular data product is determined largely by its consumers and is likely to change over time (though like entropy, is only ever likely to increase). Guide evolution – The final goal of DPPM is to guide the evolution of the data product landscape to meet overarching organizational data goals, such as mutually exclusive or collectively exhaustive domains and data products, the identification and enablement of single-threaded ownership of product definitions, or the agile inclusion of new sources of data and creation of products to serve tactical or strategic business goals. Some principles for the management of data mesh evolution, and the evaluation of data products against organizational goals, are explored later in this post.

Challenges of DPPM In this section, we explore some of the challenges of DPPM, and the pragmatic ways some of these challenges could be addressed. Infancy Data mesh as a concept is still relatively new. As such, there is little standardization associated with practical operating models for building and managing data mesh architectures, and no access to fully fledged out-of-the-box reference operating models, frameworks, or tools to support the practice of DPPM. Some elements of DPPM are supported in disparate tools (for example, some data catalogs include basic community features that contribute to measuring value), but not in a holistic way. Over time, standardization of the processes associated with DPPM will likely occur as a side-effect of commoditization, driven by the popularity and adoption of new services that take on and automate more of the undifferentiated heavy lifting associated with mesh supervision. In the meantime, however, organizations adopting data mesh architectures are left largely to their own devices around how to operate them effectively. Resistance The purest expression of democracy is anarchy, and the more federated an organization is (itself a supporting factor in choosing data mesh architectures), the more resistance may be observed to any forms of centralized governance. This is a challenge for DPPM, because in some way it must come together in one place. Just as the Bonsai artist knows the vision for the entire tree, there must be a cohesive vision for and ability to guide the evolution of a data mesh, no matter how broadly federated and autonomous individual domains or data products might be. Balancing this with the need to respect the natural shape (and culture) of an organization, however, requires organizations that implement DPPM to think about how to do so in a way that doesn't conflict with the reality of the organization. This might mean, for example, that DPPM may need to happen at several layers—at minimum within data domains, possibly within lines of business, and then at an enterprise level through appropriate data committees, guilds, or other structures that bring stakeholders together. All of this complicates the processes and collaboration needed to perform DPPM effectively. Maturity Data mesh architectures, and therefore DPPM, presume relatively high levels of data maturity within an organization—a clear data strategy, understanding of data ownership and stewardship, principles and policies that govern the use of data, and a moderate-to-high level of education and training around data within the organization. A lack of data maturity within the organization, or a weak or immature enterprise architecture function, will face significant hurdles in the implementation of any data mesh architecture, let alone a strong and useful DPPM practice. In reality, however, data maturity is not uniform across organizations. Even in seemingly low-maturity organizations, there are often teams who are more mature and have a higher appetite to engage. By leaning into these teams and showing value through them first, then using them as evangelists, organizations can gain maturity while benefitting earlier from the advantages of data mesh strategies. The following sections explore the implementation of DPPM along the lines of people, process, and technology, as well as describing the key characteristics of data products—scope, value, risk, uniqueness, and fitness—and how they relate to data mesh practices. People To implement DPPM effectively, a wide variety of stakeholders in the organization may need to be involved in one capacity or another. The following table suggests some key roles, but it's up to an individual organization to determine how and if these map to their own roles and functions.

Senior Leadership A Chief Data Officer Ultimately accountable for organizational data strategy and implementation. Approves changes to DPPM principles and operating model. Acts as chair of, and appoints members to, the data council.

. R Data Council** Stakeholder body representing organizational governance around data strategy. Acts as steering body for the governance of DPPM as a practice (KPI monitoring, maturity assessments, auditing, and so on). Approves changes to guidelines and methodologies. Approves changes to data product portfolio (discussed later in this post). Approves and governs centrally funded and prioritized data product development activities.

Enterprise Architecture AR Head of Enterprise Architecture Responsible for development and enforcement of data strategy. Accountable and responsible for the design and implementation of DPPM as an organizational capability.

. R Domain Architect Responsible for the implementing screening, data product analysis, periodic evaluation, and optimal portfolio selection practices. Responsible for the development of methodologies and their selection criteria.

Legal & Compliance C Legal & Compliance Officer Consults on permissibility of data products with reference to local regulation. Consults on permissibility of data sharing with reference to local regulation or commercial agreements.

. C Data Privacy Officer Consults on permissibility of data use with reference to local data privacy law. Consults on permissibility of cross-entity or border data sharing with reference to data privacy law.

Information Security RC Information Security Officer Consults on maturity assessments (discussed later in this post) for information security-relevant data product capabilities. Approves changes to data product technology architecture. Approves changes to IAM procedures relating to data products.

Business Functions A Data Domain Owner Ultimately accountable for the appropriate use of domain data, as well as its quality and availability. Accountable for domain data products. Approves changes to the domain data model and domain data product portfolio.

c R Data Domain Steward Responsible for implementing data domain responsibilities, including operational (day-to-day) governance of domain data products. Approves use of domain data in new data products, and performs regular (such as yearly) attestation of data products using domain data.

. A Data Owner Ultimately accountable for the appropriate use of owned data (for example, CRM data), as well as its quality and availability.

. R Data Steward Responsible for implementing data responsibilities. Approves use of owned data in new data products, and performs regular (such as yearly) attestation of data products using owned data.

. AR Data Product Owner Accountable and responsible for the design, development, and delivery of data products against their stated SLOs. Contributes to data product analysis and portfolio adjustment practices for own data products.

** The data council typically consists of permanent representatives from each function (data domain owners), enterprise architecture, and the chief data officer or equivalent. Process The following diagram illustrates the strategic, tactical, and operational practices associated with DPPM. Some considerations for the implementation of these practices is explored in more detail in this post, though their specific interpretation and implementation is dependent on the individual organization.

Boundaries When reading this section, it's important to bear in mind the impact of boundaries—although strategy development may be established as a global practice, other practices within DPPM must respect relevant organizational boundaries (which may be physical, geographical, operational, legal, commercial, or regulatory in nature). In some cases, the existence of boundaries may require some or all tactical and operational practices to be duplicated within each associated boundary. For example, an insurance company with a property and casualty legal entity in North America and a life entity in Germany may need to implement DPPM separately within each entity. Strategy development This practice deals with answering questions associated with the overall data mesh strategy, including the following:

The overall scope (data domains, participating entities, and so on) of the data mesh The degree of freedom of participating entities in their definition and implementation of the data mesh (for example, a mesh of meshes vs. a single mesh) The distribution of responsibilities for activities and capabilities associated with the data mesh (degree of democratization) The definition and documentation of key performance indicators (KPIs) against which the data mesh should be governed (such as risk and value) The governance operating model (including this practice)

Organizational guidelines for operational processes around pre-screening and screening of data products Well-defined KPIs that guide methodology development and selection for practices like data product analysis, screening, and optimal portfolio selection Allocation of organizational resources (people, budget, time) to the implementation of tactical processes around methodology development, optimal portfolio selection, and portfolio adjustment

Key considerations In this section, we discuss some key considerations for strategy development. Data mesh structure

This diagram illustrates the analogous relationship between data products in a data mesh, and the structure of the mesh itself. The following considerations relate to screening, data product analysis, and optimal portfolio selection.

Trunk (core data products) – Core data products are those that are central to the organization's ability to function, and from which the majority of other data products are derived. These may be data products consumed in the implementation of key business activities, or associated with critical processes such as regulatory reporting and risk management. Organizational governance for these data products typically favors availability and data accuracy over agility.

Branch (cross-domain data products) – Cross-domain data products represent the most common cross-domain use cases for data (for example, joining customer data with product data). These data products may be widely used across business functions to support reporting and analytics, and—to a lesser extent—operational processes. Because these data products may consume a variety of sources, organizational governance may favor a balanced view on agility vs. reliability, accepting some degree of risk in return for being able to adapt to changes in data sources. Data product versioning can offer mitigation of risks associated with change. Leaf (everything else) – These are the myriad data products that may arise within a data mesh, either as permanent additions to support individual teams and use cases or as temporary data products to fill data gaps or support time-limited initiatives. Because the number of these data products may be high and risks are typically limited to a single process or a small part of the organization, organizational governance typically favors a light touch and may prefer to govern through guidelines and best practices, rather than through active participation in the data product lifecycle.

Data products vs. data definitions The following figure illustrates how data definitions are defined and inherited throughout the lineage of data products.

In a data mesh architecture, data products may inherit data from each other (one data product consumes another in its data pipeline) or independently publish data within (or related to) the same domain. For example, a customer data product may be inherited by a customer support data product, while another the customer journey data product may directly publish customer-relevant data from independent sources. When no standards are applied to how domain data attributes are used and published, data products even within the same data domain may lose interoperability because it becomes difficult or impossible to join them together for reporting or analytics purposes. To prevent this, it can be useful to distinguish between data products and data definitions. Typically, organizations will select a single-threaded owner (often a data owner or steward, or a domain data owner or steward) who is responsible for defining minimal data definitions for common and reusable data entities within data domains. For example, a data owner responsible for the sales and marketing data domain may identify a customer data product as a reusable data entity within the domain and publish a minimal data definition that all producers of customer-relevant data must incorporate within their data products, to ensure that all data products associated with customer data are interoperable. DPPM can assist in the identification and production of data definitions as part of its data product analysis activities, as well as enforce their incorporation as part of oversight of data product development. Service management thinking These considerations relate to data product analysis, periodic evaluation, and methodology selection. Data products are services provided to the organization or externally to customers and partners. As such, it may make sense to adapt a service management framework like ITIL, in combination with the ITIL Maturity Model, for use in evaluating the fitness of data products for their scope and audience, as well as in describing the roles, processes, and acceptable technologies that should form the operating model for any data product. At the operational level, the stakeholders required to implement each practice may change depending on the scope of the data product. For example, the release management practice for a core data product may require involvement of the data council, whereas the same practice for a team data product may only involve the team or functional head. To avoid creating decision-making bottlenecks, organizations should aim to minimize the number of stakeholders in each case and focus on single-threaded owners wherever possible. The following table proposes a subset of capabilities and how they might be applied to data products of different scopes. Suggested target maturity levels, between 1 and 5, are included for each scope. (1= Initial, 5= Optimizing)

Methodology development This practice deals with the development of concrete, objective frameworks, metrics, and processes for the measurement of data product value and risk. Because the driving factors behind risk and value are not necessarily the same between products, it may be necessary to develop several methodologies or variants thereof. Key deliverables include the following:

Well-defined frameworks for measuring risk and value of data products, as well as for determining the optimal portfolio of data products Operationally feasible, measurable metrics associated with value and risk

Key considerations A key consideration for assessing data products is that of consumer value or risk vs. uniqueness. The following diagram illustrates how value and risk of a data product are driven by its consumers.

Data products don't inherently present risk or add value, but rather indirectly pose—in an aggregated fashion—the risk and value created by their consumers. In a consumer-centric value and risk model, governance of consumers ensures that all data use meets the following requirements:

Is associated with a business case justifying the use of data (for example, new business, cost reduction through business process automation, and so on) Is regularly evaluated with reference to the risk associated with the use case (for example, regulatory reporting

The value and risk associated with the linked data products are then calculated as an aggregation. Where organizations already track use cases associated with data, either as part of data privacy governance or as a by-product of the access approval process, these existing systems and databases can be reused or extended. Conversely, where data products overlap with each other, their value to the organization is reduced accordingly, because redundancies between data products represent an inefficient use of resources and increase organizational complexity associated with data quality management. To ensure that the model is operationally feasible (see the key deliverables of methodology development), it may be sufficient to consider simple aggregations, rather than attempting to calculate value and risk attribution at a product or use case level. Optimal portfolio selection This practice deals with the determination of which combination of data products (existing, new, or potential) would best meet the organization's current and known future needs. This practice takes input from data product analysis and data product proposals, as well as other enterprise architecture practices (for example, business architecture), and considers trade-offs between data-debt and time-to-value, as well as other considerations such as redundancy between data products to determine the optimal mix of permanent and temporary data products at any given point in time. Because the number of data products in an organization may become significant over time, it may be useful to apply heuristics to the problem of optimal portfolio selection. For example, it may be sufficient to consider core and cross-domain data products (trunk and branches) during quarterly portfolio reviews, with other data products (leaves) audited on a yearly basis. Key deliverables include the following:

A target state definition for the data mesh, including all relevant data products An indication of organizational priorities for use by the portfolio adjustment practice

The following are key considerations regarding the data product half-life:

Long-term or strategic data products – These data products fill a long-term organizational need, are often associated with key source systems in various domains, and anchor the overall data strategy. Over time, as an organization's data mesh matures, long-term data products should form the bulk of the mesh. Time-bound data products – These data products fill a gap in data strategy and allow the organization to move on data opportunities until core data products can be updated. An example of this might be data products created and used in the context of mergers and acquisitions transactions and post-acquisition, to provide consistent data for reporting and business intelligence until mid-term and long-term application consolidation has taken place. Time-bound data products are considered as data-debt and should be managed accordingly. Purpose-driven data products – These data products serve a narrow, finite purpose. Purpose-driven data products may or may not be time-bound, but are characterized primarily by a strict set of consumers known in advance. Examples of this might include:

Data products developed to support system-of-record harmonization between lines of business (for example, deduplication of customer records between insurance lines of business using separate CRM systems Data products created explicitly for the monitoring of other data products (data quality, update frequency, and so on)

Portfolio adjustment This practice implements the feasibility analysis, planning and project management, as well as communication and organizational change management activities associated with changes to the optimal portfolio. As part of this practice, a gap analysis is conducted between the current and target data product portfolio, and a set of required actions and estimated time and effort prepared for review by the organization. During such a period, data products may be marked for development (new data products to fill a need), changes, consolidation (merging two or more data products into a single data product), or deprecation. Several iterations of optimal portfolio selection and portfolio adjustment may be required to find an appropriate balance between optimality and feasibility of implementation. Key deliverables include the following:

A gap analysis between the current and target data product portfolio, as well as proposed remediation activities High-level project plans and effort or budget assessments associated with required changes, for approval by relevant stakeholders (such as the data council)

Data product proposals This practice organizes the collection and prioritization of requests for new, or changes to existing, data products within the organization. Its implementation may be adapted from or managed by existing demand management processes within the organization. Key deliverables include a registry of demand against new or existing data products, including metadata on source systems, attributes, known use cases, proposed data product owners, and suggested organizational priority. Methodology selection This practice is associated with the identification and application of the most appropriate methodologies (such as value and risk) during data product analysis, screening, and optimal portfolio selection. The selection of an appropriate methodology for the type, maturity, and scope of a data product (or an entire portfolio) is a key element in avoiding either a "Kudzu" mesh or a "data desert." Key deliverables include reusable selection criteria for mapping methodologies to data products during data product analysis, screening, and optimal portfolio selection. Pre-screening This optional practice is primarily a mechanism to avoid unnecessary time and effort in the comparatively expensive practice of data product analysis by offering simple self-service applications of guidelines to the evaluation of data products. An example might include the automated approval of data products that fall under the classification of personal data products, requiring only attestation on the part of the requester that they will uphold the relevant portions of the guideline that governs such data products. Key deliverables include tools and checklists for the self-service evaluation of data products against guidelines and automated registration of approved data products. Data product analysis This practice incorporates guidelines, methodologies, as well as (where available) metadata relating to data products (performance against SLOs, service management metrics, degree of overlap with other data products) to establish an understanding of the value and risk associated with individual data products, as well as gaps between current and target capability maturities, and compliance with published product definitions and standards. Key deliverables include a summary of findings for a particular data product, including scores for relevant value, risk, and maturity metrics, as well as operational gaps requiring remediation and recommendations on next steps (repair, enhance, decommission, and so on). Screening This optional practice is a mechanism to reduce complexity in optimal portfolio selection by ensuring the early removal of data products from consideration that fail to meet value or risk targets, or have been identified as redundant to other data products already available in the organization. Key deliverables include a list of data products that should be slated for removal (direct-to-decommissioning). Data product development This practice is not performed directly under DPPM, but is managed in part by the portfolio adjustment practice, and may be governed by standards that are developed as part of DPPM. In the context of DPPM, this practice is primarily associated with ensuring that data products are developed according to the specifications agreed as part of portfolio adjustment. Key deliverables include project management and software or service development deliverables and artefacts. Data product decommissioning This practice manages the decommissioning of data products and the migration of affected consumers to new or other data products where relevant. Unmanaged decommissioning of data products, especially those with many downstream consumers, can threaten the stability of the entire data mesh, as well as have significant consequences to business functions. Key deliverables include a decommissioning plan, including stakeholder assessment and sign-off, timelines, migration plans for affected consumers, and back-out strategies. Periodic evaluation This practice manages the calendar and implementation of periodic reviews of the data mesh, both in its entirety as well as at the data product level, and is primarily an exercise in project management. Key deliverables include the following:

yearly review calendar, published and made available to all data product owners and affected stakeholders Project management deliverables and artefacts, including evidence of evaluations having been performed against each data product

Technology Although most practices within DPPM don't rely heavily on technology and automation, some key supporting applications and services are required to implement DPPM effectively:

Data catalog – Core to the delivery of DPPM is the organizational data catalog. Beyond providing transparency into what data products exist within an organization, a data catalog can provide key insights into data lineage between data products (key to the implementation of portfolio adjustment) and adoption of data products by the organization. The data catalog can also be used to capture and make available both the documented as well as the realized SLO for any given data product, and—through the use of a business glossary—assist in the identification of redundancy between data products. Service management – Service management solutions (such as ServiceNOW) used in the context of data product management offer important insights into the fitness of data products by capturing and tracking incidents, problems, requests, and other metrics against data products. Demand management – A demand management solution supports self-service implementation and automation of data product proposal and pre-screening activities, as well as prioritization activities associated with selection and development of data products.

Conclusion Although this post focused on implementing DPPM in the context of a data mesh, this capability—like data product thinking—is not exclusive to data mesh architectures. The practices outlined here can be practiced at any scale to ensure that the production and use of data within the organization is always in line with its current and future needs, that governance is implemented in a consistent way, and that the organization can have Bonsai, not Kudzu. For more information about data mesh and data management, refer to the following:

How to unlock the full value of data? Manage it like a product The data mesh shift

In upcoming posts, we will cover other aspects of data mesh operating models, including data mesh supervision and service management models for data product owners.

Maximilian Mayrhofer is a Principal Solutions Architect working in the AWS Financial Services EMEA Go-to-Market team. He has over 12 years experience in digital transformation within private banking and asset management. In his free time, he is an avid reader of science fiction and enjoys bouldering.

Faris Haddad is the Data & Insights Lead in the AABG Strategic Pursuits team. He helps enterprises successfully become data-driven.

Zero-shot and few-shot prompting for the BloomZ 176B foundation model with the simplified Amazon SageMaker JumpStart SDK

Amazon SageMaker JumpStart is a machine learning (ML) hub offering algorithms, models, and ML solutions. With SageMaker JumpStart, ML practitioners can choose from a growing list of best performing and publicly available foundation models (FMs) such as BLOOM, Llama 2, Falcon-40B, Stable Diffusion, OpenLLaMA, Flan-T5/UL2, or FMs from Cohere and LightOn. In this post and accompanying notebook, we demonstrate how to deploy the BloomZ 176B foundation model using the SageMaker Python simplified SDK in Amazon SageMaker JumpStart as an endpoint and use it for various natural language processing (NLP) tasks. You can also access the foundation models thru Amazon SageMaker Studio. The BloomZ 176B model, one of the largest publicly available models, is a state-of-the-art instruction-tuned model that can perform various in-context few-shot learning and zero-shot learning NLP tasks. Instruction tuning is a technique that involves fine-tuning a language model on a collection of NLP tasks using instructions. To learn more about instruction tuning, refer to Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart. Zero-shot learning in NLP allows a pre-trained LLM to generate responses to tasks that it hasn't been specifically trained for. In this technique, the model is provided with an input text and a prompt that describes the expected output from the model in natural language. Zero-shot learning is used in a variety of NLP tasks, such as the following:

Multilingual text and sentiment classification Multilingual question and answering Code generation Paragraph rephrasing Summarization Common sense reasoning and natural language inference Question answering Sentence and sentiment classification Imaginary article generation based on a title Summarizing a title based on an article

Few-shot learning involves training a model to perform new tasks by providing only a few examples. This is useful where limited labeled data is available for training. Few-show learning is used in a variety of tasks, including the following:

Text summarization Code generation Name entity recognition Question answering Grammar and spelling correction Product description and generalization Sentence and sentiment classification Chatbot and conversational AI Tweet generation Machine translation Intent classification

About Bloom The BigScience Large Open-science Open-access Multilingual (BLOOM) language model is a transformer-based large language model (LLM). BLOOM is an autoregressive LLM trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for by casting them as text generation tasks. With its 176 billion parameters, BLOOM is able to generate text in 46 natural languages and 13 programming languages. For almost all of them, such as Spanish, French, and Arabic, BLOOM is the first language model with over 100 billion parameters ever created. Researchers can download, run, and study BLOOM to investigate the performance and behavior of recently developed LLMs down to their deepest internal operations. Solution overview In this post, we show how to use the state-of-the-art instruction-tuned BloomZ 176B model from Hugging Face for text generation. You can use the BloomZ 176B model with few-shot learning and zero-shot learning for many NLP tasks, without fine-tuning the model. There is no need to train a new model because models like BloomZ 176B have a significant number of parameters such that they can easily adapt to many contexts without being retrained. The BloomZ 176B model has been trained with a large amount of data, making to applicable for many general-purpose tasks. The code for all the steps in this demo is available in the following notebook. Instruction tuning The size and complexity of LLMs have exploded in the last few years. LLMs have demonstrated remarkable capabilities in learning the semantics of natural language and producing human-like responses. Many recent LLMs are fine-tuned with a powerful technique called instruction tuning, which helps the model perform new tasks or generate responses to novel prompts without prompt-specific fine-tuning. An instruction-tuned model uses its understanding of related tasks or concepts to generate predictions to novel prompts. Because this technique doesn't involve updating model weights, it avoids the time-consuming and computationally expensive process required to fine-tune a model for a new, previously unseen task. Instruction tuning involves fine-tuning a language model on a collection of NLP tasks using instructions. In this technique, the model is trained to perform tasks by following textual instructions instead of specific datasets for each task. The model is fine-tuned with a set of input and output examples for each task, allowing the model to generalize to new tasks that it hasn't been explicitly trained on as long as prompts are provided for the tasks. Instruction tuning helps improve the accuracy and effectiveness of models and is helpful in situations where large datasets aren't available for specific tasks. Prompt engineering for zero-shot and few-shot NLP tasks on BLOOM models Prompt engineering deals with creating high-quality prompts to guide the model towards the desired responses. Prompts need to be designed based on the specific task and dataset being used. The goal here is to provide the model with necessary information to generate high-quality responses while minimizing noise. This could involve keywords, additional contexts, questions, and more. A well-designed prompt can make the model more creative and generalized so that it can easily adapt to new tasks. Prompts can also help incorporate domain knowledge on specific tasks and improve interpretability. Prompt engineering can greatly improve the performance of zero-shot and few-shot learning models. Creating high-quality prompts requires careful consideration of the task at hand, as well as a deep understanding of the model's strengths and limitations. In the following table, the BloomZ 176B model is used to generate responses for various zero-shot and few-shot NLP tasks. The first column shows the task, the second column contains the prompt provided to the model (where the template text is bold and the non-bold text is the example input) and the third column is the response from the model when queried against the prompt For all the following tasks, we set the hyperparameters as 600 to be the max_length, num_return_sequences, top_k to be 250, top_p to be 0.95, and no sampling.

Multilingual text or sentiment classification (Chinese to English) 一个传奇的开端,一个不灭的神话,这不仅仅是一部电影,而是作为一个走进新时代的标签,永远彪炳史册。Would you rate the previous review as positive, neutral or negative? positive

Multilingual question answering (English to Chinese) Explain to me in Traditional Chinese what is the backpropagation 梯度反向传播算法是一种机器学习算法,用于训练人工神经网络。该算法使用梯度下降来最小化损失函数。

Code generation Write a binary search tree with O(log(n)) computational complexity. {code_start} def binary_search(a, x): low = 0 high = len(a) – 1 while low x: high = mid – 1 else: return mid return -1′ … else: return mid return low' Output is truncated

Paragraph rephrase Amazon Web Services (AWS) has announced nine major new updates for its cloud-based machine learning platform, SageMaker. SageMaker aims to provide a machine learning service which can be used to build, train, and deploy ML models for virtually any use case. During this year's re:Invent conference, AWS made several announcements to further improve SageMaker's capabilities.\n\nHow would you rephrase that briefly using English?,Amazon Web Services (AWS) has announced nine major new updates for its cloud-based machine learning platform, SageMaker. SageMaker aims to provide a machine learning service which can be used to build, train, and deploy ML models for virtually any use case. During this year's re:Invent conference, AWS made several announcements to further improve SageMaker's capabilities. \nThe above sentence is very complicated. Please provide me a simplified synonymous version consisting of multiple sentences: Amazon SageMaker gets a major update Amazon Web Services (AWS) has announced nine major new updates for its cloud-based machine learning platform, SageMaker. SageMaker aims to provide a machine learning service that can be used to build, train and deploy ML models for virtually any use case

Summarization Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\n\n===\nWrite a summary of the previous text in English: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.

Common sense reasoning and natural language inference Given that The world cup has kicked off in Los Angeles, United States. Does it follow that The world cup takes place in United States. Yes or no?, The world cup has kicked off in Los Angeles, United States. Using only the above description and what you know about the world, is \"The world cup takes place in United States\" definitely correct? Yes or no?,

Question answering question: \what are the key features of new Kindle?\\n\nContext: \The newest and most innovative Kindle yet lets you take notes on millions of books and documents, write lists and journals, and more. For readers who have always wished they could write in their eBooks, Amazon's new Kindle lets them do just that. The Kindle Scribe is the first Kindle for reading and writing and allows users to supplement their books and documents with notes, lists, and more. Here's everything you need to know about the Kindle Scribe, including frequently asked questions. The Kindle Scribe makes it easy to read and write like you would on paper The Kindle Scribe features a 10.2-inch, glare-free screen (the largest of all Kindle devices), crisp 300 ppi resolution, and 35 LED front lights that automatically adjust to your environment. Further personalize your experience with the adjustable warm light, font sizes, line spacing, and more. It comes with your choice of the Basic Pen or the Premium Pen, which you use to write on the screen like you would on paper. They also attach magnetically to your Kindle and never need to be charged. The Premium Pen includes a dedicated eraser and a customizable shortcut button. The Kindle Scribe has the most storage options of all Kindle devices: choose from 8 GB, 16 GB, or 32 GB to suit your level of reading and writing. \\n\nAnswer take notes on millions of books and documents, write lists and journals, and more

Sentence and sentiment classification Review:\nThis moive is so great and once again dazzles and delights us\nIs this movie review sentence negative or positive?\nOPTIONS:\n-positive \n-negative Positive

Imaginary article generation based on title Title: \"University has new facility coming up\"\\nGiven the above title of an imaginary article, imagine the article.\\n""" The £1.2m project will see the creation of a new teaching and learning centre at the University of the Highlands and Islands (UHI) in Inverness. It will be built on the site of the former Inverness College of Commerce. UHI said the new facility would be used for teaching and learning, as well as student support and administration. The university said it would also be used for conferences and events. The new building will be built on the site of the former Inverness College of Commerce The university said the new building would be built on the site of the former Inverness College of Commerce, which closed in 2013. It said the new facility would be used for teaching and learning, as well as student support and administration. It would also be used for conferences and events. UHI said the new facility would be built on the site of the former Inverness College of Commerce The university said the new building would be built on the site of the former Inverness College of Commerce, which closed in 2013. It said the new facility would be used for teaching and learning, as well as student support and administration. … It would also be used for conferences and events. UHI said the new facility would be built on the site of the former Inverness…

Summarizing a title based on the article \'The newest and most innovative Kindle yet lets you take notes on millions of books and documents, write lists and journals, and more. For readers who have always wished they could write in their eBooks, Amazon's new Kindle lets them do just that. The Kindle Scribe is the first Kindle for reading and writing and allows users to supplement their books and documents with notes, lists, and more. Here's everything you need to know about the Kindle Scribe, including frequently asked questions. The Kindle Scribe makes it easy to read and write like you would on paper The Kindle Scribe features a 10.2-inch, glare-free screen (the largest of all Kindle devices), crisp 300 ppi resolution, and 35 LED front lights that automatically adjust to your environment. Further personalize your experience with the adjustable warm light, font sizes, line spacing, and more. It comes with your choice of the Basic Pen or the Premium Pen, which you use to write on the screen like you would on paper. They also attach magnetically to your Kindle and never need to be charged. The Premium Pen includes a dedicated eraser and a customizable shortcut button. The Kindle Scribe has the most storage options of all Kindle devices: choose from 8 GB, 16 GB, or 32 GB to suit your level of reading and writing. \n\n \\n\\nGive me a good title for the article above. Amazon's Kindle Scribe: Everything you need to know

Summarization [Original]: Amazon scientists, in collaboration with researchers from the University of Sheffield, are making a large-scale fact extraction and verification dataset publicly available for the first time. The dataset, comprising more than 185,000 evidence-backed claims, is being made available to hopefully catalyze research and development that addresses the problems of fact extraction and verification in software applications or cloud-based services that perform automatic information extraction. [Summary]: Amazon and University researchers make fact extraction and verification dataset publicly available. ### [Original]: Prime members in the U.S. can get even more delivered to their door with a Prime membership. Members can now enjoy one year of Grubhub+ valued at $9.99 per month for free—at no added cost to their Prime membership. To activate this deal, visit amazon.com/grubhub. This new offer includes unlimited, $0 food delivery fees on orders over $12 as well as exclusive perks for Grubhub+ members and rewards like free food and order discounts. Plus, diners can "eat good while doing good" by opting into Grubhub's Donate the Change program, a donation-matching initiative that raised more than $25 million in 2021 alone, benefiting more than 20 charitable organizations across the country. [Summary]: Prime members in the U.S. can enjoy one year of Grubhub+ for free, with no food-delivery fees on eligible orders. ### [Original]: Amazon scientists, in collaboration with researchers from the University of Sheffield, are making a large-scale fact extraction and verification dataset publicly available for the first time. The dataset, comprising more than 185,000 evidence-backed claims, is being made available to hopefully catalyze research and development that addresses the problems of fact extraction and verification in software applications or cloud-based services that perform automatic information extraction. [Summary]: [Summary]: Amazon and University researchers make fact extraction and verification dataset publicly available.

Code generation description: a orange button that says stop code: Stop ### description: a blue box that contains yellow circles with red borders code: ### description: a Headline saying Welcome to AI code: code: Welcome to AI '

Name entity recognition [Text]: Fred is a serial entrepreneur. Co-founder and CEO of Platform.sh, he previously co-founded Commerce Guys, a leading Drupal ecommerce provider. His mission is to guarantee that as we continue on an ambitious journey to profoundly transform how cloud computing is used and perceived, we keep our feet well on the ground continuing the rapid growth we have enjoyed up until now. [Name]: Fred [Position]: Co-founder and CEO [Company]: Platform.sh ### [Text]: Microsoft (the word being a portmanteau of "microcomputer software") was founded by Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. Steve Ballmer replaced Gates as CEO in 2000, and later envisioned a "devices and services" strategy. [Name]: Steve Ballmer [Position]: CEO [Company]: Microsoft ### [Text]: Franck Riboud was born on 7 November 1955 in Lyon. He is the son of Antoine Riboud, the previous CEO, who transformed the former European glassmaker BSN Group into a leading player in the food industry. He is the CEO at Danone. [Name]: Franck Riboud [Position]: CEO [Company]: Danone ### [Text]: David Melvin is an investment and financial services professional at CITIC CLSA with over 30 years' experience in investment banking and private equity. He is currently a Senior Adviser of CITIC CLSA. [Name]: Fred [Position]: Co-founder and CEO [Company]: Platform.sh

Question answering Context: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production. Question: When was NLP Cloud founded? Answer: 2021 ### Context: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then. Question: What did NLP Cloud develop? Answer: API ### Context: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice. Question: When can plans be stopped? Answer: Anytime ### Context: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended. Question: Which plan is recommended for GPT-J? Answer:""" Answer: GPU plan'

Grammar and spelling correction I love goin to the beach. Correction: I love going to the beach. ### Let me hav it! Correction: Let me have it! ### It have too many drawbacks. Correction: It has too many drawbacks. ### I do not wan to go Correction: Correction: I don't want to go

Product description and generalization Generate a product description out of keywords. Keywords: shoes, women, $59 Sentence: Beautiful shoes for women at the price of $59. ### Keywords: trousers, men, $69 Sentence: Modern trousers for men, for $69 only. ### Keywords: gloves, winter, $19 Sentence: Amazingly hot gloves for cold winters, at $19. ### Keywords: t-shirt, men, $39 Sentence: Sentence: Cool t-shirt for men, for $39 only.

Sentence and sentiment classification Message: When the spaceship landed on Mars, the whole humanity was excited Topic: space ### Message: I love playing tennis and golf. I'm practicing twice a week. Topic: sport ### Message: Managing a team of sales people is a tough but rewarding job. Topic: business ### Message: I am trying to cook chicken with tomatoes. Topic: Topic: food

Chatbot and conversational AI This is a discussion between a [human] and a [robot]. The [robot] is very nice and empathetic. [human]: Hello nice to meet you. [robot]: Nice to meet you too. ### [human]: How is it going today? [robot]: Not so bad, thank you! How about you? ### [human]: I am ok, but I am a bit sad… [robot]: Oh? Why that? ### [human]: I caught flu [robot]: [robot]: Oh no, I am sorry to hear that!

Tweet generation keyword: markets tweet: Take feedback from nature and markets, not from people ### keyword: children tweet: Maybe we die so we can come back as children. ### keyword: startups tweet: Startups should not worry about how to put out fires, they should worry about how to start them. ### keyword: nlp tweet: tweet: NLP is a tool, not a goal.

Machine translation Hugging Face a révolutionné le NLP. Translation: Hugging Face revolutionized NLP. ### Cela est incroyable! Translation: This is unbelievable! ### Désolé je ne peux pas. Translation: Sorry but I cannot. ### NLP Cloud permet de deployer le NLP en production facilement. Translation: Translation: NLP Cloud makes it easy to deploy NLP in production.

Intent classification I want to start coding tomorrow because it seems to be so fun! Intent: start coding ### Show me the last pictures you have please. Intent: show pictures ### Search all these files as fast as possible. Intent: search files ### Can you please teach me Chinese next week? Intent: Intent: teach me chinese

Access the BloomZ 176B instruction-tuned model in SageMaker SageMaker JumpStart provides two ways to get started using these instruction-tuned Bloom models: Amazon SageMaker Studio and the SageMaker SDK. The following sections illustrate what each of these options look like and how to access them. Access the model with the simplified SageMaker JumpStart SDK The simplified SageMaker JumpStart SDK facilitates training and deploying built-in SageMaker JumpStart models with a couple lines of code. This gives you access to the entire library of SageMaker JumpStart models, including the latest foundation models and image generation models, without having to supply any inputs besides the model ID. You can take advantage of the model-specific default values we provide to specify the configuration, such as the Docker image, ML instance type, model artifact location, and hyperparameters, among other fields. These attributes are only default values; you can override them and retain granular control over the AWS models you create. As a result of these changes, the effort to write Python workflows to deploy and train SageMaker JumpStart models has been reduced, enabling you to spend more time on the tasks that matter. This feature is available in all Regions where JumpStart is supported, and can be accessed with the SageMaker Python SDK version 2.154.0 or later. You can programmatically deploy an endpoint through the SageMaker SDK. You will need to specify the model ID of your desired model in the SageMaker model hub and the instance type used for deployment. The model URI, which contains the inference script, and the URI of the Docker container are obtained through the SageMaker SDK. These URIs are provided by SageMaker JumpStart and can be used to initialize a SageMaker model object for deployment. Deploy the model and query the endpoint This notebook requires ipywidgets. Install ipywidgets and then use the execution role associated with the current notebook as the AWS account role with SageMaker access. Choose the pre-trained model We choose the bloomz-176b-fp16 pre-trained model:

The notebook in the following sections uses BloomZ 176B as an example. For a complete list of SageMaker pre-trained models, refer to Built-in Algorithms with pre-trained Model Table. Retrieve artifacts and deploy an endpoint With SageMaker, we can perform inference on the pre-trained model without fine-tuning it first on a new dataset. We start by retrieving the deploy_image_uri, deploy_source_uri, and model_uri for the pre-trained model. To host the pre-trained model, we create an instance of sagemaker.model.Model and deploy it. This may take a few minutes. Now we can deploy the model using the simplified SageMaker JumpStart SDK with the following lines of code:

#if no instance for this model id is available, use a smaller id model = JumpStartModel(model_id=model_id)

# ml.p4de.24xlarge is used by default. You can add the kwarg # instance_type to change this setting. predictor = model.deploy()

We use SageMaker large model inference (LMI) containers to host the BloomZ 176B model. LMI is an AWS-built LLM software stack (container) that offers easy-to-use functions and performance gain on generative AI models. It's embedded with model parallelism, compilation, quantization, and other stacks to speed up inference. For details, refer to Deploy BLOOM-176B and OPT-30B on Amazon SageMaker with large model inference Deep Learning Containers and DeepSpeed. Note that deploying this model requires a p4de.24xlarge instance and the deployment usually takes about 1 hour. If you don't have quota for that instance, request a quota increate on the AWS Service Quotas console. Query the endpoint and parse the response using various parameters to control the generated text The input to the endpoint is any string of text formatted as JSON and encoded in utf-8 format. The output of the endpoint is a JSON file with generated text. In the following example, we provide some sample input text. You can input any text and the model predicts the next words in the sequence. Longer sequences of text can be generated by calling the model repeatedly. The following code shows how to invoke an endpoint with these arguments:

predictor = retrieve_default(model_id=model_id, model_version="*", endpoint_name=endpoint=name) response = predictor.predict("How to make a pasta?") print(response["generated_text"])Python

We get the following output: ['How to make a pasta? boil a pot of water and add salt. Add the pasta to the water and cook until al dente. Drain the pasta.'] Access the model in SageMaker Studio You can also access these models through the JumpStart landing page in Studio. This page lists available end-to-end ML solutions, pre-trained models, and example notebooks. At the time of publishing the post, BloomZ 176B is only available in the us-east-2 Region.

You can choose the BloomZ 176B model card to view the notebook.

You can then import the notebook to run the notebook further.

Clean up To avoid ongoing charges, delete the SageMaker inference endpoints. You can delete the endpoints via the SageMaker console or from the SageMaker Studio notebook using the following commands: predictor.delete_model() predictor.delete_endpoint() Conclusion In this post, we gave an overview of the benefits of zero-shot and few-shot learning and described how prompt engineering can improve the performance of instruction-tuned models. We also showed how to easily deploy an instruction-tuned BloomZ 176B model from SageMaker JumpStart and provided examples to demonstrate how you can perform different NLP tasks using the deployed BloomZ 176B model endpoint in SageMaker. We encourage you to deploy a BloomZ 176B model from SageMaker JumpStart and create your own prompts for NLP use cases. To learn more about SageMaker JumpStart, check out the following:

Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart Run text generation with Bloom and GPT models on Amazon SageMaker JumpStart Generate images from text with the stable diffusion model on Amazon SageMaker JumpStart Run image segmentation with Amazon SageMaker JumpStart Run text classification with Amazon SageMaker JumpStart using TensorFlow Hub and Hugging Face models Amazon SageMaker JumpStart models and algorithms now available via API Incremental training with Amazon SageMaker JumpStart Transfer learning for TensorFlow object detection models in Amazon SageMaker Transfer learning for TensorFlow text classification models in Amazon SageMaker Transfer learning for TensorFlow image classification models in Amazon SageMaker

About the Authors Rajakumar Sampathkumar is a Principal Technical Account Manager at AWS, providing customers guidance on business-technology alignment and supporting the reinvention of their cloud operation models and processes. He is passionate about cloud and machine learning. Raj is also a machine learning specialist and works with AWS customers to design, deploy, and manage their AWS workloads and architectures. Dr. Xin Huang is an Applied Scientist for Amazon SageMaker JumpStart and Amazon SageMaker built-in algorithms. He focuses on developing scalable machine learning algorithms. His research interests are in the area of natural language processing, explainable deep learning on tabular data, and robust analysis of non-parametric space-time clustering. He has published many papers in ACL, ICDM, KDD conferences, and Royal Statistical Society: Series A journal. Evan Kravitz is a software engineer at Amazon Web Services, working on SageMaker JumpStart. He enjoys cooking and going on runs in New York City.

Deploy multi-Region Amazon RDS for SQL Server using cross-Region read replicas with a disaster recovery blueprint – Part 1

Disaster recovery and high availability planning play a critical role in ensuring the resilience and continuity of business operations. When considering disaster recovery strategies on AWS, there are two primary options: in-Region disaster recovery and cross-Region disaster recovery. The choice between in-Region and cross-Region disaster recovery depends on various factors, including the criticality of the applications and data, regulatory requirements, geographic distribution of users, cost and complexity. To implement a multi-Region disaster recovery solution on AWS, you must first identify the critical components of your infrastructure and determine the required Recovery Point Objective (RPO) and Recovery Time Objective (RTO) for each component. The RPO is the maximum amount of data loss that is acceptable, and the RTO is the maximum amount of time that can elapse before the system must be restored. As of July 2023, AWS Cloud spans across 99 Availability Zones within 31 geographic Regions around the world. AWS has also announced plans for 15 more Availability Zones and 5 more AWS Regions in Canada, Israel, Malaysia, New Zealand, and Thailand. Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud. You can select from popular engines like MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server. In this series, we are focusing on the needs of critical applications that require the best possible availability and disaster recovery across different AWS Regions. In this first post, we guide you through the process of establishing a failover strategy for applications utilizing Amazon RDS for SQL Server, Amazon Route53 and Amazon Simple Storage Service(Amazon S3) across two AWS Regions. In the next post, we will demonstrate how to actually perform a failover using this plan. Solution overview This solution is deployed in two AWS Regions with active-passive strategy where primary (active) Region host the workload and serve the traffic. The secondary (passive) Region is used for the disaster recovery. Amazon Route53 public hosted zone with failover policy is created to route internet traffic between primary and secondary Regions. Amazon Route 53 private hosted zone CNAME records to store RDS for SQL Server endpoints. The application connects to the database using these records. The following diagram illustrates the key components of this architecture.

In our example, the primary AWS Region is referred as us-east-1 and secondary AWS Region as us-east-2. In addition, note the following:

The Amazon RDS for SQL Server Multi-AZ DB instance is deployed in the primary AWS Region and the cross-Region read replica is in the secondary Region. Amazon RDS for SQL Server configures cross-Region read replicas (asynchronous replication) using Distributed Availability Groups. Application stack deployed in both Regions and maintain the same release version. Amazon Route53 public hosted zone that serve internet traffic is configured with 'failover' routing policies. Amazon Route53 private hosted zone configured with 'simple' routing policies for application to RDS for SQL Server instance connectivity.

Standby takes over primary Amazon Route 53 public hosted zone failover policies are implemented using 'standby takes over primary' approach. In this approach, Route53 health check for primary Region verifies accessibility of a HTTP endpoint. That endpoint belongs to an Amazon S3 file that is stored in secondary Region. For example, if you have an Amazon S3 bucket called examplebucket123 in AWS Region us-east-2 and file name is initiate_failover.dr, the corresponding endpoint to access this S3 file would be: This health check is deemed healthy when the HTTP response received from the designated endpoint returns a status code of 4xx or 5xx (That means, the Route 53 health check agents NOT able to resolve that Amazon S3 file endpoint). Conversely, if the response returns a status code of 2xx or 3xx, the health check fails (That means, the Route 53 health check agents were able resolve the Amazon S3 file endpoint). This method is called Route53 Invert health check. This method not only allows you to manually control failover between primary and secondary AWS Regions but also prevent accidental failovers if the resource in the standby Region fails. In event of disaster recovery, upload this file on S3 bucket in secondary Region. This will intentionally cause the Route53 health check for primary Region to fail. Refer this document for list of HTTP status codes. For a comprehensive understanding of different approaches of designing reliable failover mechanisms, refer Creating Disaster Recovery Mechanisms Using Amazon Route 53. It is important to note that automatic failover of internet traffic to the secondary Region is not recommended. In situations like a brief network glitch in primary Region, automatic failover can result into longer downtime. Prerequisites To implement this solution, you need the following:

An application stack and Amazon RDS for SQL Server multi-AZ instance in the primary Region. An application stack and Amazon RDS for SQL Server read replica instance in secondary Region(cross-replication). For instructions, refer Use cross-Region read replicas with Amazon Relational Database Service for SQL Server. Any server level objects (for example logins, SQL agent jobs and more) that are created in the primary DB instance after the creation of the read replica are not automatically replicated, and you must create them manually in the read replica. Hence make sure these objects are in sync between primary and secondary Regions. Refer to the Amazon RDS documentation for more details.

Navigate to Amazon Relational Database service (RDS) console in primary Region. Capture endpoint of the primary RDS for SQL Server instance.

Similarly, navigate to Amazon RDS Console in secondary Region. Capture endpoint of RDS for SQL Server cross-Region read replica. Navigate to Amazon Route53 console. Create a new private hosted zone.

Associate the VPCs for both Regions to the private hosted zone. This is necessary because private hosted zone routes traffic within an Amazon VPC.

Once private hosted zone is created, add two CNAME records with simple routing policy. In our example, we created below CNAME records:

rdsprimarydb.rds_sqlserver_private_hosted_zone – Connects to RDS for SQL Server in primary Region. rdssecondarydb.rds_sqlserver_private_hosted_zone – Connects to RDS for SQL Server cross-Region read replica.

After adding both CNAME records, private hosted zone should look like following screenshot.

Update your application configuration in primary Region and use CNAME record rdsprimarydb.rds_sqlserver_private_hosted_zone as the database host name in database connection string. Similarly update application in secondary Region using CNAME record rdssecondarydb.rds_sqlserver_private_hosted_zone. This step ensure that applications are not directly using RDS endpoints. Hence during failover application changes would not require. The following python code is an example code connecting to RDS for SQL Server instance using private hosted zone CNAME record:

Create new Amazon S3 buckets in both primary and secondary Regions. These S3 buckets are dedicated to host disaster recovery file. To ensure the security and integrity of the health check process, it is crucial to restrict access to this bucket exclusively for authorized personnel. You may leave these buckets empty at this point when both primary and secondary Regions are healthy. Navigate to Amazon Route53 console and create Route53 public hosted zone to manage internet traffic for your public domain.

After creating public hosted zone, navigate to Amazon Route53 health checks console. Create two new Route53 health checks one for primary Region failover and another is for secondary Region failover. These health checks will be attached to primary and secondary Region failover records.

Enter health check name for primary Region. Then select Endpoint option for what to monitor. In monitor and endpoint section, select Domain name and protocol HTTPS. Under domain name, specify Amazon S3 bucket endpoint of secondary Region. For example: examplebucket123.s3.us-east-2.amazonaws.com Under Path, specify the disaster recovery file name. For example: initiate_failover.dr Specify failover threshold and request interval in advanced configuration section. Check option for 'Invert health check status' and hit next. Submit create health check.

Repeat the same steps to create health check record for secondary Region. For the primary Region health check, specify the S3 file endpoint from the secondary Region and vice versa. In the event that the primary Region becomes inaccessible, you will be able to modify the S3 file in the secondary Region and initiate the failover. After health checks are created for primary and secondary Regions, Amazon Route53 will initiate the health check and report the status.

Navigate to the Route53 public hosted zone created at step 13 and complete the following steps:

Create record with failover routing policy. Click on Define failover record. Associate this record with application target. For example, Route53 support Application load balancer, API gateway, VPC endpoint or S3 endpoints and more. In this example, application load balance target is selected. Choose primary Region. Choose the target load balancer. Specify failover record type as Primary. Choose health check record created for primary region. Disable evaluate target health option. Submit

Repeat the similar steps to create another record for secondary record type.

Once public hosted zone configuration complete, it should look like below screenshot:

This completes deployment of disaster recovery blueprint with Amazon Route53, Amazon S3 and Amazon RDS for SQL Server. At this point, you should be able to access your application through internet traffic.

Clean up In the next post of this series, we are going to use this blueprint and show you how to perform cross-Region failover. Hence if you are planning to continue, preserve all resources created under this deployment. To delete the resources created to implement this solution, complete the following steps:

Delete public and private hosted zone you created. Change the application configuration to its original state. Delete the Amazon S3 bucket you created.

Summary In this post, we provided guidance on how to implement a cross-Region disaster recovery blueprint. The 'standby takes over primary' approach in Amazon Route53 public hosted zone policies empowers organizations to maintain control over the failover process and manually initiate failover when the primary Region becomes inaccessible. In the next post of this series, we will break down the process of promoting RDS for SQL Server in the AWS secondary Region and performing a cross-Region failover using the blueprint we have deployed. If you have any comments or feedback, leave them in the comments section.

About the author Ravi Mathur is a Sr. Solutions Architect at AWS. He works with customers providing technical assistance and architectural guidance on various AWS services. He brings several years of experience in software engineering and architecture roles for various large-scale enterprises.

Deploy multi-Region Amazon RDS for SQL Server using cross-Region read replicas with a disaster recovery blueprint – Part 2

In our previous post, we deployed multi-Region disaster recovery blueprint using Amazon Route 53, Amazon Relational Database Service (Amazon RDS) for SQL Server and Amazon Simple Storage Service (Amazon S3). In this post we walk you through the process of promoting RDS for SQL Server in the AWS secondary Region and performing a cross-Region failover using the blueprint we have deployed. A quick recap from our previous post At high level we performed the following steps:

Verify Amazon RDS for SQL Server database running with cross Region read replica. Created Amazon Route 53 private hosted zone CNAME records to configure application connectivity with RDS instance. Configured Amazon Route53 public hosted zone to manage internet traffic between primary and secondary AWS Regions. Created separate Amazon S3 buckets in both the Regions to host disaster recovery file. Tested connectivity for internet traffic using public domain name.

The architecture of this solution is depicted in the following diagram. When both primary and secondary Regions are up and running, Amazon Route53 routing internet traffic to active Region (us-east-1 in this example). No internet traffic routed to passive Region (us-east-2 in this example).

Key Considerations for Cross-Region Failover: Initiating cross-Region failover requires a thorough consideration of various factors. Here are a few essential points to consider

One or more AWS services that your application relies on in the primary Region are not responding. The application tier deployed in the secondary Region is fully updated and prepared to assume the primary role. Amazon Route 53 Application Recovery Controller provides capability of multi-Region readiness checks. Refer to Readiness check in Amazon Route 53 Application Recovery Controller for more details. The only option to restore customer services is by switching live traffic to the secondary Region. Replica lag for RDS for SQL Server read replica in secondary Region is under acceptable range for the business (RPO). Initiating a cross-Region failover with non-zero replica lag can potentially lead to data loss. Refer Troubleshooting a SQL Server read replica problem to learn more.

The following diagram illustrates the architecture during disaster recovery event:

Cross-Region Failover Steps: Let's see how this solution works by following the order of events

Declare a disaster recovery event and disable writes on primary RDS for SQL Server instance. Promote the RDS for SQL Server cross-Region read replica to a stand-alone DB instance in secondary Region. After the successful promotion of the RDS for SQL Server read replica, conduct an end-to-end validation, often referred to as a smoke test. This validation process ensures that the application is functioning correctly and ready to accept external traffic in the secondary AWS Region. Create and upload disaster recovery file on Amazon S3 bucket deployed with the disaster recovery blueprint. Use the same file name specified in HTTP endpoint of Route53 public record health check. Uploading this file will fail the Route 53 health checks for the primary Region and the failover to the secondary Region will initiate. Once the failover to the secondary Region is complete, verify the health of the application stack and databases and ensure they are functioning properly and receiving external traffic. At this step, your application downtime is complete. Enable in-Region high availability again by modifying the RDS instance in secondary Region and enable multi-AZ option. This is also a prerequisite of creating cross-Region read replica on RDS for SQL Server instance. Once the AWS Region primary Region become available, recreate cross-Region disaster recovery solution by manually creating new cross-Region RDS for SQL Server read replica. Modify the Route53 private hosted zone rds_sqlserver_private_hosted_zone and update rdsprimarydb.rds_sqlserver_private_hosted_zone CNAME record to point to newly created read replica in primary Region.

Implement the failover Use the following steps to initiate cross Region failover:

Declare disaster recovery event and disable writes on RDS for SQL Server instance in primary Region. For example, if application is accessible, stop it to prevent accidental writes on current primary database while the DR failover in-progress. In secondary Region, verify RDS for SQL Server read replica lag using Replica Lag Amazon CloudWatch metric. You may also run this query on primary database to get information about replica lag for all read replicas. Proceed to the next steps only if cross-Region replica lag is zero or under acceptable range for the business (RPO). Initiating cross-Region failover with non-zero replica lag can potentially lead to data loss. Navigate to Amazon Route 53 console. Select public hosted zone and verify the routing policies.

Navigate to Amazon Route 53 health checks dashboard. Note down URL for the S3 file endpoint for primary Region health check record. At this stage, health checks for both the Regions must have healthy status.

Navigate to private hosted zone rds_sqlserver_private_hosted_zone and verify the CNAME entries. Application stack in secondary Region connected to RDS instance using CNAME record rdssecondarydb.rds_sqlserver_private_hosted_zone.

In secondary Region, promote the cross-Region RDS for SQL Server read replica

You may also automate this step by creating an AWS Lambda function. You may find referenced python code on this GitHub repo. For instructions on creating a function, see the AWS Lambda Developer Guide. Promoting the RDS for SQL Server instance doesn't change RDS endpoint URL. So, you don't have to update Route53 private hosted zone CNAME records. Your application should be able to connect with promoted RDS for SQL Server instance using rdssecondarydb.rds_sqlserver_private_hosted_zone record. Perform an internal smoke test or application health checks and ensures that the application is functioning correctly and ready to accept internet traffic in secondary Region. Once application stack in secondary Region is ready to take internet traffic, initiate the failover process. Refer step 6 to obtain recovery file URL. Upload this recovery file on Amazon S3 bucket in secondary Region. In our example, file name is initiate_failover.dr.

Uploading this file on Amazon S3 bucket causes Route53 health check for primary Region to fail (Remember we enabled invert health check status option in Route 53 health checks). Once primary Region is marked unhealthy, Route53 initiates the failover of internet traffic to secondary Region. Failover delays are depended on health check settings, request interval and failure threshold.

Once failover is complete, your application start receiving internet traffic in secondary AWS Region.

At this step, your applications are recovered from the downtime but running in single AWS availability zone. To setup in-Region high availability, modify the RDS instance in the secondary Region and enable multi-AZ for Amazon RDS for SQL Server. Once the AWS primary Region become available again, recreate cross-Region disaster recovery solution by manually creating new cross-Region RDS for SQL Server read replica. Update Route 53 private hosted zone rds_sqlserver_private_hosted_zone and edit CNAME record value rdsprimarydb.rds_sqlserver_private_hosted_zone with new cross-Region RDS read replica endpoint.

Clean up To delete the resources created to implement this solution, complete the following steps:

Summary In this post, we explore the failover process for an application deployed in a multi-Region setup, employing an active/passive strategy. Amazon Route 53 public hosted zone policies failover traffic between the primary and the secondary Regions. Amazon Route53 private hosted zones policies take care of routing traffic to the appropriate database endpoints. It gives a uniform common endpoint for applications to use and application configuration doesn't need to change during the failures. You may use AWS Lambda function to script the manual tasks required for RDS instance promotion. You can manually trigger the function or use events. Try out this solution in your AWS account and if you have any comments or questions, leave them in the comments section.

AWS Weekly Roundup – Amazon MWAA, EMR Studio, Generative AI, and More – August 14, 2023

While I enjoyed a few days off in California to get a dose of vitamin sea, a lot has happened in the AWS universe. Let's take a look together! Last Week's Launches Here are some launches that got my attention: Amazon MWAA now supports Apache Airflow version 2.6 – Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is a managed orchestration service for Apache Airflow that you can use to set up and operate end-to-end data pipelines in the cloud. Apache Airflow version 2.6 introduces important security updates and bug fixes that enhance the security and reliability of your workflows. If you're currently running Apache Airflow version 2.x, you can now seamlessly upgrade to version 2.6.3. Check out this AWS Big Data Blog post to learn more. Amazon EMR Studio adds support for AWS Lake Formation fine-grained access control – Amazon EMR Studio is a web-based integrated development environment (IDE) for fully managed Jupyter notebooks that run on Amazon EMR clusters. When you connect to EMR clusters from EMR Studio workspaces, you can now choose the AWS Identity and Access Management (IAM) role that you want to connect with. Apache Spark interactive notebooks will access only the data and resources permitted by policies attached to this runtime IAM role. When data is accessed from data lakes managed with AWS Lake Formation, you can enforce table and column-level access using policies attached to this runtime role. For more details, have a look at the Amazon EMR documentation. AWS Security Hub launches 12 new security controls – AWS Security Hub is a cloud security posture management (CSPM) service that performs security best practice checks, aggregates alerts, and enables automated remediation. With the newly released controls, Security Hub now supports three additional AWS services: Amazon Athena, Amazon DocumentDB (with MongoDB compatibility), and Amazon Neptune. Security Hub has also added an additional control against Amazon Relational Database Service (Amazon RDS). AWS Security Hub now offers 276 controls. You can find more information in the AWS Security Hub documentation. Additional AWS services available in the AWS Israel (Tel Aviv) Region – The AWS Israel (Tel Aviv) Region opened on August 1, 2023. This past week, AWS Service Catalog, Amazon SageMaker, Amazon EFS, and Amazon Kinesis Data Analytics were added to the list of available services in the Israel (Tel Aviv) Region. Check the AWS Regional Services List for the most up-to-date availability information. For a full list of AWS announcements, be sure to keep an eye on the What's New at AWS page. Other AWS News Here are some additional blog posts and news items that you might find interesting: AWS recognized as a Leader in 2023 Gartner Magic Quadrant for Contact Center as a Service with Amazon Connect – AWS was named a Leader for the first time since Amazon Connect, our flexible, AI-powered cloud contact center, was launched in 2017. Read the full story here. Generate creative advertising using generative AI – This AWS Machine Learning Blog post shows how to generate captivating and innovative advertisements at scale using generative AI. It discusses the technique of inpainting and how to seamlessly create image backgrounds, visually stunning and engaging content, and reducing unwanted image artifacts. AWS open-source news and updates – My colleague Ricardo writes this weekly open-source newsletter in which he highlights new open-source projects, tools, and demos from the AWS Community. Upcoming AWS Events Check your calendars and sign up for these AWS events: Build On Generative AI – Your favorite weekly Twitch show about all things generative AI is back for season 2 today! Every Monday, 9:00 US PT, my colleagues Emily and Darko look at new technical and scientific patterns on AWS, inviting guest speakers to demo their work and show us how they built something new to improve the state of generative AI. In today's episode, Emily and Darko discussed the latest models LlaMa-2 and Falcon, and explored them in retrieval-augmented generation design patterns. You can watch the video here. Check out show notes and the full list of episodes on community.aws. AWS NLP Conference 2023 – Join this in-person event on September 13–14 in London to hear about the latest trends, ground-breaking research, and innovative applications that leverage natural language processing (NLP) capabilities on AWS. This year, the conference will primarily focus on large language models (LLMs), as they form the backbone of many generative AI applications and use cases. Register here. AWS Global Summits – The 2023 AWS Summits season is almost coming to an end with the last two in-person events in Mexico City (August 30) and Johannesburg (September 26). AWS Community Days – Join a community-led conference run by AWS user group leaders in your region: West Africa (August 19), Taiwan (August 26), Aotearoa (September 6), Lebanon (September 9), and Munich (September 14). AWS re:Invent (November 27 – December 1) – Join us to hear the latest from AWS, learn from experts, and connect with the global cloud community. Registration is now open. You can browse all upcoming in-person and virtual events. That's all for this week. Check back next Monday for another Weekly Roundup! — Antje P.S. We're focused on improving our content to provide a better customer experience, and we need your feedback to do so. Take this quick survey to share insights on your experience with the AWS Blog. Note that this survey is hosted by an external company, so the link doesn't lead to our website. AWS handles your information as described in the AWS Privacy Notice.

Antje Barth Antje Barth is a Principal Developer Advocate for generative AI at AWS. She is co-author of the O'Reilly books Generative AI on AWS and Data Science on AWS. Antje frequently speaks at AI/ML conferences, events, and meetups around the world. She also co-founded the Düsseldorf chapter of Women in Big Data.

Data consolidation for analytical applications using logical replication for Amazon RDS Multi-AZ clusters

Amazon Relational Database Service (Amazon RDS) Multi-AZ deployments provide enhanced availability and durability for your RDS database instances. You can deploy highly available, durable PostgreSQL databases in three Availability Zones using Amazon RDS Multi-AZ DB cluster deployments with two readable standby DB instances. With a Multi-AZ DB cluster, applications gain automatic failovers in typically under 35 seconds, up to two times faster transaction commit latency compared to Amazon Multi-AZ DB instance deployments without compromising durability, and additional read capacity. In this post, we walk through a use case of data consolidation for analytical applications using logical replication in the context of RDS Multi-AZ DB clusters. We enable and utilize logical replication in an RDS for PostgreSQL Multi-AZ DB cluster and also demonstrate logical replication consumption recovery after a DB instance failover. AWS announced support for PostgreSQL logical replication with Amazon RDS Multi-AZ DB cluster deployments. Amazon RDS for PostgreSQL supports streaming data changes using PostgreSQL's logical replication. You can now set up logical replication to send data from your Amazon RDS for PostgreSQL Multi-AZ DB cluster to other data systems, or receive changes from other systems into your Amazon RDS for PostgreSQL Multi-AZ DB cluster. Logical replication is supported for RDS Multi-AZ DB clusters running Amazon RDS for PostgreSQL version 14.8-R2 and higher and 15.3-R2 and higher. Logical Replication Overview PostgreSQL offers two types of replication – physical replication and logical replication. Both methods serve the purpose of creating redundant copies of data to ensure high availability and fault tolerance. Physical replication is also known as streaming replication. In this method, the actual data files (segments) and changes made to them are replicated from the primary database server to one or more standby servers. The replication process occurs at the binary level, meaning the entire data blocks are copied, which makes it efficient for handling large databases with high write loads. However, even small changes to a large block will result in replicating the whole block. In the replication topology, the primary and standby servers must be of the same PostgreSQL major version for compatibility and cannot be used for partial replication or selective replication of specific tables or data subsets. Logical replication, on the other hand, works at a higher level of abstraction. It replicates changes by understanding the logical structure of the data, which allows for more flexibility in terms of what is replicated and how it is consumed on the standby server. Logical replication gives you fine-grained control over both data replication and security. You can use logical replication between different platforms (for example, Linux to Windows) and different major versions of PostgreSQL, providing more flexibility during upgrades. You can also handle schema changes, such as table and column modifications, without much difficulty. Additionally, you can replicate individual rows or even specific columns, making it more efficient when dealing with selective replication or partial replication scenarios. Logical replication operates on a publish and subscribe model, utilizing change records from the PostgreSQL write-ahead log (WAL). The source, known as the publisher, transmits these changes for the specified tables to one or more recipients (subscribers) replicating the changes and ensuring synchronization between the publisher and the subscriber. The publisher's set of changes is identified through a publication, and subscribers access these changes by establishing a subscription that defines the connection to the publisher's database and its publications. PostgreSQL uses a mechanism called a replication slot to track the progress of a subscription, such as what changes still need to be sent to a subscriber. In Amazon RDS for PostgreSQL, users can manually create a logical replication slot on the writer DB instance of a Multi-AZ DB cluster. This offers more control and flexibility, and is useful for different change data capture (CDC) systems or troubleshooting existing setups. It is worth noting that the CREATE PUBLICATION/SUBSCRIPTION commands automatically set up the replication slots by default, which cover most use cases and simplifies the process. Manually creating the replication slot requires selecting a decoding plugin, which determines the format used to stream changes. RDS Multi-AZ DB clusters support three plugins: pgoutput (what PostgreSQL uses for logical replication), wal2json, and test_decoding (which is used for troubleshooting and is not recommended for production use) giving you the flexibility to capture changes in different formats for your CDC architectures and use cases. Logical replication use cases There are several common use cases for using logical replication with PostgreSQL, including:

Real-time data streaming and event-driven architectures –Using logical replication allows each change in data to be captured as a discrete event. For instance, changes such as insert, update, or delete statements in your database can each generate a distinct event. These events contain valuable information such as the modified data, changed values, timestamps, and the type of operation performed. This makes logical replication useful for implementing event-driven architectures, building data pipelines, and synchronizing data with other applications or services. To process the received events, you can subscribe to the replication stream and use messaging systems like Amazon Kinesis Data Streams, Amazon Managed Streaming for Apache Kafka (Amazon MSK), or Amazon MQ. These messaging systems provide scalable and fault-tolerant event processing capabilities, allowing you to handle a large volume of events and distribute them to multiple consumers. Additionally, consider leveraging event-driven frameworks or systems that are optimized for efficient event processing. For instance, serverless computing services like AWS Lambda can automatically run your code in response to events, ensuring scalability and fault tolerance in your architecture. Data distribution – Logical replication enables you to distribute data across multiple PostgreSQL instances or even different database systems. This is useful for scenarios where you need to share specific tables or subsets of data with other applications or databases. By using logical replication for data distribution, you can achieve better scalability, improved performance, data sharing, and integration across multiple environments and systems. Logical replication provides flexibility in choosing what data to replicate and where to replicate it, enabling you to design and implement a distributed architecture that meets your specific requirements. Data integration and ETL – Logical replication can be used for data integration and extract, transform, load (ETL) processes. You can replicate selected tables or entire databases to downstream systems where data can be transformed, merged, or loaded into other systems. You could also use logical replication to consolidate multiple databases into a single one (for example, for Data Analytics).

Solution overview In this example solution, we walk through a demonstration of enabling and utilizing logical replication in an RDS for PostgreSQL Multi-AZ DB cluster. We discuss the creation of a logical replication slot, the configuration of a Kinesis data stream, support for logical replication slots post-failover, and how to consume the replication slot data using a Python script. The following diagram illustrates our solution architecture.

Prerequisites To implement logical replication for an RDS for PostgreSQL Multi-AZ DB cluster, you must have:

An AWS Account. The AWS Command Line Interface (AWS CLI). A client running the PostgreSQL psql client and PgBench tool with Python installed. RDS for PostgreSQL Multi-AZ cluster does not support AWS free tier therefore consider costs for Amazon RDS pricing and AWS Kinesis Data Streams pricing.

Create a custom parameter group and enable logical replication First, we create a DB cluster parameter group and enable logical replication and increase the number of logical replication workers.

# Create a DB cluster parameter group aws rds create-db-cluster-parameter-group \ --db-cluster-parameter-group-name demo-pg-15-etl \ --db-parameter-group-family postgres15 \ --description "Demo RDS Multi-AZ Cluster ETL using logical replication" \ --region us-west-2

# Modify the demo DB cluster parameter group to enable logical replication and set the maximum logical replication workers to a value higher than your number of consumers aws rds modify-db-cluster-parameter-group \ --db-cluster-parameter-group-name demo-pg-15-etl \ --parameters "ParameterName=max_logical_replication_workers,ParameterValue=10,ApplyMethod=pending-reboot" \ "ParameterName=rds.logical_replication,ParameterValue=1,ApplyMethod=pending-reboot" \ --region us-west-2Bash

For more information on how to perform this step using the console, see Modifying parameters in a DB cluster parameter group and Working with parameter groups for Multi-AZ DB clusters. Create an RDS for PostgreSQL Multi-AZ cluster and DB cluster parameter group Our next step is to create an RDS for PostgreSQL Multi-AZ cluster. To do this, we use the AWS Command Line interface (AWS CLI). We configure the new DB cluster to use the custom parameter group we created previously.

# Create a demo DB cluster aws rds create-db-cluster \ --db-cluster-identifier demo-multi-az-cluster-etl-source \ --db-cluster-parameter-group-name demo-pg-15-etl \ --engine postgres \ --engine-version 15.3 \ --db-subnet-group-name default \ --master-user-password $PGPASSWORD \ --master-username $PGUSER \ --db-cluster-instance-class db.m6gd.large \ --allocated-storage 100 \ --storage-type io1 \ --iops 1000 \ --region us-west-2Bash

For instructions on how to perform this step using the AWS Management Console, see Creating a Multi-AZ cluster Create a Kinesis data stream While the DB cluster is rebooting, we can create the Kinesis data stream, which will be the target for logical replication. Kinesis Data Streams allows you to ingest, process, and analyze real-time streaming data and can be used to stream data into other targets such as Amazon Redshift.

# Create Kinesis data stream aws kinesis create-stream \ --stream-name etl_demo \ --shard-count 1 \ --region us-west-2Bash

For instructions on how to perform this step using the console, see Creating a Stream via the AWS Management Console. Create test tables and data After we have configured the DB cluster, we need to create test data. In this example, we use pgbench to create dummy tables and data:

# Create some dummy tables and data using pgbench pgbench -is 10 \ -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASESQL

Create a role and replication slot To enable logical replication, we must create a database role for replication and grant the role the necessary permissions. After the role is created, we create a logical replication slot on our source DB cluster. For this example, we use the wal2json decoding output.

# Connect to the source DB instance and create a role for replicaton psql -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASE

postgres=> CREATE USER repluser; CREATE ROLE postgres=> GRANT rds_replication to repluser; GRANT ROLE postgres=> \password repluser Enter new password for user "repluser": Enter it again:

# Create logical replication slot on source DB cluster SELECT * FROM pg_create_logical_replication_slot('etl_slot_wal2json','wal2json');SQL

For more information, see Create user and Logical Decoding. Consume the replication stream Finally, we consume the logical replication stream using the following Python script. The script connects to the DB cluster endpoint, starts consuming the replication stream, and forwards each record to the Kinesis data stream. The script also prints the payload to the standard output. The script has a simple retry mechanism to support reconnecting to the DB cluster endpoint if there is an interruption. To run this script, you need a client with network access to the RDS for PostgreSQL Multi-AZ DB cluster with Python and the PostgreSQL client tools installed.

$ cat consume_slot.py import boto3 import json import time import pdb import psycopg2 from psycopg2.extras import LogicalReplicationConnection import os

my_slot_name = 'etl_slot_wal2json' my_stream_name = 'etl_demo' kinesis_client = boto3.client('kinesis', region_name='us-west-2')

host = os.getenv('PGHOST') user = os.getenv('PGUSER') port = os.getenv('PGPORT') password = os.getenv('PGPASSWORD') database = os.getenv('PGDATABASE')

def consume_replication_stream(): my_connection = psycopg2.connect( f"dbname={database} host={host} port={port} user={user} password={password}", connection_factory=LogicalReplicationConnection) cur = my_connection.cursor() cur.drop_replication_slot(my_slot_name) cur.create_replication_slot(my_slot_name, output_plugin='wal2json') cur.start_replication(slot_name=my_slot_name, options={'pretty-print': 1}, decode=True) print(f'Successfully connected to slot {my_stream_name}') cur.consume_stream(consume)

def main(): retries = 10 for i in range(retries): try: consume_replication_stream() break # Success! So we break out the loop except Exception as e: print(f"Error occurred: {e}. Retrying...") time.sleep(30) # Wait for 30 seconds before retrying if i == retries - 1: # If we've retried up to the maximum retries allowed, raise the exception to be handled by the next level raise

Start a DML workload To demonstrate that data is streaming, we need to run a DML workload. In this example, we use pgbench to run the built-in tpcb-like workload for 300 seconds (5 minutes):

pgbench -T300 \ -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASE \Bash

After the workload begins, you should see the stream payload being processed in the terminal standard output:

$ python3 consume_slot.py Successfully connected to slot etl_demo { "change": [ ] } { "change": [ { "kind": "update", "schema": "public", "table": "pgbench_accounts", "columnnames": ["aid", "bid", "abalance", "filler"], "columntypes": ["integer", "integer", "integer", "character(84)"], "columnvalues": [674192, 7, 2261, " "], "oldkeys": { "keynames": ["aid"], "keytypes": ["integer"], "keyvalues": [674192] } }Bash

You can also view the payload via the Kinesis Data Viewer:

For more information on how to perform this step using the console, see Using Data Viewer in the Kinesis Console. Demonstrate DB cluster failover Now that we have validated that the logical replication stream is being consumed successfully, we can introduce a failover of the DB cluster to demonstrate that the replication slot is copied to the read replica nodes in the DB cluster and that post-failover consumption of the stream can continue. Prior to failover, let's review the slot configuration:

slot_name | plugin | slot_type | datoid | database | temporary | active | active_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | wal_status | safe_wal_size | two_phase ---------------------------------------------+----------+-----------+--------+----------+-----------+--------+------------+--------+--------------+-------------+---------------------+------------+---------------+----------- rds_us_west_2_db_knjbk2urhjnkn636iixihmgksq | | physical | | | f | t | 25517 | 338176 | 7874 | 1/B4000000 | | reserved | | f etl_slot_wal2json | wal2json | logical | 5 | postgres | f | t | | | 308164 | 1/A8210E68 | 1/A8210EA0 | reserved | | f rds_us_west_2_db_by3onqaew4kwmrlfn3zjpuigbq | | physical | | | f | t | 28390 | 338172 | 7874 | 1/B4000000 | | reserved | | f (3 rows)SQL

While our sample workload is running and being consumed, we initiate a failover:

# Failover DB cluster aws rds failover-db-cluster \ --db-cluster-identifier demo-multi-az-cluster-etl-source \ --region us-west-2 Bash

For further information on how to perform this step using the console, see Failover process for Multi-AZ DB Clusters. When the failover is complete, we can validate that the logical replication stream reconnects and continues to consume the data:

{ "kind": "insert", "schema": "public", "table": "pgbench_history", "columnnames": ["tid", "bid", "aid", "delta", "mtime", "filler"], "columntypes": ["integer", "integer", "integer", "integer", "timestamp without time zone", "character(22)"], "columnvalues": [71, 9, 175688, 3147, "2023-07-04 08:20:06.966371", null] } ] } Error occurred: terminating connection due to administrator command . Retrying... Error occurred: connection to server at "demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com" (35.163.140.202), port 5432 failed: Connection timed out Is the server running on that host and accepting TCP/IP connections? . Retrying... Successfully connected to slot etl_demo { "change": [ { "kind": "update", "schema": "public", "table": "pgbench_accounts", "columnnames": ["aid", "bid", "abalance", "filler"], "columntypes": ["integer", "integer", "integer", "character(84)"], "columnvalues": [17309, 1, 2376, " "], "oldkeys": { "keynames": ["aid"], "keytypes": ["integer"], "keyvalues": [17309] } }Bash

We can also validate the replication slot is still available after the failover:

psql -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASE \ -c "SELECT * FROM pg_replication_slots;"

slot_name | plugin | slot_type | datoid | database | temporary | active | active_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | wal_status | safe_wal_size | two_phase ---------------------------------------------+----------+-----------+--------+----------+-----------+--------+------------+--------+--------------+-------------+---------------------+------------+---------------+----------- rds_us_west_2_db_knjbk2urhjnkn636iixihmgksq | | physical | | | f | t | 904 | 271194 | 7874 | 1/94000000 | | reserved | | f rds_us_west_2_db_7jhd2ofzzow66dzv2xpsxpanfq | | physical | | | f | t | 3468 | 271202 | 7874 | 1/94000000 | | reserved | | f etl_slot_wal2json | wal2json | logical | 5 | postgres | f | f | | | 269331 | 1/900ABF88 | 1/900ABFC0 | reserved | | f (3 rows)SQL

Cleanup Don't forget to delete any unnecessary resources and drop unused replication slots. To drop unused replication slots, connect to the publisher and run the following SQL command:

SELECT pg_drop_replication_slot(slot_name)FROM pg_replication_slots WHERE slot_name IN (SELECT slot_name FROM pg_replication_slots where active = 'f');SQL

For further information on the pg_drop_replication_slot function, see Replication Management Functions. You can use the following AWS CLI command to delete the RDS for PostgreSQL Multi-AZ DB cluster with an optional DB snapshot.

aws rds delete-db-cluster \ --db-cluster-identifier demo-multi-az-cluster-etl-source \ --final-db-snapshot-identifier demo-multi-az-cluster-etl-source \ --region us-west-2Bash

For further information on how to perform this step using the console, see Deleting a Multi-AZ DB cluster. You can use the following AWS CLI command to delete the RDS for PostgreSQL Multi-AZ DB cluster parameter group.

You can use the following AWS CLI command to delete the Amazon Kinesis Data Stream.

Best practices When enabling logical replication, there are a number of best practices you should consider:

Be aware of unsupported SQL commands – Logical replication is based on the logical changes made to the data. As a result, it doesn't support automatically replicating schema changes such as ALTER TABLE and some other DDL changes. For further information on limitations of PostgreSQL replication see the next section, Current limitations in PostgreSQL logical replication. Clean up unused replication slots – If a replication slot is no longer needed, you should drop the replication slot as soon as possible to prevent accumulation of WAL files. An unused replication slot can prevent vacuum from cleaning up dead tuples and resetting the XID age horizon. Monitor disk space usage – Logical replication slots retain WAL files until they are consumed by all subscribers. This can lead to significant disk space usage if a consumer falls behind or stops consuming changes. For information on monitoring Amazon RDS using Amazon CloudWatch see, Overview of Amazon RDS and Amazon CloudWatch. Monitor replication lag – It's important to monitor the logical replication lag. High replication lag can lead to delayed updates to the consumer and increased disk space usage on the source. Monitoring logical replication lag is the responsibility of the user, for further information on gathering replication status information see, pg_stat_replication. Monitor the number of logical replication consumers – As you increase the number of replication consumers, you need to ensure that the number of consumers doesn't exceed the configured value of max_logical_replication_workers.

Current limitations in PostgreSQL logical replication Bear in mind that logical replication has certain restrictions or limitations:

Schema changes – The database schema and DDL commands are not replicated. The initial schema can be copied by hand using pg_dump --schema-only. Subsequent schema changes would need to be kept in sync manually. As a best practice, schema changes should be committed first by the subscriber, then by the publisher. Sequence data – Though logical replication replicates sequence data in serial or identity columns, in the event of switchover or failover to the subscriber database, you must update the sequences to the latest values. Large objects – Large objects are not replicated. It's also easier to store a reference to a large object on some external storage like Amazon Simple Storage Service (Amazon S3) and replicate that reference rather than storing and replicating the object itself. Truncate – Replication of TRUNCATE commands is supported, but some care must be taken when truncating groups of tables connected by foreign keys. A workaround of TRUNCATE could be DELETE. To avoid accidental TRUNCATE operations, you can REVOKE TRUNCATE privileges from tables. Partitioned tables – Logical replication treats partitioned tables as regular tables. Logical replication doesn't work at the base table level, but at the child table level. If you're using partitions, the partition hierarchy must be the same on both sides of a logical replication setup. Foreign tables, views, and materialized views – Replication is only supported by tables, including partitioned tables. Attempts to replicate other types of relations, such as views, materialized views, or foreign tables, will result in an error.

There is ongoing work in the open-source PostgreSQL developer community to address these limitations. Conclusion In this post, we provided an overview of PostgreSQL logical replication, how to enable and use logical replication with RDS for PostgreSQL Multi-AZ DB clusters, how to consume logical replication to an Amazon Kinesis Data Stream in the context of data consolidation (ETL) use cases and best practices and limitations for PostgreSQL logical replication. RDS Multi-AZ cluster deployments offer enhanced availability and durability for production database workloads. The introduction of PostgreSQL logical replication with RDS Multi-AZ clusters enables granular replication and synchronization of individual tables, providing flexibility and efficiency. As we discussed, this feature supports various use cases, such as real-time data streaming, event-driven architectures, smooth database upgrades, data distribution, data integration and ETL processes, and more. Incorporating logical replication with RDS Multi-AZ clusters can bring significant benefits to your PostgreSQL database workloads. To enhance availability, durability, and data synchronization capabilities for your applications, it's time to explore logical replication with your RDS Multi-AZ cluster topology. We encourage you to learn more by building a solution using the sample implementation provided in this post and a dataset relevant to your business. If you have questions or suggestions, leave a comment.

About the Authors Sean Massey is a Database Engineer and a Subject Matter Expert in Amazon RDS, specializing in PostgreSQL. Sean's relentless dedication to customer success is reflected in his track record of resolving critical issues in Amazon Aurora and Amazon RDS for PostgreSQL database systems. Shayon Sanyal is a Principal Database Specialist Solutions Architect and specializes in the Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL open-source database engines. In his current role, he spends his time working with customers to help design scalable, secure, and robust cloud-native architectures. Vijay Karumajji is a Principal Database Solutions Architect with Amazon Web Services. He works with our customers to provide guidance and technical assistance on database projects, helping them improve the value of their solutions when using AWS.

Accelerate SaaS Delivery onto DoD Networks with Game Warden from Second Front Systems

Blog updated on 7/8/2024 By Dylan Sims, Director of Solutions Engineering – Second Front Systems By Chris Bryant, Content Producer – Second Front Systems By Zach Green, Solutions Architect – AWS

Software-as-a-service (SaaS) products have revolutionized private sector business operations in recent years, with 94% of enterprises using cloud services as of 2022, according to zippia.com. Despite the commercial sector's demonstrated success with SaaS, the United States Department of Defense (DoD) has been slow to adopt this new delivery model, pushing many companies to deliver on-premises and hybrid solutions in addition to its purely SaaS offerings. Despite the ease of use and favorable unit economics inherent to SaaS offerings, significant bureaucratic and cybersecurity barriers exist which prevent these products from reaching the DoD market. The core issues associated with delivering software to the U.S. military are centered around contracting pathways and a web of difficult to navigate cybersecurity requirements. Game Warden on AWS removes many of the barriers to entry for software companies seeking to meet any DoD Impact Level (IL) or classification. Game Warden To solve this, Second Front Systems built the Game Warden platform, a DoD-compliant DevSecOps Platform-as-a-Service (PaaS) that accelerates software delivery onto DoD networks while supporting modern DevOps practices and adhering to stringent cybersecurity controls. Game Warden is built on AWS GovCloud (US) and provides a pathway for containerized applications to receive a Certificate to Field (CTF) on the DoD's Non-Secure Internet Protocol Router Network (NIPRNet). Game Warden enables hosted applications to inherit an Authority to Operate (ATO) while running on the platform. Second Front Systems is an AWS Partner that helps organizations streamline software delivery with its fully managed and compliance DecSecOps platform Game Warden, which is available in AWS Marketplace. Behind the Scenes Game Warden leverages a suite of AWS services for development, compliance, operations, and monitoring. Second Front Systems collaboration with AWS has helped industry-leading software companies scale their business across DoD by demystifying the ATO process and abstracting compliance requirements through an inherited security model.

Figure 1 – Game Warden's shared responsibility model. Here are some products powered by Game Warden on top of AWS that are working on DoD contracts today:

Decision Lens: Decision Lens develops integrated planning software, modernizing how the government prioritizes, plans, and funds. Customers across the DoD, intelligence community, federal civilian agencies, and state and local governments have achieved sustained operational advantages through their long-term planning, continuous medium-term prioritization, and short-term funding execution. Collaboration.Ai: Collaboration.Ai builds products that enable intelligent collaboration and harness untapped networks with an emphasis on human connections. Its innovation management platform, CrowdVector, is the technology behind the well-known AFWERX Challenge, the U.S. Air Force Guardians and Airmen Innovation Network (GAIN), and the NASA Spark program. Systems Innovation Engineering (SIE): SIE built the Supplier Capabilities Analytics and Reporting (SCAR) application to help commercial and federal customers evaluate supply chain options, identify and reduce risk, and build resiliency through requirements trade-off and risk analysis.

Game Warden's inherited security model and automated tooling is reducing ATO timelines from months to weeks, and will soon unlock new production environments in AWS Secret and Top Secret regions.

Figure 2 – Comparison of Game Warden vs. traditional ATO timelines. Solution Overview Game Warden can be thought of as a set of interconnected systems that provide an accelerated pathway for deployment of containerized applications to DoD networks. The key systems include a DevSecOps pipeline, security and compliance architecture, and hosting platform. Let's dive into each system of Game Warden and how they integrate with AWS to provide a secure, DoD-compliant hosting environment and continuous Authorization to Operate (cATO) pipeline for modern SaaS applications. DevSecOps Pipeline Game Warden integrates with an organization's existing CI/CD pipelines—automating vulnerability and malware scanning, container hardening, and deployments to hosting environments. This security and release pipeline is also an accredited pathway for cATO. The Game Warden DevSecOps pipeline is a series of streamlined phases:

Applications are developed and built external to Game Warden or using Game Warden Builder. Application components are packaged into Cloud-Native Computing Foundation (CNCF) containers pushed to the Game Warden Registry. Game Warden's pipeline is automatically triggered with each image push. Images are scanned for malware and common vulnerabilities and exploits, hardened, and deployed to a development environment for functionality testing. Each container image is hardened with custom scripts that implement Secure Technical Implementation Guide (STIG) controls. After the image has met all of the DoD's cybersecurity requirements, it can be promoted to production environments at Impact Levels 2, 4, 5, 6, AWS Secret Region, or AWS Top Secret Region.

The Game Warden pipeline emulates the Risk Management Framework process for securing software, while supporting modern CI/CD and DevSecOps practices. Software developers can fully automate their application deployments into the Game Warden development environment, with new releases taking just a few seconds before they're ready for testing. When a new version of the application is ready for release, its security posture is reviewed and the release is promoted into an accredited production hosting environment typically within 24 hours. The following diagram displays the workflow of a containerized application going through the Game Warden CI/CD pipeline. For added coverage in the development phase, learn about the upcoming Game Warden Builder.

Figure 3 – Overview of the platform pipeline. Security and Compliance Architecture Using a combination of AWS services and platform features, Game Warden meets the compliance requirements at all DoD Impact Levels and classifications. The platform's inherited security model saves customers substantial time and money by removing the infrastructure and platform compliance burden. The DoD adheres to a select set of compliance frameworks that have overlapping requirements and security goals. Game Warden is adherent to these frameworks, allowing Second Front Systems to establish and maintain a continuous Authorization to Operate. Customers operating on Game Warden benefit from inherited compliance with these standards while their application workloads are running on the Game Warden platform. DoD adopted compliance frameworks include:

DoD CC SRG National Institute of Standards and Technology (NIST) 800-171, 800-53 Center of Internet Security (CIS) Benchmarks DoD DevSecOps Reference Architecture

To aid in understanding and mapping components to compliance frameworks, AWS offers the AWS Services in Scope by Compliance Program, a directory that lists different compliance frameworks and the particular AWS services that meet each standard. Game Warden uses many AWS services and relies on this repository to ensure the services employed across the platform meet stringent DoD requirements. Game Warden leverages popular AWS services like Amazon CloudWatch, AWS CloudTrail, and Amazon Virtual Private Cloud (VPC) to monitor network, system, and API activity on the platform. Amazon GuardDuty does real-time checks for cybersecurity anomalies and is used in tandem with CloudWatch, AWS Lambda, and Amazon Simple Notification Service (SNS) to automate responses and alerting.

Figure 4 – Game Warden's security architecture on AWS. Hosting Platform In addition to providing a rapid pathway for SaaS offerings to be positioned to achieve a cATO, Game Warden includes a fully managed hosting environment built on AWS that includes development, staging, and production environments and has a connection to NIPRnet for IL4 and IL5 deployments. Game Warden also provides a pathway to deploy software to classified networks through AWS' built-in cross domain solution, AWS Diode, and managed production environments in AWS Secret and Top Secret Regions for IL6 and Top Secret / Sensitive Compartmented Information (SCI) compliance. In Game Warden, application workloads run on Big Bang, a DoD-accredited DevSecOps platform on Kubernetes, and are used in tandem with Amazon Elastic Kubernetes Service (Amazon EKS) to simplify the setup and management of secure Kubernetes clusters. Game Warden supports both multi-tenant and single-tenant use cases, enabling customers to choose between individual clusters for their end users or a shared environment to reduce consumption costs and reduce deployment complexity. After Game Warden engineers deploy applications into production, site reliability engineers (SREs) manage Day 2 operations using AWS services like CloudWatch to monitor CPU and memory utilization for core services. Game Warden also includes an observability stack in each environment which is used by customers to view application and cluster logs, and metrics. All customers benefit from 24/7 helpdesk and incident response support as tenants on Game Warden's fully managed hosting platform. Second Front Systems assumes most platform and infrastructure administrative and site reliability responsibilities, enabling customers to put more resources towards building great products. Game Warden uses AWS Identity and Access Management (IAM) to grant service-level permissions to pods in each cluster, allowing customer applications to integrate directly with popular AWS services.

Figure 5 – Game Warden's platform architecture on AWS. Conclusion The combination of Game Warden and AWS makes delivering modern software onto U.S. Department of Defense (DoD) networks faster and easier than ever before. This opens up exciting opportunities for commercial companies to unlock access to the vast DoD IT market. To help your organization build and deploy DoD-compliant software, contact Second Front Systems. You can also learn more about Game Warden in AWS Marketplace. .

Second Front Systems – AWS Partner Spotlight Second Front Systems is an AWS Partner that helps organizations streamline software delivery with its fully managed and compliance DecSecOps platform Game Warden. Contact Partner | Partner Overview | AWS Marketplace

TAGS: Authority to Operate, AWS Partner Guest Post, AWS Partner References, AWS Partner Solutions Architects (SA), AWS Partner Success Stories, SaaS on AWS, Second Front Systems, Software as a Service

Cost considerations and common options for AWS Network Firewall log management

When you're designing a security strategy for your organization, firewalls provide the first line of defense against threats. Amazon Web Services (AWS) offers AWS Network Firewall, a stateful, managed network firewall that includes intrusion detection and prevention (IDP) for your Amazon Virtual Private Cloud (VPC). Logging plays a vital role in any firewall policy, as emphasized by the National Institute of Standards and Technology (NIST) Guidelines on Firewalls and Firewall Policy. Logging enables organizations to take proactive measures to help prevent and recover from failures, maintain proper firewall security configurations, and gather insights for effectively responding to security incidents. Determining the optimal logging approach for your organization should be approached on a case-by-case basis. It involves striking a balance between your security and compliance requirements and the costs associated with implementing solutions to meet those requirements. This blog post walks you through logging configuration best practices, discusses three common architectural patterns for Network Firewall logging, and provides guidelines for optimizing the cost of your logging solution. This information will help you make a more informed choice for your organization's use case. Stateless and stateful rules engines logging When discussing Network Firewall best practices, it's essential to understand the distinction between stateful and stateless rules. Note that stateless rules don't support firewall logging, which can make them difficult to work with in use cases that depend on logs. To verify that traffic is forwarded to the stateful inspection engine that generates logs, you can add a custom-defined stateless rule group that covers the traffic you need to monitor, or you can set a default action for stateless traffic to be forwarded to stateful rule groups in the firewall policy, as shown in the following figure.

Figure 1: Set up stateless default actions to forward to stateful rule groups

Alert logs and flow logs Network Firewall provides two types of logs:

Alert — Sends logs for traffic that matches a stateful rule whose action is set to Alert or Drop. Flow — Sends logs for network traffic that the stateless engine forwards to the stateful rules engine.

To grasp the use cases of alert and flow logs, let's begin by understanding what a flow is from the view of the firewall. For the network firewall, network flow is a one-way series of packets that share essential IP header information. It's important to note that the Network Firewall flow log differs from the VPC flow log, as it captures the network flow from the firewall's perspective and it is summarized in JSON format. For example, the following sequence shows how an HTTP request passes through the Network Firewall.

When you're using a stateful rule to block egress HTTP traffic, the TCP connection will be established initially. When an HTTP request comes in, it will be evaluated by the stateful rule. Depending on the rule's action, the firewall may send a TCP reset to the sender when a Reject action is configured, or it may drop the packets to block them if a Drop action is configured. In the case of a Drop action, shown in Figure 3, the Network Firewall decides not to forward the packets at the HTTP layer, and the closure of the connection is determined by the TCP timers on both the client and server sides.

In the given example, the Network Firewall generates a flow log that provides information like IP addresses, port numbers, protocols, timestamps, number of packets, and bytes of the traffic. However, it doesn't include details about the stateful inspection, such as whether the traffic was blocked or allowed. Figure 4 shows the inbound flow log.

The alert log entry complements the flow log by containing stateful inspection details. The entry includes information about whether the traffic was allowed or blocked and also provides the hostname associated with the traffic. This additional information enhances the understanding of network activities and security events, as shown in Figure 6.

In summary, flow logs provide stateless information and are valuable for identifying trends, like monitoring IP addresses that transmit the most data over time in your network. On the other hand, alert logs contain stateful inspection details, making them helpful for troubleshooting and threat hunting purposes. Keep in mind that flow logs can become excessive. When you're forwarding traffic to a stateful inspection engine, flow logs capture the network flows crossing your Network Firewall endpoints. Because log volume affects overall costs, it's essential to choose the log type that suits your use case and security needs. If you don't need flow logs for traffic flow trends, consider only enabling alert logs to help reduce expenses. Effective logging with alert rules When you write stateful rules using the Suricata format, set the alert rule to be evaluated before the pass rule to log allowed traffic. Be aware that:

You must enable strict rule evaluation order to allow the alert rule to be evaluated before the pass rule. Otherwise the order of evaluation by default is pass rules first, then drop, then alert. The engine stops processing rules when it finds a match. When you use pass rules, it's recommended to add a message to remind anyone looking at the policy that these rules do not generate messages. This will help when developing and troubleshooting your rules.

For example, the rules below will allow traffic to a target with a specific Server Name Indication (SNI) and log the traffic that was allowed. As you can see in the pass rule, it includes a message to remind the firewall policy maker that pass rules don't alert. The alert rule evaluated before the pass rule logs a message to tell the log viewer which rule allows the traffic. This way you can see allowed domains in the logs.

alert tls $HOME_NET any -> $EXTERNAL_NET any (tls.sni; content:"www.example.com"; nocase; startswith; endswith; msg:"Traffic allowed by rule 72912"; flow:to_server, established; sid:82912;) pass tls $HOME_NET any -> $EXTERNAL_NET any (tls.sni; content:"www.example.com"; nocase; startswith; endswith; msg:"Pass rules don't alert"; flow:to_server, established; sid:72912;)

This way you can see allowed domains in the alert logs.

Log destination considerations Network Firewall supports the following log destinations:

Amazon Simple Storage Service (Amazon S3) Amazon CloudWatch Logs Amazon Kinesis Data Firehose

You can select the destination that best fits your organization's processes. In the next sections, we review the most common pattern for each log destination and walk you through the cost considerations, assuming a scenario in which you generate 15 TB Network Firewall logs in us-east-1 Region per month. Amazon S3 Network Firewall is configured to inspect traffic and send logs to an S3 bucket in JSON format using Amazon CloudWatch vended logs, which are logs published by AWS services on behalf of the customer. Optionally, logs in the S3 bucket can then be queried using Amazon Athena for monitoring and analysis purposes. You can also create Amazon QuickSight dashboards with an Athena-based dataset to provide additional insight into traffic patterns and trends, as shown in Figure 8.

Figure 8: Architecture diagram showing AWS Network Firewall logs going to S3

Cost considerations Note that Network Firewall logging charges for the pattern above are the combined charges for CloudWatch Logs vended log delivery to the S3 buckets and for using Amazon S3. CloudWatch vended log pricing can influence overall costs significantly in this pattern, depending on the amount of logs generated by Network Firewall, so it's recommended that your team be aware of the charges described in Amazon CloudWatch Pricing – Amazon Web Services (AWS). From the CloudWatch pricing page, navigate to Paid Tier, choose the Logs tab, select your Region and then under Vended Logs, see the information for Delivery to S3. For Amazon S3, go to Amazon S3 Simple Storage Service Pricing – Amazon Web Services, choose the Storage & requests tab, and view the information for your Region in the Requests & data retrievals section. Costs will be dependent on storage tiers and usage patterns and the number of PUT requests to S3. In our example, 15 TB is converted and compressed to approximately 380 GB in the S3 bucket. The total monthly cost in the us-east-1 Region is approximately $3800. Long-term storage There are additional features in Amazon S3 to help you save on storage costs:

If after implementing Network Firewall logging, you determine that your log files are larger than 128 KB, you can take advantage of S3 Intelligent-Tiering, which provides automatic cost savings for files with unknown access patterns. There is additional cost savings potential in the S3 Intelligent-Tiering optional Archive Access and Deep Archive Access tiers, depending on your usage patterns. S3 Lifecycle management can help automatically move log files into cost-saving infrequent access storage tiers after a user-defined amount of time after creation. If logs need to be kept for compliance but are too small to take advantage of S3 Intelligent-Tiering, consider aggregating and placing them in the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive tiers as a way to save on storage costs. Metadata charges might apply and can influence costs depending on the number of objects being stored.

Analytics and reporting Athena and QuickSight can be used for analytics and reporting:

Athena can perform SQL queries directly against data in the S3 bucket where Network Firewall logs are stored. In the Athena query editor, a single query can be run to set up the table that points to the Network Firewall logging bucket. After data is available in Athena, you can use Athena as a data source for QuickSight dashboards. You can use QuickSight to visualize data from your Network Firewall logs, taking advantage of AWS serverless services. Please note that using Athena to scan firewall data in S3 might increase costs, as can the number of authors, users, reports, alerts, and SPICE data used in QuickSight.

Amazon CloudWatch Logs In this pattern, shown in Figure 9, Network Firewall is configured to send logs to Amazon CloudWatch as a destination. Once the logs are available in CloudWatch, CloudWatch Log Insights can be used to search, analyze, and visualize your logs to generate alerts, notifications, and alarms based on specific log query patterns.

Figure 9: Architecture diagram using CloudWatch for Network Firewall Logs

Cost considerations Configuring Network Firewall to send logs to CloudWatch incurs charges based on the number of metrics configured, metrics collection frequency, the number of API requests, and the log size. See Amazon CloudWatch Pricing for additional details. In our example of 15 TB logs, this pattern in the us-east-1 Region results in approximately $6900. CloudWatch dashboards offers a mechanism to create customized views of the metrics and alarms for your Network Firewall logs. These dashboards incur an additional charge of $3 per month for each dashboard. Contributor Insights and CloudWatch alarms are additional ways that you can monitor logs for a pre-defined query pattern and take necessary corrective actions if needed. Contributor Insights are charged per Contributor Insights rule. To learn more, go to the Amazon CloudWatch Pricing page, and under Paid Tier, choose the Contributor Insights tab. CloudWatch alarms are charged based on the number of metric alarms configured and the number of CloudWatch Insights queries analyzed. To learn more, navigate to the CloudWatch pricing page and navigate to the Metrics Insights tab. Long-term storage CloudWatch offers the flexibility to retain logs from 1 day up to 10 years. The default behavior is never expire, but you should consider your use case and costs before deciding on the optimal log retention period. For cost optimization, the recommendation is to move logs that need to be preserved long-term or for compliance from CloudWatch to Amazon S3. Additional cost optimization can be achieved through S3 tiering. To learn more, see Managing your storage lifecycle in the S3 User Guide. AWS Lambda with Amazon EventBridge, as shown in the following sample code, can be used to create an export task to send logs from CloudWatch to Amazon S3 based on an event rule, pattern matching rule, or scheduled time intervals for long-term storage and other use cases.

GROUP_NAME = "/AnfwDemo/Anfw/Alert" DESTINATION_BUCKET = "cwexportlogs-blog" PREFIX = "network-logs" NDAYS = 1 nDays = int(NDAYS)

currentTime = datetime.datetime.now() StartDate = currentTime - datetime.timedelta(days=nDays) EndDate = currentTime - datetime.timedelta(days=nDays - 1)

fromDate = int(StartDate.timestamp() * 1000) toDate = int(EndDate.timestamp() * 1000)

def lambda_handler(event, context): client = boto3.client('logs') response = client.create_export_task( logGroupName=GROUP_NAME, fromTime=fromDate, to=toDate, destination=DESTINATION_BUCKET, destinationPrefix=BUCKET_PREFIX ) print(response)Plain text

Figure 10 shows how EventBridge is configured to trigger the Lambda function periodically.

Figure 10: EventBridge scheduler for daily export of CloudWatch logs

Analytics and reporting CloudWatch Insights offers a rich query language that you can use to perform complex searches and aggregations on your Network Firewall log data stored in log groups as shown in Figure 11. The query results can be exported to CloudWatch dashboard for visualization and operational decision making. This will help you quickly identify patterns, anomalies, and trends in the log data to create the alarms for proactive monitoring and corrective actions.

Figure 11: Network Firewall logs ingested into CloudWatch and analyzed through CloudWatch Logs Insights

Amazon Kinesis Data Firehose For this destination option, Network Firewall sends logs to Amazon Kinesis Data Firehose. From there, you can choose the destination for your logs, including Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and an HTTP endpoint that's owned by you or your third-party service providers. The most common approach for this option is to deliver logs to OpenSearch, where you can index log data, visualize, and analyze using dashboards as shown in Figure 12. In the blog post How to analyze AWS Network Firewall logs using Amazon OpenSearch Service, you learn how to build network analytics and visualizations using OpenSearch in detail. Here, we discuss only some cost considerations of using this pattern.

Figure 12: Architecture diagram showing AWS Network Firewall logs going to OpenSearch

Cost considerations The charge when using Kinesis Data Firehose as a log destination is for CloudWatch Logs vended log delivery. Ingestion pricing is tiered and billed per GB ingested in 5 KB increments. See Amazon Kinesis Data Firehose Pricing under Vended Logs as source. There are no additional Kinesis Data Firehose charges for delivery unless optional features are used. For 15 TB of log data, the cost of CloudWatch delivery and Kinesis Data Firehose ingestion is approximately $5400 monthly in the us-east-1 Region. The cost for Amazon OpenSearch Service is based on three dimensions:

Instance hours, which are the number of hours that an instance is available to you for use The amount of storage you request The amount of data transferred in and out of OpenSearch Service

Storage pricing depends on the storage tier and type of instance that you choose. See pricing examples of using OpenSearch Service. When creating your OpenSearch domain, see Sizing Amazon OpenSearch Service domains to help you right-size your OpenSearch domain. Other cost optimization best practices include choosing the right storage tier and using AWS Graviton2 instances to improve performance. For instance, allocating approximately 15 TB of UltraWarm storage in the us-east-1 Region will result in a monthly cost of $4700. Keep in mind that in addition to storage costs, you should also account for compute instances and hot storage. In short, the estimated total cost for log ingestion and storage in the us-east-1 Region for this pattern is at least $10,100. Leveraging OpenSearch will enable you to promptly investigate, detect, analyze, and respond to security threats. Summary The following table shows a summary of the expenses and advantages of each solution. Since storing logs is a fundamental aspect of log management, we use the monthly cost of using Amazon S3 as the log delivery destination as our baseline when making these comparisons.

Pattern Log delivery and storage cost as a multiple of the baseline cost Functionalities Dependencies

Amazon S3, Athena, QuickSight 1 The most economical option for log analysis. The solution requires security engineers to have a good analytics skillset. Familiarity with Athena query and query running time will impact the incident response time and the cost.

Amazon CloudWatch 1.8 Log analysis, dashboards, and reporting can be implemented from the CloudWatch console. No additional service is needed. The solution requires security engineers to be comfortable with CloudWatch Logs Insights query syntax. The CloudWatch Logs Insights query will impact the incident response time and the cost.

Amazon Kinesis Data Firehose, OpenSearch 2.7+ Investigate, detect, analyze, and respond to security threats quickly with OpenSearch. The solution requires you to invest in managing the OpenSearch cluster.

You have the flexibility to select distinct solutions for flow logs and alert logs based on your requirements. For flow logs, opting for Amazon S3 as the destination offers a cost-effective approach. On the other hand, for alert logs, using the Kinesis Data Firehose and OpenSearch solution allows for quick incident response. Minimizing the time required to address ongoing security challenges can translate to reduced business risk at different costs. Conclusion This blog post has explored various patterns for Network Firewall log management, highlighting the cost considerations associated with each approach. While cost is a crucial factor in designing an efficient log management solution, it's important to consider other factors such as real-time requirements, solution complexity, and ownership. Ultimately, the key is to adopt a log management pattern that aligns with your operational needs and budgetary constraints. Network security is an iterative practice, and by optimizing your log management strategy, you can enhance your overall security posture while effectively managing costs. For more information about working with Network Firewall, see What is AWS Network Firewall?

Sharon Li Sharon is an Enterprise Solutions Architect at Amazon Web Services based in Boston, with a passion for designing and building secure workloads on AWS. Prior to her current role at AWS, Sharon worked as a software development engineer at Amazon, where she played a key role in bringing security into the development process.

Larry Tewksbury Larry is an AWS Technical Account Manager based in New Hampshire. He works with enterprise customers in the Northeast to understand, scale, and optimize their cloud operations. Outside of work, he enjoys spending time with his family, hiking, and tech-based hobbies.

Shashidhar Makkapati Shashidhar is an Enterprise Solutions Architect at Amazon Web Services, based in Charlotte, NC. With over two decades of experience as an enterprise architect, he has a keen focus on cloud adoption and digital transformation in the financial services industry. Shashidhar supports enterprise customers in the US Northeast. In his free time, he enjoys reading, traveling, and spending time with his family.

Implementing automatic drift detection in CDK Pipelines using Amazon EventBridge

The AWS Cloud Development Kit (AWS CDK) is a popular open source toolkit that allows developers to create their cloud infrastructure using high level programming languages. AWS CDK comes bundled with a construct called CDK Pipelines that makes it easy to set up continuous integration, delivery, and deployment with AWS CodePipeline. The CDK Pipelines construct does all the heavy lifting, such as setting up appropriate AWS IAM roles for deployment across regions and accounts, Amazon Simple Storage Service (Amazon S3) buckets to store build artifacts, and an AWS CodeBuild project to build, test, and deploy the app. The pipeline deploys a given CDK application as one or more AWS CloudFormation stacks. With CloudFormation stacks, there is the possibility that someone can manually change the configuration of stack resources outside the purview of CloudFormation and the pipeline that deploys the stack. This causes the deployed resources to be inconsistent with the intent in the application, which is referred to as "drift", a situation that can make the application's behavior unpredictable. For example, when troubleshooting an application, if the application has drifted in production, it is difficult to reproduce the same behavior in a development environment. In other cases, it may introduce security vulnerabilities in the application. For example, an AWS EC2 SecurityGroup that was originally deployed to allow ingress traffic from a specific IP address might potentially be opened up to allow traffic from all IP addresses. CloudFormation offers a drift detection feature for stacks and stack resources to detect configuration changes that are made outside of CloudFormation. The stack/resource is considered as drifted if its configuration does not match the expected configuration defined in the CloudFormation template and by extension the CDK code that synthesized it. In this blog post you will see how CloudFormation drift detection can be integrated as a pre-deployment validation step in CDK Pipelines using an event driven approach. Services and frameworks used in the post include CloudFormation, CodeBuild, Amazon EventBridge, AWS Lambda, Amazon DynamoDB, S3, and AWS CDK. Solution overview Amazon EventBridge is a serverless AWS service that offers an agile mechanism for the developers to spin up loosely coupled, event driven applications at scale. EventBridge supports routing of events between services via an event bus. EventBridge out of the box supports a default event bus for each account which receives events from AWS services. Last year, CloudFormation added a new feature that enables event notifications for changes made to CloudFormation-based stacks and resources. These notifications are accessible through Amazon EventBridge, allowing users to monitor and react to changes in their CloudFormation infrastructure using event-driven workflows. Our solution leverages the drift detection events that are now supported by EventBridge. The following architecture diagram depicts the flow of events involved in successfully performing drift detection in CDK Pipelines.

The user starts the pipeline by checking code into an AWS CodeCommit repo, which acts as the pipeline source. We have configured drift detection in the pipeline as a custom step backed by a lambda function. When the drift detection step invokes the provider lambda function, it first starts the drift detection on the CloudFormation stack Demo Stack and then saves the drift_detection_id along with pipeline_job_id in a DynamoDB table. In the meantime, the pipeline waits for a response on the status of drift detection. The EventBridge rules are set up to capture the drift detection state change events for Demo Stack that are received by the default event bus. The callback lambda is registered as the intended target for the rules. When drift detection completes, it triggers the EventBridge rule which in turn invokes the callback lambda function with stack status as either DRIFTED or IN SYNC. The callback lambda function pulls the pipeline_job_id from DynamoDB and sends the appropriate status back to the pipeline, thus propelling the pipeline out of the wait state. If the stack is in the IN SYNC status, the callback lambda sends a success status and the pipeline continues with the deployment. If the stack is in the DRIFTED status, callback lambda sends failure status back to the pipeline and the pipeline run ends up in failure. Solution Deep Dive The solution deploys two stacks as shown in the above architecture diagram

The CDK Pipelines stack defines a pipeline with a CodeCommit source and drift detection step integrated into it. The pre-requisite stack deploys following resources that are required by the CDK Pipelines stack.

A Lambda function that implements drift detection step A DynamoDB table that holds drift_detection_id and pipeline_job_id An Event bridge rule to capture "CloudFormation Drift Detection Status Change" event A callback lambda function that evaluates status of drift detection and sends status back to the pipeline by looking up the data captured in DynamoDB.

The pre-requisites stack is deployed first, followed by the CDK Pipelines stack. Defining drift detection step CDK Pipelines offers a mechanism to define your own step that requires custom implementation. A step corresponds to a custom action in CodePipeline such as invoke lambda function. It can exist as a pre or post deployment action in a given stage of the pipeline. For example, your organization's policies may require its CI/CD pipelines to run a security vulnerability scan as a prerequisite before deployment. You can build this as a custom step in your CDK Pipelines. In this post, you will use the same mechanism for adding the drift detection step in the pipeline. You start by defining a class called DriftDetectionStep that extends Step and implements ICodePipelineActionFactory as shown in the following code snippet. The constructor accepts 3 parameters stackName, account, region as inputs. When the pipeline runs the step, it invokes the drift detection lambda function with these parameters wrapped inside userParameters variable. The function produceAction() adds the action to invoke drift detection lambda function to the pipeline stage. Please note that the solution uses an SSM parameter to inject the lambda function ARN into the pipeline stack. So, we deploy the provider lambda function as part of pre-requisites stack before the pipeline stack and publish its ARN to the SSM parameter. The CDK code to deploy pre-requisites stack can be found here. export class DriftDetectionStep extends Step implements pipelines.ICodePipelineActionFactory { constructor( private readonly stackName: string, private readonly account: string, private readonly region: string ) { super(`DriftDetectionStep-${stackName}`); }

public produceAction( stage: codepipeline.IStage, options: ProduceActionOptions ): CodePipelineActionFactoryResult { // Define the configuraton for the action that is added to the pipeline. stage.addAction( new cpactions.LambdaInvokeAction({ actionName: options.actionName, runOrder: options.runOrder, lambda: lambda.Function.fromFunctionArn( options.scope, `InitiateDriftDetectLambda-${this.stackName}`, ssm.StringParameter.valueForStringParameter( options.scope, SSM_PARAM_DRIFT_DETECT_LAMBDA_ARN ) ), // These are the parameters passed to the drift detection step implementaton provider lambda userParameters: { stackName: this.stackName, account: this.account, region: this.region, }, }) ); return { runOrdersConsumed: 1, }; } } TypeScript Configuring drift detection step in CDK Pipelines Here you will see how to integrate the previously defined drift detection step into CDK Pipelines. The pipeline has a stage called DemoStage as shown in the following code snippet. During the construction of DemoStage, we declare drift detection as the pre-deployment step. This makes sure that the pipeline always does the drift detection check prior to deployment. Please note that for every stack defined in the stage; we add a dedicated step to perform drift detection by instantiating the class DriftDetectionStep detailed in the prior section. Thus, this solution scales with the number of stacks defined per stage. export class PipelineStack extends BaseStack { constructor(scope: Construct, id: string, props?: StackProps) { super(scope, id, props);

const repo = new codecommit.Repository(this, 'DemoRepo', { repositoryName: `${this.node.tryGetContext('appName')}-repo`, });

const pipeline = new CodePipeline(this, 'DemoPipeline', { synth: new ShellStep('synth', { input: CodePipelineSource.codeCommit(repo, 'main'), commands: ['./script-synth.sh'], }), crossAccountKeys: true, enableKeyRotation: true, }); const demoStage = new DemoStage(this, 'DemoStage', { env: { account: this.account, region: this.region, }, }); const driftDetectionSteps: Step[] = []; for (const stackName of demoStage.stackNameList) { const step = new DriftDetectionStep(stackName, this.account, this.region); driftDetectionSteps.push(step); } pipeline.addStage(demoStage, { pre: driftDetectionSteps, }); TypeScript Demo Here you will go through the deployment steps for the solution and see drift detection in action. Deploy the pre-requisites stack Clone the repo from the GitHub location here. Navigate to the cloned folder and run script script-deploy.sh You can find detailed instructions in README.md Deploy the CDK Pipelines stack Clone the repo from the GitHub location here. Navigate to the cloned folder and run script script-deploy.sh. This deploys a pipeline with an empty CodeCommit repo as the source. The pipeline run ends up in failure, as shown below, because of the empty CodeCommit repo.

Next, check in the code from the cloned repo into the CodeCommit source repo. You can find detailed instructions on that in README.md This triggers the pipeline and pipeline finishes successfully, as shown below.

The pipeline deploys two stacks DemoStackA and DemoStackB. Each of these stacks creates an S3 bucket.

Demonstrate drift detection Locate the S3 bucket created by DemoStackA under resources, navigate to the S3 bucket and modify the tag aws-cdk:auto-delete-objects from true to false as shown below

Now, go to the pipeline and trigger a new execution by clicking on Release Change

The pipeline run will now end in failure at the pre-deployment drift detection step.

Cleanup Please follow the steps below to clean up all the stacks.

Navigate to S3 console and empty the buckets created by stacks DemoStackA and DemoStackB. Navigate to the CloudFormation console and delete stacks DemoStackA and DemoStackB, since deleting CDK Pipelines stack does not delete the application stacks that the pipeline deploys. Delete the CDK Pipelines stack cdk-drift-detect-demo-pipeline Delete the pre-requisites stack cdk-drift-detect-demo-drift-detection-prereq

Conclusion In this post, I showed how to add a custom implementation step in CDK Pipelines. I also used that mechanism to integrate a drift detection check as a pre-deployment step. This allows us to validate the integrity of a CloudFormation Stack before its deployment. Since the validation is integrated into the pipeline, it is easier to manage the solution in one place as part of the overarching pipeline. Give the solution a try, and then see if you can incorporate it into your organization's delivery pipelines. About the author:

Damodar Shenvi Wagle is a Senior Cloud Application Architect at AWS Professional Services. His areas of expertise include architecting serverless solutions, CI/CD, and automation.

TAGS: Amazon DynamoDB, Amazon EventBridge, AWS CDK, AWS CloudFormation, AWS CodeBuild, AWS CodePipeline, AWS Lambda, CICD Pipeline

Load test your applications in a CI/CD pipeline using CDK pipelines and AWS Distributed Load Testing Solution

Load testing is a foundational pillar of building resilient applications. Today, load testing practices across many organizations are often based on desktop tools, where someone must manually run the performance tests and validate the results before a software release can be promoted to production. This leads to increased time to market for new features and products. Load testing applications in automated CI/CD pipelines provides the following benefits:

Early and automated feedback on performance thresholds based on clearly defined benchmarks. Consistent and reliable load testing process for every feature release. Reduced overall time to market due to eliminated manual load testing effort. Improved overall resiliency of the production environment. The ability to rapidly identify and document bottlenecks and scaling limits of the production environment.

In this blog post, we demonstrate how to automatically load test your applications in an automated CI/CD pipeline using AWS Distributed Load Testing solution and AWS CDK Pipelines. The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define cloud infrastructure in code and provision it through AWS CloudFormation. AWS CDK Pipelines is a construct library module for continuous delivery of AWS CDK applications, powered by AWS CodePipeline. AWS CDK Pipelines can automatically build, test, and deploy the new version of your CDK app whenever the new source code is checked in. Distributed Load Testing is an AWS Solution that automates software applications testing at scale to help you identify potential performance issues before their release. It creates and simulates thousands of users generating transactional records at a constant pace without the need to provision servers or instances. Prerequisites To deploy and test this solution, you will need:

AWS Command Line Interface (AWS CLI): This tutorial assumes that you have configured the AWS CLI on your workstation. Alternatively, you can use also use AWS CloudShell. AWS CDK V2: This tutorial assumes that you have installed AWS CDK V2 on your workstation or in the CloudShell environment.

Solution Overview In this solution, we create a CI/CD pipeline using AWS CDK Pipelines and use it to deploy a sample RESTful CDK application in two environments; development and production. We load test the application using AWS Distributed Load Testing Solution in the development environment. Based on the load test result, we either fail the pipeline or proceed to production deployment. You may consider running the load test in a dedicated testing environment that mimics the production environment. For demonstration purposes, we use the following metrics to validate the load test results.

Average Response Time – the average response time, in seconds, for all the requests generated by the test. In this blog post we define the threshold for average response time to 1 second. Error Count – the total number of errors. In this blog post, we define the threshold for for total number of errors to 1.

For your application, you may consider using additional metrics from the Distributed Load Testing solution documentation to validate your load test. Architecture diagram

AWS CDK code for the CI/CD pipeline, including AWS Identity and Access Management (IAM) roles and policies. The pipeline has the following stages:

Source: fetches the source code for the sample application from the AWS CodeCommit repository. Build: compiles the code and executes cdk synth to generate CloudFormation template for the sample application. UpdatePipeline: updates the pipeline if there are any changes to our code or the pipeline configuration. Assets: prepares and publishes all file assets to Amazon S3 (S3). Development Deployment: deploys application to the development environment and runs a load test. Production Deployment: deploys application to the production environment.

The AWS Lambda (Lambda) function in the architecture contains a 500 millisecond sleep statement to add latency to the API response.

Typescript code for starting the load test and validating the test results. This code is executed in the 'Load Test' step of the 'Development Deployment' stage. It starts a load test against the sample restful application endpoint and waits for the test to finish. For demonstration purposes, the load test is started with the following parameters:

Concurrency: 1 Task Count: 1 Ramp up time: 0 secs Hold for: 30 sec End point to test: endpoint for the sample RESTful application. HTTP method: GET

Load Testing service deployed via the AWS Distributed Load Testing Solution. For costs related to the AWS Distributed Load Testing Solution, see the solution documentation.

Implementation Details For the purposes of this blog, we deploy the CI/CD pipeline, the RESTful application and the AWS Distributed Load Testing solution into the same AWS account. In your environment, you may consider deploying these stacks into separate AWS accounts based on your security and governance requirements. To deploy the solution components

Follow the instructions in the the AWS Distributed Load Testing solution Automated Deployment guide to deploy the solution. Note down the value of the CloudFormation output parameter 'DLTApiEndpoint'. We will need this in the next steps. Proceed to the next step once you are able to login to the User Interface of the solution. Clone the blog Git repository

Update the Distributed Load Testing Solution endpoint URL in loadTestEnvVariables.json. Deploy the CloudFormation stack for the CI/CD pipeline. This step will also commit the AWS CDK code for the sample RESTful application stack and start the application deployment. cd pipeline && cdk bootstrap && cdk deploy --require-approval neverBash Follow the below steps to view the load test results:

Open the AWS CodePipeline console. Click on the pipeline named "blog-pipeline". Observe that one of the stages (named 'LoadTest') in the CI/CD pipeline (that was provisioned by the CloudFormation stack in the previous step) executes a load test against the application Development environment.

Click on the details of the 'LoadTest' step to view the test results. Notice that the load test succeeded.

Change the response time threshold In this step, we will modify the response time threshold from 1 second to 200 milliseconds in order to introduce a load test failure. Remember from the steps earlier that the Lambda function code has a 500 millisecond sleep statement to add latency to the API response time.

From the AWS Console and then go to CodeCommit. The source for the pipeline is a CodeCommit repository named "blog-repo". Click on the "blog-repo" repository, and then browse to the "pipeline" folder. Click on file 'loadTestEnvVariables.json' and then 'Edit'. Set the response time threshold to 200 milliseconds by changing attribute 'AVG_RT_THRESHOLD' value to '.2'. Click on the commit button. This will start will start the CI/CD pipeline. Go to CodePipeline from the AWS console and click on the 'blog-pipeline'. Observe the 'LoadTest' step in 'Development-Deploy' stage will fail in about five minutes, and the pipeline will not proceed to the 'Production-Deploy' stage.

Click on the details of the 'LoadTest' step to view the test results. Notice that the load test failed.

Log into the Distributed Load Testing Service console. You will see two tests named 'sampleScenario'. Click on each of them to see the test result details.

From the AWS Console, go to CloudFormation and delete the stacks 'Production-Deploy-Application' and 'Development-Deploy-Application'.

Delete the CI/CD pipeline. cd pipeline && cdk destroyBash Delete the Distributed Load Testing Service CloudFormation stack.

From CloudFormation console, delete the stack for Distributed Load Testing service that you created earlier.

Conclusion In the post above, we demonstrated how to automatically load test your applications in a CI/CD pipeline using AWS CDK Pipelines and AWS Distributed Load Testing solution. We defined the performance bench marks for our application as configuration. We then used these benchmarks to automatically validate the application performance prior to production deployment. Based on the load test results, we either proceeded to production deployment or failed the pipeline. About the Authors

Usman Umar Usman Umar is a Sr. Applications Architect at AWS Professional Services. He is passionate about developing innovative ways to solve hard technical problems for the customers. In his free time, he likes going on biking trails, doing car modifications, and spending time with his family.

Krishnakumar Rengarajan Krishnakumar Rengarajan is a Senior DevOps Consultant with AWS Professional Services. He enjoys working with customers and focuses on building and delivering automated solutions that enable customers on their AWS cloud journey.

Best Practices from Pragma for Navigating the API Economy with Observability and AWS

The term "API economy" refers to businesses delivering digital services to end users, other company services, or partners. But what is the API economy? You could use APIs to improve productivity with services already built and exposed by other companies, or you could create internal services and expose them through APIs to help you reuse these services, reduce coupling, and improve maintainability. You could also develop services and expose them to a general audience or partners who need them. All of these options could be delivered via API-as-a-service, which gives you financial benefits and strategic advantages. We call this the "API economy," which allows you to establish many business models to develop and publish APIs. This post discusses the necessary aspects to achieve an observability model in the API economy, including a practice example with an architecture design and related technologies. Pragma is an AWS Advanced Tier Services Partner and AWS Marketplace Seller with Competencies in DevOps and Digital Customer Experience. Pragma has worked with big retail, banking, and insurance companies in LATAM for over 30 years. Its approach is to work alongside organizations and stakeholders to assess their business and technology needs and recommend tailor-made solution roadmaps that are focused on the end-user experience. Exploring the API Economy: API-as-a-Service API-as-a-service can be consumed by a company's internal services to transform and add value before going to end users or a partner who, having the required user authorization, works with that information and delivers their value proposition to end users. Amazon Web Services (AWS) is one of the examples of how API economy can be established. AWS uses APIs to expose core service capabilities and customers, partners, and internal AWS employees make use of these APIs, creating an API economy. A little background on how it came into existence. As Amazon was growing, the team started transitioning to a service-oriented architecture and built services in a decoupled, API-access fashion. Internal teams inside of Amazon were expected to be able to consume peer internal development team services through these APIs. As more developments were made, AWS made the APIs available to external users as well. To have a thriving API economy model, a well-defined observability architecture is required to optimize the business or improve external quality attributes. Here are some considerations that should be taken into account:

Percentage of successful and failed transactions. Number of requests by second, minutes, hours, months, or year. Configuration changes and their impact on the architecture. API consumption plan per customer. Infrastructure cost. Most and least used APIs. When an error happens, detect the error, its cause, and how to solve it.

Enabling the API Economy Based on Pragma's extensive experience, consider the following steps to establish an API economy model. Step 1: Understanding and Definitions Conducting thorough market research and understanding the target consumer is crucial to create an API economy model. Define the value proposition of the API while keeping in mind legal and regulatory restrictions. API architecture should consider business goals, processes, technical restrictions, and the current architecture context. Quality metrics should be assessed to ensure the API is reliable, secure, and efficient. Establishing a propagation and deployment strategy includes identifying the first customers who will consume the APIs and determining how they'll do so. Careful consideration should be given to the load the API will receive, taking into account the projected customer base. By projecting the load, it's possible to ensure the API can handle the expected volume of requests without experiencing performance issues. Step 2: Business Model Selection When devising an API economy model, consider diverse business models that can be utilized to maximize profits and benefits. Here are some of the most significant ones that have proven effective in the industry:

Free: The benefit is not monetary. It could be strategic, as when the company wants end users to take advantage of their service while tracking how they use it. Freemium: Users can consume the service for free with some limitations and then pay for added features. For example, the company delivers a free tier for users to see their credit card expenses, but if users want to create a budget that sends an alert when they exceed it, they have to pay an additional cost. Subscription: Customers subscribe to a standard service that could have extra payment options if the limit is exceeded. For example, users could request to know their credit score three times every month, but if the number of requests exceeds this limit they have to pay for the additional requests. Pay-per-use: The price is directly based on usage, with a specific driver such as the number of requests to the API or MB consumed. Depending on their quantity, users pay a unit/value. Revenue share: There could be a partnership between two companies, and the profits related to the API usage are distributed depending on the number of accesses to the service.

Step 3: Design Integration and Observability Prioritize designing an architecture that lays the foundation for building a scalable and reliable API economy. This involves creating a cohesive structure, defining design patterns, and adopting best practices that enable seamless communication and data exchange between components. Pragma uses dashboards that offer business and technical perspectives to ensure it can observe all APIs. These dashboards allow Pragma to analyze data and take actions related to service pricing and API consumption. A critical aspect of this architecture is observability design, which focuses on gaining insights into the system's behavior and performance. Observability involves configuring services and APIs to collect logs, metrics, and traces which can be analyzed to detect issues and make informed decisions. An example of how APIs can be shared is AWS Data Exchange, which allows customers to find third-party datasets. The exchange happens through Amazon API Gateway which integrates with provider APIs, allowing data providers to distribute and monetize API-accessible data and subscribers to find, subscribe to, and use third-party APIs in the cloud. It's imperative to know which API you'll expose and who will use it; such as:

Public API – exposed to the general public. Private API – exposed to the company's internal users. Partners API – exposed to third parties that have or will have an agreement with the company.

Step 4: Technology Selection to Implement API Lifecycle To create successful APIs, you must establish the technology stack at this stage, choosing the appropriate tools and frameworks to develop, deploy, and manage APIs efficiently. The technology stack should align with the organization's requirements and scalability needs. Additionally, defining a baseline for API codification ensures consistent and standardized API development, making it easier for developers to collaborate. The necessary infrastructure should also be put in place to support API development. A testing strategy ensures the quality of the API economy model, aligning with quality attributes metrics, strategy, and business goals. On top of that, a user-centric approach ensures a seamless and effective user experience, as the goal is to deliver reliable, user-friendly, and responsive APIs. Step 5: Implementation Taking a strategic approach to implementing an effective API economy model is important, and you first need to create APIs that align with the overall strategy. This involves identifying key functionalities and services offered through the APIs, and once the APIs have been developed the next step is to publish them. To streamline development, CI/CD pipelines should be established to automate the building, testing, and deployment of APIs. Embracing an API-oriented DevSecOps approach drives agility, faster development cycles, and improved reliability in software development and delivery. Finally, it's essential to establish governance to ensure the API economy model is continuously evolving based on user feedback and observability. This involves monitoring the performance of the APIs and making adjustments to enhance the user experience and improve overall strategy. Pragma's Approach Observability in the API economy is critical because a company needs to know if the execution is effective according to their chosen business model. Based on the insights gained, recommendations must be made to improve both profits and customer experience. Observability has three mainstays—logs, traces, and metrics—but we should consider other critical aspects related to monitoring, such as costs, infrastructure, application performance management), transactional auditory, log management, synthetic monitoring, real user monitoring, and security. Based on Pragma's experience, we'll now showcase an example of observability in the API economy and explain step by step how to implement it and how you could cater to business and technical needs. Building the Architecture Clone this repository, which was done with AWS CloudFormation. Now, change in the template api_economy_template.yaml, the domain PaymentUserPoolDomain (section for the domain you want), and username in ExampleUser (section for your email). Once you have run the template, you'll have in your AWS console the next deployed architecture.

Figure 1 – High-level architecture. This high-level architecture works with APIs that can be exposed to multiple customers with one API key and is connected to Amazon Cognito to manage the authorization process. In addition, the architecture has different services, which we'll use to answer some business and technical questions. One of the relevant services of this architecture is Amazon DevOps Guru, a machine learning (ML) service designed to detect anomalous behavior patterns by identifying problems and recommending how to solve them. Amazon EventBridge is scheduled to run an AWS Lambda function every minute, allowing DevOps Guru to detect whether the service is healthy. Next, uncomment the third line (time.sleep(120)) to modify the latency of this service, and after 10-20 minutes DevOps Guru will start to detect problems and make recommendations to improve the latency. In the following image, DevOps Guru shows an insight with high severity due to invocations timing out.

Figure 2 – DevOps Guru high severity. On the same page, DevOps Guru shows recommendations to solve the problem, such as a rollback (remember the error was caused by uncommenting the line in payment service), which is the right solution to the problem. After the rollback, wait a few minutes and DevOps Guru will show the insight was closed.

Figure 3 – Recommend rollback. Now, we'll see how to answer other technical and business questions. For that, uncomment the third line to affect the latency again. Then, get a token—you can get one by using the URL: Change REGION for the region in which you are working in the AWS console; example us-east-1. Next, change YOUR_DOMAIN and YOUR_CLIENT_ID for the data you can find in Cognito > User pools > App Integration (tab). Note the token generated above should be used by the authenticated user only, as this token is tied to an individual user account. Hence, it should be kept as a secret. If the API is called from a web or mobile application, AWS Amplify can simplify the overall authentication, including the token lifecycle management. Additionally, you can use AWS WAF and AWS Shield to block malicious requests and prevent from other web exploits and attacks. The next step is to log in and get the URL and paste in notes to get id_token. Take into account the URL contains id_token, access_token, expires_in, token_type,and you must choose just id_token. Take into account that to log in, the email address is what you put in the CloudFormation stack and the password was sent to that email. Next, import TestAPIEconomy.postman_collection.json of the GitHub in Postman, fill in the next variables, and save:

TOKEN: id_token you got previously X-API-KEY: API key you can find in API Gateway > Select your API > API Keys. Example: G4qilyc7EF3Yj7S904Sib9m6dwE6LxKAcavpeRI6 URL: API Invoke URL in stage prod. Example:

Send the request from the Postman, and you should receive the following text as a response with status 200: "Your payment has been made successfully." Create run collections with 20 iterations and a delay of 60,000, run them manually, and wait until they finish. After the previous process, observe the request trace, percentage of successful transactions, percentage of error transactions, and he transaction quantity per minute in AWS X-Ray for the API Gateway and AWS Lambda service.

Figure 4 – AWS X-Ray percentage of successful and error transactions. The error we can see now in the API Gateway is because we configured a usage plan that allows just 20 requests; after that, the limit will be exceeded and the response code will be 429: "Too many requests."

Figure 5 – Too many requests. Alternatively, with the last requests DevOps Guru also detects the anomalous latency and 4XX Error of "Too many requests," as shown in the following image. Besides, it gives some recommendations to solve this problem.

Figure 6 – Behavior anomalous in API Gateway. Next, you could see the API consumption plan per customer in API Gateway > Payment > Usage Plans > Payment_Usage_Plan > API Keys (tab) > Usage.

Figure 7 – API consumption plan per customer. Cleanup Delete all cloud formation stacks and the associated cloud watch logs group to avoid incurring additional costs. Conclusion In this post, we covered the concepts of API economy and the steps you should take to establish an API economy model effectively. You have learned the mainstays of observability and the importance of having well-defined observability architecture to have a thriving API economy model at scale. We have used AWS services such as Amazon DevOps Guru and AWS X-Ray to provide answers to improve the business. Learn more about Pragma in AWS Marketplace. .

Pragma – AWS Partner Spotlight Pragma is an AWS Partner that has worked with big retail, banking, and insurance companies in LATAM for over 30 years. Its approach is to work alongside organizations and stakeholders to assess their business and technology needs and recommend tailor-made solution roadmaps that are focused on end-user experience. Contact Pragma | Partner Overview | AWS Marketplace

TAGS: AWS Competency Partners, AWS Partner Guest Post, AWS Partner Solutions Architects (SA), AWS Partner Success Stories, AWS Service Delivery Partners, AWS Well-Architected Partners, PRAGMA

Securing Amazon API Gateway with secure ciphers using Amazon CloudFront

Enterprise customers have differing requirements based on a number of reasons including security, compliance, governance, and the industry they operate. In certain situations, customers are expected to adhere to certain protocols and standards that are non-negotiable. In this post, we explore how customers can specify TLS protocols and associated ciphers that are allowed from viewers on Amazon API Gateway. The TLS protocol provides protection against network security concerns, such as tampering between client and server communication. It is recommended to use the latest version of TLS to make sure that your API can support the most recent ciphers which offer the highest level of security. API Gateway provides numerous options for selecting TLS ciphers for the APIs provisioned for your custom domain. However, enterprise customers may require further flexibility based on their organizational policies and governance requirements. This post is intended to assist customers in this particular situation to customize API Gateway endpoints to support specific TLS cipher suites that meet their organizational compliance requirements.

Walkthrough What are Cipher Suites and how do they relate to TLS/SSL? Cipher Suites are a collection of Cryptographic algorithms (Ciphers) that allow network connections to be secure. A typical TLS/SSL Cipher suite provides a set of steps and the order under which the contained algorithms are executed. A considerable amount of processing is involved when establishing the HTTPS connection between the client and server applications. This process is referred to as a TLS/SSL handshake. During the SSL/TLS handshake, the client application provides the server application with the maximum TLS version along with the cipher suites it supports. Then, the server application makes a decision based on the best options available for both the server and the client on the right TLS version and cipher suites to be used throughout the session. Cipher suites for TLS 1.2 and TLS 1.3 TLS 1.2 and TLS 1.3 are the prevalent standards used today. The cipher suites for TLS 1.2 and TLS 1.3 are different, TLS 1.2 uses 37 cipher suites whilst TLS 1.3 uses only five cipher suites. TLS 1.3 is still considered more secure and the reason for this lies in how these cipher suites are created. If you look deeper into the composition of a TLS 1.2 cipher suite, then you find the following four ciphers: · Key exchange algorithm for protecting information when creating symmetric keys for exchange · Signature algorithm which validates the type of certificate · Bulk data encryption algorithm to encrypt messages between client and server · Message authentication code (MAC) algorithm to perform message authentication using hashes and signatures between the clients and the server An example of how the different algorithms in TLS 1.2 Cipher Suite are organized is shown here:

Looking at the composition of the cipher suite for TLS 1.3, you notice only two ciphers: one for MAC and the other for bulk data encryption. An example for a TLS 1.3 Cipher is shown here:

With TLS 1.3, the client-server TLS/SSL handshake doesn't require negotiations for key exchange and signature. This reduces the messages exchanged during the TLS/SSL handshake to just one round trip in TLS 1.3 as compared to two round trips in TLS 1.2, making TLS 1.3 faster. TLS 1.3 is also safer than TLS 1.2, as it has removed support of older, weaker, and more vulnerable algorithms which were supported in TLS 1.2 and the previous versions of TLS. TLS support in API Gateway When implementing API endpoints for your organizational needs, you can utilize API Gateway to quickly provision different types of highly scalable APIs. The three types of APIs supported by API Gateway are REST, HTTP, and WebSocket APIs. You can create an API Gateway by following the steps mentioned in the AWS documentation. Security becomes a prime concern when it comes to exposing the created API Endpoints to the client applications. API Gateway allows developers to choose a minimum version of TLS protocol to be enforced on your custom domain to provide enhanced security. Adding the security policy is supported using either AWS Management Console, AWS Command line Interface (AWS CLI), or by using AWS SDKs. To create a custom domain for API Gateway, 1. Log in to the AWS Console. 2. Navigate to API Gateway console by either searching for API Gateway or traversing through the Services menu. 3. In the API Gateway console, select "Custom domain names" as shown in the image below.

Currently, when creating the custom domain name, API Gateway provides options to choose a security policy between TLS 1.2 or TLS 1.0 as the minimum TLS version for the custom domain. The following image shows the TLS ciphers supported by an API with regional endpoint or a custom domain name with regional endpoint. Utilities like sslscan are effective at finding the TLS ciphers supported by the server.

TLS ciphers supported by an API Gateway with regional endpoint or a custom domain name with regional endpoint

API Gateway support for TLS 1.3 Currently, out of the box support for TLS 1.3 and its associated cipher suites occurs when deploying the API using edge-optimized endpoints or fronting the API using an edge-optimized custom domain name. In both cases, an AWS managed Amazon CloudFront distribution gets created using the security policy set by CloudFront, which has support for TLS 1.3 cipher suites. However, in both cases, since the CloudFront distribution created for the API is managed by AWS, it is not possible to customize the TLS version and the cipher suites used. Currently, the recommended (default) security policy for CloudFront distributions using custom SSL certificates is TLSv1.2_2021. The cipher suites supported with TLSv1.2_2021 and other security policies is highlighted in the AWS documentation for supported protocols and ciphers for CloudFront. The following image shows the TLS ciphers supported by an API with edge-optimized endpoint or using an edge-optimized custom domain name.

TLS ciphers supported by an API Gateway with edge-optimized endpoint or using an edge-optimized custom domain name

Exposing API Gateway via a custom CloudFront distribution Organizations that need flexibility in selecting specific security policies and associated TLS ciphers benefit from using an API Gateway with a regional endpoint and fronting the endpoints via a custom CloudFront distribution. Doing this allows the organizations to not only select their preferred security policy and TLS ciphers, but also use additional features provided by CloudFront including choosing default cache behavior, selecting minimum origin SSL protocol, selecting the supported HTTP versions, and adding response headers to decide on the CORS policy to be used, among others. An example architecture to achieve this looks similar to the following.

Sample architecture showing CloudFront distribution in front of a regional API Gateway

Refer to the steps highlighted in the AWS documentation for CloudFront to create the CloudFront distribution. As per the previous architecture, placing the CloudFront distribution in front of a regional API Gateway lets us configure the right Security Policy with the needed TLS 1.3 ciphers to be allowed for communication from the clients to the CloudFront. The communication between the CloudFront and the origin (Regional API Gateway) will occur based on the minimum origin SSL protocol set between CloudFront and the Origin. You can choose to limit the access to your REST Regional API Gateway endpoint exclusively to CloudFront by adding a custom header (X-Origin-Verify) to the request when CloudFront passes the request from client to API Gateway using AWS WAF in between, as highlighted in this AWS Well-Architected Lab. The same can be achieved with HTTP API Gateway endpoint using an API Gateway Lambda authorizer as highlighted in this post. The following image shows the TLS ciphers supported by the custom CloudFront distribution fronting the Regional API Gateway endpoint with security policy selected as TLSv1.

TLS ciphers supported by fronting the Regional API Gateway endpoint with security policy as TLSv1

Limiting API endpoints to support only strong cipher suites Now that we have achieved fine control on the level of security policies and associated cipher suites supported by CloudFront fronted API, let's see how this can help organizations where API endpoints are required to support only strong cipher suites and not allow handshakes from clients publishing weak cipher suites. To give a background for this use case, let's first define weak and strong ciphers. A weak cipher is one where the algorithm used to encrypt/decrypt uses a key which is not long enough, thus making it vulnerable to attacks. On the other hand, a strong cipher uses an algorithm with a longer key, thus making it more robust. You can read more about strong and weak ciphers on the owasp site. Both regional and edge optimized API Gateway endpoints have support for weak cipher suites. This makes them inappropriate to be used in organizations which have a mandate of using API endpoints supporting only strong cipher suites. The classification of cipher suites supported by regional API Gateway endpoints is shown in the following (source: ssllabs). Note that the Regional API Gateway endpoints support only TLS 1.2 as well as weak cipher suites.

Regional API Gateway endpoints support only TLS 1.2 as well as weak cipher suites

The edge optimized API Gateway endpoint falls in the same category as regional endpoints, where even though it supports TLS 1.3 along with TLS 1.2, it still has weaker cipher suites in the supported cipher suites. The classification of cipher suites in order of preference for edge optimized API Gateway endpoint is shown in the following image (source: ssllabs).

TLS version and Cipher suites supported by edge optimized API Gateway endpoint

To overcome this issue, you can use the same solution of fronting the API Gateway with CloudFront. To allow your API to use only strong cipher suites, you can configure a custom SSL certificate for the domain which can then be used as an alternate domain name for the CloudFront distribution, followed by selecting the Security policy to be the latest version. In this scenario, the security policy to be selected is TLSv1.2_2021 (recommended), as shown in the following image.

Selecting Security Policy to be used between CloudFront and clients

Looking at the classification of cipher suites where the endpoint custom domain is added as an alternate domain to the CloudFront distribution, you find that it has support for both TLS 1.3 and TLS 1.2, but it has limited its cipher suites to support only strong cipher suites. This makes the above arrangement of API Gateway fronted by CloudFront distribution with your custom domain name certificate with a security policy of TLSv1.2_2021 a perfect choice for organizations which require their API endpoints to support only the strong cipher suites for both TLS 1.3 and TLS 1.2. The following image (source: ssllabs) shows the classification of cipher suites for this arrangement.

TLS version and Cipher suites supported by API Gateway fronted by CloudFront Distribution with custom domain name certificate

Conclusion In this post, we saw how fronting API Gateway with custom CloudFront distribution provides customers with the right control and flexibility to adjust the TLS versions and the associated cipher suites for their APIs to be compliant with their organizational policies. Furthermore, by using the needed TLS ciphers, the solution also enables customers to use other associated benefits with CloudFront not limited to added reliability, low latency, and high throughput network connectivity with the help of its global edge network. Visit creating a CloudFront distribution and Amazon CloudFront Pricing to get started with CloudFront. To learn more about the best practices of building applications with CloudFront, visit AWS content delivery blogs.

Kushagra Tiwari (Kush) Kush is a Senior Applications Architect at AWS with expertise in Serverless and Containers. He has designed and architected many scalable and resilient solutions for both cloud native and hybrid products across multiple domains, including Media Systems, Finance, Insurance, Defence and Security amongst others in his 17+ years of Software experience. He is based out of Sydney and is a tea enthusiast with collection of tea leaves from over 20 countries.

Robbie Cooray Robbie Cooray is a Senior Solutions Architect at AWS since 2019, based out of Melbourne Australia. He is 11x certified, specialised provides guidance to customers in building software using serverless technologies. Robbie is passionate about designing, building and modernising enterprise solutions for retail and enterprise customers.

Simplify business intelligence identity management with Amazon QuickSight and AWS IAM Identity Center

April 2025: This post was reviewed and updated for accuracy. Amazon QuickSight is now an AWS IAM Identity Center enabled application. This capability allows administrators that subscribe to QuickSight to use IAM Identity Center to enable their users to login using their existing credentials. QuickSight is a scalable, serverless, machine learning (ML)-powered business intelligence (BI) solution that makes it simple to connect to your data, create interactive dashboards, get access to ML-enabled insights, enable natural language querying of your data, and share visuals and dashboards with tens of thousands of internal and external users, either within QuickSight itself or embedded into another application. IAM Identity Center helps you securely create or connect your workforce identities and manage their access centrally across AWS accounts and applications. IAM Identity Center is the recommended approach for workforce authentication and authorization on AWS for organizations of varying size and type. With IAM Identity Center, you can create and manage user identities in AWS, or connect your existing identity source, including Microsoft Active Directory, Okta, Ping Identity, JumpCloud, Google Workspace, and Azure Active Directory (Azure AD). With this release, administrators can create a new QuickSight account and select IAM Identity Center to configure QuickSight with their organization's identity provider or with the IAM Identity Center identity store without requiring additional single sign-on configuration in QuickSight. They can use a supported identity provider's groups to assign QuickSight roles (administrator, author and reader) to users. Amazon QuickSight users can seamlessly sign-in to QuickSight with their organization's identity provider from the AWS access portal or directly from QuickSight and share assets with users and groups. In this post, we show how to simplify your BI identity management with QuickSight and IAM Identity Center. Use case overview AnyCompany Inc. is a fictitious organization in the healthcare space. They have an application for hospitals across different regions of the country that manages business operations. AnyCompany has thousands of healthcare employees accessing the application portal. Being in the healthcare and life sciences industry, AnyCompany handles sensitive data and centrally securing user access is critical to meet business and compliance requirements. AnyCompany already configured Okta with IAM Identity Center. This configuration enables identity and access management for IAM Identity Center enabled applications. This post will go through the steps for AnyCompany to sign up to QuickSight with IAM Identity Center. Solution Overview When enabled with an external identity provider like Okta, signing up to QuickSight with IAM Identity Center enables the following sign-in flows:

The user loads the QuickSight application URL and is automatically redirected to the sign-in page of the configured IdP. Upon successful authentication, the user is redirected to QuickSight. The following diagram illustrates this flow.

The user loads the AWS access portal URL and is automatically redirected to the sign-in page of the configured IdP. After authentication, the user chooses the QuickSight application tile in the AWS access portal and is redirected to QuickSight. The following diagram illustrates this flow.

The user navigates to the application portal hosted by the IdP and is authenticated. They select the IAM Identity Center AWS access portal. The AWS access portal loads and shows enabled applications like QuickSight. The user chooses the QuickSight application tile, and is automatically redirected to QuickSight.

Prerequisites To complete this walkthrough, you must have the following:

Organizations and IAM Identity Center enabled. Okta configured as an IdP in IAM Identity Center. For instructions, refer to Okta. An AWS account for QuickSight in the same organization as IAM Identity Center. This account should not currently be subscribed to QuickSight. The administrator needs AWS Identity and Access Management (IAM) administrator access or QuickSight and IAM Identity Center permissions. For more information, refer to IAM policy examples for Amazon QuickSight.

Subscribe to QuickSight with IAM Identity Center To subscribe to QuickSight, complete the following steps:

Sign in to your AWS account and open QuickSight from the AWS Management Console. You can find it under Analytics or by searching for QuickSight.

Enter a notification email address for the QuickSight account owner or group. This email address receives service and usage notifications. Select the identity option that you want to subscribe with. For this post, we select Use AWS IAM Identity Center. Enter an account name. Choose Configure.

Next, you assign groups in IAM Identity Center to roles in QuickSight (admin, author and reader.) This step enables your users to access the QuickSight application. If you are using an external identity provider, the groups that you plan to use to assign users to roles in QuickSight need to be assigned to the IAM Identity Center application. For more details on how to do this with Okta, refer to this step from the prerequisite configuration of Okta and IAM Identity Center. If you cannot find a group from your external identity provider from QuickSight, the group likely hasn't yet been assigned to the IAM Identity Center application in your external identity provider. If you are not using an external identity provider, your IAM Identity Center groups are ready to use in QuickSight. A common way to create groups for role group assignation is to create a group of users for each role: QuickSightAdmins, QuickSightAuthors and QuickSightReaders. In Okta, you can create rules based on attributes in your external identity provider to automatically add users to those groups. Search for a group from IAM Identity Center for each QuickSight role.Note that you can add additional groups after your sign up is complete from the Manage Users page in the QuickSight administration console. From this page, click on Manage role groups. Select an IAM role to control QuickSight access to your AWS resources. For more information, see Accessing AWS resources.

Review the choices that you made, then choose Finish. Choose Go to Amazon QuickSight.

Validate user sign-in to QuickSight Now you can validate that users are able to sign-in to QuickSight using both the IdP-initiated flow and SP-initiated flow. Verify IdP initiated flow: Complete the following steps to test the IdP-initiated flow:

Navigate to the Okta console and log in with a user who is a member of one of the groups that you assigned to be QuickSight Admins, Authors or Readers.

Once logged in on the dashboard, users will be able to see the assigned Identity Center application.

Choose the AWS IAM Identity Center application from the list of applications available.

This will bring you to the IAM Identity Center landing page with a list of applications.

Choose the QuickSight application from the list to navigate to QuickSight.

Validate the SP-initiated flow: To validate the SP-initiated flow, complete the following steps:

Navigate to On the landing page, enter the account name used during QuickSight sign up, then choose Next.

Conclusion This post provided step-by-step instructions to sign up for QuickSight with IAM Identity Center and assign groups to roles in QuickSight. QuickSight automatically detects the IAM Identity Center instance in the account's organization and doesn't require additional single sign-on configuration in QuickSight. Amazon QuickSight users can seamlessly sign-in to QuickSight with their organization's identity provider from the AWS access portal or directly from QuickSight and share assets with users and groups. For more details, see Configure your Amazon QuickSight account with IAM Identity Center. For additional discussions and help getting answers to your questions, check out the QuickSight Community.

About the authors Camille Taylor is a Sr Technical Product Manager focused on QuickSight Administration and Identity Management at AWS. She has been a consultant advising Fortune 500 companies on how to derive value from their data and scale adoption of their business intelligence investments across industries. Earlier in her career, she worked in financial services consumer product valuations and analytics. In her spare time, she likes to cycle, run and engage with the latest analytical consumer tech. Srikanth Baheti is a Specialized World Wide Principal Solutions Architect for Amazon QuickSight. He started his career as a consultant and worked for multiple private and government organizations. Later he worked for PerkinElmer Health and Sciences & eResearch Technology Inc, where he was responsible for designing and developing high traffic web applications, highly scalable and maintainable data pipelines for reporting platforms using AWS services and Serverless computing. Raji Sivasubramaniam is a Sr. Solutions Architect at AWS, focusing on Analytics. Raji is specialized in architecting end-to-end Enterprise Data Management, Business Intelligence and Analytics solutions for Fortune 500 and Fortune 100 companies across the globe. She has in-depth experience in integrated healthcare data and analytics with wide variety of healthcare datasets including managed market, physician targeting and patient analytics. Sumit Dhuwalia is a Technical Account Manager at AWS focused on helping ISV customers build, deploy, and run secure, resilient, and cost-effective workloads on AWS. He has a background in analytics consulting and has helped several Fortune 500 companies across industries implement solutions spanning the entire data lifecycle, from data governance / management to advanced analytics using machine learning. Outside of work, he keeps himself busy with all things finance and reading non-fiction. Ron Cully is a Principal Product Manager at AWS where he has spent over 7 years in identity leading work on AWS IAM Identity Center, and previously on AWS Directory Service. Ron has over 25 years of industry experience in product and program management of networking and directory related products. He is passionate about delivering secure, reliable solutions that help make it easier for customers to have trustworthy experiences while taking advantage of cloud computing. Praney Mahajan is a Senior Technical Account Manager at AWS who partners with key enterprise customers as their strategic advisor. He is passionate about bridging technical solutions with business outcomes, with expertise in database, analytics, cost optimization, and operational efficiency to help customers achieve sustainable growth through innovative cloud solutions. He enjoys going on long drives with his family and playing cricket in his free time.

Bob's Used Books: A .NET Sample Application – Part 3: Infrastructure

Introduction Welcome to the third and final post in the Bob's Used Books blog post series. In the first post I discussed how to get started with Bob's Used Books and described the different debug and deployment modes you can use to test and run the application. In the second post I discussed the architecture of Bob's Used Books and provided some insight into the decisions that were made whilst building the sample application. In this post I will dive into the AWS Cloud Development Kit (AWS CDK) project that provisions AWS resources and deploys the sample application. This post references v1.0.0 of Bob's Used Books. The Bob's Used Books v1.0.0 GitHub repository can be found here. Overview The AWS CDK is an Infrastructure-as-Code (IaC) framework that enables developers to represent AWS resources and services as application code. When that application code is executed against an AWS account the resources and services defined within that code are automatically provisioned within the AWS account. IaC is often used in conjunction with DevOps to automate the provisioning of application environments as part of a CI/CD pipeline. IaC provides the following benefits:

Application environments can be version controlled by committing IaC scripts to source control Environment creation and deletion can be automated, reducing the risk of human errors Environments can be created and deleted on demand, enabling agile software development workflows and greater cost control

When an AWS CDK project is synthesized it generates an AWS CloudFormation template which is then passed to the CloudFormation service for processing. Although developers can create CloudFormation scripts directly, the AWS CDK provides higher-level constructs that typically enable developers to achieve the same outcomes with less effort. Bob's Used Books uses the AWS CDK to provision a number of AWS resources and services, including:

An Amazon Virtual Private Cloud (Amazon VPC) to host the application and its components An Amazon Simple Storage Service (Amazon S3) bucket to store book cover images An Amazon CloudFront distribution for low latency delivery of application assets An Amazon Cognito user pool and admin user for Customer Identity and Access Management (CIAM) An Amazon Relational Database Service (Amazon RDS) for SQL Server for the application backend An Amazon Elastic Cloud Compute (Amazon EC2) instance to act as the web server when the application is deployed to AWS A number of Amazon Identity and Access Management (IAM) roles, policies, and permissions that enable the application's components to communicate with each other.

As you can see, Bob's Used Books takes advantage of numerous AWS services. This is very common for applications that run in the cloud. Further, almost all applications these days have multiple non-production environments like Test and UAT in addition to a production environment. Using an IaC framework like the AWS CDK guarantees that each of those environments is provisioned quickly and consistently. With all that in mind, let's take a look at how Bob's Used Books uses the AWS CDK. CDK Stacks As discussed in the overview, applications that run in the cloud often depend upon multiple services and resources. The AWS CDK makes it easy to group these services and resources together into stacks. A stack is a group of services and resources that can be managed as a single unit. When a stack is deployed, all of the resources defined in that stack are deployed; when a stack is deleted, all the resources in that stack are deleted. For small applications it might make sense to maintain a single stack that contains all the resources for that application. As the application grows to include more services and resources though, maintaining a monolithic stack could become difficult. At that point it makes sense to split that monolithic stack into multiple stacks. Bob's Used Books has been organized into four stacks:

CoreStack – Defines an Amazon S3 bucket, an Amazon Cognito user pool, and a CloudFront distribution NetworkStack – Defines an Amazon VPC and associated resources DatabaseStack – Defines an Amazon RDS for SQL Server database EC2ComputeStack – Defines an Amazon EC2 instance that acts as the web server and deploys the application to that web server

Organizing the resources for Bob's Used Books into stacks provides greater control over resource deployment. When you are in Integrated Debugging mode (see the first post in this series for details on Integrated Debugging) you only need to deploy the Amazon S3 bucket, the Amazon CloudFront distribution, and the Amazon Cognito user pool. The application still runs locally in your development environment and doesn't require the networking resources, the database, or the web server. You deploy the resources required for Integrated Debugging by deploying CoreStack with the following command: cdk deploy BobsBookstoreCore When you want to simulate a production environment you need to deploy all of the stacks. You can do that with the following command: cdk deploy BobsBookstoreEC2 But wait, doesn't that just provision the web server and deploy the application? What about the network resources and the database? The AWS CDK project in Bob's Used Books takes advantage of a feature of the AWS CDK framework called cross-stack references. The EC2ComputeStack stack has references to resources defined in the other stacks. When you deploy the EC2ComputeStack the AWS CDK recognizes those dependencies and ensures they are also deployed. Let's take a closer look at how Bob's Used Books uses cross-stack references to manage resources. Cross-Stack References The four stacks that are defined by Bob's Used Books are instantiated in the Main method of Bookstore.Cdk/Program.cs. When EC2ComputeStack is initialized it is passed an instance of EC2ComputeStackProps: var ec2Stack = new EC2ComputeStack(app, $"{Constants.AppName}EC2", new EC2ComputeStackProps { Env = env, Vpc = networkStack.Vpc, Database = databaseStack.Database, ImageBucket = coreStack.ImageBucket, WebAppUserPool = coreStack.WebAppUserPool }); C# EC2ComputeStackProps implements the IStackProps interface and is populated with the resources created in NetworkStack, DatabaseStack, and CoreStack. For example, EC2ComputeStack deploys an Amazon EC2 instance into the VPC that was created by NetworkStack. It uses the Amazon Cognito user pool created in CoreStack to generate a user pool client app for the web server, and it uses the Amazon S3 bucket and RDS for SQL Server database to generate the appropriate access permissions for the web server. When you run cdk deploy BobsBookstoreEC2 the AWS CDK recognizes that EC2ComputeStack is dependent upon the VPC created in NetworkStack, the database created in DatabaseStack, and the bucket and user pool created in CoreStack, and it ensures they are provisioned first. Application Deployment In addition to defining compute resources for the solution, EC2ComputeStack also deploys the application to the web server. NOTE: Application deployments are typically facilitated by CI/CD pipelines rather than an IaC framework like AWS CDK, however we want to provide the .NET development community with a simple, self-contained development experience and deploying the application via the CDK achieved that goal. The Bookstore.Cdk project has a folder called EC2Artifacts that contains the following files:

bobsbookstore.conf – An Apache configuration file that defines the virtual host for Bob's Used Books. bobsbookstore.service – A Linux service that starts the application via the dotnet CLI. ssl.conf – A configuration file that is used as part of the self-signed certificate configuration. configure_ec2_web_app.sh – A bash script that configures the EC2 instance that hosts Bob's Used Books.

EC2ComputeStack uploads these files, along with the application outputs that are produced when the application is published, to an S3 bucket (this is a different bucket from the one which is defined in CoreStack). This is achieved by using the Asset class defined in the Amazon.CDK.S3.Assets namespace and is implemented in the UploadAssestsToS3 method. Once the EC2 instance is provisioned a UserData script is created that copies the files from the S3 bucket to the EC2 instance and executes configure_ec2_web_app.sh to configure the EC2 instance. configure_ec2_web_app.sh installs the latest updates, installs and configures Apache, unzips and copies Bob's Used Books to var/www/bobsbookstore, and starts bobsbookstore.service. All of these steps are performed in the ConfigureUserData method of EC2ComputeStack. Clean Up If you have deployed any of the CDK stacks that are included in Bob's Used Books you can delete the AWS services and resources that were created by opening a command-line prompt, navigating to the application solution folder, and running the following command: cdk destroy BobsBookstore* This is an important step to ensure you don't incur unnecessary costs. Conclusion Bob's Used Books takes advantage of the AWS SDK for .NET to define and provision the required AWS resources. The AWS CDK for .NET is a powerful Infrastructure-as-Code framework that enables .NET developers to represent their AWS infrastructure using a programming language with which they are familiar and productive. This is the final post in the introductory series to Bob's Used Books. The first post in this series describes how to get up and running with the application and the different debug and deployment modes that are available. The second post in the series dives deep into the architecture of Bob's Used Books and provides insight into our thought processes as we designed and built the application. This is not, however, the last you will hear about Bob's Used Books! On the contrary, we plan on using Bob's Used Books to demonstrate common .NET modernization scenarios, to demonstrate integrating with additional AWS services, and for inclusion in upcoming training and enablement content. You can download Bob's Used Books from our GitHub repository.

Brad Webber Brad is a Senior Microsoft Solutions Architect at Amazon Web Services (AWS). He started writing code when Visual Basic 6 was still a thing. These days he spends most of his time helping organizations modernize their .NET workloads and migrate them to the cloud.

Developing autonomous robots is hard. Because it requires both hardware and software to be developed, integrated and tested, doing so in a physical environment is difficult and costly. Using simulations, robot developers can test their code quickly, iterating many times before finally deploying software to the hardware for testing in the real world. Having a high-fidelity simulator allows developers to test vision, perception, and recognition systems for their robots with high confidence. Open 3D Engine (O3DE) is a community driven, open source simulator that provides that level of high-fidelity realistic rendering needed for robotic simulations. An Introduction to O3DE O3DE is a 3D engine capable of creating and running realistic 3D worlds for gaming. To achieve photo-realistic rendering, it uses a configurable, multi-threaded rendering engine known as Atom, which supports features such as physically based rendering (PBR), ray-tracing, post-processing, and forward + rendering. O3DE also is capable of simulating real-world physics through its Nvidia PhysX based physics engine. O3DE is based on a modular system architecture known as the Gems system. This allows you to select and configure any number of packages (Gems) for a project depending on its needs. Gems can come from the O3DE project itself, third-party providers, or custom made. With the recent addition of the ROS 2 gem, this enables O3DE to work with robotics simulations that use the Robotic Operating System (ROS) to develop robots. By providing scene rendering and physics modeling, robotics developers can test and train perception models that closely resemble the real world in a pure simulation environment. In order to speed up the development cycle, developers can scale out their simulations by running them in parallel in the cloud. Being able to run simulations in the cloud adds performance, cost effectiveness, and scalability to the robot development process by providing access to high performance GPU instances on demand without the need for costly investment in physical hardware or on-site infrastructure. AWS RoboMaker supports GPU-based simulation jobs by running any simulator in an OCI compliant container such as Docker. This tutorial will go over creating a Docker image from an O3DE sample application (Robot Vacuum Sample) and deploying it as a simulation job in AWS RoboMaker. Prerequisites This tutorial requires an AWS account login with Amazon ECR and AWS RoboMaker access IAM permissions. It also requires a development system, or an AWS Cloud9 development environment with the following requirements:

Linux Ubuntu 20.04 or 22.04 At least 60 GB of free disk space AWS CLI installed Docker engine installed.

Walkthrough Build the container images The Robot Vacuum Sample application comes with a Dockerfile that is used to build container images for both the simulation as well as the navigation stack. Note: All of the command line examples in this walkthrough will be based on a workspace folder $WORKSPACE, which can exist anywhere on the host system that is accessible to the currently logged in user. It is recommended to set the environment variable $WORKSPACE to $HOME/o3de-sample.

Set the environment variable for $WORKSPACE to use for this walkthrough. export WORKSPACE=$HOME/o3de-sampleBash Clone the Robot Vacuum Sample application repository. mkdir -p $WORKSPACE git clone $WORKSPACE/RobotVacuumSample Bash Build a simulation docker image named o3de_robot_vacuum_simulation. cd $WORKSPACE/RobotVacuumSample/Docker docker build --build-arg IMAGE_TYPE=simulation -t o3de_robot_vacuum_simulation . Bash Build the navigation stack docker image named o3de_robot_vacuum_navstack. cd $WORKSPACE/RobotVacuumSample/Docker docker build --build-arg IMAGE_TYPE=navstack -t o3de_robot_vacuum_navstack . Bash If your environment meets the minimum O3DE hardware requirements, you can run these docker containers by following the docker readme.

Build and push the AWS RoboMaker container images to Amazon ECR The container images created for the demo are designed to run locally on a Linux host machine. In order to run on AWS RoboMaker, the container images need to have the NICE DCV plugins installed. NICE DCV is a high-performance remote display protocol that will be used to stream the simulation from AWS RoboMaker.

Extend the simulation docker image above to enable NICE DCV by creating a file $WORKSPACE/Dockerfile.simulation.robomaker and copying the following content into it. FROM o3de_robot_vacuum_simulation:latest ENV WORKSPACE=/data/workspace WORKDIR $WORKSPACE ENV DEBIAN_FRONTEND=noninteractive

# Install NICE DCV (from RUN wget RUN gpg --import NICE-GPG-KEY RUN wget RUN tar -xvzf nice-dcv-2023.0-15487-ubuntu2204-x86_64.tgz \ && mkdir -p /etc/modprobe.d \ && apt-get install -y /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-server_2023.0.15487-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gl_2023.0.1039-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gltest_2023.0.318-1_amd64.ubuntu2204.deb

# Install xterm for direct console access RUN apt update && apt-get install -y xterm Bash Build a new docker image named o3de_robot_vacuum_simulation_robomaker. docker build -t o3de_robot_vacuum_simulation_robomaker -f $WORKSPACE/Dockerfile.simulation.robomaker $WORKSPACEBash Create a repository in the Amazon ECR for the new simulation docker image. aws ecr create-repository --repository-name o3de-robot-vacuum-simulationBash The command will return a JSON result for the new repository: { "repository": { "repositoryArn": "arn:aws:ecr:us-west-2:000000000000:repository/o3de-robot-vacuum-simulation", "registryId": "000000000000", "repositoryName": "o3de-robot-vacuum-simulation", "repositoryUri": "000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-simulation", "createdAt": 1674082385.0, "imageTagMutability": "MUTABLE", "imageScanningConfiguration": { "scanOnPush": false }, "encryptionConfiguration": { "encryptionType": "AES256" } } }

JSON Set an environment variable for the simulation Amazon ECR repository with the repositoryUri results from the JSON output. export ECR_SIMULATION_URI=000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-simulationBash Log into Amazon ECR from your local Docker runtime. aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_SIMULATION_URIBash Tag the docker image o3de_robot_vacuum_simulation_robomaker with the same repository URI. docker tag o3de_robot_vacuum_simulation_robomaker $ECR_SIMULATION_URIBash Push the new simulation docker image to Amazon ECR. docker push $ECR_SIMULATION_URIBash Note: Due to the size of the container image (~20 GB) it may take some time to complete and upload depending on your internet connectivity. Extend the navigation docker image above to enable NICE DCV by creating a file $WORKSPACE/Dockerfile.navstack.robomaker and copying the following content into it. FROM o3de_robot_vacuum_navstack:latest ENV WORKSPACE=/data/workspace WORKDIR $WORKSPACE ENV DEBIAN_FRONTEND=noninteractive

# Install NICE DCV (from RUN wget RUN gpg --import NICE-GPG-KEY RUN wget RUN tar -xvzf nice-dcv-2023.0-15487-ubuntu2204-x86_64.tgz \ && mkdir -p /etc/modprobe.d \ && apt-get install -y /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-server_2023.0.15487-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gl_2023.0.1039-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gltest_2023.0.318-1_amd64.ubuntu2204.deb RUN wget

# Install xterm for direct console access RUN apt update && apt-get install -y xterm Bash Build a new container image named o3de_robot_vacuum_navstack_robomaker. docker build -t o3de_robot_vacuum_navstack_robomaker -f $WORKSPACE/Dockerfile.navstack.robomaker $WORKSPACEBash Create a repository in Amazon ECR for the new navigation stack docker image. aws ecr create-repository --repository-name o3de-robot-vacuum-navstackBash The command will return a JSON result for the new repository: { "repository": { "repositoryArn": "arn:aws:ecr:us-west-2:000000000000:repository/o3de-robot-vacuum-navstack", "registryId": "000000000000", "repositoryName": "o3de-robot-vacuum-navstack", "repositoryUri": "000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-navstack", "createdAt": 1674088085.0, "imageTagMutability": "MUTABLE", "imageScanningConfiguration": { "scanOnPush": false }, "encryptionConfiguration": { "encryptionType": "AES256" } } } JSON Set an environment variable for the navigation stack Amazon ECR repository with the repositoryUri results from the json output. export ECR_NAVSTACK_URI=000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-navstackBash Tag the docker image o3de_robot_vacuum_navstack_robomaker with the same repository URI. docker tag o3de_robot_vacuum_navstack_robomaker $ECR_NAVSTACK_URIBash Push the new navigation stack docker image to Amazon ECR. docker push $ECR_NAVSTACK_URIBash

Create an AWS RoboMaker simulation The next step is to create an AWS RoboMaker simulation using the newly created container images. The workflow involves defining the general parameters for the simulation such as an IAM role, compute type (set to CPU and GPU), and the simulation duration. You will also need to create both a Robot application and Simulation application for the simulation. Below is a step-by-step walk through of this process.

Click on 'Create simulation job' from the 'Simulation jobs' page. (verify your selected AWS region)

Configure the simulation job details and configure the 'Compute type' to 'CPU and GPU'.

Figure 2 – Configure the simulation details such as the job duration, Failure behavior, IAM role, Compute type, and Simulation/GPU Unit Limits.

Create a new robot application named 'o3de-robot-vacuum-navstack' and select 'o3de-robot-vacuum-navstack' from ECR by clicking on 'Browse ECR'. Check on 'Run with streaming session', and set 'Launch command' to '/data/workspace/LaunchNavStack.bash'.

Figure 3 – Define a new robot application based on the uploaded 'o3de-robot-vacuum-navstack' docker image

Create a new simulation application named 'o3de-robot-vacuum-simulation' and select 'o3de-robot-vacuum-simulation' from ECR by clicking on 'Browse ECR'. Check on 'Run with streaming session', and set 'Launch command' to '/data/workspace/LaunchSimulation.bash'.

Figure 4 – Define a new simulation application based on the uploaded 'o3de-robot-vacuum-simulation' docker image

Click on 'Create' to create and launch the simulation job.

Figure 6 – The status will show 'Preparing' while the simulation job is being created and launched before you are able to take control of it.

View and control the new simulation job Once the status of the simulation job changes to 'Running', the 'Connect' buttons on each tool will be available to view and/or control the simulation job.

The 'Connect' for the Simulation application will launch the O3DE launcher that is running the simulation client.

The simulation client presents a 3rd person view of the Robot Vacuum that has been placed inside of a loft scenario. At this point, the robot is awaiting commands that will be sent to it using ROS 2. To control the robot vacuum, open the RViz 3D visualization tool which is provided as part of the ROS package. This is done by clicking 'Connect' for the Robot Application.

Through the RViz tool, you can specify points to direct the Robot Vacuum to navigate to. It will use its simulated Lidar sensor to best navigate to the desired point without colliding with any obstacle. This is done by clicking '2D Goal Pose' in RViz and specifying a point in the visualized map.

As the robot navigates the room, it maps its environment with its Lidar sensor which will also be visualized in RViz. In an autonomous robot vacuum scenario, a container with the actual automated robot code will replace the sample one in this demonstration to control the robot automatically. Cleaning up The container images created in this tutorial total around 20 gigabytes of data. If you no longer need this sample robot vacuum application, you can clear the following artifacts:

The o3de-robot-vacuum-navstack and o3de-robot-vacuum-simulation repositories in ECR The o3de-robot-vacuum-navstack robot application in AWS RoboMaker under 'Robot Applications' The o3de-robot-vacuum-simulation application in AWS RoboMaker under 'Simulation Applications'

Conclusion This concludes this tutorial on how to run a robotics simulation using O3DE in AWS RoboMaker. We walked through the steps required to prepare an O3DE container image with a robotics simulation application and executed that container in AWS RoboMaker after a few configuration steps. Now that you have completed running this sample, you can try to create your own robotic simulation project with O3DE and have it run on AWS RoboMaker. To learn more about O3DE, please visit to find out more information.

Steve Pham Steve Pham is a Senior Software Development Engineer for the Game Engine & Developer Services team at AWS. As part of the development team for O3DE for the past 7 years, he has been focused on its evolution from a windows-based 3D game engine into a broader, multi-platform 3D engine.

Matt Hansen Matt Hansen is a Principal Solutions Architect specializing in Robotics and Simulation at Amazon Web Services (AWS).

We are pleased to announce that AWS Serverless Application Model (AWS SAM) now supports building serverless GraphQL APIs with AWS AppSync. AWS SAM provides a short-hand syntax for defining AWS serverless resources. AWS AppSync enables developers to connect their applications to data and events with secure, serverless, and performant GraphQL and Pub/Sub APIs. GraphQL APIs allow developers to reduce IO latency by reducing round trips and constraining payloads to only the data needed. This blog post will demonstrate building a GraphQL API with the recently released AWS::Serverless::GraphQLApi resource. New GraphQLApi Resource With the AWS SAM GraphQLApi resource, you can declare everything needed for a GraphQL API with a single resource definition. Review the AWS::Serverless::GraphQLApi documentation for details on the resource type and its properties.

AWS::Serverless::GraphQLApi: The top-level resource, including properties for API keys, caching, custom domains, GraphQL schema, logging, and tracing. Auth: A property defining nested authorization details, including AWS Lambda authorizer, Amazon Cognito User Pool, or OIDC provider DataSource: A property defining nested datasource details like AWS Lambda or Amazon DynamoDB. Function: A property to configure code, runtime, and other pipeline function properties. Resolver: A property to configure code, runtime, pipeline function execution order, and other resolver properties.

SAM Application Walkthrough This post pairs with the pre-built demonstration application to create and view social posts. Reference the git repository README for detailed instructions on how to deploy and test the application. The following diagram provides a high-level AWS architecture of the sample application.

Figure 1: The example AWS AppSync application architecture Suppose you were to define this application with AWS CloudFormation. In that case, you'd need to configure distinct CloudFormation AppSync resources for the GraphQLApi, GraphQLSchema, ApiKey, DataSource including AWS Identity and Access Management (IAM) Role and Policy, three FunctionConfiguration(s), and two pipeline Resolver(s). Instead, with the new AWS SAM transform, you can define this application with a single AWS::Serverless::GraphQLApi resource and its inline properties. Prerequisites It's beneficial to have familiarity with GraphQL, JavaScript, and NoSQL . You will need the following:

Access to an AWS account with permissions to create CloudFormation stacks, AWS AppSync APIs, AWS IAM roles and policies, and DynamoDB tables. Install the AWS Serverless Application Model Command Line Interface (AWS SAM CLI) with version >= 1.88.0. Install a GIT client in your work environment. Install the curl client utility in your environment. Install Node.js in your environment (to run the subscription example).

Step 1: Clone the application and review the SAM template Review the template.yml file on GitHub or clone the application by executing the following commands in your CLI terminal to get started.

Read through the template, and you'll see that only two AWS SAM resources are defined, a GraphQL API and a DynamoDB table referenced as a datasource for API pipeline functions. AWS SAM simplifies your effort as a developer by taking care of the following undifferentiated heavy lifting for you:

Generating a well-scoped IAM policy and role for datasources to access DynamoDB. The createPostItem and getPostFromTable functions reference the DynamoDBPostsTable resource, but you don't have to define custom IAM resources. The GraphQLPostsAPI property SchemaUri references a relative path to the sam_graphql_api/schema.graphql file. When you deploy this template, AWS SAM CLI packages this schema for you, uploads it to S3, and then fetches the file from S3 when you deploy your template. Revisions to the file are versioned in S3. The GraphQLPostsAPI Functions dictionary defines named functions like createPostItem with a CodeUri property that references a relative path to the sam_graphql_api/createPostItem.js function code. AWS SAM CLI takes care of packaging, versioning, and uploading function files to S3 No code is defined for pipeline Resolvers mutation and query, addPost, and getPost. AWS SAM generates the pipeline resolver JS code for you, corresponding to the Runtime you define as APPSYNC_JS. This API uses a simple API Key for authorization, an easy Auth type for starting a new GraphQL API. The Auth and API Key definition only takes two lines of configuration each. AWS SAM generates these with default values so you can start testing your API immediately.

Step 2: Installing the Application The next step is to deploy this application to your AWS account so that you can explore the resources it creates and interact with the GraphQL API. Run the following command and accept all the defaults.

As AWS SAM deploys your application, it will first upload your GraphQL schema and functions to Amazon S3. Then it makes an updated copy of your template so that the code URIs reference the Amazon S3 location. Finally, AWS SAM executes your template to create resources in your AWS account, like the following screenshot. Take note of the region where you installed this application.

Figure 2: Output list of AWS resources created from the AWS SAM deploy command Step 3: Testing the Application When your deployment completes, you'll see two outputs printed to your console, providing both the application's GraphQL API endpoint and API key. Save these, as you'll need both values to test your API in the following steps.

Figure 3: CloudFormation outputs APIEndpoint and ApiKeyValue for use in testing Now you can run the following curl commands to create and retrieve a post. You'll need to use the API Endpoint and API Key captured from SAM CLI output values. Copy and run the command below, replacing placeholder angle brackets with your output values.

curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"Anonymous\", content: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\", title: \"A simple post\") { author content id } }" }' \

Capture the value of the post id field from the response output and replace the POST_ID placeholder, then issue the following command to retrieve your post details.

curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "{ getPost(id: \" \") { id author title content version ups downs } }" }' \

Step 4: Testing the subscription To test a subscription on your API, you can update the src/exports.js file with your API endpoint, API key, and the region where you installed your template.

const awsmobile = { aws_project_region: " ", // region from `sam deploy` aws_appsync_graphqlEndpoint: " ", aws_appsync_region: " ", aws_appsync_authenticationType: "API_KEY", aws_appsync_apiKey: " ", }; export default awsmobile;

Once you've updated the src/exports.js file, you can run the following commands to start a web server that hosts a web page subscribed to your API

Once the web application runs locally, open the address in a browser. Next, execute some AddPost mutations against your API so that you can see them displayed on the single-page application in real-time. Again, you'll need to replace the placeholders with your API endpoints and key before you run the commands. For this step, open a second terminal; the original terminal you opened needs to continue running the web server.

curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"AWS\", content: \"Simplifies serverless resource creation\", title: \"SAM\") { author title content id ups downs version} }" }' \ curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"AWS\", content: \"Awesome managed GraphQL\", title: \"AppSync\") { author title content id ups downs version} }" }' \ curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"AWS\", content: \"Speedy noSQL\", title: \"DynamoDB\") { author title content id ups downs version} }" }' \

If everything worked as expected, you'll see a page similar to the following.

Figure 4: Real time subscrition result. Step 5: Cleanup To avoid accruing charges for the resources you created following the instructions in this blog post, use the following command to delete your application.

Conclusion This post reviewed the new AWS SAM features to quickly build and deploy serverless GraphQL applications with AWS AppSync. We highlighted time-saving shortcuts like generated IAM policies for datasources and file packaging to improve the developer experience for the schema file and functions. The AWS SAM resource AWS::Serverless::GraphQLApi helps developers quickly build serverless GraphQL applications. AWS AppSync provides a managed GraphQL runtime to access data or events from one or more data sources with a single API and create engaging real-time experiences by publishing data from any event source to subscribed clients with built-in security, monitoring, caching, logging, and tracing. For more information, reference the AWS Serverless Application Model (AWS SAM) and AWS AppSync documentation. For more learning resources, visit and

Kevin Schwarz Kevin is Sr. Solutions Architect. Kevin brings over 20 years of technical experience building business capabilities with depth in financial services, serverless, resiliency and IoT. Outside of work, Kevin enjoys spending time with his wife and kids, running, working out, and gardening.

Navigating the investment landscape can be challenging for anyone responsible for managing pension funds. Leaders must keep up with the latest investment trends and opportunities, all while supporting the needs of stakeholders. However, traditional investment processes can be slow and outdated, posing obstacles when it comes to managing large sums of money on behalf of workers. What if there was a transformative solution for managing pension funds? Amazon Web Services (AWS) offers personalized investment solutions for investment firms using cutting-edge machine learning (ML) technologies. With AWS, pension fund managers can benefit from improved risk management through a highly reliable cloud infrastructure and operational support. Additionally, AWS supports 143 security standards and compliance certifications, more than any other offering, which can help firms in protecting sensitive data, meeting compliance requirements, maintaining business continuity, and more. These factors can help firms minimize financial risks and maximize investment opportunities. By utilizing cloud-based services, pension managers can harness advanced data analytics, gaining valuable insights that inform investment strategies. This empowers leaders to make well-informed decisions that align with the needs of stakeholders. Pension fund management from past to present The history of the pension fund industry dates back centuries, but the modern pension fund industry can be traced back to the 19th century when pension plans were established for government employees in the United States and Europe. Today, the pension fund industry is a global industry with trillions of dollars in assets under management. Before the advent of cloud technology, pension fund investment firms operated through manual processes involving a lot of paperwork, phone calls, and in-person meetings. Investment firms conducted research and analysis to identify investment opportunities; traders executed trades through brokers and trading floors; portfolio managers decided which assets to buy and sell; and reports were generally provided through paper statements or in-person meetings. As the pension fund industry has evolved over the centuries, so too has the technology used to manage and invest vast amounts of money. In the past, investment firms relied on manual processes and paper-based reporting to make informed decisions about which assets to buy and sell. However, the challenges facing pension funds today need a more sophisticated approach, one that uses data analytics and cloud technology to maximize returns while minimizing risk. How can cloud technology help pension fund managers? Pension funds manage and analyze vast amounts of data, such as investment data, member data, and regulatory data. They need to use this data to make informed investment decisions, manage risks, and comply with regulatory requirements. AWS provides a suite of data analytics tools, such as Amazon Redshift, Amazon EMR, Amazon Glue, and Amazon QuickSight, which enable pension fund managers to process and analyze large volumes of data quickly and simply. These tools can also help identify patterns and trends that are not readily noticeable to human analysts, allowing pension fund managers to make more informed investment decisions. The National Bank of Canada's Global Equity Derivatives Group (GED) used AWS to process and analyze hundreds of terabytes of financial data, conduct data manipulations in one minute instead of days, and scale and optimize its operations. Pension funds must also manage risk. The complexity of financial instruments has made it difficult to effectively manage different types of risks, such as market risk, credit risk, and operational risk. High performance computing (HPC) on AWS offers cloud-based services that provide access to HPC resources such as networking, storage, and compute on demand. By utilizing HPC, pension funds can perform complex risk simulations and stress tests to manage and mitigate risk more efficiently without the need to manage and maintain the on-premises infrastructure. For example, the Bank of Montreal (BMO) used AWS to build an elastic platform for calculating risk metrics, run multiple risk metrics and regulatory stress calculations in parallel, and scale computational capacity for future regulatory requirements. Using HPC on AWS, pension funds can analyze vast amounts of data, run simulations and models, and identify potential risks and opportunities more accurately. Additionally, HPC can reduce the time needed for complex calculations, resulting in faster decision-making and shorter time-to-market for new investment products. Through HPC on AWS, pension funds can simulate different market scenarios and evaluate their impact on the fund's portfolio. Cybersecurity is another significant challenge for the pension fund industry. Pension funds deal with vast amounts of sensitive data, including personal information of members and investment data. As cyber threats become more sophisticated, pension funds must have robust cybersecurity measures in place. AWS offers various cloud-based services that can help pension funds strengthen their cybersecurity strategies. These services include Amazon GuardDuty, AWS Security Hub, AWS Identity and Access Management (IAM), and Amazon Inspector. GuardDuty and Security Hub can provide continuous monitoring of the pension fund's AWS environment for threats and vulnerabilities, while IAM allows administrators to manage user access to resources securely. Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. Finally, pension funds must meet increasing demand for personalized investment solutions. Members want more control over their retirement savings, and they expect pension funds to provide tailored investment solutions that meet their individual needs. AWS provides a suite of services, such as Amazon Bedrock, Amazon SageMaker, and Amazon Personalize, which can help pension funds develop personalized investment solutions for their members. These technologies use machine learning (ML) and artificial intelligence (AI) to provide investment advice based on an individual's goals, risk tolerance, and financial situation. What's next for pension fund management in the cloud? To embrace the benefits of cloud technology, pension fund managers can assess the following strategies for using the cloud:

Establish a digital transformation strategy – Develop a comprehensive plan for using technology to improve operations, reduce costs, and enhance the customer experience. Invest in data analytics tools – Implement tools such as AI and ML to help manage and analyze data more efficiently and accurately. Prioritize cybersecurity – Make sure that robust cybersecurity measures are in place to protect sensitive member and investment data. Embrace cloud technology – Develop a cloud strategy that aligns with business goals and objectives and assess the suitability of cloud providers. Customers can use the AWS Cloud Adoption Framework (AWS CAF) to identify their cloud enabled strategic business outcomes. Provide personalized investment solutions – Invest in robo-advisory technologies to provide personalized investment advice to members. You can build a conversational interface such as chatbots and virtual assistants to provide personalized experience for your customers using Amazon Bedrock, a fully managed service that makes foundational models (FMs) from leading AI startups accessible via an API. Foster a culture of innovation – Encourage employees to experiment with new technologies and approaches and provide resources and support to help them do so. AWS Skill Builder provides resources and training to help customers acquire the necessary skills and knowledge to build and deploy applications on the AWS. It offers a range of learning paths, hands-on labs, and self-paced courses designed to enhance proficiency in AWS services, architectures, and best practices.

Conclusion As the pension fund industry evolves to meet the dynamic needs of their members, technology can help address some of these emerging needs. AI, ML, advanced analytics, and robo-advisory technologies can help pension funds manage data, mitigate risk, improve cybersecurity, and provide personalized investment solutions. As the pension funds industry continues to evolve, pension fund managers can embrace technology to provide better retirement savings solutions for their members. Related resources:

AWS for Capital Markets hub AWS Financial Services Competency Partners AWS Solutions Library: Financial data management solutions AWS Solutions Library: Risk management platforms

TAGS: cybersecurity, digital transformation, federal financial, financial services, public sector

John Hakim John Hakim is a senior solutions architect on the Canadian public sector financial services team at Amazon Web Services (AWS). He develops technical strategies that empower his financial services clients to develop quickly and make the most of cloud economics. With John's guidance, clients can seamlessly implement new cloud-based systems or migrate existing on-premises workloads. John makes sure his clients stay competitive in this fast-paced industry by providing guidance and support every step of the way.

Integrating IBM MQ with Amazon SQS and Amazon SNS using Apache Camel

This post is written by Joaquin Rinaudo, Principal Security Consultant and Gezim Musliaj, DevOps Consultant. IBM MQ is a message-oriented middleware (MOM) product used by many enterprise organizations, including global banks, airlines, and healthcare and insurance companies. Customers often ask us for guidance on how they can integrate their existing on-premises MOM systems with new applications running in the cloud. They're looking for a cost-effective, scalable and low-effort solution that enables them to send and receive messages from their cloud applications to these messaging systems. This blog post shows how to set up a bi-directional bridge from on-premises IBM MQ to Amazon MQ, Amazon Simple Queue Service (Amazon SQS), and Amazon Simple Notification Service (Amazon SNS). This allows your producer and consumer applications to integrate using fully managed AWS messaging services and Apache Camel. Learn how to deploy such a solution and how to test the running integration using SNS, SQS, and a demo IBM MQ cluster environment running on Amazon Elastic Container Service (ECS) with AWS Fargate. This solution can also be used as part of a step-by-step migration using the approach described in the blog post Migrating from IBM MQ to Amazon MQ using a phased approach. Solution overview The integration consists of an Apache Camel broker cluster that bi-directionally integrates an IBM MQ system and target systems, such as Amazon MQ running ActiveMQ, SNS topics, or SQS queues. In the following example, AWS services, in this case AWS Lambda and SQS, receive messages published to IBM MQ via an SNS topic:

The cloud message consumers (Lambda and SQS) subscribe to the solution's target SNS topic. The Apache Camel broker connects to IBM MQ using secrets stored in AWS Secrets Manager and reads new messages from the queue using IBM MQ's Java library. Only IBM MQ messages are supported as a source. The Apache Camel broker publishes these new messages to the target SNS topic. It uses the Amazon SNS Extended Client Library for Java to store any messages larger than 256 KB in an Amazon Simple Storage Service (Amazon S3) bucket. Apache Camel stores any message that cannot be delivered to SNS after two retries in an S3 dead letter queue bucket.

The next diagram demonstrates how the solution sends messages back from an SQS queue to IBM MQ:

A sample message producer using Lambda sends messages to an SQS queue. It uses the Amazon SQS Extended Client Library for Java to send messages larger than 256 KB. The Apache Camel broker receives the messages published to SQS, using the SQS Extended Client Library if needed. The Apache Camel broker sends the message to the IBM MQ target queue. As before, the broker stores messages that cannot be delivered to IBM MQ in the S3 dead letter queue bucket.

Deploy the broker service to allow reading messages from and writing to existing IBM MQ queues. Once the consumer or producer is migrated, migrate its counterpart to the newly selected service (SNS or SQS).

Next, you will learn how to set up the solution using the AWS Cloud Development Kit (AWS CDK). Deploying the solution Prerequisites

Step 1: Cloning the repository Clone the repository using git: git clone Step 2: Setting up test IBM MQ credentials This demo uses IBM MQ's mutual TLS authentication. To do this, you must generate X.509 certificates and store them in AWS Secrets Manager by running the following commands in the app folder:

Generate X.509 certificates: ./deploy.sh generate_secretsBash Set up the secrets required for the Apache Camel broker (replace with, for example, dev): ./deploy.sh create_secrets broker Bash Set up secrets for the mock IBM MQ system: ./deploy.sh create_secrets mockBash Update the cdk.json file with the secrets ARN output from the previous commands:

If you are using your own IBM MQ system and already have X.509 certificates available, you can use the script to upload those certificates to AWS Secrets Manager after running the script. Step 3: Configuring the broker The solution deploys two brokers, one to read messages from the test IBM MQ system and one to send messages back. A separate Apache Camel cluster is used per integration to support better use of Auto Scaling functionality and to avoid issues across different integration operations (consuming and reading messages). Update the cdk.json file with the following values:

accountId: AWS account ID to deploy the solution to. region: name of the AWS Region to deploy the solution to. defaultVPCId: specify a VPC ID for an existing VPC in the AWS account where the broker and mock are deployed. allowedPrincipals: add your account ARN (e.g., arn:aws:iam::123456789012:root) to allow this AWS account to send messages to and receive messages from the broker. You can use this parameter to set up cross-account relationships for both SQS and SNS integrations and support multiple consumers and producers.

Make sure you have the correct AWS_PROFILE and AWS_REGION environment variables set for your development account. Run yarn cdk bootstrap –-qualifier mq / to bootstrap CDK. Run yarn install to install CDK dependencies. Finally, execute yarn cdk deploy '*-dev' –-qualifier mq --require-approval never to deploy the solution to the dev environment.

Step 5: Testing the integrations Use AWS System Manager Session Manager and port forwarding to establish tunnels to the test IBM MQ instance to access the web console and send messages manually. For more information on port forwarding, see Amazon EC2 instance port forwarding with AWS System Manager.

In a command line terminal, make sure you have the correct AWS_PROFILE and AWS_REGION environment variables set for your development account. In addition, set the following environment variables:

IBM_ENDPOINT: endpoint for IBM MQ. Example: network load balancer for IBM mock mqmoc-mqada-1234567890.elb.eu-west-1.amazonaws.com. BASTION_ID: instance ID for the bastion host. You can retrieve this output from Step 4: Bootstrapping and deploying the solution listed after the mqBastionStack deployment. Use the following command to set the environment variables: export IBM_ENDPOINT=mqmoc-mqada-1234567890.elb.eu-west-1.amazonaws.com export BASTION_ID=i-0a1b2c3d4e5f67890Bash Run the script test/connect.sh. Log in to the IBM web console via using the default IBM user (admin) and the password stored in AWS Secrets Manager as mqAdapterIbmMockAdminPassword.

In the IBM MQ console, access the local queue manager QM1 and DEV.QUEUE.1. Send a message with the content Hello AWS. This message will be processed by AWS Fargate and published to SNS. Access the SQS console and choose the snsIntegrationStack-dev-2 prefix queue. This is an SQS queue subscribed to the SNS topic for testing. Select Send and receive message. Select Poll for messages to see the Hello AWS message previously sent to IBM MQ.

Access the SQS console and choose the queue with the prefix sqsPublishIntegrationStack-dev-3-dev. Select Send and receive messages. For Message Body, add Hello from AWS. Choose Send message. In the IBM MQ console, access the local queue manager QM1 and DEV.QUEUE.2 to find your message listed under this queue.

Step 6: Cleaning up Run cdk destroy '*-dev' to destroy the resources deployed as part of this walkthrough. Conclusion In this blog, you learned how you can exchange messages between IBM MQ and your cloud applications using Amazon SQS and Amazon SNS. If you're interested in getting started with your own integration, follow the README file in the GitHub repository. If you're migrating existing applications using industry-standard APIs and protocols such as JMS, NMS, or AMQP 1.0, consider integrating with Amazon MQ using the steps provided in the repository. If you're interested in running Apache Camel in Kubernetes, you can also adapt the architecture to use Apache Camel K instead. For more serverless learning resources, visit Serverless Land.

This post was contributed by Laurent Fabre, Chief Technology Officer and his team from Reezocar, and Ilan Gleiser, Pr. Specialist, Global Impact Computing, AWS and our team. Computer vision has enormous potential to revolutionize the way we think about sustainability and the circular economy. One of the key ways in which computer vision is already being applied to these areas is using powerful graphics processing units (GPUs) and AWS cloud computing services. One of the primary applications of computer vision in the circular economy is in product lifecycles. By using advanced computer vision algorithms and machine learning models, manufacturers and supply chain managers can more accurately track and analyze the lifecycle of products, from raw materials sourcing all the way through to end-of-life disposal. This allows companies to better understand the environmental impacts of their products and make more informed decisions about how to design and produce products that are more sustainable and can be easily reused or recycled. Another area in which computer vision is being applied to the circular economy is in waste reduction. By using advanced image recognition algorithms and computer vision technologies, waste management companies can better sort and categorize recyclable materials, making it easier and more cost-effective to recover and reuse valuable resources. In addition to these applications, computer vision is also being used to identify opportunities for energy and resource savings, and to monitor and analyze environmental impacts at both a local and global scale. For example, satellite imagery and other forms of remote sensing data can be used to track deforestation, monitor ocean pollution levels, and even create early warning systems to predict the impact of natural disasters on ecosystems. In this post, we'll explore how Reezocar is using computer vision to change the way they detect car damage and price used vehicles for re-sale in secondary markets. This technology can be used to determine the useful life of a car and potentially reduce the need for landfill waste, therefore aligning with the goals of the circular economy: designing-out waste and pollution from the environment. Secondary markets play a critical role in achieving a circular economy by extending the lifespan of products and reducing waste. The benefits of secondary markets go beyond environmental impact, as they also generate economic opportunities, job creation, and community empowerment. In this context, computer vision is revolutionizing the way we detect car damage. By leveraging the power of machine learning, computer vision can detect even the smallest dents and scratches on a car's body. Overall, the applications of computer vision in the circular economy are wide-ranging and growing rapidly. By enabling more efficient and sustainable resource use, computer vision has the power to drive real change in our economy and help create a more sustainable future for us all. Who is Reezocar? Reezocar is an online platform for buying and selling used cars in France. It offers customers a safe purchase guarantee and a bespoke shopping help service. With access to over six-million car ads, Reezocar makes it easy to select makes and models that match a buyer's search criteria. Customers also have the option of purchasing a certified car, with a 15-day money back guarantee included. Reezocar uses GPU-accelerated machine learning algorithms, convolutional neural networks, and a damage estimation model to calculate car prices. This system helps customers get the best deal on their purchase, as well as an accurate assessment of the vehicle's condition. The result is a fair price estimate of the car and reassurance that they're getting a good value for their money. More than 10,000 customers have already been won over by the Reezocar experience, making it one of the top on-line marketplaces for buying and selling used cars. With its reliable customer service and advanced technology, Reezocar is quickly becoming the go-to destination for those looking to purchase a used car. Reezocar is committed to its environmental mission and employs various strategies to uphold its climate-conscious values. Here are four key approaches they employ:

Extending Vehicle Lifespan: By refurbishing selected vehicles, they have successfully prolonged their lifespan by up to five years, using approximately 5% of their manufacturer's suggested retail price (MSRP) as refurbishing budget. This initiative has prevented over 30,000 tons of waste from ending up in landfills. Carbon Offset Initiatives: To counterbalance the CO2 emissions produced by these vehicles, they consistently engage in carbon offset practices through their partnership with ReforestAction. Through this collaboration, they have planted 33,000 trees, effectively trapping an estimated 5,000 tons of CO2. Discouraging New Car Production: Recognizing that the most environmentally friendly car is the one that doesn't need to be built, they actively discourage the production of new cars by prioritizing the refurbishment of used vehicles. This approach has resulted in an estimated avoidance of 40,000 tons of material, considering the increasing weight of modern cars. Efficient Supply Chain: By advocating for a shorter supply chain, they significantly reduce energy consumption during the delivery of vehicles. Additionally, this approach helps them avoid the costs associated with waste management while minimizing the pollution caused by car-related materials such as plastics, thus safeguarding the surrounding ecosystem. With a decade of experience and numerous satisfied customers, Reezocar remains dedicated to leading the way in climate action.

The rise of computer vision For decades, car buyers and dealers have relied on manual inspection to detect body damage on used cars. Until recently, physical inspections were the only way to get a full picture of the car's condition. Even then, most damage could only be detected with close examination from a trained eye. The technology leverages a combination of machine learning algorithms and convolutional neural networks to detect dents in vehicles, resulting in a faster, more streamlined, and accurate process. Initially, the technology employs machine learning algorithms to estimate an initial value of the car based on features such as its model and gearbox type (automatic vs. standard). Next, Computer vision is used to process the car's image and identify any form of damage, like scratches, dents, or severe degradation. Any detected damage will be factored into the initial value estimate by subtracting the repair costs. The proposed computer vision system determines the specific parts of the car that are damaged, which helps identify the necessary repairs and their costs. After computing the repair cost, Reezocar generates a post-repair estimate of the car's value. This estimate considers factors like age, condition, and make to ensure maximum accuracy. With these tools at their disposal, dealerships can offer more competitive pricing and decide whether a car should be refurbished or sent to the landfill. Reezocar reference architecture

Reezocar's Reference architecture aims to detect car dents using computer vision, as illustrated in Figure 1. Their approach consists of three steps: acquiring a dataset, fine tuning a convolutional neural network, and measuring the success of the model. First, they acquired and combined a synthetic and a real-world dataset to ensure a diverse and robust sample dataset. Using Amazon EC2 instances, they generated a synthetic dataset by algorithmically deforming 3D CAD models of cars. This helped to create a labeled synthetic dataset of images. Afterwards, they used Amazon SageMaker Ground Truth to annotate a set of real-world images. To achieve this, Reezocar uses Amazon SageMaker Ground Truth to acquire and annotate data, and then employs machine learning models running on GPUs to train a damage estimation model. By using Amazon SageMaker Ground Truth, Reezocar was able to avoid the need to manage their own data labeling workforce, which would have slowed down innovation and increased costs. This approach allowed Reezocar to focus on their core competencies and optimize their resources for maximum efficiency. Second, they leveraged GPUs to fine tune a convolutional neural network (CNN) such as Mask R-CNN or Detectron to do object segmentation and damage detection. This CNN model is used to segment car parts and detect car dents. Reezocar adopted a gradual approach to tackle the dent detection problem. They aimed to ensure that their system could effectively differentiate between damaged and undamaged cars. Once they were satisfied with the performance of the initial model, they submitted images with low confidence for undamaged cars and images of damaged cars to Amazon SageMaker Ground Truth to obtain around 4000 more detailed and accurate annotations, which indicate which parts are damaged. This annotated dataset was then used to train an object detection model that could accurately identify the damaged parts of a car. Reezocar uses AWS Batch to run inference and augment their data with damage information. Reezocar triggers AWS Batch conditionally by leveraging AWS Step Functions and an Amazon SQS queue. The Step Functions workflow showcased in Reference 1 checks for the arrival of a certain number of events related to the car damage data in the SQS queue. Once the required number of events is present, AWS Step Functions triggers AWS Batch to process the data using the proposed computer vision algorithm on P4d instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of the batch jobs submitted. P4d instances are a specific type of Amazon EC2 instance family that is optimized for high-performance computing and includes powerful NVIDIA GPUs. These instances provide the necessary computational power for running machine learning models and computer vision algorithms efficiently, making them suitable for Reezocar's car damage detection system. This combination of Step Functions, SQS, and AWS Batch allows Reezocar to efficiently process the data. Evaluating the model is in the eye of the beholder Finally, to evaluate the performance of their dent detection models, Reezocar used a combination of accuracy and mean Average Precision (mAP) metrics. The accuracy metric was used to assess the classifier's performance, while the mAP metric was used to evaluate the object detection model's performance. They set a target accuracy of 0.85 and a target mAP of 0.9 to ensure high levels of accuracy and reliability in their results. These metrics allowed Reezocar to measure the success of their object detection model and assess the quality of the system's output. The process is illustrated in Figure 2.

Figure 2: The damage detection system developed by Reezocar combines the generation of a synthetic dataset with real-world data labeling to ensure a diverse and robust dataset. The system is designed to detect relevant images that require labeling, which contributes to the overall efficiency and effectiveness of the damage detection system.

Reezocar's objective is to determine the refurbishing cost of cars, which involves an additional step to detect the severity of the damage in parts. The severity assessment process is illustrated in Figure 3, and the severity estimation is used to determine the repair cost of the damaged part. Table 1 shows that the repair cost estimation method involves applying a percentage of the initial price of the part, which varies according to the severity of the damage. In other words, different percentages are used based on the severity of the damage to estimate the repair cost accurately.

Figure 3: Damage severity assessment pipeline. The severity assessment is based on the normal consistency of the damaged part.

Table 1: Repair cost. Reezocar uses a percentage of the initial price of the damaged part to estimate the repair cost. The repair cost estimation process involves applying different percentages based on the severity of the damage. Through the implementation of their damage detection pipeline, Reezocar has achieved a high degree of automation with a precision rate of 86.7%. However, due to challenging light conditions and reflective materials, some cars are being mislabeled as damaged. Reezocar has identified a mislabeling rate of 15.7%, and are currently labeling data in order to retrain the damage detection model. The team was able to achieve a notable performance improvement by using Amazon Sagemaker Ground Truth to obtain 4000 labeled images, resulting in an increase in their mean Average Precision (MAP) from 0.22 to 0.3. This success has validated the effectiveness of their data-labeling pipeline and has motivated the company to pursue further data acquisition for labeling to enhance the performance of their system. As part of this effort, Reezocar is actively collecting images of damaged cars and labeling them to reach their goal of a MAP of 0.9. In the figures below, we see some qualitative results of the proposed damage detection model. For example, in Figure 4, an image of a car with damaged parts is displayed, and the proposed model accurately detects and segments the damaged parts, as shown in the second row, first column. Additionally, they used a car-part segmentation model (See Row 1; Column 3 of Figure 4) to identify which specific part is damaged.

Figure 5 shows an image of an undamaged car, while Figure 6 showcases the performance of the proposed method on scratched parts of a car, providing a closer look at the results.

Conclusion The use of computer vision to detect car dents is quickly becoming a game-changer in the automotive industry. Machine learning and convolutional neural networks are allowing for a more accurate detection of car dents than ever before, leading to improved repair and maintenance processes. Companies like Reezocar are leveraging computer vision-based damage estimation models to accurately and efficiently calculate car prices. Thanks to these models, Reezocar is now able to calculate the car price with a Mean Absolute Percentage (MAP) error of just 6%, improving the accuracy and efficiency of their pricing process. This technology is not only helping to extend the life of cars but also to reduce landfill waste. It's also helping to make the car buying process simpler and more transparent. As computer vision technology continues to improve, the way we detect and repair car dents will likely change as well. By combining the power of Amazon EC2 instances, NVIDIA GPUs, AWS Batch linked by Elastic Fabric Adapter (EFA), and machine learning powered computer vision models, Reezocar can accurately detect car dents, ultimately extending the lifespan of selected cars by up to 5 years. By refurbishing these cars and reselling them in the secondary market, Reezocar helps reduce landfill waste and promote circularity.

Laurent Fabre Laurent Fabre is the CTO of Reezocar, a leading online platform for buying and selling used cars. Laurent brings over two decades of experience in the IT industry. He is a highly skilled Cybersecurity expert, having trained at Airbus, where he developed expertise in cryptography and secure programming techniques. He also studied mathematics, with a focus on operations research, which has enabled him to leverage data analytics and metrics to solve complex problems and achieve optimal solutions. Throughout his career, Laurent has taken on numerous mission-critical tasks for both civilian and military-grade projects, often on short notice. Outside of work, Laurent indulges his passion for interpretive dancing of quantum physics, which allows him to combine his love of science and art.

Atef Shaar Atef Shaar is currently a Lead Data at Reezocar. He is developing new machine-learning-based applications for the automobile industry. Additionally, he is managing a team of data engineers and data scientists. Prior to this work, Atef Shaar worked as a Research Engineer at Télécom Paris. He graduated with a Ph.D. degree from Télécom Paris in 2018. He was a visiting research student at the National University of Singapore in 2015. He completed a master's study in International Business at Grenoble Graduate School of Business, after earning an engineering degree. His research work is related to machine learning and its application in multiple domains including marketing, distributed storage systems, and the automotive industry.

Julien Maksoud Julien Maksoud is a highly skilled engineer with over a decade of experience in the Oil & Gas industry. As a consultant, he supported various companies in Europe and Africa in analyzing their data using machine learning-based methods, providing valuable insights to decision-makers. Julien's passion for solving complex problems using machine learning led him to pursue an advanced Master's degree from Télécom Paris, which equipped him with a deeper understanding of data science, including machine learning and deep learning frameworks. Currently, Julien is working as a Data Engineer at Reezocar, where he leverages his expertise to create data pipelines on AWS for ETL and data analysis. His experience in both data engineering and data science enables him to assist with model deployment and maintenance as well. Julien is committed to delivering results that enable organizations to make informed decisions based on data-driven insights.

Tarek Ben Charrada Tarek Ben Charrada graduated with distinction, ranking 28th out of over 2800 candidates in the highly competitive "concours d'entrée aux grandes écoles" to go and earn his engineering degree from Ecole polytechnique de Tunisie. He then pursued his PhD in 3D reconstruction from a single image, demonstrating his commitment to advancing the field of computer vision and computer graphics. Today, Tarek's expertise is sought-after as a data scientist, where he leverages his skills in computer vision, speech processing and differentiable privacy to push the boundaries of what's possible in these exciting fields.

TAGS: AWS Batch, AWS ParallelCluster, HPC, Machine Learning, ML, simulations, Sustainability

Ilan Gleiser Ilan Gleiser is a Principal GenAI Specialist at AWS WWSO Frameworks team focusing on developing scalable Artificial General Intelligence architectures and optimizing foundation model training and inference. With a rich background in AI and machine learning, Ilan has published over 20 blogs and delivered 100+ prototypes globally over the last 5 years. Ilan holds a Master's degree in mathematical economics.

Francis Laurens Francis Laurens is a Senior Solutions Architect at AWS focusing on helping customers to find out the right architecture for their needs. He has a strong startup background having worked for top French Startups. During his time there, his technical proficiency in data topics helped out those startups on their path to success. He has started to use AWS in production 10 years ago when he was leading one of the first migration to Amazon Redshift in Europe.

Intelligent video and audio Q&A with multilingual support using LLMs on Amazon SageMaker

Digital assets are vital visual representations of products, services, culture, and brand identity for businesses in an increasingly digital world. Digital assets, together with recorded user behavior, can facilitate customer engagement by offering interactive and personalized experiences, allowing companies to connect with their target audience on a deeper level. Efficiently discovering and searching for specific content within digital assets is crucial for businesses to optimize workflows, streamline collaboration, and deliver relevant content to the right audience. According to a study, by 2021, videos already make up 81% of all consumer internet traffic. This observation comes as no surprise because video and audio are powerful mediums offering more immersive experiences and naturally engages target audiences on a higher emotional level. As companies accumulate large volumes of digital assets, it becomes more challenging to organize and manage them effectively to maximize their value. Traditionally, companies attach metadata, such as keywords, titles, and descriptions, to these digital assets to facilitate search and retrieval of relevant content. But this requires a well-designed digital asset management system and additional efforts to store these assets in the first place. In reality, most of the digital assets lack informative metadata that enables efficient content search. Additionally, you often need to do an analysis of different segments of the whole file and discover the concepts that are covered there. This is time consuming and requires a lot of manual effort. Generative AI, particularly in the realm of natural language processing and understanding (NLP and NLU), has revolutionized the way we comprehend and analyze text, enabling us to gain deeper insights efficiently and at scale. The advancements in large language models (LLMs) have led to richer representations of texts, which provides better search capabilities for digital assets. Retrieval Augmented Generation (RAG), built on top of LLMs and advanced prompt techniques, is a popular approach to provide more accurate answers based on information hidden in the enterprise digital asset store. By taking advantage of embedding models of LLMs, and powerful indexers and retrievers, RAG can comprehend and process spoken or written queries and quickly find the most relevant information in the knowledge base. Previous studies have shown how RAG can be applied to provide a Q&A solution connecting with an enterprise's private domain knowledge. However, among all types of digital assets, video and audio assets are the most common and important. The RAG-based video/audio question answering solution can potentially solve business problems of locating training and reference materials that are in the form of non-text content. With limited tags or metadata associated of these assets, the solution is trying to make users interact with the chatbot and get answers to their queries, which could be links to specific video training ("I need link to Amazon S3 data storage training") links to documents ("I need link to learn about machine learning"), or questions that were covered in the videos ("Tell me how to create an S3 bucket"). The response from the chatbot will be able to directly answer the question and also include the links to the source videos with the specific timestamp of the contents that are most relevant to the user's request. In this post, we demonstrate how to use the power of RAG in building a Q&A solution for video and audio assets on Amazon SageMaker. Solution overview The following diagram illustrates the solution architecture.

Convert video to text with a speech-to-text model and text alignment with videos and organization. We store the data in Amazon Simple Storage Service (Amazon S3). Enable intelligent video search using a RAG approach with LLMs and LangChain. Users can get answers generated by LLMs and relevant sources with timestamps. Build a multi-functional chatbot using LLMs with SageMaker, where the two aforementioned solutions are wrapped and deployed.

For a detailed implementation, refer to the GitHub repo. Prerequisites You need an AWS account with an AWS Identity and Access Management (IAM) role with permissions to manage resources created as part of the solution. For details, refer to create an AWS account. If this is your first time working with Amazon SageMaker Studio, you first need to create a SageMaker domain. Additionally, you may need to request a service quota increase for the corresponding SageMaker processing and hosting instances. For preprocessing the video data, we use an ml.p3.2xlarge SageMaker processing instance. For hosting Falcon-40B, we use an ml.g5.12xlarge SageMaker hosting instance. Convert video to text with a speech-to-text model and sentence embedding model To be able to search through video or audio digital assets and provide contextual information from videos to LLMs, we need to convert all the media content to text and then follow the general approaches in NLP to process the text data. To make our solution more flexible to handle different scenarios, we provide the following options for this task:

Amazon Transcribe and Amazon Translate – If each video and audio file only contains one language, we highly recommend that you choose Amazon Transcribe, which is an AWS managed service to transcribe audio and video files. If you need to translate them into the same language, Amazon Translate is another AWS managed service, which supports multilingual translation. Whisper – In real-world use cases, video data may include multiple languages, such as foreign language learning videos. Whisper is a multitasking speech recognition model that can perform multilingual speech recognition, speech translation, and language identification. You can use a Whisper model to detect and transcribe different languages on video data, and then translate all the different languages into one language. It's important for most RAG solutions to run on the knowledge base with the same language. Even though OpenAI provides the Whisper API, for this post, we use the Whisper model from Hugging Face.

We run this task with an Amazon SageMaker Processing job on existing data. You can refer to data_preparation.ipynb for the details of how to run this task. Convert video data to audio data Because Amazon Transcribe can handle both video and audio data and the Whisper model can only accept audio data, to make both options work, we need to convert video data to audio data. In the following code, we use VideoFileClip from the library moviepy to run this job: from moviepy.editor import VideoFileClip

video = VideoFileClip(video_path) video.audio.write_audiofile(audio_path)Python Transcribe audio data When the audio data is ready, we can choose from our two transcribing options. You can choose the optimal option based on your own use case with the criteria we mentioned earlier. Option 1: Amazon Transcribe and Amazon Translate The first option is to use Amazon AI services, such as Amazon Transcribe and Amazon Translate, to get the transcriptions of the video and audio datasets. You can refer to the following GitHub example when choosing this option. Option 2: Whisper A Whisper model can handle audio data up to 30 seconds in duration. To handle large audio data, we adopt transformers.pipeline to run inference with Whisper. When searching relevant video clips or generating contents with RAG, timestamps for the relevant clips are the important references. Therefore, we turn return_timestamps on to get outputs with timestamps. By setting the parameter language in generate_kwargs, all the different languages in one video file are transcribed and translated into the same language. stride_length_s is the length of stride on the left and right of each chunk. With this parameter, we can make the Whisper model see more context when doing inference on each chunk, which will lead to a more accurate result. See the following code: from transformers import pipeline import torch

device = "cuda:0" if torch.cuda.is_available() else "cpu" pipe = pipeline( "automatic-speech-recognition", model=f"openai/{whisper_model}", device=device )

generate_kwargs = {"task":"transcribe", "language":f" "} prediction = pipe( file_path, return_timestamps=True, chunk_length_s=30, stride_length_s=(5), generate_kwargs=generate_kwargs )Python The output of pipe is the dictionary format data with items of text and chunks. text contains the entire transcribed result, and chunks consists of chunks with the timestamp and corresponding transcribed result (see the following screenshot). We use data in chunks to do further processing.

As the preceding screenshot shows, lot of sentences have been cut off and split into different chunks. To make the chunks more meaningful, we need to combine sentences cut off and update timestamps in the next step. Organize sentences We use a very simple rule to combine sentences. When the chunk ends with a period (.), we don't make any change; otherwise, we concatenate it with the next chunk. The following code snippet explains how we make this change: prev_chunk = None new_chunks = [] for chunk in chunks: if prev_chunk: chunk['text'] = prev_chunk['text'] + chunk['text'] chunk['timestamp'] = (prev_chunk['timestamp'][0], chunk['timestamp'][1])

if not chunk['text'].endswith('.'): prev_chunk = chunk else: new_chunks.append(chunk) prev_chunk = NonePython Compared to the original chunks produced by the audio-to-text converts, we can get complete sentences that are cut off originally.

Chunk sentences The text content in documents is normally organized by paragraph. Each paragraph focuses on the same topic. Chunking by paragraph may help embed texts into more meaningful vectors, which may improve retrieval accuracy. Unlike the normal text content in documents, transcriptions from the transcription model are not paragraphed. Even though there are some stops in the audio files, sometimes it can't be used to paragraph sentences. On the other hand, langchain provides the recursive chunking text splitter function RecursiveCharacterTextSplitter, which can keep all the semantically relevant content in the same chunk. Because we need to keep timestamps with chunks, we implement our own chunking process. Inspired by the post How to chunk text into paragraphs using python, we chunk sentences based on the similarity between the adjacent sentences with a sentence embedding approach. The basic idea is to take the sentences with the lowest similarity to adjacent sentences as the split points. We use all-MiniLM-L6-v2 for sentence embedding. You can refer the original post for the explanation of this approach. We have made some minor changes on the original source code; refer to our source code for the implementation. The core part for this process is as follows: # Embed sentences model_name = "all-minilm-l6-v2" model = SentenceTransformer(model_name) embeddings = model.encode(sentences_all) # Create similarities matrix similarities = cosine_similarity(embeddings)

# Let's apply our function. For long sentences i reccomend to use 10 or more sentences minmimas = activate_similarities(similarities, p_size=p_size, order=order)

# Create empty string split_points = [each for each in minmimas[0]] text = ''

if text == '' and (start_timestamp == current_timestamp[1]): start_timestamp = current_timestamp[0]

if num in split_points: para_chunks.append(text) para_timestamp.append([start_timestamp, current_timestamp[1]]) text = f'{each}. ' start_timestamp = current_timestamp[1] else: text+=f'{each}. '

if len(text): para_chunks.append(text) para_timestamp.append([start_timestamp, timestamps_all[-1][1]])Python To evaluate the efficiency of chunking with sentence embedding, we conducted qualitative comparisons between different chunking mechanisms. The assumption underlying such comparisons is that if the chunked texts are more semantically different and separate, there will be less irrelevant contextual information being retrieved for the Q&A, so that the answer will be more accurate and precise. At the same time, because less contextual information is sent to LLMs, the cost of inference will also be less as charges increment with the size of tokens. We visualized the first two components of a PCA by reducing high dimension into two dimensions. Compared to recursive chunking, we can see the distances between vectors representing different chunks with sentence embedding are more scattered, meaning the chunks are more semantically separate. This means when the vector of a query is close to the vector of one chunk, it may have less possibility to be close to other chunks. A retrieval task will have fewer opportunities to choose relevant information from multiple semantically similar chunks.

When the chunking process is complete, we attach timestamps to the file name of each chunk, save it as a single file, and then upload it to an S3 bucket. Enable intelligent video search using a RAG-based approach with LangChain There are typically four approaches to build a RAG solution for Q&A with LangChain:

Using the load_qa_chain functionality, which feeds all information to an LLM. This is not an ideal approach given the context window size and the volume of video and audio data. Using the RetrievalQA tool, which requires a text splitter, text embedding model, and vector store to process texts and retrieve relevant information. Using VectorstoreIndexCreator, which is a wrapper around all logic in the second approach. The text splitter, text embedding model, and vector store are configured together inside the function at one time. Using the ConversationalRetrievalChain tool, which further adds memory of chat history to the QA solution.

For this post, we use the second approach to explicitly customize and choose the best engineering practices. In the following sections, we describe each step in detail. To search for the relevant content based on the user input queries, we use semantic search, which can better understand the intent behind and query and perform meaningful retrieval. We first use a pre-trained embedding model to embed all the transcribed text into a vector space. At search time, the query is also embedded into the same vector space and the closest embeddings from the source corpus are found. You can deploy the pre-trained embedding model as shown in Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart to create the embeddings for semantic search. In our post, we adopt similar ways to create an intelligent video search solution using a RAG-based approach with the open-source LangChain library. LangChain is an open-source framework for developing applications powered by language models. LangChain provides a generic interface for many different LLMs. We first deploy an embedding model GPT-J 6B provided by Amazon SageMaker JumpStart and the language model Falcon-40B Instruct from Hugging Face to prepare for the solution. When the endpoints are ready, we follow similar steps described Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart to create the LLM model and embedding model for LangChain. The following code snippet shows how to create the LLM model using the langchain.llms.sagemaker_endpoint.SagemakerEndpoint class and transform the request and response payload for the LLM in the ContentHandler: from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint

def transform_input(self, prompt: str, model_kwargs={}) -> bytes: self.len_prompt = len(prompt) input_str = json.dumps({"inputs": prompt , "parameters": {**model_kwargs}}) return input_str.encode("utf-8")

def transform_output(self, output: bytes) -> str: response_json = output.read() res = json.loads(response_json) print(res) ans = res[0]['generated_text'][self.len_prompt:] return ans

sm_llm = SagemakerEndpoint( endpoint_name=_MODEL_CONFIG_["huggingface-falcon-40b"]["endpoint_name"], region_name=aws_region, model_kwargs=parameters, content_handler=content_handler, ) Python When we use a SageMaker JumpStart embedding model, we need to customize the LangChain SageMaker endpoint embedding class and transform the model request and response to integrate with LangChain. Load the processed video transcripts using the LangChain document loader and create an index. We use the DirectoryLoader package in LangChain to load the text documents into the document loader: loader = DirectoryLoader("./data/demo-video-sagemaker-doc/", glob="*/.txt") documents = loader.load()Python Next, we use the embedding models to create the embeddings of the contents and store the embeddings in a FAISS vector store to create an index. We use this index to find relevant documents that are semantically similar to the input query. With the VectorstoreIndexCreator class, you can just write a few lines of code to achieve this task: index_creator = VectorstoreIndexCreator( vectorstore_cls=FAISS, embedding=embeddings, text_splitter=CharacterTextSplitter(chunk_size=500, chunk_overlap=0), ) index = index_creator.from_loaders([loader])Python Now we can use the index to search for relevant context and pass it to the LLM model to generate an accurate response: index.query(question=question, llm=sm_llm)Python Build a multi-functional chatbot with SageMaker With the deployed LLM on SageMaker, we can build a multi-functional smart chatbot to show how these models can help your business build advanced AI-powered applications. In this example, the chatbot uses Streamlit to build the UI and the LangChain framework to chain together different components around LLMs. With the help of the text-to-text and speech-to-text LLMs deployed on SageMaker, this smart chatbot accepts inputs from text files and audio files so users can chat with the input files (accepts text and audio files) and further build applications on top of this. The following diagram shows the architecture of the chatbot.

When a user uploads a text file to the chatbot, the chatbot puts the content into the LangChain memory component and the user can chat with the uploaded document. This part is inspired by the following GitHub example that builds a document chatbot with SageMaker. We also add an option to allow users to upload audio files. Then the chatbot automatically invokes the speech-to-text model hosted on the SageMaker endpoint to extract the text content from the uploaded audio file and add the text content to the LangChain memory. Lastly, we allow the user to select the option to use the knowledge base when answering questions. This is the RAG capability shown in the preceding diagram. We have defined the SageMaker endpoints that are deployed in the notebooks provided in the previous sections. Note that you need to pass the actual endpoint names that are shown in your account when running the Streamlit app. You can find the endpoint names on the SageMaker console under Inference and Endpoints. Falcon_endpoint_name = os.getenv("falcon_ep_name", default="falcon-40b-instruct-12xl") whisper_endpoint_name = os.getenv('wp_ep_name', default="whisper-large-v2") embedding_endpoint_name = os.getenv('embed_ep_name', default="huggingface-textembedding-gpt-j-6b")Python When the knowledge base option is not selected, we use the conversation chain, where we add the memory component using the ConversationBufferMemory provided by LangChain, so the bot can remember the current conversation history: def load_chain(): memory = ConversationBufferMemory(return_messages=True) chain = ConversationChain(llm=llm, memory=memory) return chain

chatchain = load_chain()Python We use similar logic as shown in the earlier section for the RAG component and add the document retrieval function to the code. For demo purposes, we load the transcribed text stored in SageMaker Studio local storage as a document source. You can implement other RAG solutions using the vector databases based on your choice, such as Amazon OpenSearch Service, Amazon RDS, Amazon Kendra, and more. When users use the knowledge base for the question, the following code snippet retrieves the relevant contents from the database and provides additional context for the LLM to answer the question. We used the specific method provided by FAISS, similarity_search_with_score, when searching for relevant documents. This is because it can also provide the metadata and similarity score of the retrieved source file. The returned distance score is L2 distance. Therefore, a lower score is better. This gives us more options to provide more context for the users, such as providing the exact timestamps of the source videos that are relevant to the input query. When the RAG option is selected by the user from the UI, the chatbot uses the load_qa_chain function provided by LangChain to provide the answers based on the input prompt. docs = docsearch.similarity_search_with_score(user_input) contexts = []

for doc, score in docs: print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}") if score <= 0.9: contexts.append(doc) source.append(doc.metadata['source'].split('/')[-1]) print(f"\n INPUT CONTEXT:{contexts}") prompt_template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.:\n\n{context}\n\nQuestion: {question}\nHelpful Answer:"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"]) chain = load_qa_chain(llm=llm, prompt=PROMPT) result = chain({"input_documents": contexts, "question": user_input}, return_only_outputs=True)["output_text"]

if len(source) != 0: df = pd.DataFrame(source, columns=['knowledge source']) st.data_editor(df)Python Run the chatbot app Now we're ready to run the Streamlit app. Open a terminal in SageMaker Studio and navigate to the cloned GitHub repository folder. You need to install the required Python packages that are specified in the requirements.txt file. Run pip install -r requirements.txt to prepare the Python dependencies. Then run the following command to update the endpoint names in the environment variables based on the endpoints deployed in your account accordingly. When you run the chatbot.py file, it automatically updates the endpoint names based on the environment variables. export falcon_ep_name= export wp_ep_name= export embed_ep_name= streamlit run app_chatbot/chatbot.py --server.port 6006 --server.maxUploadSize 6Bash

To access the Streamlit UI, copy your SageMaker Studio URL and replace lab? with proxy/[PORT NUMBER]/. For this post, we specified the server port as 6006, so the URL should look like ID>.studio. .sagemaker.aws/jupyter/default/proxy/6006/. Replace domain ID and region with the correct value in your account to access the UI. Chat with your audio file In the Conversation setup pane, choose Browse files to select local text or audio files to upload to the chatbot. If you select an audio file, it will automatically invoke the speech-to-text SageMaker endpoint to process the audio file and present the transcribed text to the console, as shown in the following screenshot. You can continue asking questions about the audio file and the chatbot will be able to remember the audio content and respond to your queries based on the audio content.

Use the knowledge base for the Q&A When you want to answer questions that require specific domain knowledge or use the knowledge base, select Use knowledge base. This lets the chatbot retrieve relevant information from the knowledge base built earlier (the vector database) to add additional context to answer the question. For example, when we ask the question "what is the recommended way to first customize a foundation model?" to the chatbot without the knowledge base, the chatbot returns an answer similar to the following screenshot.

When we use the knowledge base to help answer this question, the chatbot returns a different response. In the demo video, we read the SageMaker document about how to customize a model in SageMaker Jumpstart.

The output also provides the original video file name with the retrieved timestamp of the corresponding text. Users can go back to the original video file and locate the specific clips in the original videos.

This example chatbot demonstrates how businesses can use various types of digital assets to enhance their knowledge base and provide multi-functional assistance to their employees to improve productivity and efficiency. You can build the knowledge database from documents, audio and video datasets, and even image datasets to consolidate all the resources together. With SageMaker serving as an advanced ML platform, you accelerate project ideation to production speed with the breadth and depth of the SageMaker services that cover the whole ML lifecycle. Clean up To save costs, delete all the resources you deployed as part of the post. You can follow the provided notebook's cleanup section to programmatically delete the resources, or you can delete any SageMaker endpoints you may have created via the SageMaker console. Conclusion The advent of generative AI models powered by LLMs has revolutionized the way businesses acquire and apply insights from information. Within this context, digital assets, including video and audio content, play a pivotal role as visual representations of products, services, and brand identity. Efficiently searching and discovering specific content within these assets is vital for optimizing workflows, enhancing collaboration, and delivering tailored experiences to the intended audience. With the power of generative AI models on SageMaker, businesses can unlock the full potential of their video and audio resources. The integration of generative AI models empowers enterprises to build efficient and intelligent search solutions, enabling users to access relevant and contextual information from their digital assets, and thereby maximizing their value and fostering business success in the digital landscape. For more information on working with generative AI on AWS, refer to Announcing New Tools for Building with Generative AI on AWS.

About the authors Gordon Wang is a Senior AI/ML Specialist TAM at AWS. He supports strategic customers with AI/ML best practices across many industries. He is passionate about computer vision, NLP, generative AI, and MLOps. In his spare time, he loves running and hiking. Melanie Li is a Senior AI/ML Specialist TAM at AWS based in Sydney, Australia. She helps enterprise customers build solutions using state-of-the-art AI/ML tools on AWS and provides guidance on architecting and implementing ML solutions with best practices. In her spare time, she loves to explore nature and spend time with family and friends. Guang Yang is a Senior Applied Scientist at the Amazon Generative AI Innovation Center, where he works with customers across various verticals and applies creative problem solving to generate value for customers with state-of-the-art generative AI solutions. Harjyot Malik is a Senior Program Manager at AWS based in Sydney, Australia. He works with the APJC Enterprise Support teams and helps them build and deliver strategies. He collaborates with business teams, delving into complex problems to unearth innovative solutions that in return drive efficiencies for the business. In his spare time, he loves to travel and explore new places.

As customers accelerate their migrations to the cloud and transform their businesses, some find themselves in situations where they have to manage data analytics in a multi-cloud environment, such as acquiring a company that runs on a different cloud provider. Customers who use multi-cloud environments often face challenges in data access and compatibility that can create blockades and slow down productivity. When managing multi-cloud environments, customers must look for services that address these gaps through features providing interoperability across clouds. With the release of the Amazon Athena data source connector for Google Cloud Storage (GCS), you can run queries within AWS to query data in Google Cloud Storage, which can be stored in relational, non-relational, object, and custom data sources, whether that be Parquet or comma-separated value (CSV) format. Athena provides the connectivity and query interface and can easily be plugged into other AWS services for downstream use cases such as interactive analysis and visualizations. Some examples include AWS data analytics services such as AWS Glue for data integration, Amazon QuickSight for business intelligence (BI), as well as third-party software and services from AWS Marketplace. This post demonstrates how to use Athena to run queries on Parquet or CSV files in a GCS bucket. Solution overview The following diagram illustrates the solution architecture.

The Athena Google Cloud Storage connector uses both AWS and Google Cloud Platform (GCP), so we will be referencing both cloud providers in the architecture diagram. We use the following AWS services in this solution:

Amazon Athena – A serverless interactive analytics service. We use Athena to run queries on data stored on Google Cloud Storage. AWS Lambda – A serverless compute service that is event driven and manages the underlying resources for you. We deploy a Lambda function data source connector to connect AWS with Google Cloud Provider. AWS Secrets Manager – A secrets management service that helps protect access to your applications and services. We reference the secret in Secrets Manager in the Lambda function so we can run a query on AWS and it can access the data stored on Google Cloud Provider. AWS Glue – A serverless data analytics service for data discovery, preparation, and integration. We create an AWS Glue database and table to point to the correct bucket and files within Google Cloud Storage. Amazon Simple Storage Service (Amazon S3) – An object storage service that stores data as objects within buckets. We create an S3 bucket to store data that exceeds the Lambda function's response size limits.

The Google Cloud Platform portion of the architecture contains a few services as well:

Google Cloud Storage – A managed service for storing unstructured data. We use Google Cloud Storage to store data within a bucket that will be used in a query from Athena, and we upload a CSV file directly to the GCS bucket. Google Cloud Identity and Access Management (IAM) – The central source to control and manage visibility for cloud resources. We use Google Cloud IAM to create a service account and generate a key that will allow AWS to access GCP. We create a key with the service account, which is uploaded to Secrets Manager.

Prerequisites For this post, we create a VPC and security group that will be used in conjunction with the GCP connector. For complete steps, refer to Creating a VPC for a data source connector. The first step is to create the VPC using Amazon Virtual Private Cloud (Amazon VPC), as shown in the following screenshot.

Then we create a security group for the VPC, as shown in the following screenshot.

For more information about the prerequisites, refer to Amazon Athena Google Cloud Storage connector. Additionally, there are tables that highlight the specific data types that can be used such as CSV and Parquet files. There are also required permissions to run the solution. Google Cloud Platform configuration To begin, you must have either CSV or Parquet files stored within a GCS bucket. To create the bucket, refer to Create buckets. Make sure to note the bucket name—it will be referenced in a later step. After you create the bucket, upload your objects to the bucket. For instructions, refer to Upload objects from a filesystem.

The CSV data used in this example came from Mockaroo, which generated random test data as shown in the following screenshot. In this example, we use a CSV file, but you can also use Parquet files.

Additionally, you must create a service account to generate a key pair within Google Cloud IAM, which will be uploaded to Secrets Manager. For full instructions, refer to Create service accounts.

After you create the service account, you can create a key. For instructions, refer to Create and delete service account keys.

AWS configuration Now that you have a GCS bucket with a CSV file and a generated JSON key file from Google Cloud Platform, you can proceed with the rest of the steps on AWS.

On the Secrets Manager console, choose Secrets in the navigation pane. Choose Store a new secret and specify Other type of secret. Provide the GCP generated key file content.

The next step is to deploy the Athena Google Cloud Storage connector. For more information, refer to Using the Athena console.

On the Athena console, add a new data source. Select Google Cloud Storage.

For Data source name, enter a name. For Lambda function, choose Create Lambda function to be redirected to the Lambda console.

In the Application settings section, enter the information for Application name, SpillBucket, GCSSecretName, and LambdaFunctionName.

You also have to create an S3 bucket to reference the S3 spill bucket parameter in order to store data that exceeds the Lambda function's response size limits. For more information, refer to Create your first S3 bucket.

After you provide the Lambda function's application settings, you're redirected to the Review and create page.

Confirm that these are the correct fields and choose Create data source.

Now that the data source connector has been created, you can connect Athena to the data source.

On the Athena console, navigate to the data source. Under Data source details, choose the link for the Lambda function.

You can reference the Lambda function to connect to the data source. As an optional step and for validation, the variables that were put into the Lambda function can be found within the Lambda function's environment variables on the Configuration tab.

Because the built-in GCS connector schema inference capability is limited, it's recommended to create an AWS Glue database and table for your metadata. For instructions, refer to Setting up databases and tables in AWS Glue.

Query the data Now you can run queries on Athena that will access the data stored on Google Cloud Storage.

On the Athena console, choose the correct data source, database, and table within the query editor. RunSELECT * FROM [AWS Glue Database name].[AWS Glue Table name]in the query editor.

As shown in the following screenshot, the results will be from the bucket on Google Cloud Storage.

The data that is stored on Google Cloud Platform can be accessed through AWS and used for many use cases, such as performing business intelligence, machine learning, or data science. Doing so can help unblock developers and data scientists so they can efficiently provide results and save time. Clean up Complete the following steps to clean up your resources:

Delete the provisioned bucket in Google Cloud Storage. Delete the service account under IAM & Admin. Delete the secret GCP credentials in Secrets Manager. Delete the S3 spill bucket. Delete the Athena connector Lambda function. Delete the AWS Glue database and table.

Troubleshooting If you receive a ROLLBACK_COMPLETE state and "can not be updated error" when creating the data source in Lambda, go to AWS CloudFormation, delete the CloudFormation stack, and try recreating it. If the AWS Glue table doesn't appear in the Athena query editor, verify that the data source and database values are correctly selected in the Data pane on the Athena query editor console. Conclusion In this post, we saw how you can minimize the time and effort required to access data on Google Cloud Platform and use it efficiently on AWS. Using the data connector helps organizations become multi-cloud agnostic and helps accelerate business growth. Additionally, you can build out BI applications with the discoveries, relationships, and insights found when analyzing the data, which can further your organization's data analysis process.

About the Author Jonathan Wong is a Solutions Architect at AWS assisting with initiatives within Strategic Accounts. He is passionate about solving customer challenges and has been exploring emerging technologies to accelerate innovation.

AWS Women in Cloud program helps Brazilian on path to solutions architect role

I joined Amazon Web Services (AWS) as a Solutions Architect in September 2022, a few months after participating in AWS Women in Cloud (WiC), a workforce development initiative designed to inspire, motivate, and connect female learners to AWS training and cloud-related job opportunities. As I approach my one-year "Amaversary," as we Amazonians like to call it, I'm moved to share my journey with the hopes it inspires even one woman to see her future in my story. Since I was young, I dreamed of working for a big tech company, but this dream always seemed far from attainable. You see, I was born and raised in a city of 140,000 inhabitants in the interior of Rio de Janerio State in Brazil. Growing up in such a small and remote part of Brazil presented me with few opportunities for professional growth, especially when it came to tech. Education as my first step Determined not to let geography stand in the way of my dreams, I began studying information systems at UniFOA in 2013. There, I found great inspiration in one of my professors who taught the last class on Friday nights. He worked at a big tech company for years, and even though the class was late and he was tired, he still had a sparkle in his eyes every time he talked about his professional journey. He, too, came from a small town in Brazil and I was inspired by his story, which showed me that my dream was feasible to achieve. Thanks to this inspiration and my hard work, I secured a Systems Analyst role when I graduated in 2016. Though I entered the tech workforce, I was still living in my small hometown and my opportunities for growth felt limited. Over the next three years, I planned and dreamed for what was to come next. Making a big move In 2019, I decided to move to Brazil's largest city, São Paulo, where a single neighborhood has a population that exceeded that of my entire hometown. At first, everything felt daunting.. I had never been away from home and the move required a lot of courage and determination. But I eventually found a job as a developer at a consultancy firm, where I continued to sharpen my tech skills. I've always loved and had a strong affinity for the field of data and enjoyed helping people solve their problems. So, when I was offered a new position as a Data Engineer at a financial services company in 2020, I immediately accepted. Though I had heard of Amazon Web Services (AWS), I had little prior knowledge of cloud computing and had never worked in the AWS Cloud. My fascination with AWS's data services quickly grew and I decided to pursue the AWS Certified Cloud Practitioner certification so I could further specialize in cloud computing and add more value to my work as a Data Engineer. While taking courses at Paulista Faculty of Informatics and Administration I studied for the certification exam through AWS Academy, which provides higher education institutions around the world with a free, ready-to-teach AWS Cloud computing curriculum. Empowered by stories from Women in Cloud Not one to sit still, I next set a goal to become a Solutions Architect. Though this goal seemed like a stretch at the time, my history taught me that I could do anything I set my mind to. And, lucky for me, my AWS Academy professor had invited me to participate in the WiC event, held at the AWS Brazil office. Without hesitation, I accepted the invitation, as my dream was closer than ever: I was going to step into the AWS Brazil office for the first time! The all-day event included hands-on activities to practice my cloud skills, innovation workshops, and opportunities to hear from women who have excelled in their IT careers. Employers with available positions attended the event hoping to meet cloud-skilled candidates to fill these open roles. Leaving the event, I was deeply impacted, motivated, determined, and empowered. I completed the AWS Academy course, obtained my first AWS Certification, and began applying for Solutions Architect positions. Dreams become a reality WiC aims to build a bridge to employment by connecting cloud-skilled female job seekers with employers looking to hire for in-demand, open roles. And, about three months later, I was called to participate in the selection process for a Solutions Architect role at AWS. My heart almost leaped out of joy. I went through the process, and a few days later, I received the long-awaited email saying, "Congratulations, Inaiah. You have been approved for the Solutions Architect position at Amazon Web Services!" After all my hard work, my dream of becoming a Solutions Architect was finally coming true, thanks to the "Women in Cloud" event that allowed me to connect with other women in tech who I now get to call my colleagues. This year I attended the third edition of WiC in Brazil and had the privilege of sharing my story on stage. As I looked out into the audience of hundreds of women who were just like me a short time ago, one thing became clear: while I look forward to continuing to grow professionally, my passion is to impact and motivate other women with my experience. We are all capable of reaching our goals! About Women in Cloud AWS Women in Cloud (WiC) is a workforce development initiative to inspire, motivate, and connect female learners to AWS training and cloud-related job opportunities, promoting and growing women empowerment in IT. WIC has conducted three events in Brazil since June 2022, impacting over 1,000 women through networking, mentorship, training learners in AWS Academy Cloud Foundations, and connecting cloud-skilled talent with entry-level employment opportunities. Connect with Talent from AWS Education Programs AWS Education Programs are preparing graduates for cloud roles in cities and towns around the world. We designed our curriculum by working backward from employer requirements so that graduates are qualified and prepared with the skills needed. Find cloud-skilled talent to fill your open roles today.

While working with many of our customers, a recurring question has been "How can we be notified when users login to key accounts so we can take action if needed?" This post shows how to implement a flexible, simple, and serverless solution that creates notifications when sensitive accounts are logged in to. Alerting on high risk account access is advised as part of multi-account best practices. And although this approach is something we recommend, customers currently must develop their own method. In this post, we will walk you through a solution that will allow you to implement a flexible and scalable framework that can trigger notifications when users access accounts of interest, use specified roles, or leverage the root user principal in AWS account(s) or Organizational Units (OUs) they specify. The notifications generated can be emailed or sent to another AWS service to trigger additional automated actions, allowing you to reduce risk from unauthorized access to your environments. Prerequisites You will need to verify the following:

Your AWS Organization must have all features enabled. Make sure you do not have an explicit deny Service Control Policy (SCP) set at the Organization level that could prevent stacksets from being deployed. You must enable trusted access for stacksets to run the management account stack.

Key Concepts Before we dive deep into this solution, let's review some key concepts that are centric to the logic:

AWS root account user is the email address and password that were used to create an AWS account initially. The AWS root account user has the highest level of privileges and its use should be restricted to a small list of actions that require it. IAM identities are created using AWS Identity and Access Management (IAM) and take the form of either an IAM user with long-term credentials, or an IAM role with short-term credentials. AWS accounts can contain many users and roles. AWS Organizations allows for the creation and management of AWS accounts. Organizational Units (OUs) are containers for accounts. An OU can also contain other OUs. Root OU is the parent container for all the accounts for your organization. If you apply a policy to the root, it applies to all OUs and accounts in the organization. CloudFormation Stacksets allow you to deploy and manage configurations across multiple AWS accounts.

Solution The solution we will be walking you through details the steps needed to build a multi-account access notification system using AWS Organizations for account management, Amazon EventBridge to identify key login actions, AWS Lambda to act on the events, Amazon Simple Notification Service (SNS) to send notifications, and Amazon DynamoDB for storing configuration data. The solution is also fully compatible with AWS Control Tower.

The LoginNotification Configuration Database DynamoDB table is where the rules are stored to determine what login activities trigger a notification. By default, you will have a single rule in the table that causes the AWS root account to trigger a notification, which will look like the following:

Records in the LoginNotification Configuration Database have the following attributes: "id" is what the rule applies to. This can be an AWS account number, Organizational Unit, or Organizational root. There is a special ID called "DEFAULT" which applies to all accounts "inclusions" is a stringset field that contains a list of principals you want to include in your login notifications. Principals are evaluated against the "userIdentity" field in AWS Management Console sign-in events from CloudTrail. Items in the "inclusions" stringset are evaluated for a complete or partial match for the "userIdentity" field in the login event. "exclusions" is a stringset field that indicates principals which should not generate notifications. The same evaluation logic applies to exclusions as inclusions. There are 2 reserved words in the "inclusions" and "exclusions" list

ROOT means the AWS root account user. ALL means match all principals.

When a console login event is received it is evaluated in the following order:

By default, no notifications will be sent. If the account where the login occurred is not defined in the LoginNotification Configuration database then only the "DEFAULT" rule is applied. If the account where the login occurred is defined in, or is under an OU that is defined in the LoginNotifications Configuration database then all inclusions and exclusions that apply to the account and higher-level OUs are consolidated into a combined list If any of the entries in the combined "inclusions" list are a partial or complete match for the "userIdentity" from the event, a notification will be sent, unless there is an entry in the combined "exclusions" list that also matches the "userIdentity" from the event. "exclusions" take priority over "inclusions".

We recommend adding additional rows beyond the default entry that fit your own best practices. You may add rules at any of four different levels: Account, OU, Root OU, and Default. Exclusions always take precedence over inclusions. As an example, consider the DynamoDB table below:

In this example you can see two rows that represent specific accounts (rows 1 and 3), a row that represents an OU (row 2) and the default entry (row 4). Let us consider the effect on account "123456789012" in this set of entries. First, we would apply the DEFAULT rule which indicates we should include ROOT logins and any with "aperson" in the login credentials. If account "123456789012" was part of "OU-ABCD-EFGHIJKLM", we would also apply an inclusion for "athirdperson" due to that entry. And finally, since we have an entry for that account, we would apply the exclusion for "aperson" which would remove that string from our combined inclusions. We would be left with any logins for ROOT or for "anotherperson" for this account. Walkthrough In order to install this solution, you will need to deploy two CloudFormation stacks. These stacks can be created in any order.

CentralEventBus stack. This stack creates the EventBridge central event bus, DynamoDB tables, and Lambda functions necessary for this solution. This should be deployed in a security audit account. If you're using AWS Control Tower this should be deployed in your Audit account. ManagementAccount stack. This must be deployed in your AWS Organizations management account. This stack deploys the stackset that will deploy the EventBridge forwarding rules to your existing and new AWS accounts. It also deploys a cross account role that the LoginNotifierParser Lambda function uses to query AWS organizations to determine which rules apply to each account. This stack requires trusted account access with AWS Organizations to be enabled for your AWS Organization.

Step 1: Deploying the CentralEventBus stack We'll start with the CentralEventBus stack. You should deploy this to your Audit account, or a similar functioning account. In order to deploy the solution, sign in to the AWS Management Console and navigate to the CloudFormation console. Next, download the CentralEventBus template and save it, and deploy the solution template using Create Stack. The CentralEventBus template takes the following parameters:

AWSSSOHomeRegion indicates the home region for your IAM Identity Center (successor to AWS SSO). The default here is us-east-1. CacheExpiration is the length of time (in seconds) to cache results from AWS organization. To minimize calls to AWS Organizations and reduce execution times this solution caches information about the AWS Organizations hierarchy in a DynamoDB table. The default expiration is 300 seconds (5 minutes) EmailSubscription is the email address that will receive notification events. OrganizationID is your AWS Organizations identifier. This can be found by either navigating to the AWS Organizations console in the AWS console or by running the AWS CLI command "aws organizations describe-organization".

You will be taken to the CloudFormation Stacks screen for your CentralEventBus stack. The deployment may take a few minutes to complete. Once completed, you will see a CREATE_COMPLETE status.

After the stack has successfully completed, the email address you used in the stack details will receive an email requesting that you confirm the subscription to the SNS notification topic. Click the link in the email while logged into the account where you deployed the CloudFormation stack.

When you click on the "Confirm subscription" link you will see a confirmation screen indicating you have subscribed that email to the SNS notification topic for the login notifications.

Step 2: Deploying the ManagementAccount stack Next you will deploy the ManagementAccount stack. You should deploy this to your AWS Organizations management account. Be sure to verify you have covered the prerequisites as mentioned earlier in "Prerequisites". In order to deploy the solution, sign in to the AWS Management Console and navigate to the CloudFormation console. Next, download the ManagementAccountStack template and save it, and deploy the solution template using Create Stack. The ManagementAccount template takes the following parameters:

CentralEventBridgeAccountID is the account ID where the CentralEventBus stack is or will be deployed. CentralEventBridgeRegion is the region where the CentralEventBus stack was deployed. DeployTo is where you would want the EventBridge Notification rules to be deployed. This can either your AWS Organizations root ID or an OU. MonitoredRegions is a comma-separated list of the regions from which you want login events forwarded. "us-east-1" must be part of this list in order to forward AWS root user logins.

You will be taken to the CloudFormation Stacks screen for your ManagementAccount stack. The deployment may take some time to complete depending on how many accounts and regions are included. Once the stack is deployed you will see a CREATE_COMPLETE for the stack name you used.

Tips: It can be useful to run the AWS CLI command "sts get-caller-identity" from an account you're logged into in order to see the "UserID" information you may want to key off of for your inclusion/exclusion rules. Here is an example of the command output: [cloudshell-user@ip-10-0-0-6 ~]$ aws sts get-caller-identity { "UserId": "AROAYQDUR5EGT522XDYRZ:my-user", "Account": "1234567829012", "Arn": "arn:aws:sts:: 1234567829012:assumed-role/MyRole/my-user" } It also may be helpful to review "ConsoleLogin" events in the AWS CloudTrail console to help formulate your own best practice rules. Cleanup: In order to remove this deployed solution it is only necessary to delete both CloudFormation stacks created here. Additional costs will occur if the solution is not deleted. The cost to implement the solution will vary based on the number of regions and accounts monitored, as well as the amount of login activity. Please see the relevant service pricing pages for more information on pricing:

Amazon EventBridge Pricing AWS Lambda Pricing Amazon DynamoDB Pricing Amazon SNS Pricing

Conclusion In this post, we showed how you can implement a flexible, simple and serverless solution for alerting on sensitive account logins as part of a best practice recommendation. This is accomplished by utilizing existing AWS CloudTrail events, AWS Organizations, Amazon EventBridge, AWS Lambda, and Amazon SNS. This allows customers to send notifications to relevant support personnel or trigger additional automation tasks based on critical events, making remediation of potential security issues a simple task. Although we do not require AWS Control Tower for this solution, it is an account management best practice and works well with the architecture for the Login Notifier. You can read more about Control Tower in the documentation. About the Authors

Jeff Bodey Jeff Bodey is a Senior Solutions Architect based in Denver, Colorado. He works with customers to help them build well architected and scalable applications in AWS. Jeff enjoys playing with gadgets and probably has entirely too many in his home.

Jeff Stockamp Jeff Stockamp is a Principal Solutions Architect based in Seattle, Washington. Jeff helps guide customers as they build well architected-applications and migrate workloads to AWS. Jeff is a constant builder and spends his spare time building Legos with his son.

Thuy Uyen Tran Uyen Tran is a Greenfield Account Solutions Architect and also part of the AI/ML TFC at AWS. She is passionate about learning and facing technical challenges. Technology, innovation, and automation are her areas of interest.

Giuseppe Zappia Giuseppe Zappia is a Senior Solutions Architect at AWS, with over 20 years of experience in full stack software development, distributed systems design, and cloud architecture. In his spare time, he enjoys playing video games, programming, watching sports, and building things.

Giuseppe Zappia Giuseppe Zappia is a Senior Solutions Architect working with customers in the early stages of adopting AWS. In his spare time, he enjoys playing video games, programming, and building things.

This blog post is written by Brian Graf, Senior Developer Advocate, Amazon Lightsail and Sophia Parafina, Senior Developer Advocate. Amazon Lightsail is a virtual private server (VPS) for deploying both operating systems (OS) and pre-packaged applications, such as WordPress, Plesk, cPanel, PrestaShop, and more. When deploying these instances, you can run launch scripts with additional commands such as installation of applications, configuration of system files, or installing pre-requisites for your application. Where do I add a launch script? If you're deploying an instance with the Lightsail console, launch scripts can be added to an instance at deployment. They are added in the 'deploy instance' page:

The launch script must be added before the instance is deployed, because launch scripts can't retroactively run after deployment. Anatomy of a Windows Launch Script When deploying a Lightsail Windows instance, you can use a batch script or a PowerShell script in the 'launch script' textbox. Of the two options, PowerShell is more extensible and provides greater flexibility for configuration and control. If you choose to write your launch script as a batch file, you must add tags at the beginning and end of your code respectively. Alternatively, a launch script in PowerShell, must use the tags in a similar fashion. After the closing or tag, you must add a tag on the following line. The persist tag is used to determine if this is a run-once command or if it should run every time your instance is rebooted or changed from the 'Stop' to 'Start' state. If you want your script to run every time the instance is rebooted or started, then you must set the persist tag to 'true'. If you want your launch script to just run once, then you would set your persist tag to 'false'. Anatomy of a Linux Launch Script Like a Windows launch script, a Linux launch script requires specific code on the first row of the textbox to successfully execute during deployment. You must place '#!/bin/bash' as the first line of code to set the shell that executes the rest of the script. After first line of code, you can continue adding additional commands to achieve the results you want. How do I know if my Launch Script ran successfully? Although running launch scripts is convenient to create a baseline instance, it's possible that your instance doesn't achieve the desired end-state because of an error in your script or permissions issues. You must troubleshoot to see why the launch script didn't complete successfully. To find if the launch script ran successfully, refer to the instance logs to determine whether your launch script was successful or not. For Windows, the launch log can be found in: C:\ProgramData\Amazon\EC2-Windows\launch\Log\UserdataExecution.log. Note that ProgramData is a hidden folder, and unless you access the file from PowerShell or Command Prompt, you must use Windows File Explorer (`View > Show > Hidden items`) folders to see it. For Linux, the launch log can be found in: /var/log/cloud-init-output.log and can be monitored after your instance launches by tailing the log by typing the following in the terminal: tail -f /var/log/cloud-init-output.logPowerShell If you want to see the entire log file including commands that have already run before you opened the log file, then you can type the following in the terminal: less +F /var/log/cloud-init-output.logPowerShell On a Windows instance, an easy way to monitor the UserdataExecution.log is to add the following code in your launch script, which creates a shortcut to tail or watch the log as commands are executing: # Create a log-monitoring script to monitor the progress of the launch script execution

# Save the log-monitoring script to the desktop for the user

false PowerShell If the script was executed, then the last line of the log should say '{Timestamp}: User data script completed'. However, if you want more detail, you can build the logging into your launch script. For example, you can append a text or log file with each command so that you can read the output in an easy-to-access location: # Set the location for the log file. In this case, # it will appear on the desktop of your Lightsail instance $loc = "c:\Users\Administrator\Desktop\mylog.txt"

# Write text to the log file Write-Output "Starting Script" >> $loc

# Download and install Chocolatey to do unattended installations of the rest of the apps. iex ((New-Object System.Net.WebClient).DownloadString('

# You could run commands like this to output the progress to the log file:

# Install vscode and all dependencies choco install -y vscode --force --force-dependencies --verbose >> $loc

# Install git and all dependencies choco install -y git --force --force-dependencies --verbose >> $loc

# Completed Write-Output "Completed" >> $loc false PowerShell This code creates a log file, outputs data, and appends it along the way. If there is an issue, then you can see where the logs stopped or errors appeared. For Ubuntu and Amazon Linux 2 If the cloud-init-output.log isn't comprehensive enough, then you can re-direct the output from your commands to a log file of your choice. In this example, we create a log file in the /tmp/ directory and push all output from our commands to this file. # Create the log file touch /tmp/launchscript.log

# Add text to the log file if you so choose echo 'Starting' >> /tmp/launchscript.log

# Install software to manage independent software vendor sources sudo apt -y install software-properties-common >> /tmp/launchscript.log

# Add the repository for all PHP versions sudo add-apt-repository -y ppa:ondrej/php >> /tmp/launchscript.log

# Install Web server, mySQL client, PHP (and packages), unzip, and curl sudo apt -y install apache2 mysql-client-core-8.0 php8.0 libapache2-mod-php8.0 php8.0-common php8.0-imap php8.0-mbstring php8.0-xmlrpc php8.0-soap php8.0-gd php8.0-xml php8.0-intl php8.0-mysql php8.0-cli php8.0-bcmath php8.0-ldap php8.0-zip php8.0-curl unzip curl >> /tmp/launchscript.log

# Any final text you want to include echo 'Completed' >> /tmp/launchscript.logBash It's possible to check the logs before the launch script has finished executing. One way to follow along is to 'tail' the log file. This lets you stream all updates as they occur. You can monitor the log using: 'tail -f /tmp/launchscript.log'. Bash Using Launch Scripts from AWS Command Line Interface (AWS CLI) You can deploy their Lightsail instances from the AWS Command Line Interface (AWS CLI) instead of the Lightsail console. You can add launch scripts to the AWS CLI command as a parameter by creating a variable with the script and referencing the variable, or by saving the launch script as a file and referencing the local file location on your computer. The launch script is still written the same way as the previous examples. For a Windows instance with a PowerShell launch script, you can deploy a Lightsail instance with a launch script with the following code: # PowerShell script saved in the Downloads folder:

# Download and install Chocolatey to do unattended installations of the rest of the apps.

Write-Output "Completed" >> $locPowerShell AWS CLI code to deploy a Windows Server 2019 medium instance in the us-west-2a Availability Zone: aws lightsail create-instances \

--user-data file://~/Downloads/powershell_script.ps1PowerShell Clean up Remember to delete resources when you are finished using them to avoid incurring future costs. Conclusion You now have the understanding and examples of how to create and troubleshoot Lightsail launch scripts both through the Lightsail console and AWS CLI. As demonstrated in this blog, using launch scripts, you can increase your productivity and decrease the deployment time and configuration of your applications. For more examples of using launch scripts, check out the aws-samples GitHub repository. You now have all the foundational building blocks you need to successfully script automated instance configuration. To learn more about Lightsail, visit the Lightsail service page.

Loupe live shopping platform upgrades user experience with Amazon IVS

Designed for card collectors, live e-commerce platform Loupe connects buyers with small businesses across the US to provide on-demand access to sports cards 24/7. Loupe focuses on fostering a community through real-time chat and big, exciting, memorable moments. After completing a thorough vetting process, vendors can live stream sales via the platform, which supports web, iOS, and Android. To provide customers with a high-quality experience, Loupe powers its live streams using Amazon Interactive Video Service (Amazon IVS) on Amazon Web Services (AWS). "Unlike a static marketplace, Loupe is a fully interactive live experience. It's also a reliable destination for in-demand collectibles, built on community and trust. Our initial live stream solution wasn't allowing us to hit the high bar we wanted," explained Loupe founder and CEO, Eric Doty. "When we implemented Amazon IVS for our live streams, the video quality jumped substantially, and our users immediately noticed it. I wish we had it from day one." Released in October 2020, Loupe was inspired by the team's own lackluster card collecting experiences and the rise of "box breaking"—the term for someone buying cards from a seller who's opening them on a live stream. While revitalizing the sports card hobby, box break purchases often required several time-consuming steps across different platforms. Loupe unifies these steps, allowing customers to complete the purchasing journey in as little as 10 seconds without leaving the app.

"In a typical sports card break, the seller might live stream on a social platform, you message them to make a purchase, they reply and direct you to their website. You then find the item and purchase it with an online payment application, which sends the seller an email about the transaction, so they can open the product on the live stream. It's way too many unnecessary steps," noted Brett Makedonski, Loupe director of communications. "Our team has a background in video games and designing cutting-edge viewing experiences. We saw how the sports card industry is lagging and could be so much better. By streamlining the experience, collectors are more likely to buy in because we've removed a lot of the friction seen in other e-commerce platforms." Loupe developers leverage the Amazon IVS broadcast SDK and player SDK to custom integrate the live streaming service within the platform, and the IVS Mobile Broadcast SDK enables sellers to go live straight from the Loupe mobile application. Amazon IVS powers these live streams, which are configured to record to Amazon Simple Storage Service (Amazon S3) automatically. Loupe makes the files available for on-demand viewing or download through the Amazon CloudFront content delivery network in case sellers or buyers want to share highlights of the stream on their own social channels. About a month after the live stream, the video is archived to Amazon S3 Glacier storage, economically preserving the content while allowing the app to highlight more recent streams. Amazon Elastic Compute (Amazon EC2) and Amazon Container Service (Amazon ECS) allow Loupe to seamlessly run a scalable computing infrastructure to power its applications. The team also relies on Amazon CloudWatch for gathering and analyzing performance data. The migration from the previous live stream solution and tech stack was complete in about two months and has been live since June 2022. The move to Amazon IVS and AWS has allowed Loupe to double the max live stream resolution, increase the max bitrate by 8x, and increase the frame rate by 4x, in addition to gaining live stream stability and customization capabilities. Though sports cards are the primary offering, the functionality Loupe has built with AWS could easily apply to nearly any live streaming e-commerce or auction-based marketplace. "Our focus is on providing a quality experience to a specific audience that is enthusiastic, engaged, and discerning. They're willing to pay for premium collectibles—and many are repeat customers, but only when their expectations are met. High-quality video is central to our success, and we've seen a significant increase in viewership and business since implementing Amazon IVS," Doty concluded. With the integration of Amazon IVS, Loupe has created an elevated experience for the card collector community by streamlining the traditional buying and unboxing process. By providing customers with more personalized and engaging shopping experiences, innovative solutions like Amazon IVS will continue to help retailers stay relevant and competitive in an ever-evolving retail landscape. To learn more about building with interactive live video, check out:

Josh Walters Josh Walters manages growth marketing programs for Amazon Interactive Video Service, focusing on building, scaling, and improving initiatives that increase awareness among developers creating interactive live streaming experiences.

Importance and Impact of Compliance for SaaS Solutions on AWS

Legal requirements: SaaS companies are often subject to various regulations and laws related to data protection, privacy, and security. Compliance with these regulations is not only legally required, but can also help to avoid costly fines and legal actions. Trust and reputation: Customers trust SaaS companies with their sensitive data and expect that it will be protected. Compliance with industry standards and regulations can help to build and maintain trust. Accessing new markets: Compliance with certain standards and regulations may be required to access new markets or to work with certain customers, particularly in highly regulated industries. Risk management: Compliance can help SaaS companies to identify and manage risks associated with data protection and security. Requirements often include implementing policies, procedures, and controls to help mitigate risks and prevent data breaches.

Building with a plan for compliance can also influence the design and architecture of your SaaS solution, and compliance can help avoid technical debt down the road. In this post, we'll break down some of the factors SaaS providers need to consider in designing and building their compliant solution, and highlight resources and services within Amazon Web Services (AWS) that you can leverage in your journey. Compliance as the Foundation of Security Security and privacy frameworks provide the tools to build out cybersecurity programs, stand up policies and procedures, and implement necessary technical controls to safeguard the confidentiality, availability and integrity of information. These frameworks provide a blueprint for managing risk and reducing vulnerabilities and are designed to consider the risks organizations face and how attackers can exploit security weaknesses. For SaaS providers, these frameworks can be used to demonstrate their solutions meet the security requirements of customers, whether with government laws and regulations of their specific industries. In some cases, organizations choose to pursue an attestation or certification such as, Service Organization Control 2 (SOC 2) and/or International Organization for Standardization 27001 (ISO 27001) to improve business operations and boost their competitive advantage. In other cases, businesses must meet regulatory compliance obligations and comply with laws for how to handle payment cards, personally identifiable information (PII), and other sensitive data. Security requirements of frameworks generally map to a group of controls that each define some aspect of security. These controls may require self-attestation of compliance, or attestation by a third party, which may take the form of periodic reviews, or continual compliance with systems that monitor the state of compliance with a control. Achieving compliance will be an ongoing process, but regular monitoring and reporting can help make adhering to these frameworks a standard part of business operations. Good security controls, data privacy, and data management should be foundational components of a SaaS application from the beginning. Shared Responsibility Model and Compliance For SaaS providers building on AWS, it's important to understand the Shared Responsibility Model in terms of compliance. AWS is continuously audited and maintains certifications and accreditations across the globe, including SOC 2, ISO 27001, Federal Risk and Authorization Management Program (FedRAMP), Payment Card Industry Data Security Standard (PCI DSS), and others to help customers meet security and compliance requirements. When systems are built in the AWS cloud, AWS and its customers share those compliance responsibilities. Some controls can be inherited from AWS, some are shared, and some controls are the customers responsibility, as Figure 1 below describes. Just as AWS has a SOC 2 attestation, FedRAMP authorization, and ISO certification, SaaS providers that need to be compliant will have to go through the same compliance processes. It's important SaaS providers understand which components they are responsible for within the scope of services that comprise their environment.

Figure 1 – AWS Shared Responsibility Model. A few resources to mention that are important as you plan your SaaS architecture:

Customer Compliance Guides (CCG) provide a consolidated view of AWS security practices based on the configurable options for AWS services and compliance topics and control requirements mapped to common frameworks (SOC 2, ISO 27001, FedRAMP, National Institute of Standards and Technology Cybersecurity Framework (NIST CSF)). AWS Services in Scope by Compliance Program is a list of services assessed and approved for specific compliance frameworks and regulations. For example, for Health Insurance Portability and Accountability Act (HIPAA), customers may use any AWS service in an account designated as a HIPAA account, but only process, store, and transmit protected health information (PHI) in HIPAA-eligible services defined on this page and in the Business Associate Addendum (BAA). AWS Artifact provides access to AWS compliance reports and agreements between AWS and you as a SaaS provider which may be required by your compliance auditor. Some reports will list inheritable controls for "of the cloud," helping customers understand the scope of controls that are shared and their responsibility.

Compliance Impact on SaaS Architecture In the following sections, we'll examine how compliance concerns influence our thinking on a number of core SaaS concepts. In doing so, we'll refer back to the AWS whitepaper about SaaS Architecture Fundamentals and the SaaS Lens for the AWS Well-Architected Framework. Please refer to these guides for general SaaS guidance. Tenant Isolation Implications Tenant isolation is the process of protecting tenant resources, and denying any attempts to access other tenants' resources. The options for isolating tenant infrastructure, in SaaS, break down primarily into "silo" and "pool." Check out the SaaS whitepaper's "Full stack silo and pool" section for background reading.

Figure 2 – Silo vs. Pool model of tenant isolation. It's a misconception that compliance frameworks or regulations, particularly those covering highly regulated industries, prohibit the use of pooled resources. In fact, most frameworks offer no guidance on tenant isolation, even FedRAMP primarily specifies "a full application test which attempts to use provisional access of one tenant to compromise another tenant." (FedRAMP Penetration Test Guidance) Be prepared to prove you will "introduce isolation strategies across all layers of the architecture, providing specific constructs that ensure that any attempt to access a tenant resource is valid for the current tenant context." (SaaS Lens) If you're going to pool resources, the focus is on ensuring isolation policies are applied at every layer of our application. ln Figure 3, you can see how isolation policies need applied at our API, compute, and storage layers. AWS Identity and Access Management (IAM) is essential for isolation of pooled resources, and techniques like Dynamic Policy Generation and Attribute Based Access Control (ABAC) provide mechanism to apply isolation policies.

Figure 3 – Isolation policies in a SaaS solution. Despite the lack of explicit prohibition of pooled resources in most compliance frameworks, SaaS providers that serve highly-regulated industries should still consider customer preference. You may need silo resources to accommodate silo resource requests as higher cost pricing tiers. Compliant Tenant Onboarding In silo environments, you must ensure all tenant environments are running the same compliant version of software. As you onboard new tenants, and as we deploy new versions of our software, you need to be able to provide assurances the version every tenant is running meets our compliance standards.

Figure 4 – Compliant tenant onboarding. AWS Organizations and AWS Control Tower can help you centrally provision tenants and set up Service Control Policy (SCP) guardrails that can prevent accidental modification of accounts. Landing Zones Accelerator on AWS (LZA) can assist with setting up security configurations with log archive and audit accounts, so evidence of activity is available to auditors. Compliance Implications for Data Storage Discussions around data in a SaaS solution begin with understanding how you'll partition your tenant data. In some ways, this is similar to tenant isolation, but consider there is "often a temptation to view data partitioning and tenant isolation as interchangeable. These two concepts are not meant to be equivalent. When we talk about data partitioning, we are talking about how tenant data is stored for individual tenants." (SaaS Whitepaper) You need to consider how you partition data for compliance purposes just like tenant isolation. This may include making decisions to segregate data based on its sensitivity, in addition to isolation concerns.

Figure 5 – Segregating sensitive SaaS data. Data partitioning decisions may align with data encryption requirements, as well as backup and restore policies, data retention, data sovereignty, and offboarding, including decisions about how to configure storage and databases around the capabilities of the services you are using. For example, an Amazon Simple Storage Service (Amazon S3) bucket and its folders can be configured to encrypt with AWS Key Management Service (AWS KMS) by default, offering the option of data encrypted with tenant-specific keys.

Figure 6 – Encrypting tenant Amazon S3 data. Multi-Tenant Auditing, Logging, and Monitoring You need to inject tenancy into everything our application emits, including logs, traces, and metrics. For compliance purposes, it's important to ensure logs are aggregated into a centralized logging solution, and access to those logs is restricted in an audit account. For verifying the history of access to your resources, turning on AWS CloudTrail for all accounts and regions. Eensure access logs are turned on for services that produce them; for example, Logging calls to Amazon API Gateway and Logging Amazon Cognito API Calls. Managing Tenant Identity You also need to manage tenant users and their access to your SaaS solution; this includes ensuring users are authenticated and connected to a specific tenant context. This context contains the information you pass along to every layer of your application, like logging and monitoring as we just discussed, and in fact every layer of your solution. Our compliance story is strengthened by a single source of truth for tenant identity, rather than tenant services and logic spread throughout our solution. Leveraging an identity provider (IdP) can provide controls you can inherit around your identity management, simplifying your compliance story. Getting Started with Security and Compliance We've looked at some of the reasons SaaS compliance may be challenging, so let's examine the journey towards compliance. For many SaaS providers, this may mean making incremental progress even if you don't yet know what compliance frameworks your customers will require. Many controls and requirements defined by compliance frameworks are founded on the best practices for cloud architecture. AWS has several free resources that can ensure your solution is following best practices, which can help prepare your architecture in anticipation of future compliance requirements.

AWS Startup Security Baseline: Minimized set of free controls, customized for the needs of early stage startups. AWS Well-Architected Tool: AWS best practices right in your AWS console, with additional reviews like the SaaS Lens as well. AWS Foundational Technical Review (FTR): For AWS Partners, FTRs combine a self-assessment with an AWS validation of your SaaS workload. Center for Internet Security (CIS) AWS Benchmarks and CIS Controls: AWS CIS Benchmarks provide a secure configuration baseline agreed on by security experts from around the industry ,and CIS Controls are a comprehensive best-practice guide for system and network security.

Members of the AWS Partner Network (APN) can leverage programs, including the AWS Global Security and Compliance Accelerator (GSCA), that help SaaS providers meet their customers' compliance needs. The AWS SaaS Factory program provides expert guidance to SaaS providers at any stage of the SaaS journey. Summary In this post, we examined a number of aspects of compliance on AWS and reviewed concerns you should keep in mind as you design and build your SaaS applications, cross-referencing the best practices for SaaS with compliance requirements. We've provided a number of free resources you can leverage to align with compliance best practices while building our SaaS solution. Finally, we discussed how you can use AWS compliance services to monitor your solution. About AWS SaaS Factory AWS SaaS Factory helps organizations at any stage of the SaaS journey. Whether looking to build new products, migrate existing applications, or optimize SaaS solutions on AWS, we can help. Visit the AWS SaaS Factory Insights Hub to discover more technical and business content and best practices. SaaS builders are encouraged to reach out to their account representative to inquire about engagement models and to work with the AWS SaaS Factory team. Sign up to stay informed about the latest SaaS on AWS news, resources, and events. About AWS Global Security and Compliance Acceleration Global Security and Compliance Acceleration on AWS Program (GSCA) supports businesses globally that need to meet security, privacy, and compliance requirements for financial services, healthcare, privacy, and public sector. This includes both commercial and public sector workloads. AWS Partners with compliances needs are encouraged to reach out to their account representative to understand program requirements and to engage with the GSCA team.

TAGS: APN Programs, AWS Partner Solutions Architects (SA), AWS SaaS Factory, SaaS Insights Hub, SaaS Journey Framework, SaaS Lens, SaaS on AWS, Software as a Service

Integrationworx Launches Amazon QuickSight Objects Management Tool OverSight with Support from SaaS Factory

Integrationworx enables customers to address various business challenges by leveraging data. It offers a diverse range of services such as analytics, data integration, data management, training, and mentoring. Integrationworx has worked with multiple customers who employ formal software development lifecycle (SDLC) processes to manage their Amazon QuickSight-based business intelligence (BI) solutions across segregated Amazon Web Services (AWS) accounts. Throughout these engagements, Integrationworx witnessed, first-hand, the complexity and effort required to migrate Amazon QuickSight assets across operating environments. This led to the idea of providing a set of operations and management tools, as an add-on to QuickSight, that helps customers manage their QuickSight SDLC in a low-friction, reliable, and repeatable way. With support from AWS SaaS Factory, Integrationworx built and launched its OverSight (OVS) solution on AWS. Oversight is a as a software-as-a-service (SaaS) application designed to enable development and DevOps teams to migrate QuickSight objects across AWS accounts. It supports the creation and management of a structured and repeatable dashboard development lifecycle (DDLC).

"We had great confidence in our delivery team to architect and implement OverSight from the very beginning. Our partnership with AWS SaaS Factory immediately turbo-charged our team with guidance and best-practices that would have taken us much more time to discover on our own, accelerating our time to market two-fold." ~ Glen Douglas, Chief Architect and Product Manager at Integrationworx

The AWS SaaS Factory team spoke with Glen Douglas, OverSight Chief Architect and Product Manager at Integrationworx, to learn more about OverSight, the value its new solution brings to customers, and the key lessons learned from the journey to SaaS on AWS. Check out OverSight in AWS Marketplace >> Q&A with Integrationworx AWS SaaS Factory: Can you tell us about yourself and role at Integrationworx? Glen Douglas: I've been working in the information technology industry for over 30 years, beginning in application development and gradually moving into roles of solution and enterprise architecture over the past 20. I am one of the five founding partners at Integrationworx, and among other roles I am the visionary and chief architect for OverSight. AWS SaaS Factory: Can you tell us a bit about the history of Integrationworx as a Systems Integrator? Glen Douglas: Integrationworx helps people and companies solve business problems using data. This includes empowering and transforming the business, dealing with data from mergers and acquisitions, managing data/information as an asset, discovering efficiencies, and identifying new markets and opportunities. Our service offerings include a wide range of data challenges, such as analytics, data integration, and data management. We pride ourselves in training and mentoring, and our relationship with AWS allows us to offer managed services if needed/desired. Integrationworx has covered all North America for over 18 years. AWS SaaS Factory: Who are your customers? Glen Douglas: OverSight is specifically targeted at AWS customers that use Amazon QuickSight to deliver business intelligence and analytics solutions:

Within their own organization and departments. As part of product and/or solution offering to their customers. As service offerings to other customers.

Our customers look to OverSight to provide a simple, fast, and reliable service to manage and copy QuickSight objects across AWS accounts they manage, without the need to learn complex APIs and command line interface (CLI) commands. This could be for a variety of use cases, including:

Support of formal SDLC processes across segregated environments. Supporting QuickSight-based solution deployments. General migration or duplication of QuickSight assets across environments.

AWS SaaS Factory: How does OverSight work and what are some of the key customer benefits? Glen Douglas: OverSight is a web application and set of integration services, which a customer can use to manage their QuickSight environments. The web application provides an easy-to-use interface and release management workflow for defining and managing groups of QuickSight assets called "Applications." Applications represent a logical container for managing, versioning, and distributing a set of QuickSight objects as a single unit of work. Once defined, OverSight users can create versioned "Releases" of their Applications representing a point-in-time snapshots of their assets, which in turn can be used as deployment packages and/or backups of their work. Once created, Releases can be "deployed" to another QuickSight account; all with the click of a button. OverSight customers realize a number of key benefits including:

Ready-to-use solution to quickly copy QuickSight objects across accounts. Turn-key release management tool to support their QuickSight SDLC. Version control, and backup and recovery of QuickSight objects. Zero investment of knowledge acquisition and retention of complicated QuickSight CLI and API. SaaS-based delivery of service for a monthly subscription.

AWS SaaS Factory: How can users get started using OverSight? Glen Douglas: Users can sign-up for a free 21-day trial of OverSight via AWS Marketplace, and be operating in a matter of minutes. End users can use the OverSight web application to configure one or more of their QuickSight (AWS) accounts to work with the OverSight integration service. With the configuration in place, end users can start defining their Applications and creating releases and deployments to other QuickSight accounts they have configured. AWS SaaS Factory: Can you walk us through the architecture? What are some of the AWS services you leveraged? Glen Douglas: The OverSight architecture is an adaptation of the AWS Serverless SaaS reference architecture, introduced to us by the SaaS Factory team. OverSight is based entirely on AWS serverless technologies, which allows us to minimize our operational costs and provide an extremely cost-effective solution to our customers. Some of the key services employed in the OverSight end-to-end architecture include:

Amazon CloudFront: Provides content delivery of web application(s) representing frontend user interface (UI) to customers and internal back-office management. AWS AppSync: Hosts GraphQL APIs used between our presentation layer and microservices. AWS Lambda: Hosts microservice workloads for managing core application functionality and interfacing to QuickSight. Amazon DynamoDB: Provides application data persistence using fast, flexible NoSQL database service. AWS Identity and Access Management (IAM): Securely manage trusted-relationships between OverSight service and customer AWS accounts and access to other AWS services and resources. Amazon Cognito: Hosts user/tenant pools and application authentication, authorization, and user management.

The solution employs numerous other AWS services including Amazon Simple Storage Service (Amazon S3), Amazon CloudWatch, Amazon API Gateway, AWS CodePipeline, and AWS CloudFormation. AWS SaaS Factory: What were some of the technical challenges when moving to a SaaS delivery model? Glen Douglas: Prior to engaging with AWS SaaS Factory, our first product release of OverSight was designed as a customer-hosted solution and employed a CLI-like interface implemented in AWS Step Functions. Moving from this customer-hosted architecture to a SaaS-based solution delivery required us to rethink the following:

Design and implementation of our security model to allow OverSight to manipulate QuickSight objects within the customer's AWS accounts. Supporting multi-tenancy within all layers of the solution (UI, services, security, and data persistence). Functional design and feature bundling to accommodate future usage and pricing models. Integration with AWS Marketplace.

AWS SaaS Factory: Can you share how AWS SaaS Factory helped you address these technical challenges? Glen Douglas: The transition of OverSight to a SaaS-based delivery model was drastically accelerated with the help of the SaaS Factory team and guidance they provided. Initial workshops provided a forum for our team to review architecture goals and vision, and receive feedback and suggestions on best practices for SaaS delivery implementation. A key factor in our success was the AWS SaaS Factory team's introduction of the Serverless SaaS reference architecture and solution, which immediately addressed a number of backlog items in our solution architecture. The reference architecture formed the basis for discussions on a variety of relevant design subjects, including support of multi-tenancy, cross-account security, and scalability. Regular touchpoint meetings with the SaaS Factory team provided a venue for deep-dive discussions on specific design patterns and reviews of our solution components as they evolved. AWS SaaS Factory: Can you walk us through how the SaaS Factory team supported your business efforts? Glen Douglas: The SaaS Factory team brought entirely different perspectives to both our approach to our go-to-market (GTM) strategy and solution architecture. Business workshops provided us with deep market research on the evolution of SaaS, as well as current trends and approaches to consider in our feature-based bundling and pricing. AWS SaaS Factory: What advice would you share with other organizations as they navigate to a SaaS delivery model? Glen Douglas: First, do your research to ensure your solution is suitable for a SaaS-based delivery model and be prepared to refactor or redesign aspects of your solution to leverage SaaS delivery capabilities. Take advantage of AWS best practices and leverage relationships with your AWS Partner Solution Architects to engage experts as needed. Seek out an AWS SaaS Factory team subject matter expert (SME) as they can provide both technical and business guidance on your SaaS journey, without re-inventing the wheel. Finally, look for opportunities to employ serverless technologies within your architecture to reduce operational costs and simplify performance efficiency within your offering. About AWS SaaS Factory AWS SaaS Factory helps organizations at any stage of the SaaS journey. Whether looking to build new products, migrate existing applications, or optimize SaaS solutions on AWS, we can help. Visit the AWS SaaS Factory Insights Hub to discover more technical and business content and best practices. SaaS builders are encouraged to reach out to their account representative to inquire about engagement models and to work with the AWS SaaS Factory team. Sign up to stay informed about the latest SaaS on AWS news, resources, and events. Integrationworx – AWS Partner Spotlight Integrationworx offers services and solutions for business data including data integration, data management and analytics. Contact Integrationworx | Partner Overview | AWS Marketplace

TAGS: AWS Partner References, AWS Partner Solutions Architects (SA), AWS SaaS Factory, Integrationworx, SaaS Factory Insights Hub, SaaS on AWS, Software as a Service

10DLC Registration Best Practices to Send SMS with AWS End User Messaging

Updated 08/04/2025 to include updates to abandoned cart use cases What is 10DLC? Ten-Digit Long Code, or more commonly shortened as 10DLC, is intended specifically for sending Application-to-Person (A2P) SMS in the United States only. If you don't send text messages to recipients in the US, then 10DLC doesn't apply to you. 10DLC was designed to cover the volume and throughput middle ground between toll-free numbers on the low end and short codes on the high end. All senders using 10DLC are required to register both their company and their campaign(s), which is managed by a third-party company called The Campaign Registry (TCR). TCR maintains an industry-wide database of companies and use cases that are authorized to send messages, to US registered handsets, using 10DLC phone numbers. How to Register for 10DLC Registration can be done within the AWS console as well as programmatic registration via the SMS V2 API.

Navigate to AWS End User Messaging Select "Registrations" from the left hand rail Click "Create registration" button

If you have not already registered a company then select registration type "US 10DLC brand registration" as the Registration type and give it a "Registration friendly name" you will recognize later and proceed with the best practices below. If you have already successfully registered a company and require additional vetting proceed to "Additional Vetting" below If you have already successfully registered a company and completed the additional vetting process proceed to "Campaign Registration" below

To help ensure your registration is approved during this vetting process follow these best practices when registering. Who Should Register for a 10DLC? The information provided during registration should be for the company from whom SMS messages will be sent from.

Example 1: Company X wants to send their customers alerts via SMS should their account be compromised and there is a need to reset passwords.

In this example the company being registered is Company X.

Example 2: Company Y is an Independent Software Vendor(ISV) with 100s of their customers using their software platform. Company Z wants to give their customers the ability to send SMS from within their platform.

In this example each of Company Y's customers who want to send SMS will need to provide their information. Each of these customers will need their own separate 10DLC for each use case that Company Y wants to enable for their customers. Company Y should define very clearly for their customers the types of messages that can be sent as each of their customers will be expected to send only messages that align with the Campaign(Use-Case) that they register for.

Example 3: Company Z is an Independent Software Vendor(ISV) with 100s of their customers using their software platform. Company Z wants to provide One-Time Password(OTP) codes via SMS.

In this example the company being registered will be Company Z.

10DLC Registration Best Practices As you progress through the steps of 10DLC registration follow these best practices to ensure a smooth process. Begin here if you have not registered your company(ies) yet. Company Registration Info and Additional Company and Contact Info

Best practices for Company Registration and Additional Company and Contact Info

Make sure to enter all information correctly. Dependent on the country in which you have a Tax ID, enter into the Tax ID field one of the following:

If you select "PUBLIC_PROFIT" as your "Legal form of organization" you MUST fill out the following fields and complete the external brand verification shown in the screenshots below in the section titled "Public Profit Brand Verification Email Process"

Stock symbol Stock exchange Brand verification email – Make sure to provide your personal company email. You will receive an email from [EMAIL] to complete the brand verification.

Select the vertical that most closely aligns with your business Make sure that your website is publicly accessible. Your registration will be denied if the reviewer cannot access the site. It is a hard requirement to have both a support email and phone number

Make sure your support email and support phone number are both active

Make sure that your Company name and Email/Website domains match

If you register the company Amazon Inc. but then list a support email of [EMAIL] your registration will likely be rejected if you are considered a large enough brand that should have a dedicated email domain.

Public Profit Brand Verification Email Process – Required if you selected "PUBLIC_PROFIT" as your "Legal form of organization" Once you submit your Brand Registration you will receive an email from [EMAIL] to complete the brand verification. This may take 1-3 days to arrive. Step 1: Example email you will receive below

Once you have completed and submitted your registration, as soon as you see your Brand Registration Status show as "Complete" you are ready to move on to "Brand Vetting." Read "Additional Company Vetting for Potential Increased Quotas" below for next steps.

Additional Company Vetting for Potential Increased Quotas Once you have completed the initial Company registration you have the following quotas assigned to your business:

AT&T: 1.25 Messages Per Second(MPS) or 75 Transactions Per Minute(TPM) T-Mobile = 2000 messages/day

The quotas above do not mean that you cannot message recipients who use other carriers, these are just limits that these carriers have published. If the throughput above isn't enough for your business's needs you can apply for US 10DLC brand vetting, for a $40 fee.

Click the "Create Registration" button again and select "US 10DLC brand vetting" as the "Registration type." Select the radio button for the brand you previously registered. This vetting will be applied to that brand.

If you have multiple brands you will need to do this for each of them

The Campaign Registry, a third-party provider, will then do a deeper vetting of the information you have already provided and will give your company a score that will determine the throughput and volume apportioned to you. Read here for a detailed breakdown of the possible scores and the quotas that are attached to them. Note: Vetting doesn't guarantee that your carrier throughput or daily volume will increase. It is possible for the vetting results to decrease carrier throughput and daily volume. 10DLC Campaign Registration Once you have completed the registration process and the optional additional vetting you will need to register your Campaigns, which should align with your use-case(s). If you would like more detail for each of the 10DLC Campaign types that End User Messaging supports you can read more here.

Provide a clear and comprehensive overview of the campaign's objectives and interactions the end-user would experience after opting in. Make sure to identify who the sender is, who the recipient is, and why messages are being sent to the intended recipient

Example: One-Time Password messages are sent by Company X to its customers for purposes of authentication to log into our application

Select the segment in which your company most closely operates in

You can either enter the URL if it is publicly accessible or upload a file that contains them The file can be up to 500KB and valid file formats are PNG, JPEG, and PDF Please read this blog for the boiler plate verbiage that needs to be included

You can either enter the URL if it is publicly accessible or upload a file that contains it The file can be up to 500KB and valid file formats are PNG, JPEG, and PDF Please read this blog for the boiler plate verbiage that needs to be included

The primary purpose of the Opt-in workflow is to demonstrate that the end user explicitly consents to receive text messages and understands the nature of the program. Your application is being reviewed by a 3rd party reviewer so make sure to provide clear and thorough information about how your end-users opt-in to your SMS service and any associated fees or charges. If the reviewer cannot determine how your opt-in process works then your application will be denied and returned.

This blog details the requirements that carriers have for a compliant SMS opt-in process Note: If you have a use case that is internal to your business, you are still required to demonstrate explicit opt-in consent from the recipients. There are no exceptions to having an opt-in workflow.

The description has to be between 40 – 2048 characters and must not contain leading or trailing spaces The Opt-in location must include the following at the point of opt-in, whether that be online, paper form, or verbal:

Program (brand) name Link to a publicly accessible Terms & Conditions page Link to a publicly accessible Privacy Policy page Message frequency disclosure. Customer care contact information Opt-out information "Message and data rates may apply" disclosure.

If your Opt-in process requires a log-in, is not yet published publicly, is a verbal opt-in, or if it occurs on printed sources such as fliers and paper forms then make sure to thoroughly document how this process is completed by the end-user receiving messages. Provide a screenshot of the Call to Action in such cases using the optional "Campaign opt-in screenshot" field below

Use this if your Opt-In Workflow requires a log-in, is not yet published publicly, is a verbal opt-in, or if it occurs on printed sources such as fliers and paper forms The supported file type are PNG, JPEG, and PDF and the maximum file size is 500KB

This is optional but if you plan on allowing for opt-in by texting into your originator you should indicate that keyword here

Provide the exact message that will be sent back to your end-users letting them know that they have successfully registered

"Welcome to AnyCo! Reply YES to confirm your subscription. Msg & data rates may apply. Text STOP to opt out and HELP for support."

Brand Name It is best practice, but not required, to do a "double opt-in" as seen in the example where the recipient will text back "YES" to confirm that they did want to register. Include "Msg & data rates may apply" as seen in the example Include opt-out language as seen in the example Include Help language as seen in the example

The "Help message" is the response that is required to be sent to end-users when they text the keyword "HELP" (or similar keywords). The purpose is to provide information to the end-user related to how they can get support or opt-out of the messaging program. The message has to be a minimum of 20 characters and a maximum of 160 characters The message must include:

Program (brand) name OR product description. Additional customer care contact information.

It is mandatory to include a phone number and/or email for end-user support

The following is an example of a HELP response that complies with the requirements of the US mobile carriers:

ExampleCorp Account Alerts: For help call 1-888-555-0142 or go to example.com. Msg&data rates may apply. Text STOP to cancel.

The "Stop message" is the response that is required to be sent to end-users when they text the keyword "STOP" (or similar keywords). End-users are required to be opted out of further messages when they text the STOP (or equivalent) keyword to your number and confirms with them that they will no longer receive messages for the program. The message has to be a minimum of 20 characters and a maximum of 160 characters The message must include:

Program (brand) name OR product description Confirmation that no further messages will be delivered

The following is an example of a compliant STOP response:

You are unsubscribed from ExampleCorp Account Alerts. No more messages will be sent. If you need help call 1-888-555-0142.

Campaign Capabilities Number capability: Choose whether or not the numbers you associate to an approved campaign can support voice outbound calling in addition to SMS. If you only require SMS you can leave the default selection of SMS-only. If you require voice calling, you should select voice as well. Selecting voice will increase the registration processing time. Message Type: The content of your messages need to align with the Campaign Type and Message Type that you select here — if it's misaligned your registration will be denied. You can't change the message type on a campaign after it's in an approved state.

Campaign Use Case End User Messaging supports all of the standard use cases available to be sent via 10DLC and a single Special use case for communications from a non-religious registered 501(c)(3) charity aimed at providing help and raising money for those in need. For a more detailed listing of the campaign use cases supported visit this page.

Select the Use case that most closely aligns to your use case.

All of the information that you provide during this process needs to align with this selection or your registration will be rejected Make sure to ONLY select a Sub use case if you select a use case of MIXED or LOW_VOLUME

Note: The "Low Volume" and "Mixed" campaigns have lower quotas which are the same as a company that does not opt for the increased vetting detailed above:

For each of the Yes/No drop down selections make sure to be truthful. These registrations are being done by humans who will be checking each of these. An untruthful answer can cause your registration to be rejected.

If you plan on using links within your messages remember that generic URL shorteners e.g. "bit.ly/LONGLINK" will be rejected. If you would like to use shorteners make sure that it is a branded shortener such as "any.co/LONGLINK"

NOTE: If you are planning on sending abandoned cart messages there are additional requirements. These requirements include:

Additional information in your Privacy Policy and Terms and Conditions Opt-in process must be double opt-in and include specific verbiage for the use case Content restrictions on time and frequency

Details about this can be found in: CTIA Guidelines 3.16 Section 6.2 of T-Mobile's code of conduct

Subscriber opt-in is automatically set to "Yes" on your behalf. Explicit opt-in is required of all end-users regardless of your use case.

You are required by carriers to opt-out end users at their request. This is generally done through the opt-out keyword 'STOP'. More information related to opt-outs and how to manage them effectively can be found here

Carriers require that your SMS numbers reply to the 'HELP' keyword or similar at all times regardless of the numbers opt-in status. More information related to HELP auto-response requirements can be found in End User Messaging best practices documentation here

If you are a 1st party lender you can get approval for transactional use cases (loan transaction receipts, OTPs, etc.). If your company is related to the lending business then you must mark this as "yes"

If you have supplied messaging examples with an embedded link you must mark this as a "yes." If this is misaligned with your content then your registration will be rejected

Note: Generic link shorteners such as Bitly or TinyURL should not be used and may cause your registration to be rejected. Make sure that any links in your sample messages are branded and consistent with your domain

If you have supplied messaging examples with an embedded phone number you must mark this as a "yes." If this is misaligned with your content then your registration will be rejected

There is a potential to be rejected or for the campaign to be suspended later if your content includes age gated material and you do not mark "yes" here If they are do they need to do anything different here?

Message Samples Sample messages should reflect actual messages to be sent under the campaign you are registering for. It is critical to ensure that there is consistency between the use case, your campaign description, and the content of the messages.

Sample messages should reflect actual messages to be sent under campaign Indicate any templated fields that are variable with brackets and make sure to be clear with what information may be replaced

Example: Hi, [FirstName] this is Amazon inc. letting you know that your delivery is ready

Each sample message has to be a minimum of 20 characters. If you plan to use multiple message templates for this 10DLC campaign, include them as well Sample messages should identify who is sending the message (brand name)

Make sure your messaging does not involve prohibited content such as cannabis, hate speech, etc. and that your use case is compliant with AWS Messaging Policy

What to do if your 10DLC campaigns are rejected If your Company registration or Campaign registration is rejected please follow the steps here to create a case and the AWS Support team will provide information about the reasons that your 10DLC campaign registration was rejected in your AWS Support case.

How to Set Up a Platform That Effectively Supports Your Development Teams

Many of my conversations with AWS customers are about their attempts to build developer experience platforms that simplify software development and operations, automate deployments, improve software quality, reduce costs, and ensure security and compliance. Unfortunately, not all platforms live up to their expectations. The most frequently cited problem is development teams rejecting the platform that is supposed to support them. Other problems include a lack of collaboration between the platform teams and the development teams and complex platform requirements. In this blog post, I will give you an overview of how you can set up a platform to effectively support your development teams. The Purpose of Platforms The main purpose of IT platforms is to provide delivery teams with simple, efficient, and stress-free toolchains to build and run software so that they can focus on solving business problems faster. A platform team is an enabling organizational unit

that provides tooling-as-a-service to other IT teams, mostly software and data engineering teams; that helps to onboard, support, and advise these teams on the platform's tools and services; that facilitates alignment between the users of the platform on the scope, roadmap, and priority of the platform's tools and services; and that is accountable for ensuring product development teams adhere to defined global guidelines regarding security, compliance, and financial aspects

Platform Scope Developer experience platforms can offer a wide range of tools and services designed to support the development and operations of transactional and analytical applications. Typical products include tools and services for software development and testing, CI/CD pipelines, monitoring and logging, containerization and orchestration, service and data discovery, analytics, security, collaboration, and billing and backup services. In some organizations, platforms provide hosting services for deploying and running applications. Platform Size As a rule of thumb, 10% to 20% of a company's product development should consist of platforms. This may vary over time and depends on the scope of the platform. Resources may be greater at the beginning of platform development and scaled down as the functionality becomes sufficient and the platform scope experiences fewer changes. Characteristics of a Platform Team Platform teams are product teams for internally used products. They are small, diverse, cross-functional, and self-organizing. They include every role required to design, build, and operate platform products. Effective teams include 6–10 people, with some leeway. The roles required depend on your organization's current operating model and maturity. Typical roles include product owners, software developers, and sometimes quality assurance (QA) engineers and IT administrators. Platform product owners should understand the socio-technical domain of developer tooling (probably as former users of the platform's products) and have ideation, communication, facilitation, and delivery management skills. Organizations sometimes underestimate the role of the product owner—this is a mistake. The product owner plays a key role in the success of a platform. Successful platform teams work backward from user needs. It is important to have team members with both operational and development backgrounds. Developers should be able to work across the technology stack of their team's responsibility with expertise in a specific area (these are called T- or V-shaped developers). In some organizations, developers are also responsible for testing their products. Other organizations add the role of a dedicated QA engineer, a software developer focused on test automation. I recommend empowering and training developers to quality assure their own products. The fewer roles there are on a team, the more flexible and effective it is. If the QA engineer is a dedicated role, they should be part of the platform team instead of a separate QA team. This allows them to take responsibility for a specific part of the platform over an extended period and run their services independently. How Do You Know If Your Platform Is Good? It is difficult to measure whether a platform is achieving its goal, which makes it important to gather the right data and anecdotes. A good indicator is the net promoter score (NPS), a metric based on asking platform users if they would recommend the service to a new member. Another indicator is the adoption rate of nonmandatory services in delivery teams. These two metrics are general indicators, but you can get very specific metrics about a new version of a platform service by deploying it in an A/B test. In this method, some delivery teams use the new service while a control group doesn't. You can measure and compare certain capabilities like cycle time, throughput, release frequency, or duration for both groups. How to Get Platform Adopted I have successfully used three strategies to overcome problems in getting platform services adopted:

Involve product team representatives in making decisions for the platform's development—but don't treat them as just a sounding board; grant them decision authority regarding the scope and prioritization of the platform's products. Rotate engineers between the user teams and platform teams. This way, the engineers who will use the platform are actively involved in building it and can ensure that it is designed to meet their needs. Conversely, the platform engineers have to eat their own dog food while working on the consuming engineering teams. A reasonable rotation length is a few weeks but no longer than a quarter. Avoid speculative design and gold plating at all costs. Don't generalize too soon. Only build a product when other consuming teams have committed to using it. It is even better to prototype platform products for a single team (perhaps with additional capacity from the platform teams) to learn and use in one context. Only move the product to platform ownership and generalize it when two other teams commit to using it.

Conclusion Platforms are an important part of larger product development organizations. Among other benefits, they can implement centralized and automated that policies improve your compliance and security. When set up correctly, they can improve the quality, cycle time, throughput, and motivation of your delivery teams. The principles I've given here should help you get the most of your platform and the team that creates and manages it. What are your experiences with developer experience platforms? I would be interested in hearing about some of them.

TAGS: Agility, Best Practices, Business Value, Cloud Transformation, Culture, DevOps, Digital Transformation, enterprise strategy, Migration, Organizational Agility

Matthias Patzak Matthias joined the Enterprise Strategist team in early 2023 after a stint as a Principal Advisor in AWS Solutions Architecture. In this role, Matthias works with executive teams on how the cloud can help to increase the speed of innovation, the efficiency of their IT, and the business value their technology generates from a people, process and technology perspective. Before joining AWS, Matthias was Vice President IT at AutoScout24 and Managing Director at Home Shopping Europe. In both companies he introduced lean-agile operational models at scale and led successful cloud transformations resulting in shorter delivery times, increased business value and higher company valuations

How Amazon Shopping uses Amazon Rekognition Content Moderation to review harmful images in product reviews

Customers are increasingly turning to product reviews to make informed decisions in their shopping journey, whether they're purchasing everyday items like a kitchen towel or making major purchases like buying a car. These reviews have transformed into an essential source of information, enabling shoppers to access the opinions and experiences of other customers. As a result, product reviews have become a crucial aspect of any store, offering valuable feedback and insights to help inform purchase decisions. Amazon has one of the largest stores with hundreds of millions of items available. In 2022, 125 million customers contributed nearly 1.5 billion reviews and ratings to Amazon stores, making online reviews at Amazon a solid source of feedback for customers. At the scale of product reviews submitted every month, it is essential to verify that these reviews align with Amazon Community Guidelines regarding acceptable language, words, videos, and images. This practice is in place to guarantee customers receive accurate information regarding the product, and to prevent reviews from including inappropriate language, offensive imagery, or any type of hate speech directed towards individuals or communities. By enforcing these guidelines, Amazon can maintain a safe and inclusive environment for all customers. Content moderation automation allows Amazon to scale the process while keeping high accuracy. It's a complex problem space with unique challenges and requiring different techniques for text, images, and videos. Images are a relevant component of product reviews, often providing a more immediate impact on customers than text. With Amazon Rekognition Content Moderation, Amazon is able to automatically detect harmful images in product reviews with higher accuracy, reducing reliance on human reviewers to moderate such content. Rekognition Content Moderation has helped to improve the well-being of human moderators and achieve significant cost savings.

Moderation with self-hosted ML models The Amazon Shopping team designed and implemented a moderation system that uses machine learning (ML) in conjunction with human-in-the-loop (HITL) review to ensure product reviews are about the customer experience with the product and don't contain inappropriate or harmful content as per the community guidelines. The image moderation subsystem, as illustrated in the following diagram, utilized multiple self-hosted and self-trained computer vision models to detect images that violate Amazon guidelines. The decision handler determines the moderation action and provides reasons for its decision based on the ML models' output, thereby deciding whether the image required a further review by a human moderator or could be automatically approved or rejected.

With these self-hosted ML models, the team started by automating decisions on 40% of the images received as part of the reviews and continuously worked on improving the solution through the years while facing several challenges:

Ongoing efforts to improve automation rate – The team desired to improve the accuracy of ML algorithms, aiming to increase the automation rate. This requires continuous investments in data labeling, data science, and MLOps for models training and deployment. System complexity – The architecture complexity requires investments in MLOps to ensure the ML inference process scales efficiently to meet the growing content submission traffic.

Replace self-hosted ML models with the Rekognition Content Moderation API Amazon Rekognition is a managed artificial intelligence (AI) service that offers pre-trained models through an API interface for image and video moderation. It has been widely adopted by industries such as ecommerce, social media, gaming, online dating apps, and others to moderate user-generated content (UGC). This includes a range of content types, such as product reviews, user profiles, and social media post moderation. Rekognition Content Moderation automates and streamlines image and video moderation workflows without requiring ML experience. Amazon Rekognition customers can process millions of images and videos, efficiently detecting inappropriate or unwanted content, with fully managed APIs and customizable moderation rules to keep users safe and the business compliant. The team successfully migrated a subset of self-managed ML models in the image moderation system for nudity and not safe for work (NSFW) content detection to the Amazon Rekognition Detect Moderation API, taking advantage of the highly accurate and comprehensive pre-trained moderation models. With the high accuracy of Amazon Rekognition, the team has been able to automate more decisions, save costs, and simplify their system architecture.

Improved accuracy and expanded moderation categories The implementation of the Amazon Rekognition image moderation API has resulted in higher accuracy for detection of inappropriate content. This implies that an additional approximate of 1 million images per year will be automatically moderated without the need for any human review. Operational excellence The Amazon Shopping team was able to simplify the system architecture, reducing the operational effort required to manage and maintain the system. This approach has saved them months of DevOps effort per year, which means they can now allocate their time to developing innovative features instead of spending it on operational tasks. Cost reduction The high accuracy from Rekognition Content Moderation has enabled the team to send fewer images for human review, including potentially inappropriate content. This has reduced the cost associated with human moderation and allowed moderators to focus their efforts on more high-value business tasks. Combined with the DevOps efficiency gains, the Amazon Shopping team achieved significant cost savings. Conclusion Migrating from self-hosted ML models to the Amazon Rekognition Moderation API for product review moderation can provide many benefits for businesses, including significant cost savings. By automating the moderation process, online stores can quickly and accurately moderate large volumes of product reviews, improving the customer experience by ensuring that inappropriate or spam content is quickly removed. Additionally, by using a managed service like the Amazon Rekognition Moderation API, companies can reduce the time and resources needed to develop and maintain their own models, which can be especially useful for businesses with limited technical resources. The API's flexibility also allows online stores to customize their moderation rules and thresholds to fit their specific needs. Learn more about content moderation on AWS and our content moderation ML use cases. Take the first step towards streamlining your content moderation operations with AWS.

About the Authors Shipra Kanoria is a Principal Product Manager at AWS. She is passionate about helping customers solve their most complex problems with the power of machine learning and artificial intelligence. Before joining AWS, Shipra spent over 4 years at Amazon Alexa, where she launched many productivity-related features on the Alexa voice assistant. Luca Agostino Rubino is a Principal Software Engineer in the Amazon Shopping team. He works on Community features like Customer Reviews and Q&As, focusing through the years on Content Moderation and on scaling and automation of Machine Learning solutions. Lana Zhang is a Senior Solutions Architect at AWS WWSO AI Services team, specializing in AI and ML for Content Moderation, Computer Vision, Natural Language Processing and Generative AI. With her expertise, she is dedicated to promoting AWS AI/ML solutions and assisting customers in transforming their business solutions across diverse industries, including social media, gaming, e-commerce, media, advertising & marketing.

New – Amazon EC2 M7a General Purpose Instances Powered by 4th Gen AMD EPYC Processors

In November 2021, we launched Amazon EC2 M6a instances, powered by 3rd Gen AMD EPYC (Milan) processors, running at frequencies up to 3.6 GHz, which offer you up to 35 percent improvement in price performance compared to M5a instances. Many customers who run workloads that are dependent on x86 instructions, such as SAP, are looking for ways to optimize their cloud utilization. They're taking advantage of the compute choice that EC2 offers. Today, we're announcing the general availability of new, general purpose Amazon EC2 M7a instances, powered by the 4th Gen AMD EPYC (Genoa) processors with a maximum frequency of 3.7 GHz, which offer up to 50 percent higher performance compared to M6a instances. This increased performance gives you the ability to process data faster, consolidate workloads, and lower the cost of ownership. M7a instances support AVX-512, Vector Neural Network Instructions (VNNI) and brain floating point (bfloat16). These instances feature Double Data Rate 5 (DDR5) memory, which enable high-speed access to data in-memory, and deliver 2.25 times more memory bandwidth compared to M6a instances for lower latency. M7a instances are SAP-certified and ideal for applications that benefit from high performance and high throughput, such as financial applications, application servers, simulation modeling, gaming, mid-size data stores, application development environments, and caching fleets. M7a instances feature sizes of up to 192 vCPUs with 768 GiB RAM. Here are the detailed specs:

Name vCPUs Memory (GiB) Network Bandwidth (Gbps) EBS Bandwidth (Gbps)

M7a instances have up to 50 Gbps enhanced networking and 40 Gbps EBS bandwidth, which is similar to M6a instances. But you have a new medium instance size, which enables you to right-size your workloads more accurately, offering 1 vCPUs, 4 GiB, and the largest size offering 192 vCPUs, 768 GiB. Additionally, with M7a instances you can attach up to 128 EBS volumes to an instance compared to up to 28 EBS volume attachments with M6a instances. The new instances are built on the AWS Nitro System, a collection of building blocks that offloads many of the traditional virtualization functions to dedicated hardware for high performance, high availability, and highly secure cloud instances. Now Available Amazon EC2 M7a instances are now available today in AWS Regions: US East (Ohio), US East (N. Virginia), US West (Oregon), and EU (Ireland). As usual with Amazon EC2, you only pay for what you use. For more information, see the Amazon EC2 pricing page. To learn more, visit the EC2 M7a instance and AWS/AMD partner page. You can send feedback to [EMAIL] AWS re:Post for EC2, or through your usual AWS Support contacts. — Channy

Channy Yun (윤석찬) Channy is a Lead Blogger of AWS News Blog and Principal Developer Advocate for AWS Cloud. As an open web enthusiast and blogger at heart, he loves community-driven learning and sharing of technology.

Introducing customer-defined partition keys for Amazon Timestream: Optimizing query performance

Amazon Timestream is a fully managed, scalable, and secure time series database designed for workloads such as infrastructure observability, user behavioral analytics, and Internet of Things (IoT) workloads. It's built to handle trillions of events per day, and designed to scale horizontally to meet your needs. With features like multi-measure records and scheduled queries, Timestream enables you to analyze time series data and gain valuable insights in a cost-effective way. The flexible data model of Timestream allows you to store and query data in a way that makes sense for your use case. Whether you are tracking performance metrics for a fleet of devices, or analyzing customer behavior in real time, Timestream can address your needs. And now, with the introduction of customer-defined partition keys, you can access your time series data faster than before due to query optimizations tailored to your specific needs. The need for customer-defined partition keys Many customers and internal services, like our internal Alexa Voice Service (AVS) team, uses Timestream because their use cases require highly scalable ingestion and optimized query execution. As their systems scaled over time, we learned that though the existing partitioning schema enabled scaling, there was still a need for a flexible options that could accommodate query patterns more attuned to their use cases. This differed from the fleet-wide trends tracking and analysis on measures we had initially optimized for. With this in mind, we are excited to announce the launch of customer-defined partition keys, a new feature that provides you the flexibility you need to speed up your queries and derive insights more efficiently based on your specific time series data-related needs. Partitioning is a technique used to distribute data across multiple physical storage units, allowing for faster and more efficient data retrieval. With customer-defined partition keys feature, customers can create a partitioning schema that better aligns with their query patterns and use cases. In Timestream, a record is a single data point in a time series. Each record contains three parts:

Timestamp – This usually indicates when data was generated for a given record Set of dimensions – This is metadata that uniquely identifies an entity (for example, a device) Measures – These represent the actual value being measured by a record and tracked in a time series

To learn more about Timestream key concepts, refer to Amazon Timestream concepts. By allowing you to choose a specific dimension to partition your data on, Timestream reduces the amount of data scanned during queries, thereby substantially reducing query latency for access patterns that match your partitioning schema. For instance, filtering by customer ID, device ID, or location was a very common access pattern that many of our customers use. By allowing you to choose any of such dimensions as your partition key, you can optimize your queries and get the most out of your time series data. With this added flexibility, Timestream will be able to better adapt to specific customer workloads, and we are excited to see the innovative ways you will use this new feature to extract more value from your data. How customer-defined partition keys work To get started, simply create a new table and select a specific partition key based on your most common access pattern. Usually, this is your main filtering condition in your query predicate (for example, customer ID or device ID). Identifying and selecting the right dimension by which to partition will ensure you get the best possible performance from the feature. Also, you can optionally configure your timestream table to accept only records containing non-null values for your partition key. This will further optimize partitioning the data in your table for the best query performance. When choosing the partition key type, don't forget to choose Dimension if you want to choose you own dimension. If you chose Measure name, Timestream will adopt the default partitioning schema, which is better suited to analyze fleet-wide variations of a specific measure over time.

When we talk about common access patterns, we're referring to the types of filters or predicates that are typically used when querying time series data. These filters may include things like a specific customer ID, device ID, or any other high-cardinality dimension that's important to your business. For this hypothetical use case where the partitioning key is DeviceModel, it could be something like the following code:

Select DeviceModel, segment, Event_name, count(event_name) as occurrences from events where event_type = 'voice chrome latency' and DeviceModel in ('xxx-xxx-xx' , 'yyy-yyy-yy', 'zzz-zzz-zz', '%-012') and time >= ago(1h) group by 1, 2,3 order by occurrencesSQL

For example, if you're using Timestream to store data from smart devices or IoT applications, you might want to partition your data based on the device model. This would allow you to quickly retrieve data for a specific set of devices without having to scan through all of the data in the table. Similarly, if you're storing data about customer interactions, consider partitioning your data based on the customer ID to quickly retrieve all interactions for a specific customer. This will also provide visibility of all customer-associated devices. In cases where you could have multiple different access patterns that could be derived from the same raw data, you could even generate a scheduled query with a different partition key optimized for different use cases, for example, one to understand behavior by device model and another for root cause analysis related to a specific customer issue. By understanding your most common access patterns, you can choose the dimension that will be most beneficial as a partition key. This will allow Timestream to optimize query performance for those specific access patterns, leading to faster and more efficient analysis of your time series data. After you choose your partitioning dimension, Timestream automatically partitions the ingested data to optimize the performance of queries that are based on such partitioning. It's important to carefully consider the query patterns and select the most appropriate dimension as the partition key because once the table is partitioned, it cannot be changed. To ensure the accuracy of your data and maximize query efficiency, we are also launching a new capability called schema enforcement. This will allow you to configure the table so that it will reject write requests that don't contain the dimension that is being used as the partition key. This will help ensure that your data is properly partitioned, leading to improved query performance and faster insights. It's recommended to set the enforcement level to REQUIRED, which ensures that you will get the full benefits of your partitioning schema. However, we understand that there may be cases where a small amount of data might not contain the partitioning dimensions, and it's still the customer's preference to accept those records. In such cases, setting the enforcement level to OPTIONAL can be useful. All records that don't contain the required dimension will still be written to the system, but will be collocated together. The enforcement level can be changed at any time during the table existence. Using customer-defined partition keys: A case study Let's look at an example of how our internal AVS device monitoring teams benefited from customer-defined partition keys feature to monitor the performance of millions of in-field connected device performance. AVS device monitoring team ingests data from millions of connected devices on a daily basis, including device metrics, system health metrics, and cloud-side metrics. The goal is to ensure a delightful customer experience across all their devices. With such a large volume of data, it's important to have an efficient way of analyzing data at the entity level. In this particular use case, an entity is an individual device or all the devices owned by a particular form factor segment . The device monitoring systems needs to analyze device and cloud side metrics near real time, detect any anomalies, and proactively mitigate these issues to minimize long-lasting impact on customer engagement. The following diagram illustrates the architecture that was implemented for the above use case.

One possible data model could include data for region, device type, health metric, measure and more. By defining a partition key on Device Type, for example, it optimizes entity-level queries, such as root cause analysis of issues or deviations based on this particular attribute. This partitioning schema efficiently collocates data in a way that closely resembles their specific query patterns, thereby reducing the amount of data scanned during queries. This improved query performance, allowing for faster insight generation or issue detection, so the team could act quickly to provide the best possible query latency. Let's look at an example data model.

This data model contains few dimensions with a cardinality that could be in the millions. In this data model, device monitoring team chose the device_type dimension as the partition key. Let's look at a few examples that would see performance benefits if data is partitioned by app_name, Device_type, or device_model (Timestream allows only one partition-key to be chosen on a table). With this schema, we could get very valuable insights, such as the following:

If your primary use case requires finding all devices used by a specific application ranked by times used, your ideal partition key is app_name. If your primary use case is to find all failed and successful request events for a specific segment and calculate the average latency or outcome experience by device type, your ideal partition key is segment. If your primary use case is to calculate latencies related to successful interactions, responses, and successful prompt detection for all domain events, your ideal partition key is Domain. If your goal is to generate dashboards that show the most problematic device type, based on failed events around the world, your ideal partition key is device_type)

By utilizing customer-defined-partition-key feature, in conjunction with other features like scheduled queries, we have observed improved levels of query performance and further cost optimization thanks to more efficient data access patterns. To learn more about scheduled queries, refer to Using scheduled queries in Timestream. Now it's to optimize your queries! Customer-defined partition keys feature can be a powerful tool for optimizing query performance on entity-level analysis. By understanding the most common access patterns and choosing a high-cardinality dimension that fits most data access needs, you can benefit from faster query times and improved insights. In the above use case for device monitoring, optimizing queries for specific dimension-level analysis helped the team to better understand their device performance and improve their service accordingly. With the added benefit of schema enforcement, you can ensure that your tables are properly configured according to your specific needs, so you can decide to either to reject or allow write requests that don't contain the dimension being used as the partition key. Get started with Amazon Timestream and Customer Defined Partition Keys for Free, taking advantage of our 30-day Free trial and start enhancing your time-series workloads!

Victor Servin is a Senior Product Manager for the Amazon Timestream team at AWS, bringing over 18 years of experience leading product and engineering teams in the Telco vertical. With an additional 5 years of expertise in supporting startups with Product Led Growth strategies and scalable architecture, Victor's data-driven approach is perfectly suited to drive the adoption of analytical products like Timestream. His extensive experience and commitment to customer success allows him to help customers to efficiently achieve their goals. Yogeesh Rajendra is a tenured Engineering Tech Lead at Alexa Amazon in Bellevue, WA, USA. With over 10 years of expertise in software development and AWS technologies, he excels in building highly scalable and robust distributed systems for customers. Yogeesh's focus is on handling high-volume, high-variance, and high-velocity data, ensuring that customer experiences on Alexa-enabled devices worldwide are seamless and never compromised. His technical leadership and collaborative approach enable him to deliver innovative solutions, continuously enhancing the capabilities and performance of Alexa devices. Ramsundar Muthusubramanian is a Senior Data Engineer at Alexa Amazon in Bellevue. With over 12 years of experience in Data Warehousing and Analytics, Ram had delivered big data projects like Realtime Engine Telemetrics, Demand Forecasting Data Analytics, Realtime Certification Analytics using a range of AWS Services like Redshift, Kinesis Streams, Dynamo DB, Kinesis Firehose, Lambda. Excels in designing data lake architectures and solving data problems with best data modeling practices. Seshadri Pillailokam is a Senior Software Dev Manager, Alexa Developer Experience and Customer Trust. He runs the engineering org that builds few monitoring products for Alexa using various AWS technologies. For the last few years, he has been solving security, and risk challenges through various big data technologies, to keep his customers secure. Previously he worked for over a decade on developer facing tools and technologies for Amazon, and others. Brutus Martin is a Software Development Manager, Alexa Developer Experience and Customer Trust. His team is responsible for building platforms, tools and systems that are required to monitor 3P contents & devices to ensure customer safety and trust and better experience. Brutus has 17 years of experience in building software services and products. His team's main expertise is in building distributed scalable and extensible systems to detect anomalies, analyzing digital contents across text, image, audio and video in terms of policy, security and functional infractions that could cause a trust busting experience for customers. Brutus also manages data team that integrates data from various systems across Alexa teams to build business and technical insights.

Security is fundamental for each product and service you are building with. Whether you are working on the back-end or the data and machine learning components of a system, the solution should be securely built. In 2022, we discussed security in our post Let's Architect! Architecting for Security. Today, we take a closer look at general security practices for your cloud workloads to secure both networks and applications, with a mix of resources to show you how to architect for security using the services offered by Amazon Web Services (AWS). In this edition of Let's Architect!, we share some practices for protecting your workloads from the most common attacks, introduce the Zero Trust principle (you can learn how AWS itself is implementing it!), plus how to move to containers and/or alternative approaches for managing your secrets. A deep dive on the current security threat landscape with AWS This session from AWS re:Invent, security engineers guide you through the most common threat vectors and vulnerabilities that AWS customers faced in 2022. For each possible threat, you can learn how it's implemented by attackers, the weaknesses attackers tend to leverage, and the solutions offered by AWS to avert these security issues. We describe this as fundamental architecting for security: this implies adopting suitable services to protect your workloads, as well as follow architectural practices for security. Take me to this re:Invent 2022 session!

Statistics about common attacks and how they can be launched

Zero Trust: Enough talk, let's build better security What is Zero Trust? It is a security model that produces higher security outcomes compared with the traditional network perimeter model. How does Zero Trust work in practice, and how can you start adopting it? This AWS re:Invent 2022 session defines the Zero Trust models and explains how to implement one. You can learn how it is used within AWS, as well as how any architecture can be built with these pillars in mind. Furthermore, there is a practical use case to show you how Delphix put Zero Trust into production. Take me to this re:Invent 2022 session!

AWS implements the Zero Trust principle for managing interactions across different services

A deep dive into container security on AWS Nowadays, it's vital to have a thorough understanding of a container's underlying security layers. AWS services, like Amazon Elastic Kubernetes Service and Amazon Elastic Container Service, have harnessed these Linux security-layer protections, keeping a sharp focus on the principle of least privilege. This approach significantly minimizes the potential attack surface by limiting the permissions and privileges of processes, thus upholding the integrity of the system. This re:Inforce 2023 session discusses best practices for securing containers for your distributed systems. Take me to this re:Inforce 2023 session!

Migrating your secrets to AWS Secrets Manager Secrets play a critical role in providing access to confidential systems and resources. Ensuring the secure and consistent management of these secrets, however, presents a challenge for many organizations. Anti-patterns observed in numerous organizational secrets management systems include sharing plaintext secrets via unsecured means, such as emails or messaging apps, which can allow application developers to view these secrets in plaintext or even neglect to rotate secrets regularly. This detailed guidance walks you through the steps of discovering and classifying secrets, plus explains the implementation and migration processes involved in transferring secrets to AWS Secrets Manager. Take me to this AWS Security Blog post!

An organization's perspectives and responsibilities when building a secrets management solution

Conclusion We're glad you joined our conversation on building secure architectures! Join us in a couple of weeks when we'll talk about cost optimization on AWS. To find all the blogs from this series, visit the Let's Architect! list of content on the AWS Architecture Blog.

Luca Mezzalira Luca is Principal Solutions Architect based in London. He has authored several books and is an international speaker. He lent his expertise predominantly in the solution architecture field. Luca has gained accolades for revolutionizing the scalability of front-end architectures with micro-frontends, from increasing the efficiency of workflows, to delivering quality in products.

Federica Ciuffo Federica is a Solutions Architect at Amazon Web Services. She is specialized in container services and is passionate about building infrastructure with code. Outside of the office, she enjoys reading, drawing, and spending time with her friends, preferably in restaurants trying out new dishes from different cuisines.

Vittorio Denti Vittorio Denti is a Machine Learning Engineer at Amazon based in London. After completing his M.Sc. in Computer Science and Engineering at Politecnico di Milano (Milan) and the KTH Royal Institute of Technology (Stockholm), he joined AWS. Vittorio has a background in distributed systems and machine learning. He's especially passionate about software engineering and the latest innovations in machine learning science.

Zamira Jaupaj Zamira is an Enterprise Solutions Architect based in the Netherlands. She is highly passionate IT professional with over 10 years of multi-national experience in designing and implementing critical and complex solutions with containers, serverless, and data analytics for small and enterprise companies.

Deploying Amazon VPC IP Address Manager (IPAM) using AWS Service Catalog

Planning, tracking, and monitoring IP addresses for large-scale networks can challenging. Network Administrators often use a combination of spreadsheets, confluence pages, and home-grown tools to track IP address assignments across Amazon Virtual Private Clouds (Amazon VPCs), AWS Regions, and AWS accounts. However, these methods are largely manual and prone to errors, and even a minor mistake can cause IP address conflicts that can cause issues in establishing bidirectional connectivity. This problem is amplified in large enterprise networks, where the AWS environment spans multiple AWS Organizational Units (OUs), AWS accounts, or even AWS Organizations. This is where Amazon VPC IP Address Manager (IPAM) comes in. IPAM simplifies IP address planning, tracking, and monitoring for your enterprise. With IPAM, you can release applications more quickly because developers don't have to wait for the networking team to manage IP addresses. You can find overlapping IP addresses and fix them before issues arise with network connectivity. IPAM can notify you if your IP address pools are nearing exhaustion—it lets you quickly and efficiently perform routine IP address management activities. If you combine IPAM with the power of Infrastructure-as-Code (IaC), then you can deploy IPAM across your environment quickly and in compliance with best practices. In this post, we describe a solution to turn on IPAM using AWS Service Catalog, and we walk you through it step-by-step. AWS Service Catalog uses AWS CloudFormation to abstract the underlying complexity and provides standardized deployments. Solution overview The solution described in this post uses AWS Service Catalog to provision VPC IPAM pools across multiple accounts. We create two AWS Service Catalog Portfolios: (1) IPAM Main Portfolio which comprises three Service Catalog products, and (2) IPAM Spoke Portfolio which comprises one Service Catalog product. A Service Catalog product is an IT service or application that you want to make available for deployment on AWS. Using AWS Resource Access Manager (AWS RAM), we share these products with specific AWS accounts, or the entire Organization. This makes the products available to both existing and future accounts in your organization. Users with access to the AWS Service Catalog can provision these products in a self-service manner. This solution uses CloudFormation to create the Service Catalog portfolios and its products, as shown in the following diagram (figure 1).

Figure 1: Amazon VPC IP Address Management using AWS Service Catalog

Portfolios and products overview IPAM Main Portfolio: This portfolio comprises three Service Catalog products and is shared with the NetworkHub/Networking account where the IPAM pools are to be provisioned.

Delegate IPAM Product: This product delegates the NetworkHub account as the IPAM Delegated Administrator account. IPAM CloudFormation Macro Product: This product deploys a CloudFormation Macro and the underlying AWS Lambda function which is used to perform the template processing for the macro. The Lambda function accepts the provided IPAM template (ipam-product.yml) and returns the processed template as a response. This CloudFormation Macro processes the ipam-product.yml base template to dynamically render a CloudFormation template for creating multiple IPAM resources from a single resource definition. It is primarily used to replicate the IPAM resource block for all provided AWS Regions in the pOperatingRegionList parameter value in the source template. It dynamically creates multiple IPAM Pool resources from the defined CIDR blocks in the template, for all provided pOperatingEnvironmentsList (Prod,NonProd). IPAM Product: This product is used to provision the IPAM Scope and IPAM Pools, as well as share the created IPAM Pools across multiple AWS accounts. The input is a base CloudFormation template with IPAM Scope, IPAM Pool, and AWS RAM resources. The previous CloudFormation Macro processes this template, and the processed template contains the resource blocks for each Regional and environment-specific pool described in the input parameters. It also creates resources blocks for sharing the IPAM pools across multiple AWS accounts.

IPAM Spoke Portfolio: This portfolio comprises one service catalog product and is shared across all spoke accounts.

IPAM Spoke Product: This product creates a VPC and its subnets in the spoke account using the provisioned IPAM pool to get the CIDRs for the VPC.

We assume that you have the following AWS accounts and OU structure in your environment:

Management Account – the root user for this account and the AWS Identity and Access Management (IAM) user or IAM administrator user for this account have full access to all resources within your organization. A Network Hub Account – the account which is to be used as the IPAM Delegated Administrator account.

Note that to follow along with this post, create a Network Hub Account if you don't have one already.

Resource sharing with Organizations is turned on. We assume you are familiar with the most common IPAM constructs.

An example walkthrough In this section, we consider a top-level pool (e.g., 10.0.0.0/8) that we use as an input for our IPAM solution. Two Regional pools are created from the CIDRs sourced from the top-level pool. A Production IPv4 pool and a Non-Production IPv4 pool are then created from the Regional pool. The following diagram (figure 2) shows an example of the IPAM pool hierarchy for IPv4 across multiple Regions within a top-level IPAM pool for the Organization. We do not discuss IPv6 in this post, but it follows the same pattern as IPv4.

Deployment instructions The deployment of IPAM has two phases: Phase 1: Deploy the AWS Service Catalog Portfolio in the Management Account Step 1 – Create an S3 bucket to hold the Service Catalog Portfolio CloudFormation templates

Download the s3Bucket.yml CloudFormation template to create an Amazon Simple Storage Service (Amazon S3) bucket. This template creates two S3 buckets. One bucket named control-tower-storage-{AccountId}-{Region} to hold the Service Catalog Product template files. And the second S3 bucket named control-tower-uploadbuckets3accesslogs-{AccountId}-{Region} to log the Amazon S3 server access logs of the control-tower-storage-{AccountId}-{Region} bucket. Server access logging provides detailed records for the requests that are made to the bucket. This access log information can be useful in security and access audits. Navigate to the CloudFormation console in the Management Account, and in the left pane select Stacks. Create a stack using the downloaded s3Bucket.yml template and provide your organization ID in the template parameters section. Once the stack is deployed, navigate to the Amazon S3 console and select the control-tower-storage-{AccountId}-{Region} bucket that you just created using the CloudFormation template. Upload the four Service Catalog product template files (ipam-delegate.yml, ipam-macro.yml, ipam-product.yml, ipam-spoke-product.yml) to this S3 bucket. The following diagram (figure 3) shows the CloudFormation templates uploaded to the S3 bucket.

Figure 3: IPAM Service Catalog Product templates uploaded to the S3 bucket

Step 2 – Enable trusted access for AWS Service Catalog in Organizations

Sign in to the AWS Organizations console of the Management Account. You must sign in as an IAM user, assume an IAM Role with admin privileges, or sign in as the root user (not recommended) in the Organization's management account. On the Services page, find the row for the Service Catalog service that you want to enable trusted access. Choose Enable trusted access. In the confirmation dialog box, check the box to Show the option to enable trusted access, enter enable in the box, and then choose Enable trusted access. The following diagram (figure 4) shows the trusted access enabled for AWS Service Catalog.

Step 3 – Deploy the Service Catalog Portfolios using CloudFormation

Download the ipamportfolio.yml template and navigate to the CloudFormation console to create a stack using the this template. Update the appropriate Amazon S3 URLs of the above four templates in the parameters section of the ipamportfolio.yml template, and provide the organization ID. This template creates the service catalog portfolios (IPAM Main Portfolio and IPAM Spoke Portfolio), creates the service catalog products (it does not provision the product yet – follow Phase 2 for provisioning the products) for IPAM automation, and shares the portfolio with the provided Organization. Validate the successful deployment of the resources via the CloudFormation console (as shown in the following diagram (figure 5)) and Portfolios tab on the AWS Service Catalog console (as shown in the following diagram (figure 6)).

Phase 2: Setup user access and provision the above created IPAM products using AWS Service Catalog The IPAM Service Catalog product comprises four main deployment steps: Step 1 – Delegate an IPAM Administrator Account using the Delegate IPAM Product in your Management Account

Sign in to your AWS Management Console of the Management Account. Navigate to the AWS Service Catalog console, and select Portfolios on the left navigation pane. Select the Local tab on the top and choose IPAM Main Portfolio that was created in Phase 1. On the portfolio details page, choose the Access tab. Choose Grant Access. Add the IAM Role, user, or group that you want the end users to use to launch the product. Select Grant Access. Login back to the Management Account using the IAM Role or User or Group granted access above, if not already logged-in via that IAM Role or User or Group. In the left navigation pane, choose Products. Select Delegate IPAM Product and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the product parameters. Select Launch Product. This product delegates the NetworkHub account as the IPAM delegated administrator account.

Step 2 – Deploy the CloudFormation Macro definition using the IPAM CloudFormation Macro Product in your NetworkHub Account

Sign in to your AWS Management Console of the NetworkHub Account via the AWS role that has Administrator access. Navigate to the AWS Service Catalog console, and select Portfolios on the left navigation pane. On the Imported tab, choose IPAM Main Portfolio that was shared by the Management Account. On the portfolio details page, choose the Access tab. Choose Grant Access. Add the IAM Role, user, or group that you want the end users to use to launch the product. Select Grant Access. Login back to the NetworkHub Account using the IAM Role or User or Group granted access above, if not already logged-in via that IAM Role or User or Group. In the left navigation pane, choose Products. In the left navigation pane, choose Products. Select IPAM CloudFormation Macro Product, and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the product parameters. Select Launch Product.

Note that this template does not take-in any input parameters and is used to process the IPAM template provided in the next section. Step 3 – Deploy the AWS Service Catalog IPAM Product in your NetworkHub Account

Sign in to your AWS Management Console of the NetworkHub Account via the AWS role that was granted access to the portfolios in the previous step. Navigate to the AWS Service Catalog console, and choose Portfolios. In the left navigation pane, choose Products. Select IPAM Product and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the following product parameters:

pOrgId: ID of Organization pOperatingRegionList: A list of Regions where the IPAM is allowed to manage IP address CIDRs.

pMainPoolCIDRIPv4List: IPv4 CIDR provisioned to the main IPAM pool (TOP-LEVEL-POOL).

pRegionalPool1CIDRIPv4List: The list of CIDRs for Regional pools (one per Region) provisioned from the main IPAM pool, given in order as pOperatingRegionList.

pOperatingEnvironmentsList: A list of Environments where the IPAM is allowed to manage IP address CIDRs

pPoolCIDRIPv4ListProd: The list of production pool CIDRs (one per Region) provisioned from the respective Regional IPAM pool, given in order as pOperatingRegionList.

pPoolCIDRIPv4ListNonProd: The list of non-production pool CIDRs (one per Region) provisioned from the respective Regional IPAM pool, given in order as pOperatingRegionList.

Step 4 – Deploy the Spoke VPC Product using the IPAM Spoke Product in your Spoke Account

Sign in to your AWS Management Console of the Spoke Account. Navigate to the AWS Service Catalog console, and choose Portfolios. On the Imported tab, choose Ipam Spoke Portfolio. On the portfolio details page, choose the Access tab. Choose Grant Access. Add the IAM Role, user, or group that you want the end users to use to launch the product. Select Grant Access.

Note that if you have a common IAM Role deployed across all your spoke accounts, the previous five steps can be eliminated by updating the IPAMSpokeProductLaunchConstraintRole parameter in the ipamportfolio.yml template, with the local IAM Role name. Then, un-comment the IPAMSpokeProductLaunchRoleConstraint resource and update your ipamportfolio.yml stack.

Login back to the Spoke Account using the IAM Role or User or Group granted access previously, if not already logged-in via that IAM Role or User or Group. In the left navigation pane, choose Products. Select IPAM Spoke Product and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the following product parameters:

pVpcSize: Size for the VPC. Either small (supports 256 IP addresses), medium (supports 512 IP addresses), or large (supports 1024 IP addresses). pNumberOfAZs: Number of Availability Zones (AZs). The value must lie between two and four. IPAMRegion: Region of IPAM Scope

Select Launch Product. Validate the VPC creation. The VPCs are created based on the size that you provide (small, medium, or large) and the CIDRs in the IPAM pool provisioned for this Organization.

Cleanup steps All the resources deployed through CloudFormation templates should be deleted after successful testing and validation to avoid any unwanted costs.

Go to the Service Catalog console, identify the Provisioned Products from the left navigation pane, and terminate the product. This would terminate the corresponding CloudFormation stacks on your behalf. Go to the CloudFormation console, identify the stack used to deploy the Service Catalog Portfolio, and delete them. In spoke accounts, you can shut down the provisioned AWS Service Catalog product(s), which would terminate the corresponding CloudFormation stacks on your behalf.

Note that in a multi account setup, you must navigate through account boundaries and follow the previous steps where products were deployed. Conclusion In this blog post, we demonstrated how to deploy AWS VPC IPAM programmatically, by utilizing AWS Service Catalog. The AWS Service Catalog includes the IPAM portfolios and products which can be created using the provided CloudFormation templates. We also demonstrated how to create IPAM pools for different regions from a single resource definition using CloudFormation Macros, thereby eliminating the need for duplicating resources manually in the CloudFormation templates.

Mokshith Kumar Mokshith Kumar is a Senior Cloud Infrastructure Architect at AWS. He thrives on learning new technologies and solving complex customer challenges. He enjoys interacting with customers and strives to help accelerate their cloud adoption journey by offering technical guidance and implementing AWS solutions. He holds a master's degree specializing in Computer Networks from the University of Southern California. Off work, Mokshith is an avid swimmer and enjoys listening to music.

Raunak Tibrewal Raunak Tibrewal is a Senior Product Manager at AWS. He has spent 10+ years in the computer networking industry, and he is currently working in the Amazon VPC team building capabilities to simplify IP management for AWS customers. In his spare time, he enjoys traveling, hiking and other outdoor activities.

TAGS: Amazon VPC, Amazon VPC IP Address Manager, AWS CloudFormation, AWS Control Tower

AWS recognized as a Leader in 2023 Gartner Magic Quadrant for Contact Center as a Service with Amazon Connect

Gartner, a company that delivers actionable, objective insight to executives and their teams, has published the 2023 Gartner Magic Quadrant for Contact Center as a Service (CCaaS). Amazon Web Services (AWS) was named a Leader for the first time since Amazon Connect, our flexible, AI-powered cloud contact center, was launched in 2017. We believe this Leader placement reflects our rapid pace of innovation that enables companies of all sizes to deliver superior customer experiences at lower cost.

AWS's Ability to Execute and Completeness of Vision are the reasons why it was named a Leader in the CCaaS space. In the Gartner Critical Capabilities research, AWS positioned first in Agile Contact Center, second in Global Contact Center, and third in Digital Customer Service Center and Customer Engagement Center use cases. Since Amazon Connect was launched in 2017, we've continued to launch new capabilities for omnichannel customer experience, agent productivity, and analytics, insights, and optimization. Vice President of Amazon Connect Pasquale DeMaio reflects, "We're extremely excited about how we're enabling our customers to deliver bar raising experiences for their customers and agents. We've made huge investments over the last year to launch significant new capabilities like agent step-by-step guidance, case management, forecasting, capacity planning, and scheduling, and screen recording. We're energized by the momentum we've maintained and believe these expanded capabilities are a key factor to Gartner recognizing us as a Leader." Today, Amazon Connect customers like Priceline, Deliveroo, Unum, Capital One, Fujitsu, Intuit, John Hancock, New York Times, and National Australia Bank use Amazon Connect to deliver better customer experiences. The Gartner report offers insightful guidance as you evaluate the right cloud contact center solution for your business. Access a complimentary copy of the 2023 Gartner Magic Quadrant for CCaaS report here. This graphic was published by Gartner, Inc. as part of a larger research document and should be evaluated in the context of the entire document. The Gartner document is available upon request from AWS. GARTNER is a registered trademark and service mark of Gartner and Magic Quadrant is a registered trademark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. All rights reserved. Gartner does not endorse any vendor, product or service depicted in its research publications and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.

TAGS: Amazon Connect, AWS, CCaaS, Contact Center, Contact Centre, customer, Customer Experience, experience, Gartner, Magic, Quadrant

Leveraging AWS Analytic Services and HCLTech Frameworks for OLAP Solutions

Online analytical processing (OLAP) is a method of organizing datasets in a multi-dimensional format for quick analysis. This provides deeper insights for decision-makers by enabling them to analyze datasets, understand trends and patterns, perform root cause analysis, identify drivers, and act. Multi-dimension analysis is widely adopted by analysts, knowledge users, and power users for their decision support process. In this post, we will explore utilizing Amazon Web Services (AWS) analytic services and migration tools together with HCLTech frameworks to orchestrate OLAP solutions. HCLTech is an AWS Premier Tier Services Partner with Competencies in Migration, DevOps, SAP, Storage, Mainframe Modernization, and Cloud Operation Services. HCLTech is also a member of the AWS Managed Service Provider (MSP) and Well-Architected partner programs. OLAP Architecture and Components There are four types of online analytical processing:

Multi-dimensional OLAP (MOLAP): Stores aggregated and computed datasets in the proprietary multi-dimensional (in memory or on disk) cube format for analysis. Offers high performance and optimized storage. Relational OLAP (ROLAP): Stores datasets on relational data store to perform multi-dimensional analysis. Data aggregation and computation happens on the fly. Hybrid OLAP (HOLAP): Stores aggregated datasets in MOLAP cube and detail datasets in ROLAP database for analysis. Virtual OLAP (VOLAP): Enables multi-dimensional analysis over the datasets available in various data sources through federated queries, optimized columnar storage, and distributed and parallel processing.

AWS services for OLAP are comprised of four layers, as described below: Insights and Visualization This layer provides capabilities to interact with datasets using reports and dashboards. It enables end users to perform ad-hoc analysis, author reports, dashboards, scorecards, and charts from the underlying datasets with role-based access controls (RBAC). Amazon QuickSight addresses the capabilities in this layer Governance and Access This layer provides the capability to catalog the underlying datasets and enables users to perform search, discovery, and secure data access. It enables federated access to a variety of datasets available in the underlying persistent storages, and to centrally manage permissions on data being shared across your organization. Key AWS services include AWS Glue Data Catalog, Amazon Athena, and Amazon Redshift. Compute and Storage This layer provides capabilities to persist any kind of datasets and perform data computations (ELT/ETL) over the same. Depending on the data requirements (cold, warm, hot), data will be stored and processed by Amazon Simple Storage Service (Amazon S3), Amazon EMR, Amazon Redshift Serverless, and AWS Glue. Ingestion This layer provides the capability to bring all source and upstream datasets onto an AWS data lake or staging layer. It meets all the requirements of batch, real-time streaming, change data capture, and secure transfers.

Figure 1 – AWS services for OLAP. AWS Services for OLAP Solution Next, we will discuss the AWS services leveraged in the top three layers for the OLAP solution. Amazon QuickSight Capabilities Preparing the QuickSight SPICE Cube Amazon QuickSight uses SPICE (Super-fast, Parallel, In-memory Calculation Engine) to store data in a multi-dimensional format for faster processing of analytical queries. The cubes are encrypted and compressed for storage optimization. As part of the demonstration, we'll leverage a sample retail dataset which comprises product dimension, branches, daily sales transactions, and revenue target. As a broader process, sales data is copied to Amazon S3, loaded to Amazon Redshift Serverless data mart, and reported in QuickSight. Now, let's embed OLAP capability for the above process. While preparing your retail dataset for analysis in QuickSight, choose "Query Mode" as SPICE to import the dataset and build the in-memory cube. This imports the dataset from Amazon Redshift Serverless views/tables. Note that we need to regularly refresh the in-memory cube to update it with the latest/incremental records.

Figure 2 – Amazon QuickSight – SPICE configuration. Next, go to the dataset and select "Refresh Property" to see the status of current and historical refreshes. We can schedule the cube refresh in any given frequency, and many users can access a SPICE dataset concurrently without performance issues. Prepare Multi-Dimensional Analysis Preparing hierarchies, level, and drill-downs navigations over the datasets is seamless in QuickSight. Note that QuickSight provides the option to convert the date field to year, month, or day automatically. The above features are available in charts as well.

Figure 3 – Amazon QuickSight hierarchy, levels, drill down, and slice/dice. Amazon QuickSight provides ability to drill up and down levels, and also to slice and dice rows and columns as shown above. Drill Through Navigation from Summary to Detail Reports We can use the action options to drill through the navigation, from summary report to detail report, and pass the filter context for the same. This provides the flexibility of splitting the complexities of the reports while navigating seamlessly without losing the context.

Figure 4 – Amazon QuickSight – Drill through navigation. Aggregate and Table Calculations Amazon QuickSight provides rich sets of table calculations to discover how dimensions influence measures or each other. With the help of this table and aggregate calculation, we can address the mulit-dimensional expressions (MDX) equivalent expressions. Provided below are some of the business scenarios:

QTD: QuickSight calculate the quarter to date (QTD) automatically based on the time period hierarchy. YTD: To calculate the year to date (YTD) value, use the following table function:

PY Act: To calculate the prior year actual value for the given date:

Figure 5 – Amazon QuickSight – PY Act, Forward One Month, Trail One Month.

T1M: To calculate the trailing one month (T1M), which is the sum of a given month and previous month value:

F1M: To calculate the forward one month (F1M), which is the sum of a given month and next month value:

sum(actual)+periodOverPeriodLastValue(sum(actual),{sale_date},MONTH,-1) We can use Amazon QuickSight parameter control to customize the above calculations for specific months. Machine-Learning Powered Forecasting We can use QuickSight machine learning-powered forecasting to forecast complex, real-world scenarios such as data with multiple seasonality. Auto Narratives QuickSight automatically interprets the data, charts, and tables in the dashboard and provides a number of suggested insights in natural language. The suggested insights you can choose from are ready-made and come with words, calculations, and functions. These narratives can be customized per user requirements. QuickSight and Redshift Serverless Integrations Amazon QuickSight can perform ROLAP by directly integrating with Amazon Redshift using direct query mode. All of the QuickSight reports and insights developed in SPICE can be seamlessly repointed to its source Redshift tables, views, and materialized views. Redshift views or materialized views encapsulate complexities of underlying data models (tables and joins) and create a subset of contextual datasets that can act as a base view for OLAP analysis. We can embed window functions in the SQLs for complex analytical scenarios, pre-compute, aggregate, and store the processed result set in materialized view. Materialized views offer significant performance benefits compared to live table SQL or view queries. Also, we can extend the QuickSight insights from Redshift managed tables to Amazon S3 datasets using Amazon Redshift Spectrum.

Figure 6 – Amazon Redshift Spectrum view. We can query the S3 dataset by creating an external table from Redshift, as shown above. We can also combine datasets that include data stored in Redshift and in S3. Then, we can access them using the SQL syntax in Redshift. QuickSight and Amazon Athena Integrations Amazon Athena provides virtualization infrastructure where it submits a single SQL query to analyze data in relational, nonrelational, object, and custom data sources running on-premises or in the cloud. Athena accesses AWs Glue Data Catalog which maintains registry of sources metadata, and AWS Glue crawler continuously scans the source metadata and maintains up to date changes.

Figure 7 – Amazon QuickSight integration with Athena. By leveraging the Amazon QuickSight Athena Connector, we can compose federated queries and generate OLAP insights. QuickSight and Amazon EMR Integrations Amazon QuickSight supports big data connectors to connect with Apache Spark on Amazon EMR. We can leverage Spark tables or through results of Spark SQL queries. For this process, QuickSight requires your Spark server to be secured and authenticated using LDAP, which is available to Spark version 2.0. Spark SQL can also be configured to use AWS Glue Data Catalog as its meta store.

Figure 8 – QuickSight integration with Apache Spark on Amazon EMR. AWS Glue Crawlers can automatically infer schema from source data in S3 and store the associated metadata in the data catalog. QuickSight requires LDAP as authentication protocol for authenticating Spark cluster; hence, we need to install and configure OpenLDAP in Amazon EMR and enable Thrift Server. OLAP Solution Deployments on AWS In this section, we'll cover OLAP deployment types which involves provisioning of set of AWS analytic services based on business functional and technical requirements, along with HCLTech accelerators to fast track the deployments.

Figure 9 – AWS OLAP deployment types. Use Case Requirement #1 A customer wants to perform self-service business intelligence with multi-dimensional data analysis requiring quick query response and ad-hoc analysis over GBs of data volume, with acceptable data latency for reporting MOLAP is the appropriate deployment type, and you can leverage Amazon QuickSight SPICE which supports a maximum of 2,000 fields per cube with 24 million records (25 GB dataset) for standard edition and one billion records (one TB dataset) for enterprise edition. Use Case Requirement #2 A customer wants a managed report/scorecard in addition to dashboards and analysis over complex multi-dimensional data model, with TBs of datasets and zero data latency. ROLAP is the deployment type, and Amazon QuickSight's integration with Redshift Serverless leverages a massively parallel processing query optimizer and takes advantage of the columnar-oriented data storage. It can process complex analytic queries that often include multi-table joins, subqueries, and aggregation. Generally, customers keep the cold data, unstructured data, and historical transactional data in Amazon S3 for low-cost, efficient storage. Redshift Spectrum can create complex queries of churning both Redshift-managed tables containing hot data and S3 datasets. Use Case Requirement #3 A customer wants to perform analysis over a big data lake with petabytes of datasets. ROLAP is the deployment type and leverages the Amazon EMR SparkSQL connector with QuickSight. Amazon EMR provides a simple and cost-effective way to run highly distributed processing frameworks such as Spark. Use Case Requirement #4 A customer wants to perform hybrid data analysis over various data sources through virtualization. VOLAP is the deployment type, and QuickSight's integration with Amazon Athena can run OLAP analysis over federated queries churning datasets across S3, Redshift, and Amazon Relational Database Service (Amazon RDS). This offers zero footprint traversing of analytical queries from data mart to data lake. Advantages of AWS and HCLTech There are several advantages for customers when deploying or migrating OLAP workloads onto AWS:

Interoperability of analytical insights: Several AWS analytic services seamlessly integrate with your datasets, including Amazon QuickSight, Amazon Redshift, Amazon S3, Amazon Athena, Amazon EMR, and AWS Glue Data Catalog. AWS serverless and cost-effective deployment: With low-cost data storage, serverless performance at scale, and pay-as-you-go compute services, AWS offers fully managed and cost-effective infrastructure for managing OLAP workload deployments. Quicker deployment: AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (DMS) make it easy to migrate relational databases, data warehouses, and NoSQL databases to AWS and unlock analytic insights. HCLTech's ADvantage Migrate fast tracks AWS data lake/data mart implementation and data migration to AWS, optimizes production release cycles, and drives productivity by adopting modular design approaches (reusability of codes) and template-based data preparation and testing activities.

Conclusion In this post, we briefed you on orchestrating online analytical processing (OLAP) workloads using AWS analytics services and their key features in addressing OLAP requirements. Unlike legacy products that require separate infrastructure and licenses, AWS analytics services offer seamless OLAP analysis over the data residing in the data lake, data mart, data warehouse, with the flexibility to scale up and down at a lower cost. .

HCLTech – AWS Partner Spotlight HCLTech is an AWS Premier Tier Services Partner and MSP that serves hundreds of global enterprises to solve day-to-day and complex challenges with a dedicated full-stack business unit. Contact HCLTech | Partner Overview | AWS Marketplace | Case Studies

TAGS: AWS Competency Partners, AWS MSP Partner Program, AWS Partner Guest Post, AWS Partner Success Stories, AWS Premier Tier Services Partners, AWS Public Sector Partners, AWS Service Delivery Partners, AWS Solution Provider Partners, AWS Well-Architected Partners, HCLTech, Managed Service Provider

It's important to be aware of the case sensitivity of text data in PostgreSQL when performing queries or comparisons to ensure that the results match your intended expectations. Case also affects how the database performs sorting operations. By default, PostgreSQL is case sensitive when sorting or comparing string values. For example, PostgreSQL considers "amazon" and "Amazon" to be different values, and this impacts how they are sorted. If you want to search for a specific value regardless of its case, you need to use a case-insensitive comparison function or normalize the data to a consistent case. Case insensitivity in PostgreSQL allows for greater flexibility and ease of use when working with data. In this post, we discuss different options to deal with the case-insensitive data in PostgreSQL and their use cases, pros, and cons. Solution overview Case insensitivity in PostgreSQL can improve productivity, reduce errors, and make it easier to work with data and identifiers in a flexible and consistent way. However, there are some performance tradeoffs to consider. PostgreSQL provides different options to achieve case insensitivity in data. We discuss the following options in this post:

Using case-insensitive comparison functions Using ILIKE or ~~* operators for comparison Using the CITEXT data type Using a custom = (equal to) operator Using a case-insensitive ICU collation

Prerequisites If you want to follow along this post's examples, complete the following prerequisite steps:

Create an Amazon Aurora PostgreSQL-Compatible Edition cluster or an Amazon Relational Database Service (Amazon RDS) for PostgreSQL instance if you don't already have one. For instructions, refer to Create an Aurora PostgreSQL DB cluster or Create a PostgreSQL DB instance respectively. Create an Amazon Elastic Compute Cloud (Amazon EC2) instance to install the PostgreSQL client to access the Aurora PostgreSQL or RDS for PostgreSQL instance. For instructions, refer to Create your EC2 resources and launch your EC2 instance. Alternatively, you can set up connectivity between your RDS database and EC2 compute instance in 1-click Install the PostgreSQL client. On Amazon Linux, we can use the following commands to download the psql command line tool:

Option 1: Using case-insensitive comparison functions In this option, we discuss the PostgreSQL in-built comparison functions LOWER and UPPER. These functions allow you to convert a string to lowercase or uppercase at runtime, respectively. The lower function takes a string as an argument and returns a new string with all the characters in lowercase. For example:

postgres=# SELECT pg_catalog.lower('Print IN LOWER'); lower ---------------- print in lower (1 row)SQL

The upper function, on the other hand, takes a string as an argument and returns a new string with all the characters in uppercase. For example:

postgres=# SELECT pg_catalog.upper('Print in upper'); upper ---------------- PRINT IN UPPER (1 row)SQL

This is a straightforward way of dealing with case insensitivity in PostgreSQL. When performing a search on a text column, you can use the lower or upper function to convert the search term to lowercase or uppercase and then search for that term in the lowercased or uppercased text column. This ensures that the search is case insensitive, so you can find matches regardless of the case of the text data. Also, when importing data from external sources, it's common to use the lower and upper functions to standardize the case of text data. This helps ensure that all text data is in the same case format, making it easier to compare and manipulate. Benefits This option offers built-in functions and is simple to use. Drawbacks If you want to use an index for case-insensitive searches or comparisons, you'll need to create a specialized index such as an expression index or a specialized data type like citext instead of relying on a standard text index. Additionally, PostgreSQL can't use an expression index if the expression is not present, so you would need to add another index for case-sensitive searches. Complete the following steps:

CREATE TABLE public.case_test_opt1 ( id SERIAL PRIMARY KEY, first_name TEXT NOT NULL, last_name TEXT NOT NULL, email TEXT NOT NULL );SQL

Insert some sample data. The following query inserts 1 million records:

INSERT INTO public.case_test_opt1 (first_name, last_name, email) SELECT 'Foo', 'Bar' || n::text, 'foo.bar' || n::text || [EMAIL] FROM generate_series(1, 1000) n;SQL

Run the following query, which doesn't use the index created in the previous step because it's a normal index. You can see the full table scan (seq scan) as highlighted:

postgres=> EXPLAIN ANALYZE SELECT * FROM public.case_test_opt1 WHERE pg_catalog.UPPER(email) = [EMAIL]

QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Gather (cost=1000.00..16084.00 rows=5000 width=37) (actual time=0.428..171.612 rows=1000 loops=1) Workers Planned: 2 Workers Launched: 2 -> Parallel Seq Scan on case_test_opt1 (cost=0.00..14584.00 rows=2083 width=37) (actual time=0.237..163.207 rows=333 loops=3) Filter: (upper(email) = [EMAIL] Rows Removed by Filter: 333000 Planning Time: 0.082 ms Execution Time: 171.681 ms (8 rows) SQL

Run the same query to see if the functional index is utilized:

QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------------ Bitmap Heap Scan on case_test_opt1 (cost=139.18..7761.06 rows=5000 width=37) (actual time=0.331..1.314 rows=1000 loops=1) Recheck Cond: (upper(email) = [EMAIL] Heap Blocks: exact=1000 -> Bitmap Index Scan on case_opt1_email_upper_idx (cost=0.00..137.93 rows=5000 width=0) (actual time=0.200..0.200 rows=1000 loops=1) Index Cond: (upper(email) = [EMAIL] Planning Time: 0.149 ms Execution Time: 1.375 ms (7 rows)SQL

Option 2: Using ILIKE or ~~* operators for comparison In this option, we discuss PostgreSQL built-in string comparison operators ILIKE and ~~*. They are equivalent in functionality, but ILIKE is the standard SQL-compliant operator whereas ~~* is a PostgreSQL-specific operator. The ILIKE operator matches two strings, ignoring differences in letter case. It's equivalent to the LIKE operator but case insensitive. For example, the expression 'hello' ILIKE 'HelLO' would evaluate to TRUE:

postgres=# SELECT 'hello' ILIKE 'HelLO'; ?column? ---------- t (1 row)SQL

The ~~* operator is the PostgreSQL-specific version of ILIKE, and performs a case-insensitive string comparison similar to ILIKE. For example, the expression 'hello' ~~* 'HelLO' would evaluate to TRUE:

postgres=# SELECT 'hello' ~~* 'HelLO'; ?column? ---------- t (1 row)SQL

When accepting user input in a search form or filter, you can use the ILIKE or ~~* operator to search for matching records in a case-insensitive manner. This ensures that users can find the records they're looking for regardless of the case of the input. You can also use these operators when searching for specific text patterns in a large dataset. This is useful for scenarios such as finding all the product names that contain a certain keyword, regardless of the case of the keyword. Benefits This option offers built-in operators and is simple to use. Drawbacks You cannot use B-tree indexes, even expression indexes, with the ILIKE or ~~* operators. However, you can use PostgreSQL's GIN or GiST index types to build indexes for these types of operators. More details on GIN and GiST indexes including the differences can be found in Preferred Index Types for Text Search documentation. Keep the following in mind:

You must use the pg_trgm extension. This extension is available in Amazon Aurora and Amazon RDS, but it is not enabled by default. You can install it in your database using "CREATE EXTENSION pg_trgm;" GIN indexes can be significantly larger than B-tree indexes, especially for data types with many distinct values or long text fields. This can impact the speed of index updates and maintenance. Creating a GIN index, as well as inserts and updates, can take longer than when using a B-tree index. GIN indexes are optimized for certain query patterns, such as searching for multiple values or ranges within the indexed data. However, they may not be as effective for other types of queries, such as equality or range queries on a single column.

Using the same table that we created earlier (case_test_opt1), we can create a normal, functional, and GIN index to see the index creation time difference:

For this data set, the GIN index creation time took almost three times longer than the B-tree indexes from the previous example. The following query shows that PostgreSQL scans the GIN index when using the ILIKE operator:

postgres=> EXPLAIN ANALYZE SELECT * FROM public.case_test_opt1 WHERE email ILIKE 'FOO.BAR500@EXAMPLE%';

QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------- Bitmap Heap Scan on case_test_opt1 (cost=344.77..709.64 rows=99 width=37) (actual time=84.869..86.652 rows=1000 loops=1) Recheck Cond: (email ~~* 'FOO.BAR500@EXAMPLE%'::text) Heap Blocks: exact=1000 -> Bitmap Index Scan on case_opt1_email_tgrm_idx (cost=0.00..344.74 rows=99 width=0) (actual time=84.740..84.740 rows=1000 loops=1) Index Cond: (email ~~* 'FOO.BAR500@EXAMPLE%'::text) Planning Time: 0.256 ms Execution Time: 86.729 ms (7 rows) SQL

Option 3: Using the CITEXT data type citext is a case-insensitive text data type in PostgreSQL. It's an extension module that provides a case-insensitive version of the built-in text data type. When citext is used for a column, any string value inserted into that column is automatically converted to lowercase, and any queries on that column are case insensitive. To use citext, you must first enable the citext extension in your database using the following command:

After that, you can create a table with a citext column using following statement:

CREATE TABLE public.case_citext_opt3 ( id INT PRIMARY KEY, name CITEXT );SQL

If you're migrating data from a system that uses case-insensitive text types, using citext can simplify the migration process by allowing you to maintain the same data format. citext can help prevent data entry errors or inconsistencies in your data. With citext, you don't need to use lower and upper functions in your queries to perform case-insensitive searches or comparisons. This can make your queries simpler and easier to read. Benefits This option offers a built-in extension with the citext data type. Additionally, migration from other databases with case-insensitive data types is straightforward. Drawbacks A drawback to this option is that you can't perform case-sensitive searches if needed. Also, if you have a citext type on one side of an operator and a text on the other for a query, then it might not work as expected. So, we need to be careful with such queries. Option 4: Using a custom = operator PostgreSQL allows users to define custom operators to extend the built-in set of operators. Custom operators can be defined to perform specific operations that aren't supported by the built-in operators, or to provide shorthand notations for frequently used operations. In this option, we show how to create a new = operator to deal with the case-insensitive data. Complete the following steps:

Create a new schema to define the operator and grant necessary privileges on the schema to public:

CREATE SCHEMA util; GRANT USAGE ON SCHEMA util TO PUBLIC;SQL

Create a function to define the functionality of the operator. This function basically changes the case of right and left operands to lowercase so that comparison is done without modifying the query to use any built-in functions like lower or upper.

CREATE OR REPLACE FUNCTION util.case_ins_cmp(text, text) RETURNS BOOLEAN LANGUAGE sql immutable as $$ select pg_catalog.lower($1) operator(pg_catalog.=) pg_catalog.lower($2) $$;SQL

CREATE OPERATOR util.= ( LEFTARG = TEXT, RIGHTARG = TEXT, PROCEDURE = util.case_ins_cmp);SQL

CREATE TABLE public.case_test_opt4 ( id SERIAL PRIMARY KEY, first_name TEXT NOT NULL, last_name TEXT NOT NULL, email TEXT NOT NULL ); SQL

INSERT INTO public.case_test_opt4 (first_name, last_name, email) VALUES ('foo', 'bar', [EMAIL] ('foo', 'BAR', [EMAIL] ('foo', 'BaR', [EMAIL]

postgres=# SELECT * FROM public.case_test_opt4 WHERE last_name='bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] (1 row)SQL You can see only one row based on the case. Set the search_path to use the util schema before pg_catalog and check the results:

postgres=# SET search_path TO util, pg_catalog; SET postgres=# SELECT * FROM public.case_test_opt4 WHERE last_name='bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 2 | foo | BAR | [EMAIL] 3 | foo | BaR | [EMAIL] (3 rows)SQL

You can see all three rows regardless of case. Alternatively, you can use the absolute path of the operator using schema instead of setting the search_path:

postgres=# SELECT * FROM public.case_test_opt4 WHERE last_name operator(util.=) 'bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 2 | foo | BAR | [EMAIL] 3 | foo | BaR | [EMAIL] (3 rows)SQL

postgres=# CREATE INDEX ind_opt4 ON public.case_test_opt4(pg_catalog.lower(last_name)); CREATE INDEX postgres=# ANALYZE case_test_opt4; ANALYZE postgres=# EXPLAIN ANALYZE SELECT * FROM public.case_test_opt4 WHERE last_name OPERATOR (util. =) 'bar';

QUERY PLAN -------------------------------------------------------------------------------------------------------------------------- Index Scan using ind_opt4 on case_test_opt4 (cost=0.42..8.44 rows=1 width=40) (actual time=0.071..0.072 rows=3 loops=1) Index Cond: (lower(last_name) = 'bar'::text) Planning Time: 0.322 ms Execution Time: 0.084 ms (4 rows)SQL

Benefits One benefit of this option is that you might not need to change the application queries to add any built-in functions like lower and upper or add any operators like ~~* or ILIKE if you choose to add the schema (in which the operator is created) to the search_path and set it to the database. Additionally, you can set the search_path as a switch to make the data case insensitive if you want only part of your application to behave like case insensitive without making any changes to the application. Drawbacks You need to add the search_path (with the util schema before pg_catalog) to the application queries or alter the database to set the search_path:

In general, custom operators can be a powerful tool for developers and DBAs to extend the functionality of PostgreSQL and make it simpler to work with some operations. However, it's important to use them judiciously and follow best practices for naming, documentation, and testing to ensure that they are consistent and efficient. Clean up after you have completed the testing:

Option 5: Using a case-insensitive ICU collation "Custom collations" are a feature of the ICU collation provider. ICU allows extensive control over collation behavior by defining new collations with collation settings as a part of the language tag. These settings can modify the collation order to suit a variety of needs. You can go through Manage collation changes in PostgreSQL on Amazon Aurora and Amazon RDS for more information on collations. The following is a test case with custom collation to access case-insensitive data:

CREATE COLLATION public.case_insensitive (provider = icu, locale = 'und-u-ks-level2', deterministic = false);SQL

CREATE TABLE public.case_test_opt5 ( id SERIAL PRIMARY KEY, first_name TEXT NOT NULL COLLATE case_insensitive, last_name TEXT NOT NULL COLLATE case_insensitive, email TEXT NOT NULL COLLATE case_insensitive );

INSERT INTO public.case_test_opt5 (first_name, last_name, email) VALUES ('foo', 'bar', [EMAIL] ('foo', 'BAR', [EMAIL] ('foo', 'BaR', [EMAIL]

postgres=> SELECT * FROM public.case_test_opt5 WHERE last_name='bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 2 | foo | BAR | [EMAIL] 3 | foo | BaR | [EMAIL] (3 rows)SQL

You can see all three rows regardless of case. Benefits Like Option 4, you don't need to change the application queries to add any built-in functions like lower or upper or add any operators like ~~* or ILIKE. Also, you don't need to set the search_path. Drawbacks The major drawback is pattern matching does not work for nondeterministic collations that is case-insensitive collations. As of PostgreSQL 15, this also does not address the ability to index nondeterministic collations. Also, the sorting of the data might change as well based on the collation that you use. You have to deal with sorting if you are changing the collation. For example, the following has the order of last_name as bar, BAR, BaR:

if we modify the column's collation and run a query we observe that the order of column changed to bar, BaR, BAR:

postgres=> ALTER TABLE public.case_test_opt5 ALTER COLUMN last_name TYPE text COLLATE "en_US"; ALTER TABLE Time: 57.316 ms postgres=> SELECT * FROM public.case_test_opt5 ORDER BY last_name; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 3 | foo | BaR | [EMAIL] 2 | foo | BAR | [EMAIL] (3 rows)SQL

You can see that changing the collation changed the order of the rows. Although custom collations in PostgreSQL can provide many benefits, it's important to carefully consider the potential drawbacks before implementing them in a production environment. Proper planning, testing, and ongoing maintenance can help ensure that custom collations work effectively and don't negatively impact database performance or compatibility. Clean up after you have completed the testing:

Conclusion PostgreSQL provides several options for dealing with case insensitivity, each with its own advantages and disadvantages. The best approach depends on the specific requirements of your application and the data being stored in the database. In this post, we discussed a few options with their pros and cons. Based on your requirements, you can choose the right option for your application. If you have any questions or suggestions about post, leave a comment.

About the authors Baji Shaik is a Sr. Lead Consultant with AWS ProServe, Global Competency Centre. His background spans a wide depth and breadth of expertise and experience in SQL and NoSQL database technologies. He is a Database Migration Expert and has developed many successful database solutions addressing challenging business requirements for moving databases from on premises to Amazon RDS and Amazon Aurora PostgreSQL/MySQL. He is an eminent author, having written several books on PostgreSQL. A few of his recent works include "PostgreSQL Configuration," "Beginning PostgreSQL on the Cloud," and "PostgreSQL Development Essentials." Furthermore, he has delivered several conference and workshop sessions. Scott Mead is a Database Engineer at AWS.

How Ontraport reduced data processing cost by 80% with AWS Glue

This post is written in collaboration with Elijah Ball from Ontraport. Customers are implementing data and analytics workloads in the AWS Cloud to optimize cost. When implementing data processing workloads in AWS, you have the option to use technologies like Amazon EMR or serverless technologies like AWS Glue. Both options minimize the undifferentiated heavy lifting activities like managing servers, performing upgrades, and deploying security patches and allow you to focus on what is important: meeting core business objectives. The difference between both approaches can play a critical role in enabling your organization to be more productive and innovative, while also saving money and resources. Services like Amazon EMR focus on offering you flexibility to support data processing workloads at scale using frameworks you're accustomed to. For example, with Amazon EMR, you can choose from multiple open-source data processing frameworks such as Apache Spark, Apache Hive, and Presto, and fine-tune workloads by customizing things such as cluster instance types on Amazon Elastic Compute Cloud (Amazon EC2) or use containerized environments running on Amazon Elastic Kubernetes Service (Amazon EKS). This option is best suited when migrating workloads from big data environments like Apache Hadoop or Spark, or when used by teams that are familiar with open-source frameworks supported on Amazon EMR. Serverless services like AWS Glue minimize the need to think about servers and focus on offering additional productivity and DataOps tooling for accelerating data pipeline development. AWS Glue is a serverless data integration service that helps analytics users discover, prepare, move, and integrate data from multiple sources via a low-code or no-code approach. This option is best suited when organizations are resource-constrained and need to build data processing workloads at scale with limited expertise, allowing them to expedite development and reduced Total Cost of Ownership (TCO). In this post, we show how our AWS customer Ontraport evaluated the use of AWS Glue and Amazon EMR to reduce TCO, and how they reduced their storage cost by 92% and their processing cost by 80% with only one full-time developer. Ontraport's workload and solution Ontraport is a CRM and automation service that powers businesses' marketing, sales and operations all in one place—empowering businesses to grow faster and deliver more value to their customers. Log processing and analysis is critical to Ontraport. It allows them to provide better services and insight to customers such as email campaign optimization. For example, email logs alone record 3–4 events for every one of the 15–20 million messages Ontraport sends on behalf of their clients each day. Analysis of email transactions with providers such as Google and Microsoft allow Ontraport's delivery team to optimize open rates for the campaigns of clients with big contact lists. Some of the big log contributors are web server and CDN events, email transaction records, and custom event logs within Ontraport's proprietary applications. The following is a sample breakdown of their daily log contributions:

Ontraport's solution uses Amazon Kinesis and Amazon Kinesis Data Firehose to ingest log data and write recent records into an Amazon OpenSearch Service database, from where analysts and administrators can analyze the last 3 months of data. Custom application logs record interactions with the Ontraport CRM so client accounts can be audited or recovered by the customer support team. Originally, all logs were retained back to 2018. Retention is multi-leveled by age:

Less than 1 week – OpenSearch hot storage Between 1 week and 3 months – OpenSearch cold storage More than 3 months – Extract, transform, and load (ETL) processed in Amazon Simple Storage Service (Amazon S3), available through Amazon Athena

The following diagram shows the architecture of their log processing and analytics data pipeline.

Evaluating the optimal solution In order to optimize storage and analysis of their historical records in Amazon S3, Ontraport implemented an ETL process to transform and compress TSV and JSON files into Parquet files with partitioning by the hour. The compression and transformation helped Ontraport reduce their S3 storage costs by 92%. In phase 1, Ontraport implemented an ETL workload with Amazon EMR. Given the scale of their data (hundreds of billions of rows) and only one developer, Ontraport's first attempt at the Apache Spark application required a 16-node EMR cluster with r5.12xlarge core and task nodes. The configuration allowed the developer to process 1 year of data and minimize out-of-memory issues with a rough version of the Spark ETL application. To help optimize the workload, Ontraport reached out to AWS for optimization recommendations. There were a considerable number of options to optimize the workload within Amazon EMR, such as right-sizing Amazon Elastic Compute Cloud (Amazon EC2) instance type based on workload profile, modifying Spark YARN memory configuration, and rewriting portions of the Spark code. Considering the resource constraints (only one full-time developer), the AWS team recommended exploring similar logic with AWS Glue Studio. Some of the initial benefits with using AWS Glue for this workload include the following:

AWS Glue has the concept of crawlers that provides a no-code approach to catalog data sources and identify schema from multiple data sources, in this case, Amazon S3. AWS Glue provides built-in data processing capabilities with abstract methods on top of Spark that reduce the overhead required to develop efficient data processing code. For example, AWS Glue supports a DynamicFrame class corresponding to a Spark DataFrame that provides additional flexibility when working with semi-structured datasets and can be quickly transformed into a Spark DataFrame. DynamicFrames can be generated directly from crawled tables or directly from files in Amazon S3. See the following example code:

connection_type = 's3', connection_options = {'paths': [s3:// ]}, format = 'json')Bash

It minimizes the need for Ontraport to right-size instance types and auto scaling configurations. Using AWS Glue Studio interactive sessions allows Ontraport to quickly iterate when code changes where needed when detecting historical log schema evolution.

Ontraport had to process 100 terabytes of log data. The cost of processing each terabyte with the initial configuration was approximately $500. That cost came down to approximately $100 per terabyte after using AWS Glue. By using AWS Glue and AWS Glue Studio, Ontraport's cost of processing the jobs was reduced by 80%. Diving deep into the AWS Glue workload Ontraport's first AWS Glue application was a PySpark workload that ingested data from TSV and JSON files in Amazon S3, performed basic transformations on timestamp fields, and converted the data types of a couple fields. Finally, it writes output data into a curated S3 bucket as compressed Parquet files of approximately 1 GB in size and partitioned in 1-hour intervals to optimize for queries with Athena. With an AWS Glue job configured with 10 workers of the type G.2x configuration, Ontraport was able to process approximately 500 million records in less than 60 minutes. When processing 10 billion records, they were able to increase the job configuration to a maximum of 100 workers with auto scaling enabled to complete the job within 1 hour. What's next? Ontraport has been able to process logs as early as 2018. The team is updating the processing code to allow for scenarios of schema evolution (such as new fields) and parameterized some components to fully automate the batch processing. They are also looking to fine-tune the number of provisioned AWS Glue workers to obtain optimal price-performance. Conclusion In this post, we showed you how Ontraport used AWS Glue to help reduce development overhead and simplify development efforts for their ETL workloads with only one full-time developer. Although services like Amazon EMR offer great flexibility and optimization, the ease of use and simplification in AWS Glue often offer a faster path for cost-optimization and innovation for small and medium businesses. For more information about AWS Glue, check out Getting Started with AWS Glue.

About the Authors Elijah Ball has been a Sys Admin at Ontraport for 12 years. He is currently working to move Ontraport's production workloads to AWS and develop data analysis strategies for Ontraport. Pablo Redondo is a Principal Solutions Architect at Amazon Web Services. He is a data enthusiast with over 16 years of FinTech and healthcare industry experience and is a member of the AWS Analytics Technical Field Community (TFC). Pablo has been leading the AWS Gain Insights Program to help AWS customers achieve better insights and tangible business value from their data analytics initiatives. Vikram Honmurgi is a Customer Solutions Manager at Amazon Web Services. With over 15 years of software delivery experience, Vikram is passionate about assisting customers and accelerating their cloud journey, delivering frictionless migrations, and ensuring our customers capture the full potential and sustainable business advantages of migrating to the AWS Cloud.

Natural language processing (NLP) is the field in machine learning (ML) concerned with giving computers the ability to understand text and spoken words in the same way as human beings can. Recently, state-of-the-art architectures like the transformer architecture are used to achieve near-human performance on NLP downstream tasks like text summarization, text classification, entity recognition, and more. Large language models (LLMs) are transformer-based models trained on a large amount of unlabeled text with hundreds of millions (BERT) to over a trillion parameters (MiCS), and whose size makes single-GPU training impractical. Due to their inherent complexity, training an LLM from scratch is a very challenging task that very few organizations can afford. A common practice for NLP downstream tasks is to take a pre-trained LLM and fine-tune it. For more information about fine-tuning, refer to Domain-adaptation Fine-tuning of Foundation Models in Amazon SageMaker JumpStart on Financial data and Fine-tune transformer language models for linguistic diversity with Hugging Face on Amazon SageMaker. Zero-shot learning in NLP allows a pre-trained LLM to generate responses to tasks that it hasn't been explicitly trained for (even without fine-tuning). Specifically speaking about text classification, zero-shot text classification is a task in natural language processing where an NLP model is used to classify text from unseen classes, in contrast to supervised classification, where NLP models can only classify text that belong to classes in the training data. We recently launched zero-shot classification model support in Amazon SageMaker JumpStart. SageMaker JumpStart is the ML hub of Amazon SageMaker that provides access to pre-trained foundation models (FMs), LLMs, built-in algorithms, and solution templates to help you quickly get started with ML. In this post, we show how you can perform zero-shot classification using pre-trained models in SageMaker Jumpstart. You will learn how to use the SageMaker Jumpstart UI and SageMaker Python SDK to deploy the solution and run inference using the available models. Zero-shot learning Zero-shot classification is a paradigm where a model can classify new, unseen examples that belong to classes that were not present in the training data. For example, a language model that has beed trained to understand human language can be used to classify New Year's resolutions tweets on multiple classes like career, health, and finance, without the language model being explicitly trained on the text classification task. This is in contrast to fine-tuning the model, since the latter implies re-training the model (through transfer learning) while zero-shot learning doesn't require additional training. The following diagram illustrates the differences between transfer learning (left) vs. zero-shot learning (right).

Yin et al. proposed a framework for creating zero-shot classifiers using natural language inference (NLI). The framework works by posing the sequence to be classified as an NLI premise and constructs a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class politics, we could construct a hypothesis of "This text is about politics." The probabilities for entailment and contradiction are then converted to label probabilities. As a quick review, NLI considers two sentences: a premise and a hypothesis. The task is to determine whether the hypothesis is true (entailment) or false (contradiction) given the premise. The following table provides some examples.

A man inspects the uniform of a figure in some East Asian country. Contradiction The man is sleeping.

An older and younger man smiling. Neutral Two men are smiling and laughing at the cats playing on the floor.

A soccer game with multiple males playing. entailment Some men are playing a sport.

How to deploy pre-trained zero-shot text classification models using the SageMaker JumpStart UI and run inference on the deployed model using short text data How to use the SageMaker Python SDK to access the pre-trained zero-shot text classification models in SageMaker JumpStart and use the inference script to deploy the model to a SageMaker endpoint for a real-time text classification use case How to use the SageMaker Python SDK to access pre-trained zero-shot text classification models and use SageMaker batch transform for a batch text classification use case

SageMaker JumpStart provides one-click fine-tuning and deployment for a wide variety of pre-trained models across popular ML tasks, as well as a selection of end-to-end solutions that solve common business problems. These features remove the heavy lifting from each step of the ML process, simplifying the development of high-quality models and reducing time to deployment. The JumpStart APIs allow you to programmatically deploy and fine-tune a vast selection of pre-trained models on your own datasets. The JumpStart model hub provides access to a large number of NLP models that enable transfer learning and fine-tuning on custom datasets. As of this writing, the JumpStart model hub contains over 300 text models across a variety of popular models, such as Stable Diffusion, Flan T5, Alexa TM, Bloom, and more. Note that by following the steps in this section, you will deploy infrastructure to your AWS account that may incur costs. Deploy a standalone zero-shot text classification model In this section, we demonstrate how to deploy a zero-shot classification model using SageMaker JumpStart. You can access pre-trained models through the JumpStart landing page in Amazon SageMaker Studio. Complete the following steps:

In SageMaker Studio, open the JumpStart landing page. Refer to Open and use JumpStart for more details on how to navigate to SageMaker JumpStart. In the Text Models carousel, locate the "Zero-Shot Text Classification" model card. Choose View model to access the facebook-bart-large-mnli model. Alternatively, you can search for the zero-shot classification model in the search bar and get to the model in SageMaker JumpStart. Specify a deployment configuration, SageMaker hosting instance type, endpoint name, Amazon Simple Storage Service (Amazon S3) bucket name, and other required parameters. Optionally, you can specify security configurations like AWS Identity and Access Management (IAM) role, VPC settings, and AWS Key Management Service (AWS KMS) encryption keys. Choose Deploy to create a SageMaker endpoint.

This step takes a couple of minutes to complete. When it's complete, you can run inference against the SageMaker endpoint that hosts the zero-shot classification model. In the following video, we show a walkthrough of the steps in this section.

Use JumpStart programmatically with the SageMaker SDK In the SageMaker JumpStart section of SageMaker Studio, under Quick start solutions, you can find the solution templates. SageMaker JumpStart solution templates are one-click, end-to-end solutions for many common ML use cases. As of this writing, over 20 solutions are available for multiple use cases, such as demand forecasting, fraud detection, and personalized recommendations, to name a few. The "Zero Shot Text Classification with Hugging Face" solution provides a way to classify text without the need to train a model for specific labels (zero-shot classification) by using a pre-trained text classifier. The default zero-shot classification model for this solution is the facebook-bart-large-mnli (BART) model. For this solution, we use the 2015 New Year's Resolutions dataset to classify resolutions. A subset of the original dataset containing only the Resolution_Category (ground truth label) and the text columns is included in the solution's assets.

The input data includes text strings, a list of desired categories for classification, and whether the classification is multi-label or not for synchronous (real-time) inference. For asynchronous (batch) inference, we provide a list of text strings, the list of categories for each string, and whether the classification is multi-label or not in a JSON lines formatted text file.

The result of the inference is a JSON object that looks something like the following screenshot.

We have the original text in the sequence field, the labels used for the text classification in the labels field, and the probability assigned to each label (in the same order of appearance) in the field scores. To deploy the Zero Shot Text Classification with Hugging Face solution, complete the following steps:

On the SageMaker JumpStart landing page, choose Models, notebooks, solutions in the navigation pane. In the Solutions section, choose Explore All Solutions.

On the Solutions page, choose the Zero Shot Text Classification with Hugging Face model card. Review the deployment details and if you agree, choose Launch.

The deployment will provision a SageMaker real-time endpoint for real-time inference and an S3 bucket for storing the batch transformation results. The following diagram illustrates the architecture of this method.

Perform real-time inference using a zero-shot classification model In this section, we review how to use the Python SDK to run zero-shot text classification (using any of the available models) in real time using a SageMaker endpoint.

First, we configure the inference payload request to the model. This is model dependent, but for the BART model, the input is a JSON object with the following structure:

{ "inputs": # The text to be classified "parameters": { "candidate_labels": # A list of the labels we want to use for the text classification "multi_label": True | False } }Python

Note that the BART model is not explicitly trained on the candidate_labels. We will use the zero-shot classification technique to classify the text sequence to unseen classes. The following code is an example using text from the New Year's resolutions dataset and the defined classes:

classification_categories = ['Health', 'Humor', 'Personal Growth', 'Philanthropy', 'Leisure', 'Career', 'Finance', 'Education', 'Time Management'] data_zero_shot = { "inputs": "#newyearsresolution :: read more books, no scrolling fb/checking email b4 breakfast, stay dedicated to pt/yoga to squash my achin' back!", "parameters": { "candidate_labels": classification_categories, "multi_label": False } }Python

Next, you can invoke a SageMaker endpoint with the zero-shot payload. The SageMaker endpoint is deployed as part of the SageMaker JumpStart solution.

The inference response object contains the original sequence, the labels sorted by score from max to min, and the scores per label:

{'sequence': "#newyearsresolution :: read more books, no scrolling fb/checking email b4 breakfast, stay dedicated to pt/yoga to squash my achin' back!", 'labels': ['Personal Growth', 'Health', 'Time Management', 'Leisure', 'Education', 'Humor', 'Career', 'Philanthropy', 'Finance'], 'scores': [0.4198768436908722, 0.2169460505247116, 0.16591140627861023, 0.09742163866758347, 0.031757451593875885, 0.027988269925117493, 0.015974704176187515, 0.015464971773326397, 0.008658630773425102]}Python

Run a SageMaker batch transform job using the Python SDK This section describes how to run batch transform inference with the zero-shot classification facebook-bart-large-mnli model using the SageMaker Python SDK. Complete the following steps:

Format the input data in JSON lines format and upload the file to Amazon S3. SageMaker batch transform will perform inference on the data points uploaded in the S3 file. Set up the model deployment artifacts with the following parameters:

model_id – Use huggingface-zstc-facebook-bart-large-mnli. deploy_image_uri – Use the image_uris Python SDK function to get the pre-built SageMaker Docker image for the model_id. The function returns the Amazon Elastic Container Registry (Amazon ECR) URI. deploy_source_uri – Use the script_uris utility API to retrieve the S3 URI that contains scripts to run pre-trained model inference. We specify the script_scope as inference. model_uri – Use model_uri to get the model artifacts from Amazon S3 for the specified model_id.

#set model id and version model_id, model_version, = ( "huggingface-zstc-facebook-bart-large-mnli", "*", )

# Retrieve the inference Docker container URI. This is the base Hugging Face container image for the default model above. deploy_image_uri = image_uris.retrieve( region=None, framework=None, # Automatically inferred from model_id image_scope="inference", model_id=model_id, model_version=model_version, instance_type="ml.g4dn.xlarge", )

# Retrieve the inference script URI. This includes all dependencies and scripts for model loading, inference handling, and more. deploy_source_uri = script_uris.retrieve(model_id=model_id, model_version=model_version, script_scope="inference")

# Retrieve the model URI. This includes the pre-trained model and parameters. model_uri = model_uris.retrieve(model_id=model_id, model_version=model_version, model_scope="inference") Python

Use HF_TASK to define the task for the Hugging Face transformers pipeline and HF_MODEL_ID to define the model used to classify the text:

# Hub model configuration < hub = { 'HF_MODEL_ID':'facebook/bart-large-mnli', # The model_id from the Hugging Face Hub 'HF_TASK':'zero-shot-classification' # The NLP task that you want to use for predictions }Python For a complete list of tasks, see Pipelines in the Hugging Face documentation. Create a Hugging Face model object to be deployed with the SageMaker batch transform job:

# Create HuggingFaceModel class huggingface_model_zero_shot = HuggingFaceModel( model_data=model_uri, # path to your trained sagemaker model env=hub, # configuration for loading model from Hub role=role, # IAM role with permissions to create an endpoint transformers_version="4.17", # Transformers version used pytorch_version="1.10", # PyTorch version used py_version='py38', # Python version used )Python

# Create transformer to run a batch job batch_job = huggingface_model_zero_shot.transformer( instance_count=1, instance_type='ml.m5.xlarge', strategy='SingleRecord', assemble_with='Line', output_path=s3_path_join("s3://",sagemaker_config['S3Bucket'],"zero_shot_text_clf", "results"), # we are using the same s3 path to save the output with the input )Python

Start a batch transform job and use S3 data as input:

You can monitor your batch processing job on the SageMaker console (choose Batch transform jobs under Inference in the navigation pane). When the job is complete, you can check the model prediction output in the S3 file specified in output_path. For a list of all the available pre-trained models in SageMaker JumpStart, refer to Built-in Algorithms with pre-trained Model Table. Use the keyword "zstc" (short for zero-shot text classification) in the search bar to locate all the models capable of doing zero-shot text classification. Clean up After you're done running the notebook, make sure to delete all resources created in the process to ensure that the costs incurred by the assets deployed in this guide are stopped. The code to clean up the deployed resources is provided in the notebooks associated with the zero-shot text classification solution and model. Default security configurations The SageMaker JumpStart models are deployed using the following default security configurations:

The models are deployed with a default SageMaker execution role. You can specify your own role or use an existing one. For more information, refer to SageMaker Roles. The model will not connect to a VPC and no VPC will be provisioned for your model. You can specify VPC configuration to connect to your model from within the security options. For more information, see Give SageMaker Hosted Endpoints Access to Resources in Your Amazon VPC. Default KMS keys will be used to encrypt your model's artifacts. You can specify your own KMS keys or use existing one. For more information, refer to Using server-side encryption with AWS KMS keys (SSE-KMS).

To learn more about SageMaker security-related topics, check out Configure security in Amazon SageMaker. Conclusion In this post, we showed you how to deploy a zero-shot classification model using the SageMaker JumpStart UI and perform inference using the deployed endpoint. We used the SageMaker JumpStart New Year's resolutions solution to show how you can use the SageMaker Python SDK to build an end-to-end solution and implement zero-shot classification application. SageMaker JumpStart provides access to hundreds of pre-trained models and solutions for tasks like computer vision, natural language processing, recommendation systems, and more. Try out the solution on your own and let us know your thoughts.

About the authors David Laredo is a Prototyping Architect at AWS Envision Engineering in LATAM, where he has helped develop multiple machine learning prototypes. Previously, he has worked as a Machine Learning Engineer and has been doing machine learning for over 5 years. His areas of interest are NLP, time series, and end-to-end ML. Vikram Elango is an AI/ML Specialist Solutions Architect at Amazon Web Services, based in Virginia, US. Vikram helps financial and insurance industry customers with design and thought leadership to build and deploy machine learning applications at scale. He is currently focused on natural language processing, responsible AI, inference optimization, and scaling ML across the enterprise. In his spare time, he enjoys traveling, hiking, cooking, and camping with his family. Dr. Vivek Madan is an Applied Scientist with the Amazon SageMaker JumpStart team. He got his PhD from University of Illinois at Urbana-Champaign and was a Post Doctoral Researcher at Georgia Tech. He is an active researcher in machine learning and algorithm design and has published papers in EMNLP, ICLR, COLT, FOCS, and SODA conferences.

TAGS: Hugging Face, Natural Language Processing, sagemaker, Text Classification, Zero-Shot Learning

Amazon Translate enhances its custom terminology to improve translation accuracy and fluency

Amazon Translate is a neural machine translation service that delivers fast, high-quality, affordable, and customizable language translation. When you translate from one language to another, you want your machine translation to be accurate, fluent, and most importantly contextual. Domain-specific and language-specific customizable terminology is a key requirement for many government and commercial organizations. Custom terminology enables you to customize your translation output such that your domain and organization-specific vocabulary, such as brand names, character names, model names, and other unique content (named entities), are translated exactly the way you need. To use the custom terminology feature, you should create a terminology file (CSV or TMX file format) and specify the custom terminology as a parameter in an Amazon Translate real-time translation or asynchronous batch processing request. Refer to Customize Amazon Translate output to meet your domain and organization specific vocabulary to get started on custom terminology. In this post, we explore key enhancements to custom terminology, which doesn't just do a simple match and replace but adds context-sensitive match and replace, which preserves the sentence construct. This enhancement aims to create contextually appropriate versions of matching target terms to generate translations of higher quality and fluency. Solution overview We use the following custom terminology file to explore the enhanced custom terminology features. For instructions on creating a custom terminology, refer to Customize Amazon Translate output to meet your domain and organization specific vocabulary.

Exploring the custom terminology feature Let's translate the sentence "she was a great tutor" with Amazon Translate. Complete the following steps:

On Amazon Translate console, choose Real-time translation in the navigation pane. Choose the Text tab. For Target language, choose French. Enter the text "she was a great tutor."

As shown in the following screenshot, the translation in French as "elle était une excellente tutrice."

Under Additional settings¸ select Custom terminology and choose your custom terminology file.

The translation in French is changed to "elle était une excellente éducatrice."

In the custom terminology file, we have specified the translation for "tutor" as "éducateur." "Éducateur" is masculine in French, whereas "tutor" in English is gender neutral. Custom terminology did not perform a match and replace here, instead it used the target word and applied the correct gender based on the context. Now let's test the feature with the source sentence "he has 10 sheep." The translation in French is "il a 10 agneaux." We provided custom terminology for "sheep" as "agneau." "Agneau" in French means "baby sheep" and is singular. In this case, the target word is changed to inflect plural. The source sentence "walking in the evening is precious to me" is translated to "me promener le soir est précieux pour moi." The custom terminology target word "promenant" is changed to "promener" to inflect the correct verb tense. The source sentence "I like burger" will be translated to "J'aime les sandwichs" to inflect the correct noun based on the context. Now let's test sentences with the target language as Spanish. The source sentence "any action-specific parameters are listed in the topic for that action" is translated to "odos los parámetros especificos de acción aparecen en el tema de esa acción" to inflect the correct adjective. The source sentence "in order for us to help you, please share your name" will be translated to "pour que nous puissions vous aider, veuillez partager votre nom." Some words may have entirely different meanings based on context. For example, the word "order" in English can be a sequence (as is in the source sentence) or a command or instruction (as in "I order books"). It's difficult to know which meaning is intended without explicit information. In this case, "order" should not be translated as "commande" because it means "command" or "instruct" in French. Conclusion The custom terminology feature in Amazon Translate can help you customize translations based on your domain or language constructs. Recent enhancements to the custom terminology feature create contextually appropriate versions of matching terms to generate translations of higher quality. This enhancement improves the translation accuracy and fluency. There is no change required for existing customers to use the enhanced feature. For more information about Amazon Translate, visit Amazon Translate resources to find video resources and blog posts, and refer to AWS Translate FAQs.

About the Authors Sathya Balakrishnan is a Senior Consultant in the Professional Services team at AWS, specializing in data and ML solutions. He works with US federal financial clients. He is passionate about building pragmatic solutions to solve customers' business problems. In his spare time, he enjoys watching movies and hiking with his family. Sid Padgaonkar is the Senior Product Manager for Amazon Translate, AWS's natural language processing service. On weekends, you will find him playing squash and exploring the food scene in the Pacific Northwest.

Use a reusable ETL framework in your AWS lake house architecture

Data lakes and lake house architectures have become an integral part of a data platform for any organization. However, you may face multiple challenges while developing a lake house platform and integrating with various source systems. In this blog, we will address these challenges and show how our framework can help mitigate these issues. Lake house architecture using AWS Figure 1 shows a typical lake house implementation in an Amazon Web Services (AWS) environment.

In this diagram we have five layers. The number of layers and names can vary per environmental requirements, so check recommended data layers for more details.

Landing layer. This is where all source files are dropped in their original format. Raw layer. This is where all source files are converted and stored in a common parquet format. Stage layer. This is where we maintain a history of dimensional tables as Slowly Changing Dimension Type 2 (SCD2). Apache Hudi is used for SCD2 in the Amazon Simple Storage Service (Amazon S3) bucket, and an AWS Glue job is used to write to Hudi tables. AWS Glue is used to perform any extract, transform, and load (ETL) job to move, cleanse, validate, or transform files between any two layers. For details, see using the Hudi framework in AWS Glue. Presentation layer. This is where data is being cleansed, validated, and transformed, using an AWS Glue job, in accordance with business requirements. Data warehouse layer. Amazon Redshift is being used as the data warehouse where the curated or cleansed data resides. We can either copy the data using an AWS Glue python shell job, or create a Spectrum table out of the Amazon S3 location.

The data lake house architecture shows two types of data ingestion patterns, push and pull. In the pull-based ingestion, services like AWS Glue or AWS Lambda are used to pull data from sources like databases, APIs, or flat files into the data lake. In the push-based pattern, third-party sources can directly upload files into a landing Amazon S3 bucket in the data lake. Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is used to orchestrate data pipelines that move data from the source systems into a data warehouse. Amazon EventBridge is used to schedule the Airflow directed acyclic graph (DAG) data pipelines. Amazon RDS for PostgreSQL is used to store metadata for configuration of the data pipelines. A data lake architecture with these capabilities provides a scalable, reliable, and efficient solution for data pipelines. Data pipeline challenges Maintaining data pipelines in a large lake house environment can be quite challenging. There are a number of hurdles one faces regularly. Creating individual AWS Glue jobs for each task in every Airflow DAG can lead to hundreds of AWS Glue jobs to manage. Error handling and job restarting gets increasingly more complex as the number of pipelines grows. Developing a new data pipeline from scratch takes time, due to the boilerplate code involved. The production support team can find it challenging to monitor and support such a large number of data pipelines. Data platform monitoring becomes arduous at that scale. Ensuring overall maintainability, robustness, and governability of data pipelines in a lake house is a constant struggle. The benefits of a data pipeline framework Having a data pipeline framework can significantly reduce the effort required to build data pipelines. This framework should be able to create a lake house environment that is easy to maintain and manage. It should also increase the reusability of code across data pipelines. Effective error handling and recovery mechanisms in the framework should make the data pipelines robust. Support for various data ingestion patterns like batch, micro batch, and streaming should make the framework versatile. A framework with such capabilities will help you build scalable, reliable, and flexible data pipelines, with reduced time and effort. Reusable ETL framework In a metadata-driven reusable framework, we have pre-created templates for different purposes. Metadata tables are used to configure the data pipelines. Figure 2 shows the architecture of this framework:

In this framework, there are pre-created AWS Glue templates for different purposes, like copying files from SFTP to landing bucket, fetching rows from a database, converting file formats in landing to parquet in the raw layer, writing to Hudi tables, copying parquet files to Redshift tables, and more. These templates are stored in a template bucket, and details of all templates are maintained in a template config table with a template_id in Amazon Relational Database Service (Amazon RDS). Each data pipeline (Airflow DAG) is represented as a flow_id in the main job config table. Each flow_id can have one or more tasks, and each task refers to a template_id. This framework can support both the type of ingestions—pull-based (scheduled pipelines) and push-based (initiated pipelines). The following steps show the detailed flow of the pipeline in Figure 2.

To schedule a pipeline, the "Scheduled DAG Invoker Lambda" is scheduled in EventBridge, with flow_id of the pipeline as the parameter. The source drops files in a landing bucket. An event is initiated and calls the "Triggered DAG Invoker" Lambda. This Lambda function gets the file name from the event to call the Airflow API. A Lambda function queries an RDS metadata table with the parameter to get the DAG name. Both of the Lambda functions call the Airflow API to start the DAG. The Airflow webserver locates the DAG from the S3 location and passes it to the executor. The DAG is initiated. The DAG calls the functions in the common util python script with all required parameters. For any pipeline, the util script gets all the task details from the metadata table, along with the AWS Glue template name and location. For any database or API connectivity, the util function gets the secret credentials from AWS Secrets Manager based on the secret_id. The AWS Glue template file from the S3 location starts the AWS Glue job using Boto3 API by passing the required parameters. Once the AWS Glue job completes successfully, it deletes the job. If the pipeline contains any Lambda calls, the util script calls the Lambda function as per the configuration parameter. If the AWS Glue job fails due to any error in Step #11, the script captures the error message and sends an Amazon Simple Notification Service (Amazon SNS) notification.

For developing any new pipeline, the developer must identify the number of tasks that need to be created for the DAG. Identify which template can be used for which task, and insert configuration entries to the metadata tables accordingly. If there is no template available, create a new template to reuse later. Finally, create the Airflow DAG script and place it in the DAG location. Conclusion The proposed framework leverages AWS native services to provide a scalable and cost-effective solution. It allows faster development due to reusable components. You can dynamically generate and delete AWS Glue jobs as needed. This framework enables jobs tracking by configuration tables, supports error handling, and provides email notification. You can create scheduled and event-driven data pipelines to ingest data from various sources in different formats. And you can tune the performance and cost of AWS Glue jobs, by updating configuration parameters without changing any code. A reusable framework is a great practice for any development project, as it improves time to market and standardizes development patterns in a team. This framework can be used in any AWS data lake or lake house environments with any number of data layers. This makes pipeline development faster, and error handing and support easier. You can enhance and customize even further to have more features like data reconciliation, micro-batch pipelines, and more. Further reading:

Land data from databases to a data lake at scale using AWS Glue blueprints Creating a source to Lakehouse data replication pipe using Apache Hudi, AWS Glue, AWS DMS, and Amazon Redshift Temporal data lake architecture for benchmark and indices analytics

Ashutosh Dubey Ashutosh is a Global Technical leader and Solutions Architect at Amazon Web Services based out of New Jersey, USA. He has extensive experience specializing in the Data, Analytics, and Machine Learning field, and has helped Fortune 500 companies in their cloud journey to AWS.

Prantik Gachhayat Prantik is an Enterprise Architect at Infosys with 19+ years of experience in various technology fields and business domains. He has a proven track record helping large enterprises modernize digital platforms and delivering complex transformation programs. Prantik specializes in architecting modern data and analytics platforms in AWS. Prantik loves exploring new tech trends and enjoys cooking.

Build a digital asset tokenization framework for financial services use cases using Amazon Managed Blockchain – Part 1

This is the first post in a series of posts covering digital asset tokenization in financial services, a topic which is seeing tremendous interest in the sector. The series aims to be a guide for financial services customers looking to learn more about the topic, and who may be considering building a digital asset capability for their business on AWS. The series explains key concepts, offers guidance on business case alignment, highlights high level and technical design considerations, and finally presents a reference solution architecture of an illustrative business use case using Amazon Managed Blockchain and other relevant services. In this initial post we explore core concepts in digital asset tokenization for financial services and highlight applicable business use cases. Overview of digital asset tokenization in financial services Throughout this post, the terms "Distributed Ledger Technology" (DLT) and "Blockchain" are regarded as being sufficiently synonymous to simply abbreviate to "DLT" for brevity. A digital asset in the context of financial services refers to a digital representation (or "digital twin") of an underlying financial asset. Such digital assets often take the form of digital tokens typically implemented and operated on, but not exclusively, a blockchain or similar distributed ledger technology (DLT). On such a platform, the tokens can be issued, distributed, and traded as financial instruments themselves on a secondary market. The advent of DLT technology has attracted the attention of many industries, but perhaps none more so than financial services given the disruptive potential of the technology to the sector. Digital asset tokenization is a use case in the industry which aligns exceptionally well with the enabling characteristics of distributed ledger technology. Industry impact and potential of digital asset tokenization In a 2021 study conducted by Deloitte 76% of respondents saw digital asset tokenization as a significant long term opportunity, with corporate investors (comprising 31% of surveyed companies) reporting significant uptake in digital assets. In another study, 97% of institutional investors agree that "tokenization will revolutionize asset management" and a March 2023 study by Citibank estimates a Total Addressable Market Size of $4-5 trillion for distributed ledger technology (DLT)-based tokenized securities. An analysis by the World Economic Forum (Global Agenda Council) valued the total tokenized market to be 10% of global GDP by 2030:

Fig 1. Tokenization of illiquid assets to be a $16 trillion worth opportunity globally. Source: World Economic Forum – Global Agenda Council, BCG Analysis With the advent of distributed ledger technologies, a new range of capabilities addressing pain points and new business opportunities within financial services has emerged. Many of these new capabilities involve a new class of assets which are either entirely new native digital assets or digital twins of existing assets and financial instruments. Although not necessarily implemented on DLTs (or similar technology such as centralised cryptographic immutable ledgers), many digital asset providers and institutions have chosen this technology. Many large financial institutions, such as Goldman Sachs, J.P.Morgan, Santander, Societe General, and the European Investment Bank have issued digital asset financial instruments on public DLTs, or are offering new financial products which provide some form of exposure to crypto-currencies or integrate the technology into their existing portfolios. Similarly, digital asset working groups to formulate strategies around DLT technology are now commonplace within financial services institutions. The transformative potential of digital asset tokenization relates to the fact that the financial services industry has multiple layers of intermediaries and participants in distinct silos but operating on the same data which leads to significant duplication and therefore reconciliation. For example, the existing industry infrastructure supporting post-trade settlement on funds and bonds flow through intermediaries such as Central Counterparty Clearing Houses (CCPs). Such intermediaries exist to reduce counterparty risk and settlement failures by providing a trusted third party to become a counterparty to both sides of a transaction. A technology platform that can offer the same degree of settlement and counterparty risk reduction directly without multiple intermediaries offers significant cost reduction and efficiently gains. Digital asset tokens can directly exploit these benefits as all participants in the token lifecycle can transact directly with the token. In addition, the token itself executes the business logic workflow, without reliance on trusted third parties to govern and execute transactions. Some business use cases enabled by Digital Asset Tokenization

Cross-border Payments. A token, as a representation of value, is agnostic to political and economic borders and thus can be easily transferred between accounts which are located in different jurisdictions Regulated Decentralised Finance (De-Fi). By issuing a token on a platform operated by regulated entities employing full KYC/AML controls, innovations such as peer to peer lending/settlement and deposit tokens offered by decentralised finance can be leveraged in a regulated setting. The regulated entities provide the platform but do not function as an intermediatory between issuers and investors. Central Bank Digital Currency (CBDC). Tokens running on a programmable network representing a unit value held at a central bank. A unit of digital currency represented by a fungible token of currency. Delivery v Payment (DvP). During settlement of a tokenised trade transaction the smart contract ensures that the security tokens and the payment tokens are swapped simultaneously. This ensures that one party cannot receive the payment without delivering the securities, or vice versa. Bond, Repo, Euro Commercial Paper Transactions on DLT. Transactions consist with the issuance by the originating institution of a series of tokens on a DLT, where investors purchase and pay for the tokens using traditional fiat.

What is a digital asset token? A digital asset token represents a transferrable and quantifiable unit of value, or a claim on an underlying asset of value, that exists on an electronic programmable network. Digital asset tokens can either be native or non-native to the network. A native token is one that is an integral component of the network, with the network serving as its principal record-keeping system. A prominent example of a native token is Bitcoin. Here, the token (Bitcoin) is inherently bound to its originating network, and it cannot exist independently of that network. On the other hand, a non-native token typically signifies a claim on an underlying asset. A banknote, for instance, provides an apt example of a non-native token. Unlike naturally occurring entities such as gold, a banknote isn't native to our world. Instead, it embodies a claim on an asset (such as gold) held at a central bank.

Fig 2. A digital asset token comprises of codified rules, behaviours, and data. For example, how to pay a dividend, to whom, and when. What is tokenization? Tokenization is the procedure of converting a traditional asset into digital form and subsequently recording it on a programmable platform, such as one enabled by a DLT. However, the value of tokenization goes far beyond the relatively simple digitization of an asset. The fundamental differentiation of tokenization is of "portable representation of value" which refers to how a digital token, secured by cryptography and enabled by a distributed ledger or blockchain, can represent ownership in an immutable, inimitable form and be transferrable between multiple parties in a decentralized network, where trust between participants is not a prerequisite. Tokens can be generated and are linked to, or symbolize various types of assets such as securities, equity shares, participation certificates, debt instruments, shares of investment collective schemes (commonly referred to as funds), and structured products. Tokenization can rightly be perceived as the transformation of conventional securities into digital assets, commonly referred to as ledger-based securities. Why digitally tokenize a financial asset or claim? Despite the advent of electronic trading, global telecommunications and the internet, financial industry market infrastructure exist in silos with distinct tech stacks dating back many decades. Eliminating intermediaries in order to simplify operating models and lower cost, is a key benefit of tokenized representations of financial assets and their corresponding lifecycle. The decentralized nature of DLT networks that often underpin digital asset tokenization platforms enable all eligible participants direct access to the digital representation, without the involvement of intermediaries. Furthermore, tokenization has the potential to significantly improve liquidity for financial instruments which have traditionally proven to be illiquid. Fractional ownership, accessibility of trading infrastructure, execution automation and programmability (using smart contracts), transparency and interoperability across DLT networks all contribute to enhanced liquidity. This, in turn, can lead to price stabilization, settlement and counterparty risk reduction, transaction cost reduction, and reduced friction around price discovery and trading. Fractional ownership is probably the fundamental innovation tokenization brings. For example, investors in a tokenised bond would no longer be bound to the face value of a bond; for example, a $100,000 bond would become 100,000 units/tokens of $1, thus providing fractional ownership and increasing illiquidity to a broader base of potential investors. It is important to note that not all financial products are suitable for tokenization. Assets like fixed income and private equity are more constrained compared to the efficient public equities market as it stands today. It is therefore key to ensure the asset class being considered for tokenization is suitable and the benefits of tokenization are likely to be realized. What is a digital asset tokenization platform? A digital asset tokenization platform converts rights to an asset into a digital token onto a digital and programmable platform which manages the progression of the token through its business lifecycle. The state of the token is maintained on a shared multi-party ledger which is accessible to entitled participants. Participants may transact directly with each other on the platform for example, issuers transacting directly with investors, direct peer to peer trading across secondary markets, regulators monitoring transactions in real-time and settlement services executing directly on the platform, as illustrated in the following diagram:

Fig 3. The traditional method for managing financial asset lifecycles involves a "duplicated system of record." In this approach, all participants keep duplicate data in separate databases. Because of the ongoing risk of data errors, these databases need frequent reconciliation. The registry, or 'golden record' of the asset is stored by a centralised intermediatory such as a Central Security Depositary (CSD). Data is accessed by the various participants involved in the asset lifecycle, such as investment banks, corporate investors, private investors, funds, and custodians (and others service providers such as CSDs) through Application Program Interfaces (APIs). With a decentralized tokenization platform, the programmable tokens are codified on a shared platform with no single centralized entity having overall control, and all participants having direct access to the asset they are eligible to transact with thus eliminating the need for intermediaries. The state of the digital asset, for example current owner, is an attribute of the token itself – not a data record maintained by all participants. Digital asset 'smart contracts' are an intrinsic attribute of a token on the platform, codifying token behaviour and enabling automation of the business lifecycle events. The quality, reliability, consistency and auditability of a digital asset tokenization workflow implementation is therefore critically important to ensure regulatory, contractual, and compliance standards are met – perhaps more so than with conventional approaches, due to the characteristic immutability and automation of the platform. The programmability of the token's lifecycle on the platform leads to another benefit of 'composability' which refers to the ease by which smart contract of differing tokens can interoperate seamlessly to build innovative financial product. For example, if two different tokens are created using smart contracts which know how to interoperate with each other, the tokens can be easily swapped thus forming a financial swap instrument. Similarly, one token could be programmed to be equivalent in value to a number of other known tokens forming a structured hierarchy of tokens, the exact structure of which is being configured to achieve a specific business goal or end-client requirement. The lifecycle of a digital asset token, representing a claim on an underlying asset, generally follows the same lifecycle as conventional security processing; namely origination, issuance and distribution, trading, clearing, settlement, and maturity. However, a token-based platform will differ in some key regards:

The platform is more decentralized – there is no, or considerably less centralised entities, which govern the platform. Token ownership, for example, can be transferred directly between participants on the platform without passing through a central entity. With traditional centralized ledgers, the services which comprise the operation of a security; for example; custody settlement, escrow, collateral, and regulatory actions are distinct processes which are executed by various agencies running their own applications and databases. The attributes of the security, such as the owner, the custodian, the identification, the issuance value, are represented by duplicated and distributed data records across the network. A tokenization platform however, codifies these fragmented and siloed data records into a programmable unit which can exist in a distinct form on the platform. The token's lifecycle is operated in an automated manner in response to events originating externally to the platform.

The following lists the key lifecycle stages and how a tokenized asset can benefit from these stages executed by a tokenization platform:

Origination and issuance process can be significantly automated by smart contract, reducing time and cost. Trading can execute in real-time directly between counterparties on the same platform, further reducing capital requirements, counterparty risk, and settlement times. Settlement and clearing time is reduced and processes streamlined, drastically reducing errors and costly reconciliation and exception processing workflows. Servicing could be programmed via a smart contract to auto-trigger pay-outs for corporate actions. In the case of loan securitisation, borrower repayments could be pooled and distributed to investors via token smart contracts.

Fig 4. A digital asset token exists on the platform and transits various states aligned to the business workflow. Its state is visible to all eligible participants. Benefits at each stage are illustrated. Conclusion In this post, we explored the fundamental concepts underpinning digital asset tokenization within the realm of the financial services industry. Common reasons for building a platform to facilitate digital asset tokenization were highlighted, and how the traditional asset lifecycle is disrupted and enhanced using tokenization. In conclusion, digital asset tokens can take many forms, from tokenised credit cards on a cellphone, to a multi-million-dollar bond on the public Ethereum DLT. It is important to realise a DLT is not a pre-requisite for enabling tokenization, it is one facilitating technology that has the potential to fulfil many of the aspired benefits of tokenization. It is critically important to carefully consider the asset's attributes and operational mechanisms when considering tokenization, as different types of asset may yield differing degrees of benefit. Assets with little manual intervention, a clear legal framework, and clear regulation have a higher chance of capturing the various benefits of tokenization. In the next part in the series, we will look into the considerations involved in envisioning a tokenisation framework on AWS and present potential solution architectures on key aspects. In addition, the benefits of using a manged blockchain service, such as Amazon Managed Blockchain will be discussed. Newly launched capabilities such as Amazon Managed Blockchain Query and Amazon Managed Blockchain Access provide actionable real-time data across different blockchains with standardized APIs. For example, a tokenization architecture would utilize Amazon Managed Blockchain Query to extract token balances, transactions, and token events from the Ethereum blockchain in real-time, enabling rapid integration with actionable analytics and applications. Amazon Managed Blockchain Access provides public blockchain nodes for Ethereum and Bitcoin, as well as the ability to create private blockchain networks with the Hyperledger Fabric framework. These capabilities enable developers and architects to focus on differentiating innovation rather than node hosting infrastructure which offers significant architectural acceleration when planning and implementing a tokenization architecture.

Steven Bacci is a Principal Specialist Solution Architect within the Worldwide Specialist Blockchain/Web3 team at Amazon Web Services. He works closely with the product and engineering team for Amazon Managed Blockchain, advising customers globally on business case alignment, technical architectural designs, and application engineering for Blockchain and Web3 solutions with a specific focus on the financial services sector.

Measure and track cloud efficiency with sustainability proxy metrics, Part I: What are proxy metrics?

Sustainability has become an important decision-making factor for customers, employees, regulators, investors, and partners. Customers have started their journey towards a sustainable business and operations. If you're building, deploying, and maintaining IT infrastructure, reducing its environmental impact is an important step in reaching company-wide sustainability goals. Thus, sustainability has become a non-functional requirement in modern software and systems architecture, along with security, maintainability, reliability and others. When it comes to architecting workloads in the cloud, sustainability is a shared responsibility between AWS and customers. Whereby AWS optimizes for sustainability of the cloud, customers are responsible for sustainability in the cloud. Customers optimize their service usage and resource efficiency. This blog post series provides an overview on how you can establish a sustainability proxy metrics showback mechanism for teams wanting to optimize their AWS usage for sustainability. In Part I, we introduce the concept of proxy metrics and the importance of normalization. We also show examples of how customers have used this concept to reduce the environmental impact of their applications. In Part II: Establish a metrics pipeline, we discuss how you set up a proxy metrics data pipeline to establish a sustainability proxy metrics showback mechanism. Optimize your workloads with proxy metrics Every optimization should start with a goal informed with metrics or KPIs: Reducing cost, increasing performance, or reducing greenhouse gas emissions. The AWS Customer Carbon Footprint Tool (CCFT) provides the important output metric of the greenhouse gas emissions associated with a customer's AWS service usage. This emission data is used for reporting and understanding the high-level impact on a monthly basis. However, while AWS is working to increase scope and granularity of the CCFT (read this blog), a practice of continuous optimization cycles calls for fine-grained metrics. Absolute emissions don't expose the efficiency of a workload. Emissions are the outcome of multiple factors including factors which are not in the responsibility of application teams such as the usage of an application, or the carbon intensity of energy. For these purposes we complement the carbon emissions reported by the AWS Customer Carbon Footprint Tool with dependent metrics that we call sustainability proxy metrics. We have also launched the Sustainability Proxy Metrics Dashboard (you can access the dashboard from this link), as part of the Cloud Intelligence Dashboards. Good sustainability proxy metrics serve as fine-grained substitutes to carbon emissions, that provide insights into workload efficiency. Metrics that we track in near real-time and break down to application teams and resources, so they are suitable for fast optimization cycle times. They are tangible metrics that reflect resource usage, in terms of Compute, Storage and Networking (read these blogs).

As depicted in Figure 1 on the right, calculating Greenhouse Gas Emissions for AWS service usage is dependent on multiple data sources. This includes the energy required to run cloud resources (Scope 1 & 2) and the indirect emissions associated with the lifecycle of physical resources, up- and downstream in the value chain (Scope 3). Similarly, cost is a simple function of AWS service usage. But even though cost reflects usage, volume based discounts reduce cost while not reducing associated emissions. Also, the pricing structure of certain services does not reflect every aspect of resource usage – consider data transfer pricing and how there is no charge for inbound data transfer across all services in all regions or how data transfer charges don't differ depending on end customer proximity. AWS service usage in turn depends on and is used by a customer's operational processes to fulfill business needs, completing the data flow on the left. All of this comes back to efficiency and using the least amount of resources to fulfill business needs. Normalize metrics to allow for comparison We sometimes see customers counting the number of Amazon EC2 instances, or the amount of instance hours to quantify resource consumption. These metrics do not help to compare applications, identify top contributors to consumption or spot trends. Some applications run instances only for minutes before termination. Others run a single instance for a whole month. In a similar way the instance size matters. Instead of just using instance hours, you have to factor in the amount of vCPUs of an instance. We call this normalization. There are many ways to normalize:

Normalize resource usage: Use the information about the instance type and multiply the instance hours with the number of vCPUs. Alternatively, take normalization factors into account such as those used by Amazon EC2 Reserved Instances. The same applies to other services like Amazon S3 or Amazon EBS in which you take the GB hours instead. For KPIs, calculate the ratio of desired usage in relation to total usage. That's already the case with CPU utilization. If your goal is Amazon EC2 Spot adoption, then that is all spot hours divided by all vCPU hours. And if it's AWS Graviton adoption, then it is all Graviton vCPU hours divided by the total vCPU hours. You define a minimum target percentage for your application teams for this type of KPI. Use a scoring system to weight services and features differently and incentivize application teams to use resource-efficient services. For example, weight the Amazon S3 Standard storage class higher than Amazon S3 Intelligent-Tiering, by applying a multiplier, as the service description of the latter provides flexibility to AWS to optimize for using less energy and less hardware to provide the service. The goal for application teams is to drive down the weighted usage. Resource efficiency is using the least amount of resources to fulfill business needs. Your KPIs or metrics have to factor this in by normalizing the resource usage by business unit metrics. We will dive deeper into this in the next section.

Normalize by business metrics An increase in resource usage is not a cause for alarm when your business grows, but a steady consumption at dropping customer demand is. Factoring in business metrics in your KPIs helps to track and communicate efficiency over time. Business metrics are specific to the purpose of a workload. Examples include the number of monthly active users, or insurance policies managed, or successful calls to an API. You divide your resource usage by a business metric (read this user guide "Evaluate specific improvements") to calculate a sustainability KPI, like vCPU hours per transaction, as depicted in the equation below. Ideally, you want to see your sustainability KPIs go down or at least, stay on level. You will find the related concept of unit metrics for cost in the blog post "choose, create, and track your unit metrics for your applications".

In AWS re:Invent 2022 Build a cost-, energy-, and resource-efficient compute environment (CMP204) (watch the recording) Arm – a global semiconductor industry leader – present how they measured, tracked, and reduced the impact of Electronic Design Automation (EDA) jobs. They used Amazon EC2 instances' vCPU hours to calculate KPIs for Amazon EC2 Spot adoption, AWS Graviton adoption, and the resources needed per job. Similarly, Amazon Prime Video explain in AWS re:Invent 2022 Architecting sustainably and reducing your AWS carbon footprint (SUS205) (watch the recording) how they used the following sustainability proxy metrics to quantify and track the effectiveness of optimizations:

Playback experience: Infrastructure cost ($) per 1.000 concurrent streams Content delivery: Delivery bandwidth (Gbps) per stream Content discovery experience: Normalized Instance Hour (NIH) per 1000 page impressions Customer acquisition: Infrastructure cost ($) per subscription

Optimizing towards their goals, Prime Video implemented tradeoffs between sustainability goals and other non-functional requirements. To match the provisioning of resources to the spikey demand from viewers of "Thursday Night Football" they implemented automated contingency switches that turn off non-critical customer experience features if the system was under duress. Conclusion In this post we've covered the motivation for sustainability proxy metrics and KPIs. We explained the concept of usage based metrics, normalization and inclusion of business metrics, and shared examples on how customers are using these metrics to optimize for sustainability. In Part II of this blog post series, we'll dive deeper into how you set up a proxy metrics data pipeline to establish a sustainability proxy metrics showback mechanism for teams wanting to optimize their AWS usage for sustainability and for organizational adoption of efficiency best practices at scale. For detailed information on how to optimize your workloads for sustainability, please refer to the AWS Well Architected Sustainability Pillar. If you're interested to start measuring and optimizing sustainability proxy metrics for your applications, please locate the "Sustainability Proxy Metrics Dashboard" and implement today.

Katja Philipp Katja Philipp is a Senior Prototyping Architect at Amazon Web Services. She works with customers to build innovative prototypes using AI, ML, and generative AI technologies that solve real business challenges. Katja is passionate about sustainability, serverless and how technology can be leveraged to solve current challenges for a better future.

Jonas Bürkel Jonas Bürkel is a Solutions Architect at AWS based in Germany. He helps customers in the manufacturing industry build their solutions in the cloud to meet their unique business and technical requirements. Jonas is also passionate about sustainability and how technology can help us become more efficient.

Steffen Grunwald Steffen Grunwald is a Principal Sustainability Solutions Architect at Amazon Web Services. He supports customers in solving their sustainability challenges through the cloud. Having a long software engineering background, he loves to dive deep into application architectures and development processes to drive sustainability, performance, cost, operational efficiency, and increase the speed of innovation.

Introducing S3 cross-region support in the AWS SDK for Java 2.x

AWS SDK for Java team is excited to introduce the latest addition to the AWS SDK for Java 2.x: the Amazon Simple Storage Service (Amazon S3) Cross-Region Client feature. With this new feature, you can effortlessly access Amazon S3 buckets in different AWS Regions by using a single client configured for cross-region access. Join us as we explore the possibilities and convenience offered by this exciting enhancement. Motivation A common challenge when working with Amazon S3 buckets is the need to know the bucket's region to interact with it. Users might not always have explicit knowledge of the bucket's region, resulting in additional calls to determine the region. The new Amazon S3 Cross-Region client feature addresses this problem by enabling seamless access to S3 buckets across different regions. In this blog post, we explore the significance of this feature and how to use it to simplify cross-region S3 bucket access. Getting Started To begin, add the Amazon S3 dependency to your project for SDK versions 2.20.111 or higher: software.amazon.awssdk s3 2.20.111 XML By default, the cross-region feature is disabled. To enable it for an S3Client, set crossRegionAccessEnabled to true on the S3Client Builder: S3AsyncClient client = S3AsyncClient.builder() .crossRegionAccessEnabled(true) .build();Java Routing Requests to the Correct Region When you reference an existing bucket in a request, such as with the 'putObject' request, the SDK initiates a request to the region configured for the client. If the bucket does not exist in that specific region, the error response includes the actual region where the bucket resides. Subsequently, the SDK makes the request to the correct region using the region information obtained from the error response. To optimize future requests to the same bucket, the SDK caches this region in the client. Considerations for Enabling Cross-Region Bucket Access When you enable cross-region bucket access, it's important to keep in mind that the first API call may incur increased latency if the bucket is not in the client's configured location. However, subsequent calls benefit from cached region information, resulting in improved performance. Please take note of the potential latency for the initial cross-region bucket access call and plan accordingly. This feature does not affect access to the bucket; the user still needs to be authorized to access the bucket in whatever region it resides. Conclusion Cross-region bucket access simplifies the management of Amazon S3 buckets by allowing you to access a bucket without needing to know the bucket's region in advance. It is important to understand the routing process, caching capabilities, and increased latency for the first API call to buckets in different Regions. The Amazon S3 Cross-Region Client feature, available in the AWS SDK for Java 2.0, provides a valuable functionality. We hope you'll find this new feature useful. If you have any question, comments, concerns, or ideas, please open an issue or bug report on the GitHub issues page.

John Viegas John Viegas serves as a Software Development Engineer on the Java SDK team at Amazon Web Services (AWS). With a passion for enhancing the developer experience, he actively engages in projects and tools designed to elevate software development. For further insights into his contributions, feel free to explore his GitHub profile under the handle @joviegas.

Customers running Google Kubernetes Engine (GKE) clusters can now use the Amazon Elastic Kubernetes Service (Amazon EKS) to visualize GKE cluster resources. This post describes how to use Amazon EKS Connector to connect a GKE cluster to the Amazon EKS console. The EKS console provides a single pane of glass to visualize all your Kubernetes clusters. Customers that prefer a graphical user interface use the Amazon EKS console to view the Kubernetes cluster's status, configuration, and workloads. With Amazon EKS Connector, customers can also view their GKE cluster's information along with Amazon EKS clusters. In addition to GKE, Amazon EKS Connector allows you to register and connect any conformant Kubernetes cluster to Amazon EKS. Any external cluster information shown in the EKS console is view-only. Solution You can connect your GKE clusters to Amazon EKS using the AWS CLI, the AWS Management Console, or eksctl. This post uses eksctl. You can find the steps for AWS Management Console or AWS CLI here. You'll need the following tools:

You'll also need a GKE cluster on which you can install EKS Connector. With eksctl, connecting an external Kubernetes cluster to Amazon EKS is a two-step process:

Generate external cluster configuration eksctl makes it easy to register clusters by creating the required AWS resources and generating Kubernetes manifests for EKS Connector. Register your GKE cluster: eksctl register cluster \ --name \ --provider GKE \ --region Bash Please change the Region in the command above to match your environment. eksctl register cluster registers the external cluster to Amazon EKS and creates three files.

These manifests will create eks-connector statefulset in a new namespace. The manifests permit EKS connector to get and list resources in all namespaces in the applied cluster. eksctl also creates an IAM role that EKS Connector uses to invoke Systems Manager APIs. { "Version": "2012-10-17", "Statement": [ { "Sid": "SsmControlChannel", "Effect": "Allow", "Action": [ "ssmmessages:CreateControlChannel" ], "Resource": "arn:aws:eks:*:*:cluster/*" }, { "Sid": "ssmDataplaneOperations", "Effect": "Allow", "Action": [ "ssmmessages:CreateDataChannel", "ssmmessages:OpenDataChannel", "ssmmessages:OpenControlChannel" ], "Resource": "*" } ] }Bash The next step is to apply these manifests to the GKE cluster. Install EKS Connector on GKE Apply the manifests that eksctl generated in the previous step: kubectl apply -f eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yamlBash Verify that EKS Connector pods are running: kubectl get all --namespace eks-connector

NAME READY STATUS RESTARTS AGE pod/eks-connector-0 2/2 Running 0 5m49s pod/eks-connector-1 2/2 Running 0 5m31s

NAME READY AGE statefulset.apps/eks-connector 2/2 5m51sBash EKS Connector uses user impersonation to authorize against the GKE cluster's API server. Kubernetes administrators can further customize EKS Connector's permissions to limit its access to cluster resources. Please see Granting access to a user to view a cluster to configure more restrictive access. For this post, we have used a role that permits view access to all cluster resources. Once the GKE cluster is registered, you can use eksctl to list your clusters, including your GKE cluster. ~# eksctl get clusters 2021-11-11 06:32:42 [i] eksctl version 0.74.0-rc.0 2021-11-11 06:32:42 [i] using region us-east-1 NAME REGION EKSCTL CREATED eks-cluster-aws us-east-1 True gke-connected-by-eks-connector us-east-1 FalseBash You can also use the AWS CLI to list and describe registered clusters: ~# aws eks describe-cluster –name

{ "cluster": { "name": "gke-connected-by-eks-connector", "arn": "arn:aws:eks:us-east-1:831141539580:cluster/gke-connected-by-eks-connector", "createdAt": "2021-11-10T21:33:43.179000+00:00", "status": "ACTIVE", "tags": {}, "connectorConfig": { "activationId": "397acad4-4ecf-4fca-8beb-e71bc6128481", "activationExpiry": "2021-11-13T21:33:42.779000+00:00", "provider": "GKE", "roleArn": "arn:aws:iam::831141539580:role/eksctl-20211110213333349284" } } }Bash The Amazon EKS console will now show the GKE cluster along with your EKS clusters and any other registered external clusters. The user or role with which you log in to the AWS Management Console and the role used to generate manifests in the previous step should be the same.

Here's a screenshot of the EKS console that shows an overview of the GKE cluster's data plane.

You can view node level details such as its compute resources, kernel, and operating system details. The panel also displays the resource allocation and pods running on the specific node.

The Workloads tab shows workloads (pods, deployments, statefulsets, and daemonsets) running in the cluster. It also allows filtering by namespace or resource type. You can select a workload to view pod details such as events, status, labels, and annotations.

When pods contain multiple containers, you can further drill down to view container-level details such as image, mounts, ports, and environment variables.

The EKS Console also provides information on the objects in JSON format. Here's information about a node in JSON:

Similarly, you can enable the Raw view toggle to view the JSON of other Kubernetes objects in the cluster. Tags can be applied to registered clusters to help you track each cluster's owner, organization, cluster function, and so on. You can then search and filter the clusters based on the tags that you add.

Cleanup Run the following command to delete resources created in this post: kubectl delete -f eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yaml

eksctl deregister cluster Bash Conclusion Using EKS connector, you can view any conformant Kubernetes cluster information in the Amazon EKS console. You can connect any conformant Kubernetes cluster, including Amazon EKS Anywhere clusters running on-premises, self-managed clusters on Amazon Elastic Compute Cloud (Amazon EC2), and other Kubernetes clusters running outside of AWS such as GKE. Regardless of where your cluster is running, you can use the Amazon EKS console to get a centralized view of all connected clusters and the Kubernetes resources running on them. EKS Connector is open-source. Visit the EKS documentation for more details. *Google Kubernetes Engine and icon are trademarks of Google LLC. AWS is not affiliated with Google LLC or Google Kubernetes Engine.

Gokul Chandra Gokul is a Specialist Solutions Architect at Amazon Web Services. He assists customers in modernizing with containers helping them to use AWS container services to design scalable and secure applications. He is passionate about cloud native space and Kubernetes. Gokul's areas of interest include Containers, Microservices, Public & Private Cloud Platforms, Cloud Native for Telco, Edge Computing, Hybrid & Multi Cloud Architectures and NFV. You can find him in Medium @gokulchandrapr and Linkedin @gokulchandra.

Re Alvarez-Parmar In his role as Containers Specialist Solutions Architect at Amazon Web Services, Re advises engineering teams with modernizing and building distributed services in the cloud. Prior to joining AWS, he spent more than 15 years as Enterprise and Software Architect. He is based out of Seattle. Connect on LinkedIn at: linkedin.com/in/realvarez/

This post was co-written with Sparsh Agarwal, Senior Product Manager at Salesforce Introduction Amazon EKS Anywhere (Amazon EKS-A) takes the power of Amazon Elastic Kubernetes Service (Amazon EKS) beyond the AWS cloud and enables you to run Amazon EKS on your own infrastructure. It provides an installable software package for creating and operating Kubernetes clusters on-premises and automation tooling for cluster lifecycle support. Amazon EKS Anywhere can be installed on bare metal servers, VMware vSphere, CloudStack, Nutanix, AWS Snowball Edge and Docker. It allows you to leverage the benefits of EKS across hybrid environments, ensuring consistency and flexibility in your Kubernetes deployments. Amazon EKS Anywhere brings a consistent Amazon Web Services (AWS) management experience to your data center, building on the strengths of Amazon EKS Distro, an open-source distribution for Kubernetes used by Amazon EKS. MuleSoft is an AWS partner that accelerates the adoption of key cloud services while allowing customers to safely unlock the data inside legacy, on-premises, or SaaS applications. MuleSoft Anypoint Runtime Fabric is a powerful runtime environment for Mule applications, providing enhanced performance, scalability, and high availability. You typically create Mule application to perform system integrations. Mule apps are configured to run in Mule Runtime. A request to a Mule app triggers Mule to encode the request and data in a Mule Event, and to pass it to either single or multiple threads. In this post, we'll explore how to deploy Anypoint Runtime Fabric on Amazon EKS Anywhere, allowing you to run Mule applications in a Kubernetes environment across on-premises. By combining the capabilities of Anypoint Runtime Fabric and Amazon EKS Anywhere, organizations can unlock the potential of running Mule applications in Kubernetes clusters deployed on their infrastructure. This provides a unified and scalable runtime environment for Mule applications, enabling efficient resource utilization, simplified management, and seamless integration with other services in the cloud and on-premises. Throughout this post, we guide you through the process of setting up Amazon EKS Anywhere on VMware vSphere and deploying Anypoint Runtime Fabric on Amazon EKS Anywhere. By following the provided instructions and best practices, you can harness the power of MuleSoft and Amazon EKS Anywhere to build and manage resilient, high-performing Mule applications across hybrid environments. Solution overview The solution consists of setting up your VMware vSphere environment to deploy an Amazon EKS Anywhere Management cluster. This cluster will be used to deploy the workload cluster where the actual mule applications will be running. The management cluster also hosts the necessary management components for the workloads cluster such as Ingress, registry and monitoring components. In the workload cluster, the mule applications runs along with the necessary Mule runtime and runtime fabric agents.

Figure 1: MuleSoft Runtime Anytime Fabric Architecture on EKS-Anywhere Prerequisites

Administrative machine with machine requirements as listed in the EKS-A product page with the following tools installed:

AWS Command Line Interface (AWS CLI) version 2.10 or higher Helm eksctl eks-anywhere (v0.14.0 or later) kubectl rtfctl

VMware vSphere infrastructure with required resources as listed in EKS-A documentation Prepare vSphere for EKS Anywhere Amazon EKS-A Curated Packages are only available to customers with the Amazon EKS-A Enterprise Subscription. To request a free trial, use the link MuleSoft Anypoint Runtime Fabric License. To request a license, connect with your MuleSoft Account representative through this contact form Create a vSphere Cluster on VMware

Create an initial cluster on your vSphere environment that will be used as management cluster (to create and manage other clusters). Create one or more workload clusters from the management cluster.

Walkthrough Deploying MuleSoft Anypoint Runtime Fabric Execute all commands mentioned in this post on the Administrative Machine. Validate that your Kubernetes environment is ready for installation. Activation data can be obtained from the MuleSoft activation documentation page:

You can then apply your Mule license key to this installation:

kubectl CLI is used to access the Amazon EKS-A cluster and verifies MuleSoft deployment. For example, you can use the following command to verify MuleSoft rtf namespace and verify the deployed pods and their state:

kubectl get pods -A NAMESPACE NAME READY STATUS capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-7d6b5746b6-jw7bb 1/1 Running capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-574cbcd9d7-jzswd 1/1 Running capi-system capi-controller-manager-5b64bc4449-mcw5c 1/1 Running capv-system capv-controller-manager-64c4954b7c-85r25 1/1 Running cert-manager cert-manager-7568b959dc-6mtnc 1/1 Running eksa-packages cron-ecr-renew-27782220-lh8b8 0/1 Completed eksa-packages cron-ecr-renew-27782520-z84jw 0/1 Completed eksa-packages cron-ecr-renew-27782820-29g47 0/1 Completed eksa-packages eks-anywhere-packages-5d6df98848-mrmbm 1/1 Running eksa-packages eksa-auth-refresher-kxnds 0/1 Completed eksa-system eksa-controller-manager-6d5f44755c-m9bc9 1/1 Running kube-system vsphere-cloud-controller-manager-d7r5k 1/1 Running kube-system vsphere-cloud-controller-manager-fdm7m 1/1 Running kube-system vsphere-csi-node-6v497 3/3 Running rtf agent-5488fbd6db-m9zj8 2/2 Running rtf agent-upgrade-jqzs5 0/1 Completed rtf cluster-status-27783040-dlq8r 0/1 Completed rtf initial-cluster-status-64k77 0/1 Completed rtf mule-clusterip-service-66b4cfb785-jf6d7 1/1 Running rtf resource-cache-7d86dd7996-tk5ql 2/2 Running rtf rtf-install-job-m7l7z 0/1 Completed Apache Configuration

Figure 2: MuleSoft Anypoint Platform Runtime Manager console Deploy a sample Hello world Mule application and expose it with an load balancer We'll use MetalLB as the external LB in this case. Step 1: Select a range of underlay IP's that will be used for the LoadBalancer. In the setup below the underlay IP Classless Inter-Domain Routing(CIDR) is 10.2.16.0/24:

kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.2.16.14 Ready 50d v1.23.7-eks-7709a84 10.2.16.14 10.2.16.14 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket 10.2.16.15 Ready control-plane,master 11d v1.23.7-eks-7709a84 10.2.16.15 10.2.16.15 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket 10.2.32.21 Ready control-plane,master 11d v1.23.7-eks-7709a84 10.2.32.21 10.2.32.21 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket 10.2.64.29 Ready 50d v1.23.7-eks-7709a84 10.2.64.29 10.2.64.29 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket Apache Configuration

Step 2: Create a package configuration file for MetalLB. From Step 1, we see that the address range for four IP's 10.2.16.30-10.2.16.33 are not used by any other service. In the following code snippet, we use this IP range for the MetalLB Load Balancer. This IP range should not be used by any other service.

cat metallb.yaml apiVersion: packages.eks.amazonaws.com/v1alpha1 kind: Package metadata: creationTimestamp: null name: generated-metallb namespace: eksa-packages spec: packageName: metallb config: | IPAddressPools: - name: default addresses: - 10.2.16.30-10.2.16.33 L2Advertisements: - IPAddressPools: - default EOF Apache Configuration

Step 3: Install the EKS-A Package using EKS Anywhere CLI (don't use kubectl and always use sudo as docker.socket permissions, which needs sudo in this environment). Before the package creation create a namespace for MetalLb controllers.

sudo eksctl anywhere create packages -f metallb.yaml —kubeconfig ~/.kube/config ----------------------- The Amazon EKS Anywhere Curated Packages are only available to customers with the Amazon EKS Anywhere Enterprise Subscription ----------------------- package.packages.eks.amazonaws.com/generated-metallb created Apache Configuration

Step 4: Check if all components of MetalLB are in running state.

kubectl get all -n metallb-system NAME READY STATUS RESTARTS AGE pod/generated-metallb-controller-5dbdbdfff9-wb66j 1/1 Running 0 105s pod/generated-metallb-speaker-9bnv4 1/1 Running 0 105s pod/generated-metallb-speaker-cpstb 1/1 Running 0 105s pod/generated-metallb-speaker-kpt2m 1/1 Running 0 105s pod/generated-metallb-speaker-stjdf 1/1 Running 0 105s

NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/generated-metallb-speaker 4 4 4 4 4 kubernetes.io/os=linux 105s

NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/generated-metallb-controller 1/1 1 1 105s

NAME DESIRED CURRENT READY AGE replicaset.apps/generated-metallb-controller-5dbdbdfff9 1 1 1 105s Apache Configuration

Step 5: Install Nginx Ingress controller (there is enterprise version and community versions available – in this installing the community version).

helm upgrade --install ingress-nginx ingress-nginx --repo --namespace ingress-nginx --create-namespaceApache Configuration

Step 6: Verify that the ingress-controller service procured an external-ip (in this case its provided by MetalLB from the range provided). Once available, the curl should provide a reply as shown below.

kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.111.28.84 10.2.16.30 80:31385/TCP,443:32224/TCP 74s ingress-nginx-controller-admission ClusterIP 10.109.26.230 443/TCP 74s curl 10.2.16.30 404 Not Found 404 Not Found nginx Apache Configuration

Step 7: Create an Ingress configuration manifest. Below is the sample hello world mule application that can be accessed with hostname www.helloworld.com and path /helloWorld (www.helloworld.com/helloWorld).

cat helloworld-sample-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: helloworld-ingress namespace: rtf spec: ingressClassName: nginx rules: - host: www.helloworld.com http: paths: - pathType: Prefix backend: service: name: helloworld port: number: 8081 path: /helloWorld EOF kubectl apply -f helloworld-sample-ingress.yaml kubectl get ing -A NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE default demo-localhost nginx demo.localdev.me 10.2.16.30 80 33h f1039f97-fc61-4966-b65c-a0b96be70e54 helloworld-ingress nginx www.helloworld.com 10.2.16.30 80 7m53s f1039f97-fc61-4966-b65c-a0b96be70e54 helloworld-rtf-ingress-template nginx rtf.example.com 10.2.16.30 80 33h Apache Configuration

Note: Copy the IP address nginx ingress for the next step. Step 8: In order to test the hello world application, add the static IP 10.2.16.30 to /etc/hosts file to resolve www.helloworld.com. Usually in general production setting, this IP will be added to customers IPAM like InfoBlox, etc. In this case, we set this static IP to the LoadBalancer service IP of the ingress-controller service.

Step 9: Now you should be able to access the service with the path configured and the specific host in the specification.

Operational support: There are no upfront commitments or fees to use Amazon EKS Anywhere. Customers can optionally purchase Amazon EKS Anywhere Enterprise Subscriptions for access to EKS-A Curated Packages as well as 24/7 support from AWS for all bundled tooling. Version support: Refer to the official Amazon EKS Anywhere and Kubernetes version support policy page. Pricing: Amazon EKS Anywhere Enterprise Subscription option available with EKS-A, which is required to get support for the EKS-A clusters and access to additional paid features such as Amazon EKS Anywhere Curated Packages.

Cleaning up To delete the resources provisioned in the blog, please execute the following commands. Kubectl delete ns metallb-system Kubectl delete ns ingress-nginx Kubectl delete ns rtfBash Please make sure to delete the EKS Anywhere cluster using the steps mentioned in the link. Conclusion In this post, we showed you how to deploy MuleSoft Anypoint Runtime Fabric on Amazon EKS Anywhere to bring the power and flexibility of MuleSoft's runtime environment to on-premises and edge locations. This combination allows organizations to build and manage scalable, resilient, and high-performing Mule applications across AWS cloud and on-premises. By following the step-by-step instructions in this blog post, you can start leveraging the capabilities of MuleSoft Anypoint Runtime Fabric on Amazon EKS Anywhere. To configure Mulesoft Anypoint Runtime Fabric on Amazon EKS, you can follow the quickstart link to deploy using CloudFormation For more information on getting started with Amazon EKS Anywhere, check out the EKS-A workshop, EKS-A documentation, or frequently asked questions. Check out the EKS-A GitHub repository and join the community Slack channel in the Kubernetes workspace to contribute.

Sparsh Agarwal, Salesforce Sparsh Agarwal is a senior product manager at Salesforce focused on transforming runtime fabric into the K8s platform used for scaling Salesforce automations. Before Salesforce, he drove data platforms at Microsoft from infancy to an industry leader and extended machine learning products to the Edge platform.

Kranthi Pullagurla Kranthi Pullagurla has over 20+ years' experience across Application Integration and Cloud Migrations across Multiple Cloud providers. He works with AWS Partners to build solutions on AWS that our joint customers can use. Prior to joining AWS, Kranthi was a strategic advisor at MuleSoft (now Salesforce). Kranthi has experience advising C-level customer executives on their digital transformation journey in the cloud.

Vikram Venkataraman Vikram Venkataraman is a Principal Solution Architect at Amazon Web Services and also a container enthusiast. He helps organization with best practices for running workloads on AWS. In his spare time, he loves to play with his two kids and follows Cricket.

Modernize Mainframe Applications for Hybrid Cloud with IBM and AWS

Visit the AWS IBM Services Partnership page Engage through the IBM Z and the Cloud Modernization Center

IBM – AWS Partner Spotlight IBM is an AWS Premier Tier Services Partner and MSP that offers consulting services and software solutions. IBM has assembled an ecosystem of highly experienced professionals dedicated to deploying customer solutions on AWS. Contact IBM | Partner Overview *Already worked with IBM? Rate the Partner *To review an AWS Partner, you must be a customer that has worked with them directly on a project.

Introduction: Signed certificates are digital passports, authenticating system identities, and ensuring secure data transmission. As organizations expand their SAP landscapes across cloud environments and integrate with third-party applications, properly implemented certificates become essential in preventing security breaches and unauthorized access, while maintaining smooth operation of business-critical systems. For many SAP administrators and security professionals, managing certificates might seem like yet another complex task in an already demanding environment. This is where AWS Certificate Manager (ACM) comes into play. AWS Certificate Manager is a service that is used to provision, manage, and deploy public and private SSL/TLS certificates. You can then use these certificates to terminate traffic on any compute workload that requires a certificate, such as those running on EC2 instances, like SAP workloads. With the announcement of publicly exportable certificates via ACM, customers can now leverage ACM to create a Personal Security Environment (PSE) for their SAP systems. For details regarding the launch of publicly exportable certificates via ACM, check out the blog post AWS Certificate Manager introduces exportable public SSL/TLS certificates to use anywhere from my colleague Channy Yun. Prerequisites: To use the certificates from ACM in SAP workloads, please see the prerequisites below:

A method to validate domain ownership for the domain in which you are requesting the certificate An exported public certificate requested and validated in ACM with the proper tags where applicable OpenSSL installed in your operating system or access to AWS Cloudshell

Note: this blog post does not cover how to request the certificate, as this has already been covered in the launch blog mentioned earlier Generating the .p12 file: Once you have exported the certificate from ACM, you will have three files:

With those 3 files saved in a folder, open your terminal of choice and execute the below OpenSSL commands from said folder First, export the private key: # openssl pkcs8 -topk8 -in private_key.txt -out private_key.pem -nocrypt Note: Ensure you have the pass phrase for the private_key.txt file generated when exporting the certificate from ACM. Then, confirm the certificate matches the exported key: # openssl x509 -noout -modulus -in certificate.txt | openssl md5 # openssl rsa -noout -modulus -in private_key.pem | openssl md5 Now let's build out the PKCS#12 file: # openssl pkcs12 -export \ # -inkey private_key.pem \ # -in certificate.txt \ # -certfile certificate_chain.txt \ # -name "CN= " \ # -out sap_ssl.p12 Note: Take note of the export password as it will be required when importing the .p12 file into SAP. Converting the .p12 into an SAP PSE: If you are working with an SAP ABAP based system, you can perform this task by logging into the system and entering transaction STRUST. From there, set to change mode and select PSE -> Import:

Provide the .p12 file and the export password you defined. Note: If you are missing a certificate from the chain (i.e. Starfield Services Root Certificate Authority" you can download the certificate from the Amazon trust repository. You can now save the PSE according to your requirements (for example, as the SAPSSLS.pse). This will replace your current PSE. If a PSE already exists in the system, ensure you have backed it up (if required).

If you are building the PSE in a non-ABAP environment (such as SAP HANA or an SAP Web Dispatcher), you can use the below steps to generate the SAP PSE In the same folder where you generated the PKCS#12 file, execute the below command: # openssl pkcs12 -export -out certificate.pfx -inkey private_key.txt -in certificate.txt -certfile certificate_chain.txt Note: You will be asked for the pass phrase for the private_key.txt file and the export password for the PKCS#12 file Now copy the certificate.pfx along with the root and certificate chain files into your SAP server and execute the following command: # sapgenpse import_p12 -r certificate_chain.txt -r root.crt -p certificate.pfx Note: You will be asked for the PKCS#12 encryption password and to set a passphrase for the SAP PSE Since the PSE is protected with credentials, you will need to generate the cred_v2 file by executing the below command with the adm user: # sapgenpse seclogin -p [ -O ] [ -x ] For more details on PSE credentials, refer to SAP Note # 1525059 – Troubleshooting problems with PSE credentials (SAP S-User required) Pricing: For details and up-to-date information on pricing for public certificates in ACM, refer to AWS Certificate Manager pricing Conclusion: AWS Certificate Manager (ACM) provides a solution for managing certificates in SAP environments. With the ability to export public certificates, organizations can now easily create Personal Security Environments (PSE) for both ABAP and non-ABAP based SAP systems. This integration simplifies certificate management, enhances security, and maintains compliance requirements for SAP landscapes. By following the steps outlined in this blog post, SAP administrators can implement secure certificate management using ACM, ensuring their SAP systems remain protected while reducing the operational overhead of certificate management. Customers can further leverage tagging to have a centralized view of their spending in certificate management. Want to know more about running your SAP landscapes on AWS? Visit our SAP on AWS page.

Migrate centralized crypto exchange workloads to AWS – Part 2

This is the English version of the original post published in the AWS Indonesia blog channel. Part 1 of our series introduced key components for running centralized cryptocurrency exchanges (CEX) on AWS. Part 2 (this post) builds upon that knowledge and guides you through migrating CEX workloads to AWS successfully. In the upcoming Part 3 we will explore how recommendations from Part 1 and Part 2 were used in real migration scenario. Our guidance comes from real-world migration projects of Indonesian cryptocurrency exchanges. We focus on practical lessons learned from actual migrations that were built upon more generic prescriptive guidance for large migrations. These insights can help both crypto exchanges and high-performance trading platforms. In this post, we discuss how to plan your AWS migration strategically, execute it securely, and validate its success through testing. We share best practices for Multi-AZ deployments, auto scaling configurations, and database management. We also show you how to perform a smooth cutover and optimize your post-migration environment. Design and plan your move to AWS Moving your crypto exchange to AWS starts with good planning. Before you begin the move, you must understand your current system and decide how to set it up in AWS. This section shows you the key steps to plan your move, from mapping your system design to setting up AWS accounts and checking service limits. Document your workload architecture design before migrating it to AWS Migration to AWS starts with knowing your current crypto exchange setup. Before moving, you must plan how your system will handle high availability, performance scaling, network speed, and multi-cloud operations. By mapping out these key areas now, you can build a strong AWS system that grows with your needs. For guidance, refer to the reference architecture in Part 1 of this series. In the following sections, we discuss the key design decisions to make on this stage. Design for high availability Your crypto exchange must maintain continuous operation. This means deploying your services across multiple Availability Zones. Set up your databases with automatic failover capabilities to prevent downtime. Distribute your application servers across different zones to support service continuity. Remember that Amazon Simple Storage Service (Amazon S3) automatically stores data across multiple zones for built-in redundancy. For more information, refer to the following resources:

Multi-AZ DB cluster deployments for Amazon RDS Tutorial: Set up a scaled and load-balanced application Amazon ECS capacity and availability Deploying Applications with High Availability on Kubernetes Amazon S3 backups

Plan your scaling for performance Crypto exchanges serve massive number of users and process large transaction volumes. Your scaling strategy should include Amazon EC2 Auto Scaling or Kubernetes Horizontal Pod Autoscaler with Amazon Elastic Kubernetes Service (Amazon EKS) for applications to handle varying loads. Consider using a combination of Amazon Elastic Compute Cloud (Amazon EC2) On-Demand and Spot Instances to manage costs effectively. For databases, solutions like Amazon Aurora Serverless can help handle unpredictable transaction patterns. For more information, refer to the following resources:

Auto Scaling launch templates Creating resources for Amazon ECS cluster auto scaling using the AWS Management Console Scale pod deployments with Horizontal Pod Autoscaler Getting Started with Karpenter

Minimize latency impact in multi-cloud migrations For crypto exchange workloads, latency is a critical performance factor that can be compromised in multi-cloud environments. When applications run on AWS while databases remain with other cloud providers, the increased latency can significantly impact performance. To minimize this impact in microservices architectures, prioritize migrating related services and their databases as complete units to AWS, limiting cross-cloud communication to only non-essential service interactions. Optimize network performance with cluster placement groups For processing hot paths in trading systems where every microsecond matters, network latency between instances can significantly impact performance. Amazon EC2 cluster placement groups help optimize these critical paths by placing EC2 instances close to each other in the same Availability Zone, reducing the physical network distance between components. By implementing cluster placement groups strategically along processing hot paths, trading systems can achieve more consistent and lower latency network performance, which is crucial for competitive market making and exchange operations. For more information, refer to Optimize tick-to-trade latency for digital assets exchanges and trading platforms on AWS. Establish foundation for your AWS accounts Before beginning your migration, it's important to establish a well-structured AWS organizational framework. AWS Organizations and AWS Control Tower serve as the foundation for managing multiple AWS accounts effectively. Your organizational structure should reflect your business needs, with separate accounts for different purposes such as business units, development environments, or specific workloads. Configure a dedicated management account and use a root user only for tasks that truly require it. AWS Control Tower plays an important role in standardizing account creation and management, making sure accounts adhere to your regulatory requirements and security standards. It's important to set up your management accounts several days before migration begins, because they require manual configuration, including payment methods to prevent operational disruptions. Security and compliance form the basis of your AWS organization. Service Control Policies (SCPs) help you maintain control by setting maximum permissions across your organization, so you can restrict actions like resource deletion or limit EC2 instance types. Backup policies provide consistent data backup across all accounts, while maintaining compliance with retention requirements and disaster recovery standards. Threat detection and security monitoring are essential components of your AWS environment. Amazon GuardDuty provides real-time threat detection using machine learning to identify potential cyber attacks and malware. AWS Security Hub offers a centralized dashboard for security monitoring, collecting security checks, and providing recommendations based on best practices. These services should be enabled organization-wide and monitored from designated administrator accounts. Activity tracking and auditing capabilities are fundamental for maintaining security and compliance. AWS CloudTrail records API activity in your AWS environment, providing audit trails for security investigations and operational troubleshooting. By centralizing these logs through a designated administration account, you maintain complete visibility across your organization. The organizational structure of your accounts on AWS creates a secure, manageable foundation for your crypto exchange operations on AWS, providing proper governance while maintaining operational efficiency. Increase service quotas AWS services come with default quotas that require careful review before you start your migration journey. These quotas represent the maximum limits for various service resources and API operations in your AWS account. You can quickly check both default and current quotas through the Service Quotas dashboard on the AWS Management Console. Quota management requires advance planning and proactive action. Request quota increases well before your migration project starts, because approval processes can take from minutes to days, with larger quota requests taking longer. Waiting until the last minute to request quota increases can cause significant project delays—even a single day's delay in quota approval can impact your entire migration timeline. This is particularly important for crypto exchanges, where service capacity directly affects trading operations.By understanding and managing your service quotas early in the planning phase, you can achieve smooth resource provisioning during migration and avoid unexpected constraints that could affect your project timeline or operational capabilities. Plan migration implementation details Good planning helps make your AWS migration successful. A clear plan helps your team understand what work needs to be done, when to do it, and how long it will take. This makes it straightforward to track progress and make sure nothing important is missed.The following are the main technical decisions to make:

Choose how to move your servers—either set them up fresh or use AWS Application Migration Service Pick a way to move your database—either export/import or use AWS Database Migration Service (AWS DMS) If changing database type, check if the AWS Schema Conversion Tool (AWS SCT) can help Choose how to move files—either download/upload or use AWS DataSync If you are using Kubernetes, check what changes your Kubernetes template files need Update your continuous integration and continuous delivery (CI/CD) pipeline system to deploy on the right AWS accounts according to the organizational structure you designed earlier Look for code that needs updates to work with AWS tools Convert infrastructure code from previous cloud providers to options compatible with AWS

Some of these activities can be done in parallel, whereas others must be done one after another. Using project management tools that your team knows well can help keep everyone informed about the progress. A clear and detailed plan can help your team work better together during the migration. Test your migration plan with proof of concept Before starting your full migration, it's helpful to run small tests to check if your plan will work. The following are the main areas to test:

Virtual machine migration – Test if AWS Application Migration Service can move your servers to AWS correctly. Even though AWS supports many operating systems, you might face issues with SSH access or server identity changes. Finding these problems early helps you fix them before the real migration starts. If your Linux-based EC2 instance becomes inaccessible, use the EC2Rescue tool to remediate common problems. Database migration – Try moving a small part of your database with AWS DMS. This helps you check if you need to make any changes in your source database before the migration. It also validates that data moves correctly between different database versions. During the proof of concept, the pre-migration assessments in AWS DMS tasks can help identify problems that might prevent a migration task from running as expected. If during migration you are changing database engines, test schema conversion with the AWS SCT. Transaction hot path testing – Check if your transaction processing path keeps latency low after moving to AWS. Test how orders flow from inbound APIs, to matching and performing trade, to delivering updated market data to the end-users. Check if transactions and market data move quickly enough. Testing early helps find slow spots before you move everything. Multi-cloud performance – Check if your applications work well when some parts are on AWS while others are still on your old infrastructure. This is important because network delays between different clouds can affect how fast your crypto exchange works. Code changes – Test how your application code works with AWS services. For example, check if your storage management code works with Amazon S3. Also test if your CI/CD pipeline and infrastructure code work properly with AWS. Consider using Amazon Q Developer to help migrate, test, and review your code. AWS Support Plan – Remember to upgrade your AWS Support Plan to at least Business level. This gives you quick access to AWS support teams if you need help solving problems during testing.

This early testing helps you find and fix problems before they affect your real migration. It also helps your team learn how AWS services work in practice. Review third-party licenses for AWS migration When moving to AWS, you must check your third-party software licenses carefully. Many software vendors tie their licenses to specific cloud providers, which means your current licenses might not work after moving to AWS. To avoid problems during migration, contact your software vendors early to understand their AWS licensing requirements and make any needed license changes or purchases before starting the migration process. Migration best practices when moving to AWS Moving a crypto exchange to AWS requires the following proven practices to achieve a successful transition. These best practices, drawn from real implementations with Indonesian crypto exchanges, help you protect sensitive data, maintain performance, and minimize disruption during migration.

Daily progress monitoring – Have short daily meetings with your team to check progress and solve any problems quickly. This helps keep your migration on track and lets you fix issues before they become bigger problems. Secure data transfer – Crypto exchanges handle sensitive data that needs protection. Use AWS Site-to-Site VPN when moving your data to AWS. This creates a secure connection whether you're using the internet or AWS Direct Connect. AWS migration tools like AWS Application Migration Service, AWS DMS, and Data Sync work well with VPNs. AWS migration tools – Make your migration faster by using AWS tools designed for this purpose. Move your virtual machines with AWS Application Migration Service, transfer databases with AWS DMS, and sync your files with Data Sync. For container workloads on Kubernetes, you can use your existing CI/CD pipeline to deploy to Amazon EKS. Database setup – AWS DMS doesn't automatically move everything. You must manually sync database items like functions, stored procedures, and indexes. This step is important for both application features and performance. Getting help – If you face technical problems you can't solve, contact AWS Support. AWS Support can help with migration issues, especially when dealing with unusual problems not covered in AWS documentation.

Following these best practices helps reduce risks and facilitates a smooth transition of your crypto exchange to AWS. By focusing on security, using the right tools, and maintaining proper oversight, you can achieve a successful migration while keeping your trading platform reliable and performant. Testing after your move to AWS After you complete the migration process, good testing helps check that your crypto exchange works well on AWS. Testing shows if your system works correctly and runs as fast as before. Your testing plan should include unit testing, functional testing, end-to-end integration testing, load testing, and performance testing. After you validate the system works correctly, run failover tests for critical components of your system, such as switching to hot standby applications and database point-in-time recovery procedures. Setting up test environments with the right data is important for good testing. AWS Application Migration Service helps by offering test instance features before the final cutover. For database testing, Amazon Aurora or Amazon Relational Database Service (Amazon RDS) blue/green deployment lets you create separate test instances while keeping production data intact and up-to-date. Container workloads on Amazon EKS can be tested using your current CI/CD pipeline in separate environments. For storage testing, you can create new S3 buckets and copy files from production. Using tools your team knows well makes testing less complicated and more consistent.Writing down test information helps track your progress. Keep records of what you test and what you find. Fix problems you find before moving to the next step. Testing with real-life data amounts and users shows if your system can handle actual work demands.This complete testing approach helps make sure your move to AWS goes well. Your crypto exchange needs to work reliably and quickly within the defined latency bounds. Testing helps make sure everything works right before going live. Final cutover after your move to AWS After testing is done and you pick a date to go live, it's time to switch your crypto exchange to AWS. This cutover requires careful planning to keep your service running smoothly. These are the key steps to follow:

Make sure your old and new databases match perfectly. Use Amazon CloudWatch to check AWS DMS metrics and verify the latest data is in place. Compare files between old and new storage systems. Use CloudWatch to check DataSync metrics and count files to make sure they match. For virtual machines using AWS Application Migration Service, monitor respective CloudWatch metrics to make sure all data is copied before switching. Point your CI/CD pipeline to AWS instead of your old provider. Update DNS records and system settings to point to AWS. This includes load balancer addresses and database connection details. Have a backup plan, and keep your old system ready in case you need to switch back quickly. Set up your applications and servers on AWS before the switch. This leaves fewer tasks for the actual cutover day.

The cutover to AWS works best when you prepare well. Check each step carefully as you go. Having a backup plan helps you react quickly if something unexpected happens. Remember that good planning makes the cutover easier and safer for your crypto exchange.This careful approach helps protect your trading service during the move to AWS. Your users should see little or no downtime during the cutover. Keep your team ready and focused until the cutover is complete. Next steps after your move to AWS Moving to AWS is just the start. Now you need to make sure your crypto exchange keeps running well. Good monitoring and post-migration cleanup help your system stay healthy and cost-effective. The following are the main tasks to handle:

Set up CloudWatch to watch your system and send alerts if problems happen. Remove old systems and accounts from your previous provider that you don't use anymore. Clean up migration tools like AWS DMS, AWS Application Migration Service, and DataSync when you're done with them. Update your system documents with new AWS information about setup and structure. Monitor your AWS costs and look for ways to save money. Start an AWS Support Business Plan if you haven't, to get help quickly if you need it.

Taking care of these tasks helps your crypto exchange run better on AWS. Thorough and continuous monitoring lets you spot and fix problems early. Removing old systems saves money and keeps things simple. Having current documents helps your team work better.Remember to check your system regularly after the move. Watch how it runs and look for ways to make it better. We discussed many AWS tools that help you keep your crypto exchange running smoothly. Your team should know how to use these tools and when to ask AWS for help.Keeping your system healthy on AWS takes ongoing work. But with good monitoring and regular updates, your crypto exchange can run well and grow over time. Stay focused on both daily operations and long-term improvements to get the most from AWS. Conclusion Moving a crypto exchange to AWS takes careful planning and work. In this post, we discussed how to do it safely and successfully. We shared what we learned from helping real exchanges move to AWS.Good planning makes a big difference in your move to AWS. Start by designing your system well and setting up AWS accounts the right way. Check AWS limits early and test your plans before making big changes. Use AWS tools to help move your data safely. Take time to test all parts of your system before going live.AWS gives crypto exchanges a strong base to grow and try new things. The tools and features AWS help you run a safe and fast trading system. As crypto trading keeps changing, AWS helps you change with it. In upcoming Part 3 of this series, we will provide real examples from Indonesian crypto exchanges. We show how they handled challenges and made AWS work for them. To learn more about running your crypto exchange on AWS, talk to your AWS team.

Agung Sidharta Agung is a Startups Solutions Architect who loves to work with customers solving their problems. In his spare time, he enjoys traveling, reading IT-related materials, and walking in the surrounding environment with his family and little dog.

Angga Pradikta Angga is a Solutions Architect at AWS. Angga helps customers navigate their cloud journey by designing and implementing secure, scalable, and cost-effective solutions.

Nikolay Vlasov Nikolay is the global lead for Distributed Ledger Technology infrastructure at AWS Worldwide Specialist Solutions Architect organization. He helps customers run workloads for decentralized web and ledger technologies on AWS.

David-Paul Dornseifer David is a Blockchain Development Architect at AWS. He focuses on helping customers design, develop and scale end-to-end blockchain solutions. He primarily focusses on confidential compute and digital asset custody related solutions as well as latency optimization for critical CEX workloads.

Migrate centralized crypto exchange workloads to AWS – Part 1

This is the English version of the original post published in the AWS Indonesia blog. Cryptocurrency exchanges are digital marketplaces where users trade various digital currencies like Bitcoin and Ethereum. The exchanges provide a secure environment where millions of people worldwide can participate in the cryptocurrency economy.The cryptocurrency trading landscape features two distinct types of exchanges: centralized (CEXs) and decentralized (DEXs). Centralized exchanges operate like traditional financial institutions, with companies managing user transactions and maintaining control over digital assets. In contrast, decentralized exchanges function through blockchain networks, without intermediaries and giving traders direct control over their funds. Indonesia has emerged as a significant player in the global cryptocurrency market, ranking third in worldwide crypto adoption in 2024. The country hosts 30 officially registered centralized cryptocurrency exchanges, demonstrating the robust growth of digital asset trading in the region. In 2024, AWS has gained recognition as one of two cloud providers recommended by Indonesia's financial authority, PT Central Finansial X (CFX), for cryptocurrency exchange operations. This three-part series explores the world of centralized cryptocurrency exchanges and their implementation on AWS Cloud infrastructure. We share practical insights and technical guidance for building reliable, secure, and scalable cryptocurrency trading platforms. Part 1 (this post) examines the business model of centralized exchanges and presents reference architectures based on AWS services. Part 2 focuses on migration strategies and lessons learned, and upcoming Part 3 showcases real-world implementations from Indonesian cryptocurrency exchanges. The series aims to help both new and established exchanges understand the benefits of cloud infrastructure in the digital asset trading ecosystem. Crypto exchange business blueprint Let's first review key components of the CEX business blueprint, as illustrated in the following diagram.

1. Trade engine At the center of cryptocurrency exchanges lies the trade engine. These engines work as stateless microservices that process and match trading orders. They communicate through specialized messaging systems for order management and trade matching. When users place orders, the trading engine processes each request and matches buyers with sellers at the best available prices. The system stores all trading information in specialized services called trade feeds and order books. Crypto exchanges also use blockchain nodes and wallets to settle and store users' crypto assets, acting as custodians. They can build their own blockchain infrastructure or use specialized services. Modern trading engines integrate with blockchain technology to make asset trading faster and more efficient. This integration allows for quick order processing while maintaining secure asset management on the blockchain network. 2. Trade feeds and order books The trade engine maintains the incoming stream of trading intentions in the order books service. These books maintain all open buy orders (bids) and sell orders (asks), along with their respective prices and quantities. It's similar to a marketplace bulletin board where people actively post their intentions to buy or sell at various price points, before any actual transactions occur.Trade feeds, on the other hand, function as historical records of completed transactions, essentially documenting what has already occurred in the market. They record the final execution price of trades, the volume traded, the exact timestamp of execution, and whether it was a buy or sell transaction. You can think of trade feeds like a receipt book or transaction history—they provide a complete record of all finalized deals that have taken place on the exchange. 3. Market data distribution Real-time market data plays a crucial role in keeping trading systems accurate and efficient. External providers supply current price information to trading engines, making sure transactions occur at fair market rates. This continuous flow of data helps maintain market integrity and supports informed trading decisions. 4. Analytics and user experience The trading system collects data that helps detect unusual trading patterns and supports detailed business analysis and regulatory reporting. The collected data also enables exchanges to provide better services to users, including personalized portfolio management and improved trading experiences. 5. Liquidity providers and market makers Professional market makers and liquidity providers play essential roles in maintaining healthy trading environments. These partners help create stable markets by consistently offering trading pairs and managing price fluctuations. The entire system operates under strict security measures, protecting user assets during all trading stages and maintaining careful monitoring of all transactions. 6. Security and monitoring Security remains the top priority in cryptocurrency exchange operations. Exchanges implement comprehensive security measures throughout their systems to protect users' assets through all key stages: onboarding, trading, and off-boarding. Regular monitoring occurs at both technical and business levels, with special attention to transaction safety and trading activity oversight. Implementing the CEX blueprint on AWS The following diagram illustrates how this business blueprint can be designed with AWS services.

1. At trade The trading platform consists of four main modules: UI, API layer with core services, blockchain layer, and wallet layer. The frontend interface uses AWS Amplify for development and operation, with Amazon Route 53 handling domain names and Amazon CloudFront distributing content globally for fast access. Trading users need stable real-time connections, so we propose using WebSocket connections managed by a Network Load Balancer and Amazon EC2 Auto Scaling to maintain service during usage spikes. For core trading services, we suggest using AWS Fargate with either Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS) to run and scale containerized applications. The blockchain nodes will operate on Amazon Elastic Compute Cloud (Amazon EC2) instances using both regular x86 and cost-efficient Amazon Graviton processors. For wallet security, we recommend AWS Key Management Service (AWS KMS) and AWS Nitro Enclaves to protect blockchain keys and sensitive operations. 2. Persistent data storage We propose different storage services for different types of data:

Amazon Aurora or Amazon Relational Database Service (Amazon RDS) for accurate trading records Amazon ElastiCache for storing frequently accessed pricing data Amazon DynamoDB for handling user information with quick access Amazon OpenSearch Service for advanced data searching Amazon Simple Storage Service (Amazon S3) for storing files and documents

3. Market data distribution The platform needs to share market information in real time with many users. We propose using Amazon Kinesis or Amazon Managed Streaming for Apache Kafka (Amazon MSK) to handle continuous data streams. Amazon EventBridge will connect different parts of the system by routing events between services. This design lets services work independently while maintaining communication. EventBridge includes security features like filters, access controls, and data protection to keep events safe and trackable. 4. External provider connections For connecting with outside partners like market makers and payment services, we recommend the following network solutions:

AWS Transit Gateway for connecting multiple networks centrally NAT gateways for safe internet access AWS Site-to-Site VPN for secure connections with local networks AWS Direct Connect for fast private connections to AWS

5. Pre-trade (data analysis and machine learning) For market analysis and trading support, we suggest the following services:

Amazon S3, AWS Glue, and AWS Lake Formation to organize data Amazon Redshift and Amazon Athena to analyze large amounts of data Amazon Quick Sight to create visual reports Amazon SageMaker to build and run machine learning models Amazon Bedrock to manage AI-powered applications AWS Lambda to connect these services automatically

6. Security and system monitoring The platform needs strong security and monitoring systems:

Amazon CloudWatch to track system performance through metrics and logs AWS X-Ray to help find problems in applications AWS Identity and Access Management (IAM) to control who can access different services AWS Shield to protect against denial of service (DoS) attacks AWS CloudTrail to record all system actions Amazon GuardDuty to look for security threats AWS WAF to protect web applications from attacks AWS Security Hub to help manage all security information in one place

Conclusion In Part 1 of our series, we detailed the fundamental components of centralized cryptocurrency exchanges and their implementation using AWS services. We explored how critical elements like trade engines, order books, and market data distribution can be built using AWS Cloud services. The architecture we've presented combines security measures, scalable infrastructure, and real-time data processing capabilities essential for modern crypto trading platforms. Part 2 will focus on practical migration strategies for existing exchanges moving to AWS, including technical challenges encountered during actual implementations in Indonesia's crypto market.

Enhanced search with match highlights and explanations in Amazon SageMaker

Amazon SageMaker now enhances search results in Amazon SageMaker Unified Studio with additional context that improves transparency and interpretability. Users can see which metadata fields matched their query and understand why each result appears, increasing clarity and trust in data discovery. The capability introduces inline highlighting for matched terms and an explanation panel that details where and how each match occurred across metadata fields such as name, description, glossary, and schema. Enhanced search results reduces time spent evaluating irrelevant assets by presenting match evidence directly in search results. Users can quickly validate relevance without analyzing individual assets. In this post, we demonstrate how to use enhanced search in Amazon SageMaker. Search results with context Text matches include keyword match, begins with, synonyms, and semantically related text. Enhanced search displays search result text matches in these locations:

Search result: Text matches in each search result's name, description, and glossary terms are highlighted. About this result panel: A new About this result panel is displayed to the right of the highlighted search result. The panel displays the text matches for the result item's searchable content including name, description, glossary terms, metadata, business names, and table schema. The list of unique text match values is displayed at the top of the panel for quick reference.

Data catalogs contain thousands of datasets, models, and projects. Without transparency, users can't tell why certain results appear or trust the ordering. Users need evidence for search relevance and understandability. Enhanced search with match explanations improves catalog search in four key ways: 1) transparency is increased because users can see why a result appeared and gain trust, 2) efficiency improves since highlights and explanations reduce time spent opening irrelevant assets, 3) governance is supported by showing where and how terms matched, aiding audit and compliance processes, and 4) consistency is reinforced by revealing glossary and semantic relationships, which reduces misunderstanding and improves collaboration across teams. How enhanced search works When a user enters a query, the system searches across multiple fields like name, description, glossary terms, metadata, business names and table schema. With enhanced search transparency, each search result includes the list of text matches that were the basis for including the result, including the field that contained the text match, and a portion of the field's text value before and after the text match, to provide context. The UI uses this information to display the returned text with the text match highlighted. For example, a steward searches for "revenue forecasting," and an asset is returned with the name "Sales Forecasting Dataset Q2" and a description that contains "projected sales figures." The word sales is highlighted in the name and description, in both the search result and the text matches panel, because sales is a synonym for revenue. The About this result panel also shows that forecast was matched in the schema field name sales_forecast_q2. Solution overview In this section we demonstrate how to use the enhanced search features. In this example, we will be demonstrating the use in a marketing campaign where we need user preference data. While we have multiple datasets on users, we will demonstrate how enhanced search simplifies the discovery experience. Prerequisites To test this solution you should have an Amazon SageMaker Unified Studio domain set up with a domain owner or domain unit owner privileges. You should also have an existing project to publish assets and catalog assets. For instructions to create these assets, see the Getting started guide. In this example we created a project named Data_publish and loaded data from the Amazon Redshift sample database. To ingest the sample data to SageMaker Catalog and generate business metadata, see Create an Amazon SageMaker Unified Studio data source for Amazon Redshift in the project catalog. Asset discovery with explainable search To find assets with explainable search:

Enter the search text user-data. While we get the search results in this view, we want to get further details on each of these datasets. Press enter to go to full search.

In full search, search results are returned when there are text matches based on keyword search, starts with, synonym, and semantic search. Text matches are highlighted within the searchable content that is shown for each result: in the name, description, and glossary terms.

To further enhance the discovery experience and find the right asset, you can look at the About this result panel on the right and see the other text matches, for example, in the summary, table name, data source database name, or column business name, to better understand why the result was included.

After examining the search results and text match explanations, we identified the asset named Media Audience Preferences and Engagement as the right asset for the campaign and selected it for analysis.

Conclusion Enhanced search transparency in Amazon SageMaker Unified Studio transforms data discovery by providing clear visibility into why assets appear in search results. The inline highlighting and detailed match explanations help users quickly identify relevant datasets while building trust in the data catalog. By showing exactly which metadata fields matched their queries, users spend less time evaluating irrelevant assets and more time analyzing the right data for their projects. Enhanced search is now available in AWS Regions where Amazon SageMaker is supported. To learn more about Amazon SageMaker, see the Amazon SageMaker documentation.

Ramesh H Singh Ramesh is a Senior Product Manager Technical (External Services) at AWS in Seattle, Washington, currently with the Amazon DataZone team. He is passionate about building high-performance ML/AI and analytics products that enable enterprise customers to achieve their critical goals using cutting-edge technology.

Pradeep Misra Pradeep is a Principal Analytics and Applied AI Solutions Architect at AWS. He is passionate about solving customer challenges using data, analytics, and AI/ML. Outside of work, Pradeep likes exploring new places, trying new cuisines, and playing board games with his family. He also likes doing science experiments, building LEGOs and watching anime with his daughters.

Ron Kyker Ron is a Principal Engineer with Amazon DataZone at AWS, where he helps drive innovation, solve complex problems, and set the bar for engineering excellence for his team. Outside of work, he enjoys board gaming with friends and family, movies, and wine tasting.

Rajat Mathur Rajat is a Software Development Manager at AWS, leading the Amazon DataZone and SageMaker Unified Studio engineering teams. His team designs, builds, and operates services which make it faster and straightforward for customers to catalog, discover, share, and govern data. With deep expertise in building distributed data systems at scale, Rajat plays a key role in advancing the data analytics and AI/ML capabilities of AWS.

Kyle Wong Kyle is a Software Engineer at AWS based in San Francisco, where he works on the Amazon DataZone and SageMaker Unified Studio team. His work has been primarily at the intersection of data, analytics, and artificial intelligence, and he is passionate about developing AI-powered solutions that address real-world customer challenges.

How Zepto scales to millions of orders per day using Amazon DynamoDB

Named after a microscopic unit of time representing 10⁻²¹, Zepto is an Indian quick-commerce venture launched in 2021. The company specializes in lightning-fast delivery services, bringing groceries and essential items to urban consumers in just minutes through its technology platform and optimized logistics network. Operating in multiple cities across India, Zepto has a comprehensive catalog, including fresh produce, electronics, beauty products, clothing, and toys. Since its launch in 2021, Zepto has undergone significant growth, evolving from initial operations with a single micro-warehouse to now operating more than 1,000 stores and processing millions of orders daily. Amazon Web Services (AWS) has been a partner in Zepto's growth journey, providing reliable and scalable services and infrastructure that has helped it grow. To learn more, watch the video Zepto and AWS. In this post, we describe how Zepto transformed its data infrastructure from a centralized relational database to a distributed system for select use cases. We discuss the challenges encountered with Zepto's original architecture to support the business scale, the shift towards using key-value storage for cases where eventual consistency was acceptable, and Zepto's adoption of Amazon DynamoDB—a fully managed, serverless, key-value NoSQL database that delivers single digit milli second latency at any scale. We also discuss how Zepto overcame various challenges, including roll out to production, and the lessons learned. Previous architecture at Zepto Born in the cloud, Zepto initially had architected with a simple monolithic application serving its mobile and web applications requests called Zepto Storefront that had its transactional data stored and served from Amazon Aurora PostgreSQL-Compatible Edition, a fully managed, PostgreSQL-compatible, and ACID-compliant relational database engine that combines the speed, reliability, and manageability of Amazon Aurora with the simplicity and cost-effectiveness of open source databases. It had multiple modules, like Order Management, Inventory Management, Payments Service, and more, packaged together as a single web application and deployed on a set of Amazon Elastic Compute Cloud (Amazon EC2) instances, an AWS service that provides sizable compute capacity in the cloud. We found the monolithic architecture wasn't supporting Zepto's growing scale and business demands. To address this, Zepto started decomposing the Zepto Storefront application with a set of simplified, scalable, and robust microservices that have their own purpose-built data stores, deployed on Amazon Elastic Kubernetes Service (Amazon EKS). Amazon EKS is a fully managed Kubernetes service that you can use to run Kubernetes in both the AWS Cloud and on-premises data centers. With the journey of breaking up the monolithic application halfway through, Zepto application requests were served from both the Zepto Storefront (the monolith) application and the microservices, as shown in the following architectural diagram.

The following are some of the key data stores backing Zepto's microservices and the Zepto Storefront application:

Aurora PostgreSQL-Compatible – The microservices—User Management, Payment Services, and more—and the Zepto Storefront application where the Order Management Service (OMS) and other modules exist have their transactional data stored in their own databases set up in Aurora PostgreSQL clusters. Amazon OpenSearch Service – Amazon OpenSearch Service is an AWS managed service that you can use to run and scale OpenSearch clusters without having to worry about managing, monitoring, and maintaining your infrastructure, is used by the Search Service that serves Zepto's products search and discovery use cases. MongoDB – The microservices – Catalog, Product Service and more uses MongoDB as the primary data store for storing product, and category related information.

Zepto's data lake is set up on Amazon Simple Storage Service (Amazon S3), an object storage service that offers industry-leading scalability, data availability, security, and performance. It is kept synchronized with Zepto's transactional data stores—Aurora PostgreSQL Compatible and MongoDB—with the help of streaming change data capture (CDC) through a Kafka cluster set up with Amazon Managed Streaming for Apache Kafka (Amazon MSK). Amazon MSK is a streaming data service that manages Apache Kafka infrastructure and operations, making it simple for developers and DevOps managers to run Kafka applications and Kafka Connect connectors on AWS. Multiple MSK clusters support Zepto's requirements, such as the events cluster that facilitates communication between the applications (Zepto Storefront and the microservices), the search cluster that receive batch inference data after data analytics and processing, the CDC cluster, and more. Challenges In this section, we discuss the challenges Zepto faced with its Storefront monolithic application, specifically around OMS, one of the core modules, which managed the end-to-end lifecycle of an order. Performance challenges When Zepto received orders, each order was created as a new entry in the Aurora Customer_Orders table with an initial status of ORDER_RECEIVED. Due to the high volume of daily order processing over multiple years, the table started accumulating billions of records. This resulted in the following challenges:

Expensive reads – Although most orders process successfully, some transactions can be delayed or disrupted by various factors, such as payment failures and gateway delays. To manage these cases, Zepto's automated monitoring jobs regularly scan the Customer_Orders table to detect problematic transactions and trigger appropriate responses, including listening to payment service events, triggering orders cancellation for payment failures and more. However, as the table size grew substantially, the frequent execution of these monitoring queries began to experience performance degradation. Expensive writes – When orders move through different stages from creation to fulfillment, their status needs to be updated in the Customer_Orders table, for example ORDER_RECEIVED, PAYMENT_SUCCESSFUL and, ORDER_CONFIRMED. These status updates, triggered by events from MSK topics, required locking the corresponding order records in the table for updating. As a result, multiple concurrent updates often compete for these locks, which led to increased write latency. Autovacuum during peak hours – Although autovacuum helps improve the performance by invoking VACUUM and ANALYZE commands for removing obsolete data or tuples from the tables and updating query statistics, several parameters can affect when the process gets triggered. Zepto witnessed autovacuum triggered on the Customer_Orders table during peak hours, which competed with regular database operations for system resources and eventually impacted read and write operations on the table. Zepto also witnessed autovacuum not keeping up with very frequent data modifications that caused the query planner to use outdated statistics, resulting in inefficient query plans and slower query runtimes.

Operational challenges To support Zepto's micro-warehouses operating around the clock, the team faced several operational challenges:

Autovacuum configuration complexity – The team frequently invested time optimizing various autovacuum parameters for critical large tables, like the Customer_Orders table. However, this optimization process grew more complex over time, because team had to constantly adjust settings to handle tables with high data modification rates and evolving usage patterns driven by frequent feature additions to meet business needs. Monitoring challenges – Beyond autovacuum configuration, the team needed to continuously monitor and adjust numerous database parameters to optimize the Customer_Orders table's performance. This extensive parameter tuning and maintenance work consumed significant time that could have been better spent on core business activities. Off-peak hours activities – Scheduling resource-intensive operations on the Customer_Orders table, such as creating and dropping indexes, had to be performed during off-peak hours.

Data re-architecture Zepto began exploring methods to restructure its OMS to resolve the challenges related to performance, scalability, and operations through the microservices architecture. Because the OMS can't be functionally broken down into microservices all in one stage due to other legacy system dependencies, we came up with a hybrid approach: maintaining strongly relational use cases in Aurora PostgreSQL-Compatible while moving non-relational use cases to key-value stores like DynamoDB, which provides consistent performance regardless of scale. This initiative resulted in the creation of a new microservice called Draft-Order service, which handles orders (both successful and unsuccessful) before passing the confirmed orders for which the payment went through to the OMS. This lightweight Draft-Order service is aimed to accept orders regardless of their payment status and provide logical separation between payment-related order status updates from the order fulfillment process. The following diagram represents the order management workflow.

Zepto's teams have extensive experience with Aurora PostgreSQL-Compatible and its broad application versatility within their applications, but we chose DynamoDB for two key reasons:

DynamoDB delivers consistent single-digit millisecond performance at any scale DynamoDB's serverless nature brings in operational excellence like no provisioning servers, no version upgrades, no maintenance windows, no patching, no downtime maintenance and more Cost analysis based on Zepto's throughput requirements indicated DynamoDB to be more cost effective.

With those insights, we started designing the Draft-Order service with a DynamoDB table to manage the orders through the following workflow:

When a new order is received, an entry is added to the DynamoDB table The service monitors order-related events from payment services from the events cluster and updates the order status accordingly, such as PAYMENT_SUCCESSFUL or PAYMENT_UNSUCCESSFUL When the order reaches the successful payment completion status, the service

calls an idempotent OMS Create Order API that creates an entry in the Aurora PostgreSQL Customer_Orders table for fulfillment and other processes marks the order status as ORDER_CONFIRMED publishes the status to the events cluster

This way, payment-related update and read processes that previously happened on the Customer_Orders table through OMS are offloaded to the DynamoDB table through Draft-Order service. Query access patterns DynamoDB differs from relational databases because it requires upfront knowledge of query patterns to provide single-digit query performance at scale and efficient use of read/write capacity units; see First steps for modeling relational data in DynamoDB for more details. Therefore, our first step was to identify specific query access patterns:

Get order – Get the order using ORDER_ID, a unique identifier of an order used throughout as the referential key in other Aurora PostgreSQL tables Update order – Update the order item attributes like order status using ORDER_ID Get orders where payment was unsuccessful – Get orders that have remained in a specific status longer than the given time. This is particularly useful for identifying orders with extended payment processing issues or incomplete transactions beyond a timeframe, allowing for appropriate follow-up actions

DynamoDB schema modeling After we had identified the access patterns, we moved forward with the schema design in collaboration with the AWS team. The Draft_Orders table is the base table that stores the required details of the orders placed by Zepto's customers, with ORDER_PK (unique identifier of an order) as the partition key, and other attributes:

ORDER_PK – The partition key that can have one of two types of order identifiers:

ORDER_ID: A unique identifier used for internal service-to-service communication ORDER_CODE: A human-readable unique identifier

The prefix "ORDER_ID" or "ORDER_CODE" is added to the value for identification.

STATE – Order status like ORDER_RECEIVED, PAYMENT_SUCCESSFUL, ORDER_CONFIRMED and more. EXPIRE_AT – Time to live (TTL) DATA – Snapshot of the attributes of the order that will be stored in the Aurora PostgreSQL Customer_Orders table upon successful payment completion POS_PK– Pending Order Scan Partition Key, an attribute that helps in keeping orders in the global secondary index (GSI) uniformly across partitions. See Using Global Secondary Indexes in DynamoDB for more details. POS_SK – A time ordered key CREATED_ON and UPDATED_ON – Creation and update timestamp

Most of the values of keys and the attributes size are limited in size, except for the values of the DATA attribute, which vary in the range of tens of KBs, which required careful consideration while designing the schema; see Best practices for storing large items and attributes in DynamoDB for additional details. Because of this, we choose Snappy, a fast and efficient data compression and decompression technique for the DATA attribute values. See Example: Handling binary type attributes using the AWS SDK for Java document API for details on how to implement compression logic for binary type attributes. DynamoDB's Time To Live (TTL) feature helps in automatically removing items within a few days after their specified expiration timestamp. The Draft_Orders table uses the TTL feature where each order is configured with an EXPIRE_AT attribute. So, the order entries get deleted within few days of their expiration, leaving the table with the latest orders. During this window, scheduled jobs periodically check for orders that have remained beyond a typical timeframe, and they trigger appropriate actions to ensure all the orders reach either ORDER_CONFIRMED or ORDER_TERMINATED status. While the first three access patterns are well served with pointed queries on Draft_Orders table, the fourth access pattern requires retrieving orders that remained in ORDER_RECEIVED status longer than the given time. While it can be achieved by querying the base table using attributes that are not part of the primary key, it may not be efficient; hence, we created a GSI with POS_PK as the partition key, and POS_SK as the sort key. In addition, we selected only the CREATED_ON attribute in the GSI that is required for fetching the orders basis their creation time, rather than including all the base table attributes. This targeted approach helps minimize storage and write costs, since a smaller index with fewer projected attributes is more cost-efficient; see GSI Attribute projections for additional details. The following screenshot illustrates the GSI keys and projected attributes' details.

To prevent hot partition issues and for distributing the orders evenly across partitions, we use POS_PK; see Designing partition keys to distribute your workload in DynamoDB for more details. Additionally, this POS_PK helps filter and maintain only those orders that still need payment completion in the GSI. It is achieved by nullifying the POS_PK value in the base table upon both successful and unsuccessful payment by the Draft-Order service. When the POS_PK value is nullified in the base table, the corresponding entry eventually gets removed from the GSI, leaving only the orders requiring payment to be completed in the GSI. To take action on payment incomplete orders, the Draft-Order service runs scheduled jobs for every 5 minutes that query the GSI for payment pending orders. Some of them might get through with orders marked ORDER_CONFIRMED because their payments succeed with alternate payments during the process, whereas some might not, which eventually get marked as ORDER_TERMINATED after the configured number of retries. Current architecture Zepto continued to refactor its Storefront into purpose-built microservices with their own data stores. Bringing in the Draft-Order service backed by DynamoDB became a natural fit with the existing system of microservices, as shown in the following architectural diagram.

By enabling Amazon DynamoDB Streams, which captures a time-ordered sequence of item-level modifications in the Draft_Orders table, Zepto processes the CDC logs using the Apache Flink connector for DynamoDB that writes the stream data into the data lake for further processing. See DynamoDB Streams and Apache Flink for additional details. Rollout to production The new architecture with the DynamoDB table is designed to hold the last few days of orders, and hence data migration and back-filling from the Customer_Orders table have been completely skipped. With this new architecture in place, initially 10% of read and write operations were offloaded from the Aurora PostgreSQL Customer_Orders table to a DynamoDB Draft_Orders table. To maintain the uniqueness in the Draft_Orders table for both ORDER_CODE and ORDER_ID for any order, the team had combined two operations within a single transaction using TransactWriteItems API, as shown in the following code (see Managing complex workflows with DynamoDB transactions for additional details):

@transactional { Create Order(unique_order_code) without other order related details; Condition: if no order exists with the given unique_order_code; Create Order (unique_order_id) with order code and other order related details; Condition: if no order exists with the given unique_order_id; }Scala

The team's initial tests showed that the above statements in the DynamoDB Transaction had an average latency of approximately 18 milliseconds. After investigating, we discovered that including non-essential synchronous operations within a transaction was causing performance issues. To improve performance, we removed these operations from the transaction and instead used two separate PutItem API calls – one for creating the order with ORDER_CODE without any order related details followed by creating the order with the ORDER_ID, ORDER_CODE and other order related details in sequence. This change delivered two main benefits.

Response times became consistently fast, staying under 10 milliseconds per DynamoDB API call We cut DynamoDB's Write Capacity Unit consumption in half since we eliminated the transaction overhead. Note: Transactions in DynamoDB require two read/write operations per item, as explained in Capacity management for transactions.

The key lesson from this experience was to avoid bundling operations in transactions unless absolutely necessary. See Best practices for transactions for additional details. Following these improvements, Zepto implemented the changes gradually: first to 30% of production traffic and then expanding to full production deployment. Benefits of adopting DynamoDB The new architecture offered several improvements over the previous monolithic architecture using Aurora PostgreSQL:

The Create Order API's performance improved by 60% on average, and 40% for p99 Zepto experienced consistent, single-digit millisecond performance for operations on the DynamoDB table, which wasn't the case with the previous architecture

Reduced operational overhead: Zepto engineers were freed from the undifferentiated heavy lifting operational tasks like cluster maintenance, including patching, upgrading, and parameter fine-tuning based on multiple metrics Improved scalability: DynamoDB's serverless nature enables Zepto to scale from quiet late-night deliveries to high-demand festival rushes and sports event peaks – all without managing servers.

Conclusion In this post, we shared the challenges that Zepto experienced with its monolithic data management architecture, and how it adopted DynamoDB with a microservices-based architecture. With this migration, Zepto now handles millions of orders daily while cutting costs, achieving a 60% faster Create Order API and enhanced operational efficiency and scalability. Given these positive outcomes, Zepto is now looking to implement DynamoDB in other areas of their business. It is common to see developer teams trying to find the one-size-fits all database, which doesn't exist. We recommend developers to find a database that matches their specific application needs. AWS database services are specialized for handling specific data models and access patterns, offering optimized performance, scalability, and cost-effectiveness compared to general-purpose databases. To explore AWS' purpose-built database offerings, refer to AWS Cloud Databases.

Rahul Bennapalan Rahul is a Principal Engineer at Zepto with deep expertise in designing scalable application architectures, working with NoSQL databases, and managing cloud infrastructure. He has led the design and scaling of large-scale systems across critical domains like order management, customer support, and more.

Nitin Jain Nitin is a Principal Engineer with 7 years of experience in software development, system design, and architecture in the e-commerce industry. He has been instrumental in scaling engineering systems to handle rapid business growth, focusing on building high-performance, resilient, and scalable platforms that support expansion and innovation.

Kayalvizhi Kandasamy Kayalvizhi is a Principal Solutions Architect (APAC) at AWS. She uses her expertise to transform concepts into reality, with emphasis on AWS Cloud-based solutions, serverless architectures, and microservices. When she's not working, Kayalvizhi enjoys chess as a FIDE-rated player and shares her passion for the game by coaching her daughters the art of playing chess.

Orchestrating big data processing with AWS Step Functions Distributed Map

Developers seek to process and enrich semi-structured big data datasets with durably orchestrated network-based workflows. For example, during quarterly earnings season, finance organizations run thousands of market simulations simultaneously to provide timely insights for scenario planning or risk management—these workloads require coordination between raw datasets and on-premise servers to provide the latest market information. AWS Step Functions is a visual workflow service capable of orchestrating over 14,000 API actions from over 220 AWS services to build distributed applications. Now, Step Functions Distributed Map streamlines big data dataset transformation by processing Amazon Athena data manifest and Parquet files directly. Using its Distributed Map feature, you can process large scale datasets by running concurrent iterations across data entries in parallel. In Distributed mode, the Map state processes the items in the dataset in iterations called child workflow executions. You can specify the number of child workflow executions that can run in parallel. Each child workflow execution has its own, separate execution history from that of the parent workflow. By default, Step Functions runs 10,000 parallel child workflow executions in parallel. Distributed Map can process AWS Athena data manifest and Parquet files directly, eliminating the need for custom pre-processing. You also now have visibility into your Distributed Map usage with new Amazon CloudWatch metrics: Approximate Open Map Runs Count, Open Map Run Limit, and Approximate Map Runs Backlog Size. In this post, you'll learn how to use AWS Step Functions Distributed Map to process Athena data manifest and Parquet files through a step-by-step demonstration.

This post is part of a series of post about AWS Step Functions Distributed Map:

Processing Amazon S3 objects at scale with AWS Step Functions Distributed Map S3 prefix Optimizing nested JSON array processing using AWS Step Functions Distributed Map Orchestrating big data processing with AWS Step Functions Distributed Map

Use case: IoT sensor data processing You'll build a sample application that demonstrates processing IoT sensor data in Parquet format using Step Functions Distributed Map. These Parquet data files and a manifest file containing the list of the data files are exported from Athena. The data temperature, humidity, and lbattery level from different devices. The following table shows sample of sensor data:

Your objective is to use the Athena data manifest file, get the list of Parquet files, and iterate over the data in the files to detect anomalies and also stream the processed data through Amazon Kinesis Data Firehose to an Amazon S3 bucket for further analytics using Athena queries. Following is the criteria to detect anomaly:

Low battery conditions: less than 20% Humidity anomalies: more than 95% or less than 5% Temperature spikes: more than 35°C or less than -10°C

The following diagram represents the AWS Step Functions state machine:

The Distributed Map runs an Athena query which generates Parquet data files and an Athena manifest file (csv). The manifest file contains the list of Parquet data files. Distributed Map processes these Parquet data files in parallel using child workflow executions. You can control the number of child workflow executions that can run in parallel using MaxConcurrency parameter. See Step Functions service quotas to learn more about concurrency limits. Each child workflow execution invokes an AWS Lambda function to process the respective Parquet file. The Lambda function processes individual sensor readings and detects anomalies according to the preceeding logic and returns a processed sensor data summary response. The child workflow sends the summary response record to Amazon Kinesis firehose stream which stores the results in a specified Amazon S3 results bucket.

The following Athena Start QueryExecution state runs an UNLOAD query to generate data files in Parquet format and a manifest file in CSV. The output will be stored in the S3 bucket specified in the UNLOAD query and the manifest file will be stored in the S3 bucket configured for the Athena workgroup.

{ "QueryLanguage": "JSONata", "States": { "Athena StartQueryExecution": { "Type": "Task", "Resource": "arn:aws:states:::athena:startQueryExecution.sync", "Arguments": { "QueryString": "UNLOAD (WRITE_YOUR_SELECT_QUERY_HERE) TO 'S3_URI_FOR_STORING_DATA_OBJECT' WITH (format = 'JSON')", "WorkGroup": "primary" }, "Output": { "ManifestObjectKey": "{% $join([$states.result.QueryExecution.ResultConfiguration.OutputLocation, '-manifest.csv']) %}" }, "Next": "Next State" … }Code

The following ItemReader is configured to use a manifest type of "ATHENA_DATA" with "PARQUET" data input.

{ "QueryLanguage": "JSONata", "States": { ... "Map": { ... "ItemReader": { "Resource": "arn:aws:states:::s3:getObject", "ReaderConfig": { "ManifestType": "ATHENA_DATA", "InputType": "PARQUET" }, "Arguments": { "Bucket":"Bucket": "{% $split($substringAfter($states.input.ManifestObjectKey, 's3://'), '/')[0] %}",, "Key": "{% $substringAfter($substringAfter($states.input.ManifestObjectKey, 's3://'), '/') %}" } }, ... } }Code

Additional supported InputType options are CSV and JSONL. All objects referenced in a single manifest file must have the same InputType format. You specify the Amazon S3 bucket location of Athena manifest CSV file under Arguments. The context object contains information in a JSON structure about your state machine and execution. Your workflows can reference the context object in a JSONata expression with $states.context. Within a Map state, the Context object includes the following data:

"Map": { "Item": { "Index" : Number, "Key" : "String", // Only valid for JSON objects "Value" : "String", "Source": "String" } }Code

For each Map state iteration, Index contains the index number for the array item that is being currently processed, Key is available only when iterating over JSON objects, Value contains the array item being processed, and Source contains one of the following:

For state input, the value will be : STATE_DATA For Amazon S3 LIST_OBJECTS_V2 with Transformation=NONE, the value will show the S3 URI for the bucket. For example: S3://amzn-s3-demo-bucket. For all the other input types, the value will be the Amazon S3 URI. For example: S3://amzn-s3-demo-bucket/object-key.

Using this newly introduced Source field in the context object, you can connect the child executions with the source object. Prerequisites

Access to an AWS account through the AWS Management Console and the AWS Command Line Interface (AWS CLI). The AWS Identity and Access Management (IAM) user that you use must have permissions to make the necessary AWS service calls and manage AWS resources mentioned in this post. While providing permissions to the IAM user, follow the principle of least-privilege. AWS CLI installed and configured. If you are using long-term credentials like access keys, follow manage access keys for IAM users and secure access keys for best practices. Git Installed AWS Serverless Application Model (AWS SAM) installed Python 3.13+ installed

Set up the state machine and sample data Run the following steps to deploy the Step Functions state machine.

Clone the GitHub repository in a new folder and navigate to the project root folder.

Run the following command to install required Python dependencies for the Lambda function.

python3 -m venv .venv source .venv/bin/activate python3 -m pip install -r requirements.txtCode

Stack name: The CloudFormation stack name (for example, sfn-parquet-file-processor) AWS Region: A supported AWS Region (for example, us-east-1) Keep rest of the components to default values. Note the outputs from the AWS SAM deploy. You will use them in the subsequent steps. Run the following command to generate sample data in csv format and upload it to an S3 bucket. Replace with the value from sam deploy ouptut.

Create the Athena database and tables Before you can run queries, you must set up an Athena database and table for your data.

From Amazon Athena console, navigate to workgoups, select the workgroup named "primary". Select Edit from Actions. In the query result configuration section, select the options as follows:

Management of query results – select customer managed Location of query results – enter s3:// . Replace with the value from sam deploy output. Choose Save to save the changes to the workgroup

Select Query editor tab and run the following commands to create database and tables

Create an Athena table in database iotsensordata that references the S3 bucket containing the raw sensor data. In this case it will be . Replace with the value from sam deploy output.

CREATE EXTERNAL TABLE IF NOT EXISTS `iotsensordata`.`iotsensordata` (`deviceid` string, `timestamp` string, `temperature` double, `humidity` double, `batterylevel` double, `latitude` double, `longitude` double ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ('field.delim' = ',') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3:// /daily-data/' TBLPROPERTIES ( 'classification' = 'csv', 'skip.header.line.count' = '1' );SQL

Create an Athena table in database iotsensordata that references the S3 bucket having the analytics results streamed from Kinesis Data Firehose. Replace with value from sam deploy output. And replace with the current year (e.g 2025).

CREATE EXTERNAL TABLE IF NOT EXISTS iotsensordata.iotsensordataanalytics (deviceid string, analysisDate string, readingTimestamp string, readingsCount int, metrics struct , anomalies array , anomalyCount int, healthStatus string, timestamp string ) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES ( 'ignore.malformed.json' = 'FALSE', 'dots.in.keys' = 'FALSE', 'case.insensitive' = 'TRUE' ) STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3:// / /' TBLPROPERTIES ('classification' = 'json', 'typeOfData'='file');SQL

Start your state machine Now that you have data ready and Athena set up for queries, start your state machine to retrieve and process the data.

Run the following command to start execution of the Step Functions. Replace the and with the value from sam deploy output..

aws stepfunctions start-execution \ --state-machine-arn \ --input '{ "IoTDataBucketName": " "}'Code The Step Functions state machine has the Athena StartQueryExecution state which has an UNLOAD query that generates the sensor data files in a parquet format and a manifest file in CSV format. The manifest will have 5 rows referencing the 5 parquet files. The state machine will process these 5 parquet files in one map run. Run the following command to get the details of the execution. Replace the executionArn from the previous command.

After you see the status SUCCEEDED, run the following command from Athena query editor to check the processed output from Kinesis Data Firehose that was streamed to S3 bucket referenced by the Athena table created in step 4 of the preceding section.

If any of the sensor data exceeds the thresholds, the healthstatus attribute will be set to "anomalies_detected". The workflow produced a summary table of metadata which you can now query for reporting.

Review workflow performance Using the following observability metrics, you can review key performance behavior of your data processing workflow. The AWS/States namespace includes the following new metrics for all Step Functions Map Runs.

OpenMapRunLimit: This is the maximum number of open Map Runs allowed in the AWS account. The default value is 1,000 runs and is a hard limit. For more information, see Quotas related to accounts. ApproximateOpenMapRunCount: This metric tracks the approximate number of Map Runs currently in progress within an account. Configuring an alarm on this metric using the Maximum statistic with a threshold of 900 or higher can help you take proactive action before reaching the OpenMapRunLimit of 1,000. This metric enables operational teams to implement preventive measures, such as staggering new executions or optimizing workflow concurrency, to maintain system stability and prevent backlog accumulation. ApproximateMapRunBacklogSize: This metric shows up when the ApproximateOpenMapRunCount has reached 1,000 and there are backlogged Map Runs waiting to be executed. Backlogged Map Runs wait at the MapRunStarted event until the total number of open Map Runs is less than the quota.

The following graph shows an example of these new metrics. Use the maximum statistic to visualize these metrics. ApproximateMapRunBacklogSize metrics appear after accounts start getting throttled on the OpenMapRunLimit limit. The OpenMapRun (orange line) is the account hard limit of 1,000 shown as a static line. The ApproximateOpenMapRunCount (violet line) is the current number of active OpenMap runs. The ApproximateMapRunBacklogSize (green line) indicates the map runs waiting in backlog to be processed. When the ApproximateOpenMapRunCount is lower than 1000 (OpenMapRun limit) there are no map runs in backlog. However, when the count reaches the OpenMapRun limit, the backlog of map runs starts to build up. After the active runs complete, the backlog will start to drain out and new runs will begin execution.

Clean up To avoid costs, remove all resources created for this post once you're done. From the Athena query editor, run the following commands:

DROP TABLE `iotsensordata`.`iotsensordata`; DROP TABLE `iotsensordata`.`iotsensordataanalytics`; DROP DATABASE `iotsensordata`; SQL

Run the following commands from the AWS CLI after replacing the variable to delete the resources you deployed for this post's solution:

aws s3 rm s3:// --recursive aws s3 rm s3:// --recursive sam deleteSQL

Conclusion With this update, Distributed Map now supports additional data inputs, so you can orchestrate large-scale analytics and ETL workflows. You can now process Amazon Athena data manifest and Parquet files directly, eliminating the need for custom pre-processing. You also now have visibility into your Distributed Map usage with the following metrics: Approximate Open Map Runs Count, Open Map Run Limit, and Approximate Map Runs Backlog Size. New input sources for Distributed Map are available in all commercial AWS Regions where AWS Step Functions is available. For a complete list of AWS Regions where Step Functions is available, see the AWS Region Table. The improved observability of your Distributed Map usage with new metrics is available in all AWS Regions. To get started, you can use the Distributed Map mode today in the AWS Step Functions console. To learn more, visit the Step Functions developer guide. For more serverless learning resources, visit Serverless Land.

Optimizing nested JSON array processing using AWS Step Functions Distributed Map

When you're working with large datasets, you've likely encountered the challenge of processing complex JSON structures in your automated workflows. You need to preprocess arrays within nested JSON objects before you can run parallel processing on them. Extracting data used to require custom code and extra processing steps, delaying you from building your core application logic. With AWS Step Functions Distributed Map, you can process large datasets with concurrent iterations of workflow steps across data entries. Using the enhanced ItemsPointer feature of Distributed Maps, you can extract array data directly from JSON objects stored in Amazon S3. Alternatively, for JSON object as state input, you can use Items (JSONata) or ItemsPath (JSONPath). With this enhancement you can point directly to arrays nested within JSON structures, eliminating the need for custom preprocessing of your data. With ItemsPointer, Items, and ItemsPath you can select the nested array data and simplify your workflows. In this post, we explore how to optimize processing array data embedded within complex JSON structures using AWS Step Functions Distributed Map. You'll learn how to use ItemsPointer to reduce the complexity of your state machine definitions, create more flexible workflow designs, and streamline your data processing pipelines—all without writing additional transformation code or AWS Lambda functions.

Use case: e-commerce product data enrichment In this e-commerce use case example, you'll build a sample application that demonstrates processing of product inventory data for an e-commerce application using AWS Step Functions Distributed Map. The application receives a JSON file from an upstream application containing an array of product information. The Step Functions workflow reads the JSON file containing product data from an S3 bucket and iterates over the array to enrich each product data in the array. The following diagram presents the AWS Step Functions state machine.

The state machine reads the product-updates.json file from an input S3 bucket. The file contains a JSON array of products. The Distributed Map state in the state machine, selects the JSON array node using ItemsPointer and iterates over the JSON array. For each of the items within the array, the state machine invokes a Lambda function for data enrichment. The Lambda function adds product stock and price information to the product data. The state machine saves the updated product data in an Amazon DynamoDB table. Finally, the state machine uploads the execution metadata into an output S3 bucket. See limits related to state machine executions and task executions.

MaxConcurrency can be configured to specify the number of child workflow executions in a Distributed Map that can run in parallel. If not specified, then Step Functions doesn't limit concurrency and runs 10,000 parallel child workflow executions. You can read a JSON file from a S3 bucket using ItemReader and its sub-fields. If the JSON file, from the S3 bucket, contains a nested object structure, you can select the specific node with your data set with an ItemsPointer. For example, the following input JSON file:

{ "version": "2024.1", "timestamp": "2025-09-26T10:49:36.646197", "productUpdates": { "items": [ { "productId": "PROD-001", "name": "Wireless Headphones", "price": 79.99, "stock": 150, "category": "Electronics" }, { "productId": "PROD-002", "name": "Smart Watch", "category": "Electronics" }, … ] } }Code

The following JSONata-based workflow configuration extracts a nested list of products from productUpdates/items:

"ItemReader": { "Resource": "arn:aws:states:::s3:getObject", "ReaderConfig": { "InputType": "JSON", "ItemsPointer": "/productUpdates/items" }, "Arguments": { "Bucket": "amzn-s3-demo-bucket", "Key": "updates/product-updates.json" } }Code

For JSONPath-based workflow note that Arguments is replaced with Parameters:

The ItemReader field is not needed when your dataset is JSON data from a previous step. ItemsPointer is only applicable when the input JSON objects read from an S3 bucket. If you are using JSON as state input to a Distributed Map, then you can use the ItemsPath (for JSONPath) or Items (for JSONata) field to specify a location in the input that points to JSON array or object used for iterations. Prerequisite To use Step Functions Distributed Map, verify you have:

Set up and run the workflow Run the following steps to deploy the Step Functions state machine.

Clone the GitHub repository in a new folder and navigate to the project folder.

Stack name: Stack name for CloudFormation (for example, stepfunctions-json-array-processor) AWS Region: A supported AWS Region (for example, us-east-1) Accept all other default values. The outputs from the sam deploy will be used in the subsequent steps. Run the following command to generate product-updates.json file containing a nested JSON array of sample products and upload the product-updates.json file to the input S3 bucket. Replace InputBucketName with the value from sam deploy output.

Run the following command to start execution of the Step Functions workflow. Replace the StateMachineArn with the value from sam deploy output.

aws stepfunctions start-execution \ --state-machine-arn \ --input '{}'Code The state machine reads the input product-updates.json file and invokes a Lambda function to update the database for every product in the array after adding price and stock information. The execution metadata is also uploaded into the results bucket.

Monitor and verify results Run the following steps to monitor and verify the test results.

Run the following command to get the details of the execution. Replace executionArn with your state machine ARN.

aws stepfunctions describe-execution --execution-arn Code Wait until the status shows SUCCEEDED. Run the following commands to validate the processed output from ProductCatalogTableName DynamoDB table. Replace the value ProductCatalogTableName with the value from sam deploy output.

Check that the DynamoDB table contains the enriched product data including price and stock attributes. Example output:

{ "Items": [ { "ProductId": { "S": "PROD-005" }, "lastUpdated": { "S": "2025-10-07T20:33:34.507Z" }, "stock": { "N": "129" }, "price": { "N": "139.25" } }, { "ProductId": { "S": "PROD-003" }, "lastUpdated": { "S": "2025-10-07T20:33:34.576Z" }, "stock": { "N": "471" }, "price": { "N": "40.92" } }, … ], "Count": 5, "ScannedCount": 5, "ConsumedCapacity": null }Code

Clean up To avoid costs, remove all resources you've created while following along with this post. Run the following command after replacing the variable to delete the resources you deployed for this post's solution:

aws s3 rm s3:// --recursive aws s3 rm s3:// --recursive sam deleteCode

Conclusion In this post, you learned how to use Step Functions Distributed Map for extracting array data natively from JSON objects stored in a S3 bucket. By removing custom data extraction code, you can simplify the processing of your large-scale parallel workloads. With ItemsPointer you can extract array data within JSON files stored in a S3 bucket , and with Items(JSONata) or ItemsPath (JSONPath), you can extract arrays from complex JSON state input, adding flexibility to your workflow designs. New input sources for Distributed Map are available in all commercial AWS Regions where AWS Step Functions is available. For a complete list of AWS Regions where Step Functions is available, see the AWS Region Table. To get started, you can use the Distributed Map mode today in the AWS Step Functions console. To learn more, visit the Step Functions developer guide. For more serverless learning resources, visit Serverless Land.

By: Yaniv Ehrlich, Partner Strategy Leader, Generative Innovation and Delivery – AWS By: Cher Simon, Principal Deep Learning Architect, Generative Innovation and Delivery – AWS The AWS Generative AI Innovation Center (GenAIIC) proudly launches the Partner Agent Factory (PAF), a pioneering co-innovation program designed to accelerate the development of enterprise-grade, production-ready AI Agents and Tools with our Partner Innovation Alliance (PIA) partners. This initiative offers a unified go-to-market strategy, enabling customers to access transformative AI agent solutions on AWS Marketplace. PAF provides immersive expert guidance and support, empowering PIA partners to build end-to-end AI solutions that meet dynamic business requirements. Customers benefit from the combined expertise of GenAIIC's proven methodologies and PIA partners' industry knowledge, available through AI Agents and Tools on AWS Marketplace. This strategic collaboration fosters innovation while driving security, governance, and simplified procurement. With PAF, enterprises can rapidly adopt scalable AI Agent solutions, driving tangible business results.

"The Partner Agent Factory is a unique program to advance our mission to democratize enterprise AI adoption. By combining GenAIIC's proven methodologies with our partners' deep industry expertise, we are building AI agents that help customers create transformative business solutions that deliver measurable impact. PAF embodies our commitment to collaborative innovation, ensuring that cutting-edge AI capabilities reach companies across all sectors through trusted partnerships and rigorous quality standards." – Taimur Rashid, Director and Head of Generative Innovation and Delivery, AWS.

Accelerating AI Innovation through Expanded Strategic Partnerships The GenAIIC Partner Agent Factory (PAF) aims to amplify the success and strategic partnerships with our high-performing PIA partners, converting AI potential into tangible business value. Since its launch in 2023, GenAIIC has guided thousands of customers from AI experimentation to full-scale deployment, achieving significant productivity gains and enhanced customer experiences. Building on this momentum, GenAIIC connects customers with a global network of PIA partners to scale support for both proof-of-concept and production implementations. Through PAF, GenAIIC co-innovates enterprise-grade AI Agents and Tools alongside PIA members, combining AWS's proven AI methodologies with partners' deep industry expertise to deliver production-ready solutions that address real-world business challenges faster and more effectively than either organization could achieve alone. Unlock the Full Potential of AI with PAF PAF is designed to deliver unparalleled benefits to AWS customers and partners by fostering a dynamic environment of innovation and collaboration:

Collaborative Excellence: PAF combines the deep technical expertise of GenAIIC with the specialized domain knowledge of our PIA partners. This synergy ensures a holistic approach to solving complex business challenges with Agentic AI, leading to more effective and tailored solutions. Accelerated Innovation Pace: Integrating marketplace-ready agentic solutions and accelerators developed through the GenAIIC PAF reduces time-to-market for Agentic AI innovations and enhances organizational agility in response to dynamic business needs. Scalable Solutions: PAF is committed to supporting the expansion and adaptability of Agentic AI solutions. Our framework ensures these solutions can evolve to meet the growing and changing demands of enterprises, providing long-term value and flexibility.

PAF Pathway: Advancing from GenAIIC Bootcamp to AWS Marketplace Success To advance to the PAF, PIA partners must first complete a hands-on, immersive GenAIIC bootcamp, which provides an in-depth exploration of GenAIIC methodology. This advanced training is followed by structured PAF Academy sessions that refine the solution candidate through business strategy discovery, a science delivery deep dive, and a path-to-production demonstration showcasing tangible customer results. These components are critical for the comprehensive board review assessment, ensuring the solution meets high standards of quality, effectiveness, and industry alignment. Upon successful evaluation, solutions are published to the AWS Marketplace, offering cutting-edge Agentic AI solutions to a wide range of customers.

Figure 1: GenAIIC Partner Agent Factory Introducing the PAF AI Agents and Tools We are excited to announce the first four PAF launch partners, who will spearhead this initiative. These collaborations span various industries and AI applications, highlighting the extensive capabilities and potential of generative AI in transforming enterprise solutions. OneByZero – ETL Crew: Generative AI for ETL Modernization ETL Crew, developed by OneByZero in collaboration with GenAIIC PAF, is an Agentic AI platform that automates legacy ETL modernization for cloud migrations, delivering up to 50% faster transitions with reduced costs. The solution eliminates manual coding by intelligently transforming legacy pipelines into cloud-native architectures while ensuring consistency and compliance. Learn more.

"As part of AWS's GenAIIC Partner Innovation Alliance, we at OneByZero are proud to introduce our ETL Crew solution on the AWS Marketplace – having been rigorously supported, vetted and co-innovated through the GenAIIC Partner Agent Factory. This collaboration ensures that customers gain an enterprise-grade capability, leveraging not just the latest from AWS but also their GenAI assets, and guidance for secure and scalable architecture." – Vibhore Kumar, CTO, OneByZero. "Through our collaboration with OnebyZero (OBZ) and AWS, Krungsri has been able to accelerate our experimentation with Generative AI for code conversion, accelerating Cloud migration by more than 50%, leveraging Kumo (Krungsri's AWS based data platform) to drive efficiency by using data analytics and AI across multiple business domains and workstreams. We are excited about OBZ's ETL Crew solution, built with AWS GenAIIC Partner Agent Factory, to enhance our continued success." – K. Tul Roteseree, Head of Enterprise Data and Analytics Group, Bank of Ayudhya PCL (Krungsri).

CI&T Conversational Banking powered by Agentic Intelligence CI&T Conversational Banking, powered by Agentic Intelligence and co-innovated with GenAIIC PAF, enables customers to perform everyday banking tasks such as checking balances and loan simulations through WhatsApp with human-like conversations. This multilingual Agentic AI solution delivers 24/7 personalized service with enterprise-grade security while reducing operational costs for financial institutions. Learn more.

"We are very proud to be pioneering, together with AWS, a new generation of user experiences for financial services—more human, intuitive, and intelligent. This transformation has been accelerated through our collaboration with the AWS Generative AI Innovation Center's Partner Innovation Alliance and its Partner Agent Factory program, which have enabled us to design and scale agentic solutions that deliver tangible business outcomes for our clients." – Bruno Guicardi, President & Co-founder, CI&T. "At banQi, we believe that technology brings people closer together. That's why, together with CI&T and AWS GenAIIC Partner Agent Factory, we're building conversational AI solutions that make banking simpler, more human, and more accessible. Through this collaboration, we're delivering practical, intelligent experiences that strengthen relationships and make everyday financial life easier for our customers." – Bruno Giovani da Silva Maria, Financial Services Coordinator, banQi.

Quantiphi QAIssist – Agentic Contact Center QAIssist is an AI-powered contact center solution co-innovated by Quantiphi and GenAIIC PAF that uses autonomous agents to handle complex customer requests across voice and chat while maintaining enterprise security for regulated industries. The solution intelligently processes multi-step workflows, accesses real-time data, and escalates to human agents when needed, delivering faster and context-aware customer experiences. Learn more.

"Our collaboration with AWS through the GenAl Innovation Center (GenAIIC) Partner Innovation Alliance (PIA) has been a catalyst in accelerating hands-on generative Al innovation. The PIA Bootcamp experience paved the way for our participation in the Partner Agent Factory (PAF), enabling us to co-innovate with AWS GenAIIC experts and transform GenAl concepts into scalable, agentic solutions. This progression reflects our shared commitment to infusing GenAl into every layer of enterprise transformation-driving efficiency, innovation, and measurable business outcomes across industries." – Jim Keller, Global AWS Leader, Quantiphi. "With AWS and Quantiphi's Partner Agent Factory (PAF) solution, we're moving to a platform that facilitates a much simpler integration with our Unclaimed Property Claims-processing system. By transforming our contact center into a smarter, more efficient, and citizen-focused operation, we will be better able to serve a very fast-growing need. The modernization potentially optimizes costs while improving service accessibility, but also lays the foundation for future innovation with Agentic AI—enabling our team to deliver faster, more personalized support to the Citizens of Utah." – Dennis L. Johnston, Utah State Treasurer's Office Administrator, Unclaimed Property Division.

Crayon refiNER – An Agentic NER Solution Crayon refiNER is an AI-powered named entity recognition framework built on LLMs that automatically detects, validates, and refines entities across multiple languages and domains using intelligent tool-driven workflows. Co-innovated with AWS GenAIIC, Crayon refiNER continuously learns and improves extraction accuracy through self-refining feedback loops while providing transparent, auditable results with minimal manual intervention. Learn more.

"Partnering with AWS GenAIIC on the Partner Agent Factory Launch Program enables us to bring together innovation, expertise, and reusable GenAI assets to rapidly turn ideas into impactful solutions. As a member of GenAIIC Partner Innovation Alliance, this collaboration strengthens our ability to help customers realize tangible business value through generative AI." – Vikas Bhonsle, Executive Vice President – APAC, India and Middle East, Crayon.

Embark on Your AI Journey with PAF PAF is dedicated to delivering tangible business results through innovation, collaboration, and excellence in Agentic AI. Explore GenAIIC's PAF solutions on the AI Agents and Tools on AWS Marketplace today. Connect with us at [EMAIL] to harness the power of Agentic AI for your business success.

From Business Logic to Working Code: How Kiro Changes Who Can Build

Supply chain managers understand inventory reconciliation. Compliance officers know audit requirements. Marketing teams grasp campaign workflows. What if they could build their own enterprise applications directly from that expertise? This isn't theoretical. Citizen development tools like Kiro replace traditional coding with natural language specification. Business users describe what they need in plain English, and Kiro builds working applications that those users can iterate and refine themselves. "When inventory falls below 100 units, email the supplier and create a purchase order" becomes a working application in days. You can iterate to production in weeks instead of months. The approach succeeds because business users no longer translate their needs through analysts and developers (a process that loses critical context). Domain expertise becomes working software. The potential is massive. By 2025, 70% of new applications developed by enterprises will use low-code or no-code technologies, up from less than 25% in 2020, according to Gartner. This shift unlocks business users as an enormous workforce for application development. The driver behind this trend: IDC forecasts that by 2026, more than 90% of organizations worldwide will feel the pain of the IT skills crisis, amounting to some $5.5 trillion in losses caused by product delays, impaired competitiveness, and loss of business. How Specification-Driven Development Changes the Game Three technical capabilities make this possible. Amazon Bedrock AgentCore provides enterprise-grade runtime with 8-hour session support. (Previous solutions timed out after minutes, forcing users to restart complex workflows.) Integrated tools eliminate the need for expert developers to assemble complex toolchains. And iterative refinement lets users test with real data immediately instead of waiting for complete implementations. These capabilities solve the core failure pattern of earlier citizen development attempts. When business users have to predefine complete workflows upfront, they can't discover edge cases until deployment. As AWS VP Deepak Singh explained in an interview with SiliconANGLE, Kiro lets developers work side by side with an agent to define requirements and application structure. "You collaborate with the agent on figuring out what your requirements should be and how your application should be structured," said Singh. "As the application goes from your little fun prototype to a full application, it grows with you." Testing helps improve the speed advantage. Traditional no-code platforms require users to map complete business logic into visual workflows before seeing results. Specification-driven development lets users describe outcomes, test with real data immediately, and refine based on actual results—cutting discovery cycles from weeks to days. From Four Hours to Four Minutes: A Real Implementation Consider Sarah, a supply chain manager at a medical device manufacturer who spent four hours every Monday reconciling inventory data across three systems. IT said it would take six months to build a solution. Sarah described her reconciliation process to Kiro: "Pull data from systems A, B, and C. Flag discrepancies over $1,000. Email alerts to procurement when stock falls below safety levels. Generate weekly variance reports for finance." The first version caught 80% of her use cases. Through three iterations over two weeks—discovering edge cases, handling exceptions for returned items, and adding finance team notifications—she built a solution that handles 95% of her Monday reconciliation automatically. Two weeks of development turned four hours of manual work into a fifteen-minute automated process—months ahead of IT's six-month timeline. Building Your Citizen Development Framework When launching citizen development initiatives, three critical foundations help determine success or failure. 1. Governance without Strangulation Your governance framework should prevent problems through design rather than approval workflows. Use pre-approved connectors to enterprise systems that enforce your existing security model and prohibit direct database access. Automated scanning catches sensitive data exposure. Require approval only for external connections, not internal data sources. When Sarah's inventory tool scales to handle $2M in monthly orders, that triggers a review for enterprise scaling — recognition that the tool has outgrown its pilot, not that it's done something wrong. 2. Distributed Support Model Identify power users in each business unit: the Excel experts who already build complex macros, the process optimizers who create workarounds, and the people others ask for help. Train them as citizen development mentors who provide domain expertise while IT maintains platform infrastructure. This approach frees professional developers to focus on platform capabilities and complex integrations. When Sarah built her inventory tool, she didn't need IT to hold her hand. She asked Mike (the procurement Excel wizard) for business logic help. 3. Metrics That Matter Track development cycle time, hours saved, and process improvements. Compare citizen-built solution timelines to typical IT project durations. Monitor usage patterns—when tools scale beyond their original team, you can replicate that success enterprise-wide. Addressing Resistance Department managers often worry about losing control when teams self-serve their technology needs. Their concern stems from the decades when "IT oversight" meant project approval and resource allocation. Give these managers new responsibilities that leverage their oversight skills: identifying patterns across team-built solutions, connecting use cases between departments, and escalating complex requirements to IT. They become innovation process managers rather than request gatekeepers. The framing matters as much as the responsibilities. Present citizen development as capability building ("Build tools that make you more effective than your competitors") rather than cost reduction. Cost reduction implies more work for the same pay and triggers defensive reactions. Capability building positions managers as competitive advantage creators who develop their teams' skills. A procurement manager who helps her team build three automation tools in a quarter has accomplished a feat worth recognizing in performance reviews. Watch for resistance disguised as governance concerns. "We need more controls before expanding" often means "I'm uncomfortable with the pace of change." Address this by showing controlled expansion: Start with one additional team that has a clear use case and strong power user, succeed there, then expand. Incremental proof points overcome fear better than comprehensive governance documents. Your 30-Day Pilot Plan Start small, measure everything, and scale what works. Week 1-2: Selection and Setup Have your team download Kiro and identify three power users who spend 4+ hours weekly on repetitive tasks. Choose processes with clear rules and measurable outcomes, such as inventory reconciliation, compliance reporting, and customer onboarding workflows. Avoid anything that requires complex judgment calls or creative decisions. Week 3-4: Build and Iterate Have users describe their processes in plain English, test with real data immediately, and refine based on results. Expect iteration—first versions typically capture 80% of use cases. Document what requires IT intervention versus what users solve independently. Measurement and Scaling Track hours saved, errors caught, and processes accelerated through platform usage data and quarterly user surveys. Compare citizen development timelines to typical IT estimates for projects of a similar scope. Look for patterns in successful use cases that indicate which approaches work best and merit scaling enterprise-wide. Use these outcomes to justify a broader rollout. The New Role of Technology Leaders Technology leaders who win in this environment recognize that their strength comes not from controlling who can build but from controlling how building happens safely. Your role evolves from building every application to building the infrastructure that lets your team members build within governance frameworks you design. This requires new skills: designing governance frameworks that enable rather than restrict, identifying patterns across distributed solutions, and knowing when citizen-built applications need enterprise scaling versus remaining team-specific. Citizen-built applications that can handle millions of transactions indicate platform success, not governance failure.

TAGS: Agility, AI, Business Value, Change Leadership, governance, Leadership, Organizational Agility

Tom Godden Tom Godden is an Enterprise Strategist and Evangelist at Amazon Web Services (AWS). Prior to AWS, Tom was the Chief Information Officer for Foundation Medicine where he helped build the world's leading, FDA regulated, cancer genomics diagnostic, research, and patient outcomes platform to improve outcomes and inform next-generation precision medicine. Previously, Tom held multiple senior technology leadership roles at Wolters Kluwer in Alphen aan den Rijn Netherlands and has over 17 years in the healthcare and life sciences industry. Tom has a Bachelor's degree from Arizona State University.

Unlock Seamless iSCSI Storage Integration: A Guide to FSxN on ROSA Clusters for iSCSI

In a previous blog, we introduced an exciting feature in the Trident 25.02 release that simplifies preparing worker nodes of an OpenShift Cluster for iSCSI workloads. This enhancement eliminates the need for manual preparation, streamlining the process for Kubernetes cluster worker nodes and benefiting users of Red Hat OpenShift Service on AWS (ROSA). With this feature, provisioning persistent volume for various workloads, including virtual machines on OpenShift virtualization on bare metal nodes within a ROSA cluster, becomes effortless. In this blog, we will provide a comprehensive guide on installing Amazon FSx for NetApp ONTAP (FSxN) on AWS and utilizing it to provision storage for containers and virtual machines running on ROSA clusters. Join us as we walk you through the installation and configuration of Trident 25.02, showcasing how to create container applications and virtual machines on ROSA clusters using iSCSI volumes. Additionally, we will demonstrate that Trident supports Read Write Many (RWX) access modes for iSCSI volumes in Block mode, enabling live migrations of VMs created with iSCSI storage. Get ready to unlock seamless storage integration and enhance your ROSA cluster deployments! ROSA clusters with FSxN storage ROSA integrates seamlessly with Amazon FSx for NetApp ONTAP (FSxN), a fully managed, scalable shared storage service built on NetApp's renowned ONTAP file system. With FSxN, customers can leverage key features such as snapshots, FlexClones, cross-region replication with SnapMirror, and a highly available file server that supports seamless failover. The integration with NetApp Trident driver—a dynamic Container Storage Interface (CSI)—facilitates the management of Kubernetes Persistent Volume Claims (PVCs) on storage disks. This driver automates the on-demand provisioning of storage volumes across diverse deployment environments, making it simpler to scale and protect data for your applications. One key benefit of FSxN is that it is a true first party AWS offering just like EBS, meaning customers can retire their committed spend with AWS and get support directly from them as well. Solution overview This diagram shows the ROSA cluster deployed in multiple availability zones (AZs). ROSA cluster's master nodes, infrastructure nodes are in Red Hat's VPC, while the worker nodes are in a VPC in the customer's account. We'll create an FSxN file system within the same VPC and install the Trident provisioner in the ROSA cluster, allowing all the subnets of this VPC to connect to the file system.

Prerequisites ● AWS account ● A Red Hat account ● IAM user with appropriate permissions to create and access ROSA cluster ● AWS CLI ● ROSA CLI ● OpenShift command-line interface (oc) ● Helm 3 documentation ● A HCP ROSA cluster ● Access to Red Hat OpenShift web console Step 1: Provision FSx for NetApp ONTAP Create a multi-AZ FSx for NetApp ONTAP in the same VPC as the ROSA cluster. There are several ways to do this. We are showing the creation of FSxN using a CloudFormation (CFN) Stack

Run the command below by replacing the parameter values with your own values:

# cd rosa-fsx-netapp-ontap/fsx aws cloudformation create-stack \ --stack-name ROSA-FSXONTAP \ --template-body file://./FSxONTAP.yaml \ --region \ --parameters \ ParameterKey=Subnet1ID,ParameterValue=[subnet1_ID] \ ParameterKey=Subnet2ID,ParameterValue=[subnet2_ID] \ ParameterKey=myVpc,ParameterValue=[VPC_ID] \ ParameterKey=FSxONTAPRouteTable,ParameterValue=[routetable1_ID,routetable2_ID] \ ParameterKey=FileSystemName,ParameterValue=ROSA-myFSxONTAP \ ParameterKey=ThroughputCapacity,ParameterValue=1024 \ ParameterKey=FSxAllowedCIDR,ParameterValue=[your_allowed_CIDR] \ ParameterKey=FsxAdminPassword,ParameterValue=[Define Admin password] \ ParameterKey=SvmAdminPassword,ParameterValue=[Define SVM password] \ --capabilities CAPABILITY_NAMED_IAM Where : region-name: same as the region where the ROSA cluster is deployed subnet1_ID : id of the Preferred subnet for FSxN subnet2_ID: id of the Standby subnet for FSxN VPC_ID: id of the VPC where the ROSA cluster is deployed routetable1_ID, routetable2_ID: ids of the route tables associated with the subnets chosen above your_allowed_CIDR: allowed CIDR range for the FSx for ONTAP security groups ingress rules to control access. You can use 0.0.0.0/0 or any appropriate CIDR to allow all traffic to access the specific ports of FSx for ONTAP. Define Admin password: A password to login to FSxN Define SVM password: A password to login to SVM that will be created. Verify that your file system and storage virtual machine (SVM) have been created using the Amazon FSx console.

3. Install Trident CSI driver for the ROSA cluster a. Install Trident using the Trident certified Operator in the operator hub. For additional methods of installing Trident, refer to the Trident documentation. Ensure that all Trident pods are running after the installation is successful.

Install the operator in the trident namespace. Once the operator is installed, click on view operator. Now install the trident orchestrator by clicking on create instance.

Go to the yaml view and update the nodePrep parameter to include iscsi.

Once the orchestrator status changes to installed, ensure that all trident pods are in the running state.

Now you can log back into the ROSA worker nodes and verify that iscsid and multipathd are running and that the multipath.conf file has the required entries. [root@localhost fsx]# oc debug node/ip-10-0-0-196.us-west-2.compute.internal Starting pod/ip-10-0-0-196us-west-2computeinternal-debug-tv7vw … To use host binaries, run `chroot /host` Pod IP: 10.0.0.196 If you don't see a command prompt, try pressing enter. sh-5.1# sh-5.1# sh-5.1# chroot /host sh-5.1# sh-5.1# systemctl status iscsid ● iscsid.service – Open-iSCSI Loaded: loaded (/usr/lib/systemd/system/iscsid.service; enabled; preset: disabled) Active: active (running) since Wed 2025-08-06 20:31:37 UTC; 5min ago TriggeredBy: ● iscsid.socket Docs: man:iscsid(8) man:iscsiuio(8) man:iscsiadm(8) Main PID: 621624 (iscsid) Status: "Ready to process requests" Tasks: 1 (limit: 99844) Memory: 4.1M CPU: 5ms CGroup: /system.slice/iscsid.service └─621624 /usr/sbin/iscsid -f Aug 06 20:31:37 ip-10-0-0-196 systemd[1]: Starting Open-iSCSI… Aug 06 20:31:37 ip-10-0-0-196 systemd[1]: Started Open-iSCSI. sh-5.1# systemctl status multipathd ● multipathd.service – Device-Mapper Multipath Device Controller Loaded: loaded (/usr/lib/systemd/system/multipathd.service; enabled; preset: enabled) Active: active (running) since Wed 2025-08-06 20:31:38 UTC; 5min ago TriggeredBy: ○ multipathd.socket Process: 621707 ExecStartPre=/sbin/modprobe -a scsi_dh_alua scsi_dh_emc scsi_dh_rdac dm-multipath (code=exited, stat> Process: 621708 ExecStartPre=/sbin/multipath -A (code=exited, status=0/SUCCESS) Main PID: 621709 (multipathd) Status: "up" Tasks: 7 Memory: 18.9M CPU: 37ms CGroup: /system.slice/multipathd.service └─621709 /sbin/multipathd -d -s Aug 06 20:31:38 ip-10-0-0-196 systemd[1]: Starting Device-Mapper Multipath Device Controller… Aug 06 20:31:38 ip-10-0-0-196 multipathd[621709]: ——–start up——– Aug 06 20:31:38 ip-10-0-0-196 multipathd[621709]: read /etc/multipath.conf Aug 06 20:31:38 ip-10-0-0-196 multipathd[621709]: path checkers start up Aug 06 20:31:38 ip-10-0-0-196 systemd[1]: Started Device-Mapper Multipath Device Controller. sh-5.1# sh-5.1# sh-5.1# cat /etc/multipath.conf defaults { find_multipaths no } blacklist { device { vendor .* product .* } } blacklist_exceptions { device { vendor NETAPP product LUN } }

Configure the Trident CSI backend to use FSx for NetApp ONTAP (ONTAP SAN for iSCSI)

The Trident back-end configuration tells Trident how to communicate with the storage system (in this case, FSxN). For creating the backend, we will provide the credentials of the Storage Virtual machine to connect to, along with the cluster management lif and the SVM to use for storage provisioning. We will use the ontap-san driver to provision storage volumes in FSxN file system.

Create the backend object using the command shown and the following yaml. #cat tbc-fsx-san.yaml

apiVersion: v1 kind: Secret metadata: name: tbc-fsx-san-secret type: Opaque stringData: username: fsxadmin password: &lt;value provided for Define SVM password as a parameter to the Cloud Formation Stack&gt; --- apiVersion: trident.netapp.io/v1 kind: TridentBackendConfig metadata: name: tbc-fsx-san spec: version: 1 storageDriverName: ontap-san managementLIF: &lt;management lif of the file system in AWS&gt; backendName: tbc-fsx-san svm: &lt;SVM name that is created in the file system&gt; defaults: storagePrefix: demo nameTemplate: "{{ .config.StoragePrefix }}_{{ .volume.Namespace }}_{{ .volume.RequestName }}" credentials: name: tbc-fsx-san-secret YAML

# oc apply -f tbc-fsx-san.yaml -n trident 2. Verify backend object has been created and Phase is showing Bound and Status is Success [ec2-user@ip-10-0-128-119 ~]$ oc create -f tbc-fsx-san.yaml -n trident secret/tbc-fsx-san-secret created tridentbackendconfig.trident.netapp.io/tbc-fsx-san created [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ oc get tbc -n trident NAME BACKEND NAME BACKEND UUID PHASE STATUS tbc-fsx-san tbc-fsx-san 10d013ab-b291-4a0c-91fa-6c76eddf554e Bound Success [ec2-user@ip-10-0-128-119 ~]$ 3. Create Storage Class for iSCSI Now that the Trident backend is configured, you can create a Kubernetes storage class to use the backend. Storage class is a resource object made available to the cluster. It describes and classifies the type of storage that you can request for an application. # cat sc-fsx-san.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: sc-fsx-san provisioner: csi.trident.netapp.io parameters: backendType: "ontap-san" media: "ssd" provisioningType: "thin" fsType: ext4 snapshots: "true" storagePools: "tbc-fsx-san:.*" allowVolumeExpansion: true #oc create -f sc-fsx-san.yaml 4. Verify storage class is created [ec2-user@ip-10-0-128-119 ~]$ oc create -f sc-fsx-san.yaml storageclass.storage.k8s.io/sc-fsx-san created [ec2-user@ip-10-0-128-119 ~]$

[ec2-user@ip-10-0-128-119 ~]$ oc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE gp2-csi ebs.csi.aws.com Delete WaitForFirstConsumer true 21h gp3-csi (default) ebs.csi.aws.com Delete WaitForFirstConsumer true 21h sc-fsx-san csi.trident.netapp.io Delete Immediate true 13s [ec2-user@ip-10-0-128-119 ~]$ 5. Create a Snapshot class in Trident so that CSI snapshots can be taken # cat snapshotclass.yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: trident-snapshotclass driver: csi.trident.netapp.io deletionPolicy: Retain # oc create -f snapshotclass.yaml [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ oc create -f snapshotclass.yaml volumesnapshotclass.snapshot.storage.k8s.io/trident-snapshotclass created [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ oc get volumeSnapshotClass NAME DRIVER DELETIONPOLICY AGE csi-aws-vsc ebs.csi.aws.com Delete 22h trident-snapshotclass csi.trident.netapp.io Retain 18s [ec2-user@ip-10-0-128-119 ~]$ This completes the installation of Trident CSI driver and its connectivity to FSxN file system using iSCSI. Using ISCSI storage for container apps on ROSA

Deploying a Postgresql application using iSCSI storage class Use the following yaml file to deploy postgresql app

#cat postgres-san.yaml apiVersion: apps/v1 kind: Deployment metadata: name: postgres spec: replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:14 env: - name: POSTGRES_USER #value: "myuser" value: "admin" - name: POSTGRES_PASSWORD #value: "mypassword" value: "adminpass" - name: POSTGRES_DB value: "mydb" - name: PGDATA value: "/var/lib/postgresql/data/pgdata" ports: - containerPort: 5432 volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data volumes: - name: postgres-storage persistentVolumeClaim: claimName: postgres-pvc --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgres-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: sc-fsx-san --- apiVersion: v1 kind: Service metadata: name: postgres spec: selector: app: postgres ports: - protocol: TCP port: 5432 targetPort: 5432 type: ClusterIP #oc create namespace postgres-san root@localhost HAFSX]# oc create -f postgres-san.yaml -n postgres-san deployment.apps/postgres created persistentvolumeclaim/postgres-pvc created service/postgres createdYAML b. Verify that the application pod is running. Verify and a PVC and PV are created for the application. Note that the storage class for the PVC uses the san storage class previously created using iSCSI. c. Verify that iSCSi sessions are created in the node where the pod runs. [ec2-user@ip-10-0-128-119 ~]$ oc debug node/ip-10-0-1-192.us-west-2.compute.internal Starting pod/ip-10-0-1-192us-west-2computeinternal-debug-4j8gg … To use host binaries, run `chroot /host` Pod IP: 10.0.1.192 If you don't see a command prompt, try pressing enter. sh-5.1# chroot /host sh-5.1# iscsiadm -m session tcp: [1] 10.0.0.107:3260,1028 iqn.1992-08.com.netapp:sn.5cdf7ad172f811f0883cc908a40ebab0:vs.3 (non-flash) tcp: [2] 10.0.2.18:3260,1029 iqn.1992-08.com.netapp:sn.5cdf7ad172f811f0883cc908a40ebab0:vs.3 (non-flash) sh-5.1#

d. Verify that a lun is created Verify that a lun is created on the volume in FSxN for this application and the lun is mapped. You can log into the FSxN CLI using fsxadmin and the password you previously created. FsxId0d3560ad60fbad076::> volume show -vserver fsx Vserver Volume Aggregate State Type Size Available Used% ——— ———— ———— ———- —- ———- ———- —– fsx fsx_root aggr1 online RW 1GB 972.2MB 0% fsx trident_postgres_san_postgres_pvc_a56f2 aggr1 online RW 5.50GB 5.45GB 0% fsx vol1 aggr1 online RW 1TB 860.2GB 0% 3 entries were displayed. FsxId0d3560ad60fbad076::> lun show -volume trident_postgres_san_postgres_pvc_a56f2 -vserver fsx Vserver Path State Mapped Type Size ——— ——————————- ——- ——– ——– ——– fsx /vol/trident_postgres_san_postgres_pvc_a56f2/lun0 online mapped linux 5GB FsxId0d3560ad60fbad076::> igroup show -vserver fsx Vserver Igroup Protocol OS Type Initiators ——— ———— ——– ——– ———————————— fsx ip-10-0-1-192.us-west-2.compute.internal-239f6c7b-d5da-4c33-94a5-65d13c0caaeb iscsi linux iqn.1994-05.com.redhat:cae290244627 Using ISCSI storage for VMs on OpenShift Virtualization in ROSA 1. Verify you have baremetal nodes as worker nodes in the cluster. To be able to create VMs, you need to have bare metal nodes on the ROSA cluster. 2. Install OpenShift Virtualization using the Operator You can install OpenShift Virtualization using the OpenShift Virtualization Operator in the Operator hub. Once it is installed and configured, Virtualization will be populated in the UI of the OpenShift Console.

3. Deploy a VM using iSCSI storage class Click on Create VirtualMachine and select From template. Select Fedora VM. You can choose any OS that has source available. 4. Customize the VM Customize the VM to provide the storage class for the boot disk and create additional disks with the selected storage class. Click on Customize VirtualMachine.

5. Click on the Disks tab and click on Edit for the root disk

6. Ensure you have selected sc-fsx-san for storage class. Select Shared Access (RWX) for Access mode and Select Block for Volume Mode. Trident Supports RWX Access mode for iSCSI storage in Volume Mode Block. This setting is a requirement for the PVC of the disks so that you can perform live migration of VMs. Live migration is migration of a VM from one worker node to another for which RWX access mode is required and Trident supports it for iSCSI in Block Volume Mode.

Note: If sc-fsx-san was set as the default storage in the cluster, then this storage class will automatically be picked. 8. Add another disk Click Add disk and select empty disk (since this is just an example) and ensure that sc-fsx-san disk is chosen, Volume Mode Block and Access Mode RWX are chosen. Click Save and the Click Create VirtualMachine. The VM comes to a running state.

9. Check the VM pods, PVCs. Verify that PVCs are created using iSCSI storage class and RWX Access modes. [root@localhost fsx]# oc get pods,pvc NAME READY STATUS RESTARTS AGE pod/virt-launcher-fedora-demo-vm-lr2rj 1/1 Running 0 111s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE persistentvolumeclaim/dv-fedora-demo-vm-disk-green-impala-85-cls8e4 Bound pvc-cfceae37-d159-46d9-8925-4e3bc35ebdc6 30Gi RWX sc-fsx-san 2m22s persistentvolumeclaim/dv-fedora-demo-vm-rootdisk-2uz76f Bound pvc-98fb0938-7530-4ec8-bf36-78e7101468f5 30Gi RWX sc-fsx-san 2m22s [root@localhost fsx]#

10. Verify that a LUN is created in each volume corresponding to the disk PVCs by logging into the FSxN CLI.

FsxId0d3560ad60fbad076::> volume show -vserver fsx Vserver Volume Aggregate State Type Size Available Used% ——— ———— ———— ———- —- ———- ———- —– fsx fsx_root aggr1 online RW 1GB 971.0MB 0% fsx trident_default_prime_cc24a6a2_86ca_4e0b_a467_e9fac9e391a9_cfcea aggr1 online RW 33GB 33.00GB 0% fsx trident_openshift_virtualization_os_images_prime_2a2f581f_87a3_4e7a_8d3a_3a3d54253796_8a3b8 aggr1 online RW 33GB 31.59GB 4% fsx trident_openshift_virtualization_os_images_prime_54ef0fcf_f60e_4561_b8e2_571d1cdebc06_eaf2e aggr1 online RW 33GB 31.33GB 5% fsx trident_openshift_virtualization_os_images_prime_7e5eb814_68a6_4836_8321_b2d14d838ffc_91f5f aggr1 online RW 33GB 30.86GB 6% fsx trident_openshift_virtualization_os_images_prime_9cf9f23d_1ed8_4100_a05e_c73c938a2d78_bebc7 aggr1 online RW 33GB 30.68GB 7% fsx trident_openshift_virtualization_os_images_prime_a4d7925b_d11b_40d6_a052_1c9932e9c0c6_2e76b aggr1 online RW 33GB 31.07GB 5% fsx trident_openshift_virtualization_os_images_prime_e77ccf75_7798_43cf_85d6_f9f69f9eb200_5c85c aggr1 online RW 33GB 32.36GB 1% fsx trident_openshift_virtualization_os_images_tmp_pvc_50094ec5_587a_4de2_86a4_7f200f8e89b8_98fb0 aggr1 online RW 33GB 32.33GB 2% fsx trident_postgres_san_postgres_pvc_a56f2 aggr1 online RW 5.50GB 5.39GB 2% fsx vol1 aggr1 online RW 1TB 849.1GB 0% 11 entries were displayed. FsxId0d3560ad60fbad076::> lun show -volume trident_default_prime_cc24a6a2_86ca_4e0b_a467_e9fac9e391a9_cfcea Vserver Path State Mapped Type Size ——— ——————————- ——- ——– ——– ——– fsx /vol/trident_default_prime_cc24a6a2_86ca_4e0b_a467_e9fac9e391a9_cfcea/lun0 online mapped linux 30GB FsxId0d3560ad60fbad076::> lun show -volume trident_openshift_virtualization_os_images_tmp_pvc_50094ec5_587a_4de2_86a4_7f200f8e89b8_98fb0 Vserver Path State Mapped Type Size ——— ——————————- ——- ——– ——– ——– fsx /vol/trident_openshift_virtualization_os_images_tmp_pvc_50094ec5_587a_4de2_86a4_7f200f8e89b8_98fb0/lun0 online mapped linux 30GB FsxId0d3560ad60fbad076::> FsxId0d3560ad60fbad076::> igroup show Vserver Igroup Protocol OS Type Initiators ——— ———— ——– ——– ———————————— fsx ip-10-0-1-192.us-west-2.compute.internal-239f6c7b-d5da-4c33-94a5-65d13c0caaeb iscsi linux iqn.1994-05.com.redhat:cae290244627 fsx ip-10-0-1-227.us-west-2.compute.internal-239f6c7b-d5da-4c33-94a5-65d13c0caaeb iscsi linux iqn.1994-05.com.redhat:45b335a3f4b 2 entries were displayed.

Conclusion In this blog, we successfully demonstrated how to integrate FSx for NetApp ONTAP as a shared file system with a ROSA cluster using a Hosted Control Plane, leveraging the NetApp Trident CSI driver for iSCSI storage. We illustrated how Trident release 25.02 streamlines the preparation of worker nodes by configuring iSCSI and multipathing for iSCSI on ONTAP storage. Our comprehensive, step-by-step guide detailed the configuration of the Trident backend and storage class for iSCSI, and how to utilize them to create containers and VMs. We emphasized that the ontap-san driver supports the RWX access mode in Block Volume Mode for iSCSI, making it ideal for VM disks in OpenShift virtualization and enabling live migration of VMs. For further information on Trident, please refer to the NetApp Trident documentation. Additionally, you can find more resources, including detailed guides and videos, in the Red Hat OpenShift with NetApp section under Containers in the NetApp Solutions documentation. To clean up the setup from this post, follow the instructions provided in the GitHub repository.

Ryan Niksch Ryan Niksch is a Partner Solutions Architect focusing on application platforms, hybrid application solutions, and modernization. Ryan has worn many hats in his life and has a passion for tinkering and a desire to leave everything he touches a little better than when he found it.

Banumathy Sundhar In my current role as a Technical Marketing Engineer and in my past role as a Technology Enablement Professional, I have carried out my responsibilities in various ways. I evangelize platforms and products, deep dive into technical areas (Kubernetes, OpenShift, AWS, Azure and Google Clouds), provide live or recorded demos, share information, educate and up-skill via my blogs, live and virtual multi-day sessions. I have provided technical validation of solutions for our customers with NetApp products integrated with OpenShift clusters in a hybrid cloud environment. In my previous role, I have developed and delivered a wide variety of training on technical topics with hands-on lab-intensive content, technical presentations, certification prep sessions and lab sessions at Conferences and Instructor Summits.

Mayur Shetty Mayur Shetty is a Senior Solution Architect within Red Hat's Global Partners and Alliances organization. He has been with Red Hat for four years, where he was also part of the OpenStack Tiger Team. He previously worked as a Senior Solutions Architect at Seagate Technology driving solutions with OpenStack Swift, Ceph, and other Object Storage software. Mayur also led ISV Engineering at IBM creating solutions around Oracle database, and IBM Systems and Storage. He has been in the industry for almost 20 years, and has worked on Sun Cluster software, and the ISV engineering teams at Sun Microsystems.

Decoupling the compute and data layers with AWS MGN, AWS DataSync and Amazon FSx

As more organizations take their first steps in the cloud migration journey, we're seeing them facing a common on-premises challenge: applications that are tightly coupled with their datasets. This means that while their applications (running on physical or virtual machines) are ready to be migrated to the cloud using the rehost ("lift-and-shift") strategy, there may be massive, multi-terabyte block storage volumes attached to them that create migration bottlenecks. This traditional "lift-and-shift" of both compute and storage at once can be slow, risky, and complex. This challenge is especially acute in data-intensive industries. Consider a financial services firm that is running an on-premises data analytics application on a powerful server. Imagine a case where the C: drive hosts the operating system, database engine (e.g. SQL Server) or an application, while the business-critical data is spread across several large volumes: a 4TB D: drive holds active financial transaction data, a 6TB E: drive archives historical records, and an 8TB F: drive contains shared data marts for business intelligence reporting. While functional, this setup creates significant challenges for scaling capacity, collaboration, or ensuring business continuity—all because the data is locally stored on that single on-premises server. To overcome this, in this blog post we'll share with you our recommended strategy for decoupling these layers by first migrating your on-premises data volumes, those large D:, E:, and F: drives, to a fully managed shared storage service in the cloud, such as Amazon FSx for NetApp ONTAP, using AWS DataSync. AWS DataSync is an online data transfer service that simplifies data migration and helps you quickly, easily, and securely, transfer your file or object data to, from, and between AWS storage services. This strategy preserves file permissions and metadata while transforming your rigid, hardware-dependent storage into a flexible, enterprise-grade managed service. Additionally, by relocating your data to a managed file system, you gain access to features such as automated backups and disaster recovery, all with minimal disruption to the running application. In this blog post, we'll walk you through executing this strategy in the form of a scenario, providing you with a practical blueprint for accelerating your own cloud journey. We'll demonstrate how to prepare the on-premises server for data migration, configure a DataSync task, and perform the cutover from using the local storage drives to a remote mounted file system. We'll also show you how to streamline the compute migration by using AWS Application Migration Service (AWS MGN), and selectively replicating only the OS and application volumes. By following this decoupling strategy, when you're ready to complete the server migration using AWS MGN, your new Amazon EC2 instance can mount the exact same cloud-based shared file system that stores your decoupled datasets. Prerequisites While familiarity with these AWS services and concepts is beneficial, this post focuses on the strategic approach and best practices for a successful migration, rather than detailed technical setup. Before proceeding, ensure you have the following prerequisites in place:

An AWS account with the necessary AWS Identity and Access Management permissions required for accessing these services AWS MGN initialized in your target AWS Region. An Amazon FSx file system (e.g. for Windows File Server or for NetApp ONTAP) provisioned in your target VPC. This will serve as the destination for your data. Established network connectivity between your on-premises environment and your AWS VPC, and firewall rules in place to allow connectivity with AWS services

Important Note: This migration pattern is designed for file-level access scenarios only. If your application requires block-level disk access (such as for Windows Server Failover Clustering Quorum), this decoupling strategy is not suitable, and you should maintain the traditional approach of migrating compute and storage together. Walkthrough: The Decoupling process Step 1: Expose On-Premises Data Volumes as Shares Now that we understand the big picture, let's look at the first critical step on-premises: making the data available for migration. Remember those massive D:, E:, and F: drives that are attached directly to your single server? The beauty of this storage-first approach is that we don't need to physically move them. Instead, we'll simply "liberate" the data from the local server's direct control by exposing it to the network.

Figure 1: List the physical volumes attached to your on-premises server. Think of it this way: the data is already stored in a file system, just like a standard network drive. By enabling simple Windows File Sharing, you can turn those local volumes into network shares. It's a quick process—you'll create a share for each volume (e.g. \your-server\data-D, \your-server\data-E, \your-server\data-F) and grant a specific service account read-only permissions. This read-only access is a best practice that ensures the migration tool can't accidentally modify your source data.

Figure 2: Create SMB shares for the non-bootable drives. Once those shares are active, they become the "source" for your migration. The DataSync agent, which we'll cover in the next section, can now connect to these shares and begin its work, treating them just like any other network-accessible file system. With this one simple step, you've taken the first stride toward decoupling your storage and compute, opening the door for a smooth, non-disruptive migration. Step 2: Provision your cloud storage with Amazon FSx With your on-premises data now exposed and ready to be migrated, the next step is to create its new home in the cloud. This is where Amazon FSx for NetApp ONTAP comes in. You'll create a file system that is not only highly available and durable, but also fully managed. This new storage will have all the enterprise-grade features you're accustomed to, like data deduplication and compression to save costs, and snapshots for quick, point-in-time restores. Most importantly, it will provide the same familiar user experience you're used to on-premises. You will create a new SMB share (e.g. \fsx-ontap-endpoint\app-A) that will be the destination for our application's on-premises volumes. FSx for NetApp ONTAP seamlessly integrates with your existing Microsoft Active Directory infrastructure, allowing you to maintain security for your data at the file and folder levels. The beauty of this step is that you're not just replicating data; you're modernizing your entire storage layer. This new cloud-native storage is fully decoupled from any single compute instance, which means it can be accessed by your existing on-premises team during the migration and will be ready for your new cloud-based Amazon EC2 instances when you're ready for the cutover. Step 3: Deploy the AWS DataSync Agent With our data now accessible via SMB shares, the next step is to introduce the key player in our migration: the AWS DataSync agent. Think of the DataSync agent as a smart, automated delivery driver. The agent connects to your on-premises shares, grabs the data, and securely and efficiently delivers it to its new home in the cloud. It's the essential bridge between your legacy on-premises environment and your new AWS infrastructure. Make sure you architect and plan accordingly, so that the agent is deployed to a network that has connectivity to the servers where data is located, as well as the DataSync backend service. DataSync agents may be deployed as a Virtual Machine on supported platforms, as well as an Amazon EC2 Instance. Step 4: Create and Run the DataSync Task With all the pieces in place—your on-premises data exposed as network shares and your new FSx for NetApp ONTAP file system ready in the cloud—the final step is to orchestrate the migration itself. In the AWS console, you'll configure the transfer locations – source SMB (or NFS, if you choose to follow this strategy with a Linux server) and target FSx for ONTAP, then create a DataSync task. This task is a data transfer job that brings the source and destination locations together and sets task options for moving file or object data between AWS storage services and on-premises systems, or other cloud providers. DataSync can be tuned to meet your needs by offering a number of powerful options here, including scheduling the task for a convenient time, filtering which files and folders to include, and configuring how it handles things like symbolic links and user permissions.

Figure 3: Monitoring the DataSync data copy task When you run the task for the first time, DataSync performs a full initial transfer of all your data. After that, you can schedule the task to run on a regular basis for periodic incremental loads. DataSync will identify and transfer only the files that have changed since the last run, ensuring your cloud storage stays up to date with minimal bandwidth usage. DataSync handles the heavy lifting, ensuring a fast, secure, and reliable migration with minimal impact on your running application. Step 5: Prepare for Cutover Before the final cutover, it's crucial to prepare your on-premises environment. Start by running a final incremental DataSync task to ensure all the latest changes have been synced to your FSx file system. Once that's complete, you'll schedule a maintenance window to take the on-premises application or database offline and unmount the original D:, E:, and F: drives from your local server. With those local volumes disconnected, the final step is to remount the new cloud file system. You'll now mount your Amazon FSx share at the same mount location or drive letter, so the data is available for your on-premises server the same way it was with a local drive. This seamless transition allows your application to start using the cloud-based storage without any changes to its configuration or file paths. Step 6: Validate and Resume the Application Once the file system is remounted, the final steps are to validate and resume the application. First, perform a quick check to ensure the application can correctly read and write data to the newly mounted cloud volumes. Once you validate and confirm everything is working as expected, you can bring the application back online, pointing it to the new, cloud-based storage. Step 7: Migrate or Modernize? After a successful data migration, the next logical step is to address the compute layer. Now that the data is decoupled and residing on a cloud-native service like Amazon FSx, you can follow an efficient approach for migrating your application server. AWS MGN is the ideal service for this. When installing the MGN replication agent on your on-premises server, you can choose to selectively replicate only the OS and application volumes. With the large data volumes that were previously attached locally already migrated, this is a crucial step—you'll use a flag in the command-line installer to configure the agent to only migrate local volumes that contain the operating system and the application. In the below screenshot, the –drives command instructs MGN to only replicate the C: drive, significantly reducing the time it takes for replicating your server's data to AWS.

Figure 4: Use the –drives flag on Windows or –devices on Linux to specify the bootable drive for replication by MGN This storage-first approach gives you a powerful opportunity to modernize your compute architecture. By decoupling your application from its on-premises storage, you've paved the way to paths beyond a simple lift-and-shift migration to an EC2 instance, opening the door to explore modern, containerized platforms. For example, you can start working on modernizing your Windows-based application using AWS Transform for .NET workloads and deploy your application on Amazon EKS (Elastic Kubernetes Service), leveraging the persistent storage with native support for containerized workloads provided by FSx for ONTAP. This allows you to scale and manage your application with far greater flexibility than a traditional server. Conclusion In this post, we walked you through a strategic pattern that takes you beyond a simple lift-and-shift migration and helps you take your first step in modernizing your monolithic applications. By using AWS DataSync and AWS Application Migration Service together, you can decouple large data volumes from your application servers during a rehost migration. Consider adapting this approach to meet your migration timelines while simultaneously unlocking the benefits of cloud: improved agility, independent scalability, and enhanced resilience for your architecture.

Dor Shiloni Dor is an AWS Senior Specialist Solution Architect, a member of the EMEA Migration and Modernization team. He is passionate about working with customers to understand their needs, leveraging the cloud to architect innovative solutions and driving business value for them.

David Stenson David is a Migration and Modernization Solution Architect at AWS. In his role, he provides guidance to customers on their migration and modernization journey. He lives in Dublin, Ireland and in his spare time, likes to read and spend time with his family.

Improve Cost Visibility and Observability with AWS Cost Categories – Part 1: Fundamentals and Basic Grouping Techniques

As cloud environments continue to expand and evolve, deriving value from their cloud investments becomes paramount for businesses operating on AWS. The ability to group and analyze costs across resources and accounts is crucial for gaining visibility, identifying optimization opportunities, and making data-driven decisions. Organizations aim to accurately allocate and track cloud costs across different business units, projects, and environments to improve budgeting, enable effective chargeback processes, and make informed optimization decisions. With AWS Cost Categories, a free feature, you can create rules to flexibly group and visualize cost using various dimensions such as account, charge type, service and even other Cost Categories. In this first part of our two-part series, we will explore the fundamentals of AWS Cost Categories and demonstrate how they can transform your cost management approach. We'll cover:

Cost Categories overview and key benefits How to flexibly group and visualize costs using 'ANY' or 'ALL' operators Practical implementation examples including:

Cost grouping by region Cost grouping across multiple dimensions Splitting charges within AWS Cost Categories

Through these examples, you will learn how to implement AWS Cost Categories to gain better control and visibility over your cloud spending, enabling more accurate cost allocation, improved financial planning, and strategic decision-making. AWS Cost Categories Overview AWS Cost Categories, a feature within the AWS Cloud Financial Management product suite, allows you to create customized categories to map and organize your cloud spending based on your organizational structure and business needs. This empowers organizations to effectively manage their cloud finances, enabling accurate cost allocation, streamlined chargeback processes, and data-driven optimization decisions. This feature addresses key business needs such as tracking expenses across complex organizational structures, attributing costs to specific projects or departments, and gaining granular insights for budget planning and cost control. Using Cost Categories, you can map your cost and usage information based on defined rules using various dimensions, such as Linked Account and Cost allocation Tags. Once Cost Categories are set up and enabled, you will be able to view your cost and usage information by these Cost Categories starting at the beginning of the month or retroactively for up to 12 months in AWS Cost Explorer, AWS Budgets, Data Exports, AWS Billing Conductor, and AWS Cost Anomaly Detection. Prerequisites To follow along with this post, make sure you have the following prerequisites in place:

An AWS account. If you don't have one, follow the instructions here to create and activate it. A fundamental grasp of creating Cost Categories. Activated Cost allocation tags. Once activated, it typically takes up to 24 hours for these tags to become available for use. Appropriate IAM permissions to access and manage AWS Cost Categories (ce:CreateCostCategoryDefinition, ce:DeleteCostCategoryDefinition, ce:DescribeCostCategoryDefinition, ce:ListCostCategoryDefinitions, ce:UpdateCostCategoryDefinition).

How to flexibly group and visualize cost using Cost Categories Before diving into specific examples, let us explore various ways to group and analyze your AWS costs using Cost Categories. Throughout this guide, we will use regional cost tracking as our primary example to demonstrate these grouping capabilities. Regional cost tracking is a critical requirement for organizations operating global cloud infrastructures. Many enterprises need to monitor and analyze their AWS spending across different geographical locations to support regional business units, comply with local regulations, and optimize resource allocation. By selecting the 'Region' dimension, you can create Cost Categories that align with your global AWS footprint. This enables granular visibility into spending patterns across your operational regions, such as US East (Virginia) (us-east-1), US East (Ohio) (us-east-2), etc. With this level of regional cost breakdown, you can identify opportunities for optimization, such as resource allocation, and disaster recovery planning. Tracking costs by regions, as shown in Figure 1, helps identify where your highest expenses occur and enables strategic decisions about resource placement and regional pricing advantages.

Figure 1. Cost Explorer view of cost breakdown with group by via dimension 'Region'

Beyond granular region-level visibility, you can consolidate and categorize your cloud costs based on geographic regions that align with your organization's structure and reporting needs. For example, if your business operates across North America, Europe, Asia-Pacific, and Latin America, you can create custom Cost Categories to group your AWS spending accordingly. Instead of viewing costs broken down by individual AWS Regions like us-east-1 or eu-central-1, you can define broader categories such as 'Cost in North America', 'Cost in EMEA', 'Cost in APAC', and 'Cost in LATAM'. This allows you to gain a higher-level, regional perspective on your cloud expenditure, which may be more meaningful for your finance, operations, and executive teams. To accomplish this, you can leverage the 'Region' dimension in the AWS Cost Categories feature. Simply create a new Cost Category rule, and under the 'Region' dimension, specify the appropriate values that map to your preferred geographic groupings. Cost Grouping by Region To create a Cost Category with Region dimension:

Navigate to AWS Cost Categories by searching 'Cost Categories' in the AWS Console search bar, or access it through the Billing and Cost Management Console's left navigation menu. Click 'Create Cost Category' on the AWS Cost Categories console. Group your costs

Name your 'Cost Category' as 'Cost By AWS Regions' and click 'add rule' to group your cost. Select your rule type as 'Manually define how to group costs'. Select the dimension as 'Region' to group the cost by, select the matching operator as 'Is' and select the value for the dimension. Before creating the rule by clicking the 'Create rule' button, provide a name such as 'Cost in NAMER' for the cost group.

Figure 2 below illustrates how you can define these regional Cost Categories:

Note: 24 hours after the creation of your new Cost Categories, the status of your newly created Cost Categories will be updated from 'processing' to 'applied' on the home page of 'Cost Categories.' When you select the 'AWS Regions' Cost Category, you will see a detailed cost breakdown for each region, as shown in Figure 3. You may notice an 'uncategorized' segment in the report, which represents costs that are not tied to any specific AWS Region. By clicking on this 'uncategorized' section, you will be taken to a filtered view in Cost Explorer. This view helps you identify which services are generating these uncategorized costs, and you can apply additional filters, such as linked accounts, to determine who owns these expenses. It is important to note that not all AWS costs can be categorized by region due to the nature of certain global services or specialized pricing components. Understanding these uncategorized costs through Cost Explorer helps you identify any potential gaps in your cost allocation strategy and maintain comprehensive cost visibility across your AWS environment.

'ANY' or 'ALL' Operators: The rule creation process includes two powerful operators: 'ANY' and 'ALL.' The 'ANY' operator enables you to create more inclusive rules by allowing costs to be grouped based on multiple conditions within a single dimension. Conversely, the 'ALL' operator enables you to create more specific rules by requiring costs to match all the specified conditions. For instance, you want to consolidate all costs related to your 'North America' operations. You can use the 'Region' dimension and apply the 'ANY' operator to include multiple region values such as 'us-east-1', 'us-east-2', 'us-west-1', and 'us-west-2'. This approach ensures that your 'North America' Cost Category encompasses expenses from all these regions, offering a comprehensive overview of your spending in this geographic area. On the other hand, the 'ALL' operator allows you to create more specific rules by requiring costs to match all the specified conditions. The flexibility offered by the 'ANY' and 'ALL' operators empowers you to define Cost Categories that align precisely with your organization's needs, whether you are looking for broader, more inclusive groupings or narrowly targeted, specific cost tracking. Grouping cost with Multiple Dimensions We can also use multiple dimensions inside the Cost Category definition to group and visualize cost. In the example below, you can visualize the cost of each of their accounts by region by adding both dimensions in a single rule and choosing the appropriate cost values. To accomplish this, you can leverage the existing Cost Category 'Cost by Region' illustrated above as one of the dimensions in the AWS Cost Categories feature. To create a new multiple dimensions Cost Category 'Account spend by region' to group cost of each of their accounts by region, follow these steps:

Click 'Create Cost Category' on the AWS Cost Categories console. Grouping your costs Name your 'Cost Category' as 'Account spend by region' and click 'add rule' to group your cost. Select your rule type as Regular rule to Manually define how to group costs Add the 'Account' dimension and select the appropriate values. Click 'Add condition' to add another dimension to the rule, Select the existing Cost Category 'Cost by Region'. Provide a name such as "AccountA in NAMER" for the cost group, and then click the "Create rule" button.

By using multiple dimensions in a single Cost Category rule, as shown in Figure 4, you can effectively visualize the cost of each account by region.

Configure the Cost Category by adding individual rules that map specific AWS account and region combinations as shown in Figure 5, ensuring accurate cost allocation across geographical segments like NAMER and APAC based on AWS regional spend patterns

Splitting charges within AWS Cost Categories We can use split charges to allocate costs across multiple Cost Category values. This is useful when a cost cannot be directly attributed to a single Cost Category bucket, definition, or owner. Common examples include data transfer costs, API metrics with aggregated costs, and support costs. You can define split charge when you create or edit your Cost Categories and allocate these charges using the following allocation methods – Proportional, Fixed or Even Split. To split charges, follow these steps:

For existing Cost Categories, click 'Split charges' tab, and then 'Create Split charge'. Select the 'Source value' that you want to split and name your uncategorized costs. Chose the 'Target value' – Choose one or more target Cost Category values to allocate the source Cost Category value to. Chose any of the below appropriate 'Charge Allocation Method'

Proportional – Allocates costs using proportionally weighted target values, ideal for organizations that want cost distribution to reflect actual resource consumption. For example, if Project A uses 60% of the compute resources and Project B uses 40%, the AWS costs would be split accordingly using this method, ensuring that projects consuming more resources bear a larger portion of the costs. Fixed – To Allocate your costs by defining a percentage for each of your target values. For Organizations with strict budgetary agreements or cost-sharing arrangements. For example, if a consulting firm has three clients with predefined agreements, Client X could be allocated 50%, Client Y 30%, and Client Z 20% of the total AWS costs, regardless of actual usage. This method ensures compliance with contractual obligations and simplifies billing, even when actual resource consumption varies. Even Split – Allocate your costs evenly across your target values. For Organizations where resources are shared equally or when detailed usage tracking is not necessary. For example, if a university research department has five teams sharing AWS resources, each team would be responsible for 20% of the total AWS costs, regardless of their individual usage patterns. This method promotes collaboration and simplifies budgeting by ensuring equal cost distribution among all participating groups.

Figure 6, Cost Category – Split Charges and Figure 7, Cost Category – Create Split Charges below illustrate how you can create split charges on an existing Cost Category and allocate the uncategorized spend proportionally (using the allocation method) to existing Cost Category values:

Figure 8 below shows the Uncategorized spend 'Split-Charge' is Proportionally split across the other values in the Cost Category. In this example, there was an uncategorized cost of $7.74 that has been proportionally split across the four regions. The 'Total Allocated Cost' metric includes both the raw cost for each region and its portion of the split uncategorized cost. This provides a more comprehensive view of costs, ensuring that all expenses, including those initially uncategorized, are appropriately allocated to each region.

Clean Up While AWS Cost Categories do not incur any charges, you may want to clean up test configurations to maintain organizational clarity in your cost reporting. To Delete Test Cost Categories: If you created 'Cost Categories' for testing or learning purposes, you can remove them to keep your cost reporting organized

Navigate to the AWS Cost Categories console. Select the test 'Cost Category' you want to remove. Choose 'Delete' from the Actions menu. Confirm deletion by typing the Cost Category name.

Cost Categories are a free feature and do not generate any AWS charges. Cost Category deletion cannot be undone. Historical cost data associated with deleted 'Cost Categories' will no longer be available for reporting. It may take up to 24 hours for Cost Category deletions to fully process. Deleted Cost Categories will still appear in historical reports for the time period they were active.

Conclusion In this first part of our series, we've explored how AWS Cost Categories can transform your approach to cloud financial management. We've covered the fundamentals of creating and configuring Cost Categories, using flexible operators to define grouping rules, and implementing practical examples for regional cost tracking, multi-dimensional analysis, and charge splitting. By implementing these foundational AWS Cost Categories techniques, organizations can achieve:

Enhanced visibility into spending patterns across different regions and accounts. More accurate cost allocation aligned with business structures. Flexible grouping capabilities that adapt to your unique organizational needs. Improved financial reporting with consistent categorization across all AWS cost management tools. These capabilities enable more informed decision-making, better budget planning, and clearer cost accountability across your organization. The ability to group and analyze costs in meaningful ways provides the foundation for a mature cloud financial management practice.

In part two of this series, we will build on these fundamentals by exploring advanced techniques for hierarchical ost categorization and programmatic management. We'll demonstrate how to create multi-level organizational structures within your Cost Categories and how to automate 'Cost Category' creation and management at scale using AWS APIs and JSON templates.

Karthikeyan KM KM is a Senior Technical Account Manager supporting Enterprise Users at AWS. With over 20 years of IT experience, he focuses on designing secure, reliable, and scalable solutions while ensuring operational excellence. He is passionate about helping customers accelerate their digital transformation journeys through efficient cloud architectures that align with their business objectives.

Bhaskar Mazumdar Bhaskar is a Senior Technical Account Manager with extensive experience helping organizations optimize their cloud infrastructure and accelerate their digital transformation journeys. He focuses on designing scalable, secure, and cost-effective architectures that align with business objectives.

Vivek Somi Vivek is a seasoned Technical Account Manager focused on security and cloud financial management at Amazon Web Services (AWS). He plays a pivotal role in supporting State and Local Government customers. He acts as a trusted advisor, guiding AWS customers through the complexities of AWS services to achieve their strategic outcomes.

A guide to sending international SMS with US toll-free numbers and AWS End User Messaging

AWS End User Messaging now supports international SMS capabilities for US toll-free numbers (TFNs). This new feature allows businesses to use a single US TFN to send SMS messages to over 150 countries, simplifying global outreach. It primarily benefits customers who need to send one-way transactional alerts—like one-time passwords (OTPs) or shipping notifications—and businesses that want to rapidly prototype and test their messaging strategy in new international markets without the overhead of procuring country-specific numbers. This guide will walk you through the pros and cons of this feature and show you how to enable it and when to use it versus traditional, country-specific sending methods. What are international US toll-free numbers? An International US toll-free Number is a standard US TFN that has been enabled with the capability to send SMS messages to destinations outside of the United States. This feature is backward compatible, meaning you can enable it on any new or existing US TFNs in your account. How to enable international sending There are three primary ways to enable this feature for your US toll-free numbers:

Enable international sending when registering a new number in the console. Enable international sending for an existing number in the console. Enable international sending for an existing number via the AWS CLI.

Let's discuss these in detail. 1. Enable international sending when registering a new number in the console To enable international sending, when registering a new number in the console, do the following steps.

From the AWS End User Messaging console, choose Manage SMS.

Under Configurations in the left navigation menu, choose Phone numbers and choose Request originator.

On the Select country screen, choose United States (US) as your destination country.

On the Define use case screen, configure the various options listed for your intended Messaging use case, and select Yes to enable International sending. Choose Next.

Under the Select originator type screen, choose Toll-free. Validate your Resource policy choices and choose Next.

On the Review and request screen, verify that the information you entered is correct and choose Request. Note that the US toll-free number registration requests can take approximately 15 business days to be approved.

For more information, see Request a phone number in AWS End User Messaging SMS. 2. Enable international sending for an existing number in the AWS console If you have already acquired a TFN, you can enable the international sending feature at any time.

From the AWS End User Messaging console, under Configurations in the left navigation menu, choose Phone numbers and choose an existing toll-free number.

3. Enable international sending for an existing number in the AWS CLI The update-phone-number command allows you to modify a phone number's capabilities, while the describe-phone-numbers command allows you to verify its status. To enable International Sending, use the --international-sending-enabled flag: aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --international-sending-enabled \ --region us-east-1 Bash Note: Replace "phone-a1b2c3d4e5f67890" with your actual phone number's ID To disable International Sending, use the --no-international-sending-enabled flag: aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --no-international-sending-enabled \ --region us-east-1Bash A successful command (for update-phone-number) returns the full JSON object for the phone number. Confirm the change by checking that the InternationalSendingEnabled value is true. { "PhoneNumberArn": "arn:aws:sms-voice:us-east-1:111122223333:phone-number/phone-a1b2c3d4e5f67890", "PhoneNumberId": "phone-a1b2c3d4e5f67890", "PhoneNumber": "+18005550199", "Status": "ACTIVE", "IsoCountryCode": "US", "MessageType": "TRANSACTIONAL", "NumberCapabilities": [ "SMS" ], "NumberType": "TOLL_FREE", "MonthlyLeasingPrice": "2.00", "TwoWayEnabled": true, "InternationalSendingEnabled": true, "CreatedTimestamp": "2025-08-15T10:30:00.123Z" } JSON Use the describe-phone-numbers command with your Phone Number ID to check its current configuration at any time. aws pinpoint-sms-voice-v2 describe-phone-numbers \ --phone-number-ids "phone-a1b2c3d4e5f67890" \ --region us-east-1Bash Benefits and Limitations This feature offers a powerful new way to reach a global audience, but it's important to understand where it shines and what its limitations are. Benefits (Advantages)

Global Reach with a Single Number: Send SMS to over 150 countries using a single, existing US TFN. Simplified Management: Avoid the operational overhead and cost of purchasing and managing a fleet of country-specific phone numbers. Rapid Prototyping and Testing: Quickly test messaging campaigns in new international markets before committing to the best practice approach of acquiring dedicated in-country numbers. Cost Optimization for One-Way Alerts: Provides a cost-effective method for sending high-volume, one-way transactional messages like OTPs, appointment reminders, and shipping notifications globally. Speed to Implement: toll-free number registration is currently the fastest way to get an originator in the US and begin sending SMS. Check here for current timelines.

Two-Way SMS is Limited to the US and Canada: Reliable, two-way SMS conversations are only supported for recipients in the United States and Canada. One-Way Only for All Other Countries: For all other destinations, this is a one-way only. Best-Effort Deliverability: Sending outside of the US and Canada is on a "best-effort" basis. The phone number that appears on the recipient's device may be replaced with a local number or Sender ID, which is why two-way messaging will not work for these destinations. For more details on maximizing delivery, please read A Guide to Optimizing SMS Delivery and Best Practices. Managed Opt-Out is Not Guaranteed Internationally: The automatic STOP reply functionality does not work for destinations outside of the US and Canada. For international recipients, you must provide an alternative opt-out method. Standard Throughput (3 MPS): International TFNs have a default throughput of 3 Message Parts Per Second (MPS). For high-volume, high-throughput campaigns, dedicated country-specific numbers (like short codes) are the recommended best practice.

Understanding the Cost The pricing for this feature is straightforward:

No Additional Monthly Fees: There is no extra charge to enable the international sending capability on your US TFN. You only pay the standard monthly lease for the number itself. Pay-Per-Use Messaging: You are billed for each outbound SMS message at the standard, per-message rate for the destination country.

For a complete and up-to-date list of prices by country, please visit the AWS End User Messaging Pricing page. When to use this vs. country-specific numbers Choosing the right tool depends on your use case. Here's a simple comparison:

Use Case Use International TFN? Use Country-Specific Number (Best Practice)?

Rapidly test a new market Yes. It's the fastest way to get started. No, this approach takes more time to set up.

Send one-way alerts (OTPs, notifications) Yes. It is a simple, cost-effective solution. Yes, but it's more complex if you need to send to many countries

Require two-way conversations Only for the US and Canada. Yes. This is the required approach for reliable two-way SMS in a specific country.

Guarantee your brand/number appears consistently No, plan for Sender ID to not be preserved, as this is not guaranteed internationally. Yes. This is the primary reason to use a dedicated in-country number.

Maximize deliverability for critical campaigns No, delivery is "best-effort." Yes. A local number provides the highest chance of successful delivery.

Considerations and next steps Once you have enabled your international sending over US toll-free numbers, you can enhance your messaging strategy by considering resilience, monitoring, and scalability. The following resources provide best practices for enhancing your sending.

Monitoring Delivery: To monitor delivery rates and patterns by country, you can use Configuration Sets to create event destinations. This allows you to stream SMS events (like DELIVERED or FAILED) to services like Amazon CloudWatch or Amazon Data Firehose for analysis. Building Resilience: For implementing robust delivery, including automatic retry strategies for failed messages, we recommend reading our guide: How to build resilient SMS delivery with AWS End User Messaging. Broader Global Strategy: For a deeper look at the strategic elements of a global SMS program, our post on How to Manage Global Sending of SMS with AWS End User Messaging provides valuable insights and includes a template for organizing use cases and selecting originators.

Conclusion International SMS for US toll-free numbers is a powerful strategic tool for businesses looking to simplify their global messaging. It excels at enabling rapid testing in new markets and efficiently delivering one-way transactional alerts across the globe from a single number. However, it is not a replacement for the best practice of using dedicated, in-country phone numbers when reliable two-way conversations and guaranteed branding are critical to your campaign's success. By understanding its benefits and limitations, you can strategically use this feature to get going quickly while planning a long-term move towards country-specific codes for your most important markets.

Brett Ezell Brett Ezell is a Solutions Architect for AWS End User Messaging and Amazon Simple Email Service. Outside of work, Brett spends his time collecting vinyl, comic books, catching live shows, and training at the gym.

Bruno Giorgini Bruno Giorgini is a Sr Solutions Architect specialized in AWS Communication Developer Services. When he is not crafting innovative solutions for clients, Bruno enjoys spending quality time with his family, exploring the scenic hiking trails. His passion for technology and its potential to drive business transformation keeps him motivated to deliver impactful solutions for organizations around the world.

Know Before You Go: An AWS Partner's Guide to re:Invent 2025

We're looking forward to bringing you AWS re:Invent 2025! The conference is December 1 – 5 in Las Vegas, with virtual opportunities to watch keynotes and Innovation Talks live. All of us at Amazon Web Services (AWS) are excited to connect with AWS Partners and AWS Marketplace Sellers from around the globe. Whether you're joining us in Las Vegas or virtually, you'll have access to groundbreaking announcements, technical sessions, and unparalleled networking opportunities that will help transform your business. To maximize your re:Invent experience, we compiled essential information for our AWS Partner and AWS Marketplace communities. Reserved seating is available for registered attendees to secure seats in our most popular sessions. Sign in to the attendee portal and visit the event catalog to choose your sessions. To help plan your re:Invent experience, download the AWS Events mobile app, available for both iOS and Android. Can't make it to Las Vegas? Register now for free to watch live keynotes and Innovation Talks. Attending re:Invent for the first time? Here are some tips to make the most of your experience:

Wear comfortable shoes. You may be walking an average of 10+ miles a day. Use the complimentary shuttle service between venues to save energy for networking and sessions. Check the re:Invent mobile app for real-time shuttle tracking and plan extra time during peak hours. Get 50% off voucher for an AWS Certification exam. All in-person re:Invent attendees will receive a 50% discount voucher valid for any AWS Certification exam. The voucher can be used for online-proctored or test center-based exams. Earn your AWS Certification prior to attending re:Invent and enjoy special recognition during the event, including access to the AWS Certified Lounge to recharge and network. Stay connected—bring chargers for your devices. Visit the AWS Partner Booth in the main Expo for charging stations and a comfortable space to connect with other Partners and AWS experts while your devices power up. Stay hydrated. Visit the Hydrate & Help stations throughout the Venetian campus. Each time you refill your water bottle, you're supporting a charity in their effort to build clean water systems around the world. Have fun! re:Invent isn't just about learning—it's about community, with multiple ways to join the action. Participate in the 5K, attend Bingo After Dark, get a commemorative tattoo, test your skills at the Sports Forum, challenge fellow developers to BuilderCards, and wrap up an incredible week at re:Play, the ultimate re:Invent party.

AWS Partner Experience at re:Invent This year's AWS Partner experience delivers targeted content and activities to help you build innovative solutions, drive successful business outcomes, and create new possibilities with AWS. You'll gain practical insights on how to apply AWS technologies to better serve your customers, while learning how the AWS Partner Network (APN) and AWS Marketplace can help you scale and transform your business. Don't miss our comprehensive lineup of Partner-focused sessions and technical tracks throughout the week, anchored by the must-attend Keynote with Ruba Borno on Wednesday, December 3. Visit us at the main Expo in the Venetian. At the APN Booth, discover strategies to build, market, sell, and grow your business with AWS. Stop by the AWS Marketplace Booth to explore software applications to help you innovate and the Partner Specialization Booth where you can connect with AWS-vetted experts across 12 critical use cases, from security to AI. Stop by for personalized AWS Partner Central support to make sure you're prepared to leverage new features. Take advantage of numerous networking opportunities throughout the week to build meaningful connections with cloud innovators from around the globe. For important details about health and safety protocols, registration information, and accessibility accommodations, review the re:Invent FAQs.

You're invited to join Ruba Borno, Vice President of AWS Worldwide Channels and Alliances, for the Partnership Advantage Keynote on Wednesday, December 3, from 3 PM to 4:30 PM PT at the Venetian. Join Dr. Ruba Borno and special guests as they unveil how customers are achieving extraordinary breakthroughs with the AWS Partner Network and the AWS Marketplace. As we enter a new agentic era, listen to first-hand experience of customers and partners on how to harness agents and generative AI to accelerate innovation, drive impact, and redefine what's possible. Hear how they're laying the foundational elements to make it all a reality—modernizing securely, scaling faster, and unlocking entirely new business outcomes.

AWS CEO Keynote with Matt Garman Join Matt Garman on Tuesday, December 2, from 8 AM to 10:30 AM PT as he shares his perspective on cloud transformation. Matt will highlight how AWS is shaping the future of cloud technology through continuous innovation. Through behind-the-scenes insights and customer success stories, he'll illustrate how AWS is reinventing what's possible in cloud computing to empower customers and partners with what they need to build a better future.

Innovation Talks Dive deep with AWS thought leaders in our Innovation Talks and learn about the latest cloud technologies. Discover how advancements in AI, analytics, industries, and other key areas can help push your business forward.

Join us for AWS-led breakout sessions, a cornerstone of the AWS Partner experience (PEX) at re:Invent. These lecture-style sessions, delivered by AWS experts, customers, and Partners, provide comprehensive insights to accelerate your AWS journey. If you're attending virtually, breakouts will be available on-demand after the conference. Within the PEX track, you'll also find AWS Marketplace sessions that share the latest features and techniques to sell your products and services. To further enrich your learning experience, we offer additional session formats:

Chalk Talks: Interactive sessions with small audiences featuring a short lecture delivered by an AWS expert, followed by in-depth Q&A discussion. Workshops: Two-hour hands-on sessions where you'll work in teams or individually to solve real-world problems using AWS services. Bring your laptop to build alongside AWS Partner experts. Builder Labs: Get real-time guidance from AWS experts while you work within your own AWS sandbox environment.

➤ Explore AWS Partner-focused (PEX) content in the event catalog. See PEX sessions listed by day below and reserve your seats! Monday, December 1

PEX308-R – Architecting Agentic AI for Supply Chain: Data-to-Decision Patterns PEX404-R – Building Production-Ready AI Applications with Amazon Bedrock PEX303 – Accelerating Development and DevSecOps with Amazon Bedrock & Amazon Q PEX202 – Agentic AI's new generation of Industry and Line of Business solutions PEX314 – AWS Marketplace seller workshop: Publish, update, and manage products PEX203 – Accelerating workload modernization using next generation agentic AI PEX318 – Building Customer Trust with AI Agents: Transform Your MSP Operations PEX305 – Automotive Supply Chain Optimization using AI PEX401-R1 – Deploying Intelligent Agent Systems with MCP PEX316-R1 – Build agents & tools for Bedrock AgentCore and sell in AWS Marketplace

PEX401-R – Deploying Intelligent Agent Systems with MCP PEX319 – Architecting Compliant GenAI Systems for Healthcare Workflows PEX205 – From automation to autonomy: How AWS Partners drive SMB AI evolution at scale PEX209 – AWS Marketplace growth through product-led growth and international expansion PEX309 – Breakdown to Breakthrough: Crafting AI-Powered Smart Machine Support PEX106 – Maximizing customer outcomes: AWS Partner Led Customer Success PEX308-R1 – Architecting Agentic AI for Supply Chain: Data-to-Decision Patterns PEX208 – Transform digital experience: Accelerate AWS Partner & customer success PEX405-R – AI That Remembers: Building Memory Systems for AI Agents PEX204 – Go-to-market in today's political climate: a policy and AWS Marketplace overview PEX206 – Greenfield unlocked: Best practices for expanding new customer engagement PEX304 – Migrate and Modernize Legacy Workflows to Intelligent Agents PEX207 – AI-powered co-selling: How AWS and partners accelerate growth together

PEX316-R –Build agents & tools for Bedrock AgentCore and sell in AWS Marketplace PEX108 – AWS Generative AI Innovation Center driving enterprise success with AWS Partners PEX301 – Agentic AI Solutions for Financial Services & Insurance with AWS PEX201 – Expanding agentic AI: Accelerate, optimize and scale generative AI PEX402 – Engineering Intelligence: Multi-Agent AI Systems for Industrials PEX109 – Migration & Modernization: How AWS Partners help customers transform PEX302 – Building enterprise-grade Agentic AI factory PEX110 – Solution-based selling strategies in AWS Marketplace PEX315 – Intelligent Security Operations with AWS: AI-Powered Incident Response PEX310 – From Motion to Meaning: AI-Powered Human Movement Analysis on AWS PEX102 – Next-gen MSPs: From service providers to strategic partners. PEX307-R1 – Accelerating Cloud Migration with Agentic AI: Hands-on AWS Transform PEX320 – Agentic Code Generation for Industrial Analytics and Predictive Maintenance PEX405-R1 – AI That Remembers: Building Memory Systems for AI Agents PEX306 – Integration to impact: Accelerate AWS Partner revenue measurement PEX112 – Monetizing agentic AI solutions: A Partner's guide for AI agents & tools PEX312 – Trust and Transparency: Implementing Scalable Data Lineage for AI/ML PEX311 – Multimodal Video Understanding AI at Scale with Amazon Bedrock

PEX104 – Generative AI excellence: How AWS Partners help customers from strategy to scale PEX403 – Metadata Intelligence: The Key to Trusted and Smart Agentic AI Systems PEX103 – Accelerate your growth: AWS Partner Programs for Software Partners PEX105 – AWS Global Startup Program – how to accelerate your GTM success PEX107 – Fast Track to Growth: Leveraging Partner Programs & AWS Marketplace for success PEX404-R1 – Building Production-Ready AI Applications with Amazon Bedrock PEX113 – AI-powered co-sell: Unlock partner success with AWS Partner Central PEX313 – Customize Amazon Nova models for Agentic AI with SageMaker & Bedrock PEX101 – Accelerate new customer engagement and revenue growth

ISV Track for Software Partners New this year, the ISV track is designed for independent software vendors building, scaling, and optimizing their applications on AWS. Led by AWS experts, these sessions draw from real-world implementations to share technical patterns, implementation strategies, and architectural best practices. Whether you're migrating existing applications, building new solutions, or looking to integrate AWS services into your software, the ISV track provides targeted content to accelerate your journey, including:

ISV302 – Scaling SaaS applications with cell-based architectures ISV308 – Accelerating SaaS Implementations with Amazon Bedrock and Amazon Nova ISV314 – From Code to Market: Build and Launch AI Agents on AWS Marketplace ISV310 – Global Growth: A Framework for Expanding into New Regions ISV401 – Elevate Multi-Tenant SaaS Operations: DevOps Strategy with Kubernetes

➤ Explore all ISV track sessions in the event catalog.

Spotlight: Fast Track to Growth: Leveraging Partner Programs & AWS Marketplace for Success (PEX107) Don't miss this breakout session on Thursday, December 4 with Julia Chen, VP, AWS Partner Core, and Matt Yanchyshyn, VP, AWS Marketplace and Partner Services. Explore how AWS is transforming Partner success through profitable programs and product innovation from AWS Marketplace and Partner Services. Discover how AWS can fast-track your business growth through enhanced incentives, AI-driven automation, and global reach. Learn about new features enabling faster time-to-value, including automated AWS Specialization validation, simplified contract management, and enhanced AWS Marketplace capabilities. Whether you're focused on migration acceleration, generative AI adoption, or expanding globally, you'll leave with a clear plan to scale efficiently while increasing profitability. This session is intended for AWS Partners. ➤ Register for the PEX107 session >

AWS Marketplace Buyer Sessions In addition to PEX sessions for selling in AWS Marketplace, we offer sessions designed for buyers at all levels. Whether you're new to cloud marketplaces or an existing AWS Marketplace buyer, you'll hear from experts and customers, and get hands-on experience with AWS Marketplace features. ➤ Here are the AWS Marketplace buyer-focused sessions happening at re:Invent: Monday, December 1

AIM328: Accelerate agent solution development with third-party agents & tools AIM304-R: Build agentic observability workflows with products from AWS Marketplace AIM3318: From idea to impact: Harness AI agents and tools in AWS Marketplace

AIM304-R – Build agentic observability workflows with products from AWS Marketplace AIM3311 – Supercharge agentic workflows on AWS with third-party agents and tools

COP212 – Optimize software and AI Agent procurement with AWS Marketplace

MAM107 – Simplify discovery and deployment of software with AWS Marketplace NTA101 – Unlock Innovation: A Guide to AWS Partner Solutions in AWS Marketplace .

Visit Our Booths at the Main Expo The main Expo at the Partner Experience Pavilion features three booths: the AWS Partner Network booth for Partner growth strategies, the AWS Marketplace booth for selling and procurement solutions, and the Specialized Partner Solutions Booth for technical expertise and validation. AWS Partner Network Booth Visit the AWS Partner Network booth to learn how to build and grow your business with AWS. Connect with AWS experts to explore Partner programs, co-selling opportunities, marketing resources, and solutions across data and AI, migration and modernization, industries, public sector, SMBs, and more. Learn how to showcase your technical expertise and differentiate your business. Meet our AWS Partner Ambassadors—successful leaders passionate about sharing their AWS expertise and success stories. Visit for a chance to win prizes while discovering strategies for building solutions with AWS. New this year: Get hands-on AWS Partner Central support to ensure you're ready to take advantage of new features and maximize your partnership success. AWS Marketplace Booth Join us at the AWS Marketplace booth to connect with experts who can help you grow your business. Whether you're a software seller or buyer, our team is ready to answer technical questions, provide personalized guidance, and help you explore the latest AWS Marketplace innovations. New to AWS Marketplace? Our experts will show you how to get started. Already using AWS Marketplace? Discover new ways to optimize your experience and maximize results. Stop by for technical support and participate in our new discovery game—collect stamps as you explore the pavilion and connect with experts to win prizes. PeerSpot will be on site helping sellers capture verified customer feedback that strengthens their AWS Marketplace presence. Invite your customers at re:Invent to leave a verified review for your AWS Marketplace listings, or request your scheduling link for online reviews. Specialized Partner Solutions Booth The Specialized Partner Solutions booth highlights the value of AWS Specialization Partners, showcasing Partner solutions across 12 use cases: CloudOps, Security and Managed Security Service Providers (MSSP), Analytics, Resilience, Migration and Modernization, AI and Agentic AI, End User Computing and Connect, FinOps, Security Incident Response and Cyber Insurance, AWS Built-In, AI Security and Governance, and Database Specializations. Visit the booth to learn how AWS validates Partners' technical expertise and discover complementary Partner solutions that can expand your business opportunities. As an AWS Specialization Partner, your solutions may be featured to customers seeking your specific capabilities. New this year: Explore three expanded Partner domains—Security Incident Response & Cyber Insurance to help you prepare for and respond to security events, FinOps to optimize your cloud spend and financial operations, and AI Security & Governance to ensure responsible and secure AI implementations. Stop by for demos, one-on-one consultations, and exclusive Partner insights while entering for a chance to win prizes. AWS Partners LIVE! Join us for AWS Partners LIVE!, streaming directly from the Partner Experience Pavilion at re:Invent (Booth #1190). Our rotating crew of hosts leads dynamic conversations with AWS experts, diving deep into AI, Security, and Industries through interactive sessions. Experience unique insights, breaking news, and customer success stories—complete with games, hot takes, and more. Tuesday

AI LIVE! 10 AM – 1 PM Security LIVE! 1 PM – 3 PM; 4:30 PM – 6 PM

Security LIVE! 10 AM – 11:30 AM AI LIVE! 11:30 AM – 1:30 PM Industries LIVE! 1:30 PM – 4 PM

Can't make it to Las Vegas? Watch live on YouTube!

Lightning Theater Sessions for AWS Partners Want to learn more about partnering with AWS? Visit our Lightning Theater sessions at the main Expo in the Venetian for 20-minute presentations from the AWS Partner team covering key benefits, programs, and opportunities for AWS Partners. ➤ Here's the lineup of Lightning Theater sessions: Tuesday, December 2

PEX205 – From automation to autonomy: How AWS Partners drive SMB AI evolution at scale PEX209 – AWS Marketplace growth through product-led growth and international expansion PEX106 – Maximizing customer outcomes: AWS Partner Led Customer Success PEX208 – Transform digital experience: Accelerate AWS Partner & customer success PEX206 – Greenfield unlocked: Best practices for expanding new customer engagement PEX207 – AI-powered co-selling: How AWS and partners accelerate growth together

PEX108 – AWS Generative AI Innovation Center driving enterprise success with AWS Partners PEX109 – Migration & Modernization: How AWS Partners help customers transform PEX110 – Solution-based selling strategies in AWS Marketplace PEX102 – Next-gen MSPs: From service providers to strategic partners PEX112 – Monetizing agentic AI solutions: A Partner's guide for AI agents & tools

PEX104 – Generative AI excellence: How AWS Partners help customers from strategy to scale PEX103 – Accelerate your growth: AWS Partner Programs for Software Partners PEX105 –AWS Global Startup Program – how to accelerate your GTM success PEX113 – AI-powered co-sell: Unlock partner success with AWS Partner Central .

Attendee Guides for AWS Partner Personas Make the most of your re:Invent experience with expert guidance from AWS Ambassadors. These experienced community leaders have created tailored attendee guides packed with insider tips, strategic recommendations, and proven best practices for different Partner roles and interests. Whether you're a solutions architect, business leader, or technical professional, you'll find a curated path that matches your goals. Each guide offers session recommendations, networking opportunities, must-attend events for your role, and expert tips to maximize your experience. Explore our collection of Attendee Guides to build your optimal re:Invent experience:

AWS Marketplace Sellers by Alan Blockley, Thoughtworks Cloud administrators and operational professionals by Justin Cook, Eviden Cloud alliance leads by Ruslan Kusov, SoftServe Product and technical by Mark Ross, Eviden Sales and marketing leaders by Dorian Sezen, kloia Senior executives by Todd Bernson, BSC analytics

AWS Public Sector Partners Kick off an exciting week at the Public Sector Partner Forum on Monday, December 1, from 11:30 AM to 2 PM PT at the Encore. Join Rishi Bhaskar, Director, Worldwide Public Sector Partner Sales, AWS, for an engaging and interactive session where you'll have an opportunity to hear from guest speakers Julia Chen, VP AWS Partner Core, and Peter Amirkhan, SVP, Public Sector at Zscaler, and participate in roundtable conversations with AWS leaders and their peers. This is an invite-only event. Interested in attending? Reach out to [EMAIL] Round out your re:Invent experience at Partners in the Cloud, a Partner Appreciation Reception on Wednesday, December 3, 5:30 PM – 7 PM at the iconic Ghostbar. Located on the 55th floor of the Palms Resort & Casino's Ivory Tower, this networking event offers the perfect setting to connect with the AWS Public Sector team and fellow Partners while enjoying spectacular views of the Las Vegas Strip. Register to attend > .

Want to participate in a gamified learning event? Interested in testing your AWS knowledge at re:Invent? AWS GameDay challenges participants to use AWS solutions to solve real-world technical problems in a team-based setting. Unlike traditional workshops, GameDays are open-ended and non-prescriptive to give participants the freedom to explore and think outside the box. ➤ Here is the schedule for AWS GameDay events and sessions:

GHJ305 – Agentic AI (sponsored by Dynatrace) GHJ306 – AI-Assisted Developer Experience (sponsored by New Relic) GHJ307 – Cloud Architecture (sponsored by Datadog) GHJ308 – Security (sponsored by SentinelOne) ARC301 – AI-powered troubleshooting: From chaos to Well-Architected ARC309 – The Frugal Architect: Building cost-aware architectures CMP326 – Java Modernization and Performance Optimization SEC403 – Winning the DDos Game

No need to find a team, we'll match you with other attendees at the event. Bring your laptop, your A-game, and get ready for an unforgettable experience that will level up your AWS expertise!

To help you prepare for your re:Invent experience, AWS Partners have enhanced training and certification options available in AWS Skill Builder. Advance your professional goals with access to 900+ free courses and learning plans, gain hands-on experience in an AWS console sandbox, explore game-based learning including AWS Cloud Quest and AWS Card Clash, take AWS Certification Official Practice Exams, and discover new generative AI training content. Start your AWS Skill Builder subscription to unlock exclusive training content for your team, including AWS Builder Labs, AWS Jam Journeys, and Digital Classroom courses. Once you arrive in Las Vegas, there are plenty of opportunities to develop your skills and propel your AWS practice forward. Visit the Developers Community Pavilion at the main Expo (Booth #1502), which brings together AWS Training & Certification, Open Source, and DevEx teams to present an integrated developer journey featuring interactive learning stations focused on hands-on learning and community engagement. In the Arcade Arena, you'll find two interactive experiences: the AWS Skill Builder Meeting Simulator, where you'll practice customer conversations through AI-powered simulations, and AWS Card Clash, a strategic card game where players deploy AWS services to build architectures and score points. Connect with AWS T&C experts at the AWS Certification Lounge for networking and community building. Partner Bootcamps Join us for exclusive, hands-on technical workshops led by AWS experts (Mandalay Bay Convention Center). These advanced (300-level) to expert (400-level) bootcamps are designed specifically for AWS Partners. Bring your laptop and get ready for guided, intensive learning experiences. Reserved seating is available and space is limited. ➤ View all Partner Bootcamp sessions in the event catalog. December 1

TNC305 – Transform Business Tasks into AI Actions TNC303 – Building Secure agents with Q Index

TNC302 – Build AI Agents Ready for Enterprise TNC402 – Gen AI Security Testing & Protection Strategies

TNC403 – Migrate the Well-Architected way TNC401 – Build generative AI security runbooks with Amazon Bedrock

TNC304 – Instrument Your Serverless Apps for Full Visibility TNC301 – Accelerate VMware Migration with AWS Transform

AWS Certification Connect with fellow AWS certified professionals in our dedicated lounge space at the Venetian (open Monday – Thursday 7:30 AM to 5 PM), offering refreshments, comfortable seating, and a space to watch keynotes, charge your device, and participate in special activations for the AWS certified community. re:Invent in-person attendees receive a 50% discount on any AWS Certification exam. Vouchers are valid for online-proctored or test center exams. Join these certification exam prep sessions onsite to accelerate your AWS Certification readiness:

TNC103 – AWS Escape Room: Exam Prep for Cloud Practitioner – Monday, Dec 1 at 2:30 PM – 5:30 PM TNC105 – Prepare for your next AWS Certification exam Tuesday, Dec 2 at 11 AM – 12 PM TNC108 – The value of AWS Certifications in your enterprise strategy Tuesday, Dec 2 at 11:30 AM-12:30 PM TNC204 – AWS Escape Room: Exam Prep for AWS Certified Solutions Architect – Associate – Tuesday, Dec 2 at 12:30 PM – 3:30 PM Thursday, Dec 4 at 2:30 PM – 5:30 PM TNC109 – Exam Prep: Transform Your Team's Cloud Training with Cohort-Based Training – Wednesday, Dec 3 at 11 AM – 12 PM TNC102 – AWS Escape Room: Exam Prep for AWS Certified AI Practitioner – Wednesday, Dec 3 at 12:30 PM – 3:30 PM; Thursday, Dec 4 at 8:30 AM – 11:30 AM TNC207 – Developing AI solutions: what every developer should know Thursday, Dec 4 at 2:30 PM – 3 PM TNC106 – The Evolution of AWS Skills Validation Thursday, Dec 4 at 3 PM – 3:30 PM

Connect with AWS Ambassadors With thousands of AWS enthusiasts gathering at re:Invent each year, attendees have often requested new ways to network, discover, and connect. Again this year, we have AWS Ambassadors engaged in PeerTalk, re:Invent's networking program designed to help facilitate meaningful connections. AWS Ambassadors are presenting at re:Invent breakouts, chalk talks and workshops, and supporting Partners and customers at the APN Booth. We're proud to recognize the 2025 top AWS Ambassadors from across the world. Congratulations to all of our AWS Ambassador award winners!

Get Inspired by Our Sponsoring Partners Hundreds of AWS Partners are sponsoring re:Invent 2025 and will showcase their solutions in the main Expo. Visit their booths and Lightning Theater sessions to connect. Meet our 2025 AWS re:Invent sponsors >

Stay Connected Keep up with the latest re:Invent 2025 updates and use the hashtag #reInvent to join the conversation:

Read the APN Blog for the latest updates from re:Invent Follow the AWS Partners page on LinkedIn Follow @AWS_Partners and @awsmarketplace on X/Twitter Subscribe to the APN YouTube channel Join the AWS Partner email list for APN news and events

TAGS: APN Programs, AWS Ambassadors, AWS GameDay, AWS re:Invent, AWS re:Invent 2025

Optimizing undersea cables: how Orsted and AWS modeled seabed thermal properties

This post was contributed by Ross Pivovar, Rafał Ołdziejewski, Cindy Xin Qi Lee Offshore wind farms play a critical role in the global transition to renewable energy and clean power generation. But generating electricity is only half the battle—safely and efficiently transporting that power to the grid through undersea cables is equally important. Today, we'll explore how Ørsted, a global leader in offshore wind energy, partnered with AWS to develop a proof-of-concept for a complex thermal modeling challenge for their submarine cables, enabling more efficient operations and potentially extending cable lifespan. In this post, we describe a proof-of-concept project. The solutions, methodologies, and products mentioned haven't yet been vetted for general use in Ørsted, so they represent prototype work rather than production systems. Background: why cable thermal modeling matters Undersea cables (Figure 1) are the unsung heroes of offshore wind energy. These complex structures carry electricity from offshore wind turbines to onshore substations, ultimately connecting to the power grid. For companies like Ørsted, optimizing cable performance is critical for several reasons:

High stakes failures: Undersea cables are extremely expensive and difficult to repair when they fail. Their immense size and weight make transportation challenging, and installation requires specialized vessels, specific weather windows, and highly trained personnel. Cost-revenue balance: If cables aren't utilized to their full capacity, valuable revenue is lost. Conversely, if pushed beyond safe limits, they risk premature failure. Project viability: For newer, larger wind farms, cable capacity can be the difference between a project being financially viable or not.

The goal with thermal cable models is to optimize the use of cables—from early development to operation.

Figure 1 The diagram shows offshore wind turbines connected to an offshore substation via orange "Array Cables" (66 kV), with electricity then transmitted to shore through a green "Export Cable" (220 kV) to a transformer station. This illustration demonstrates how wind-generated electricity is collected, voltage-converted at sea, and efficiently transmitted to land for distribution.

The TEEM model: predicting cable temperature To manage their cable infrastructure, Ørsted developed a thermal model using a technique called TEEM (Thermal Electrical Equivalent Model [1]). This method transforms the complex 3D heat transfer problem of an undersea cable (Figure 2) into a simplified but accurate electrical equivalent model (Figure 3). TEEM treats the cable as a series of cylindrical layers, each with specific thermal properties.

Heat generation is represented by current sources Heat transfer resistance is represented by electrical resistors Heat storage capacity is represented by capacitors

This approach allows Ørsted to predict cable temperatures based on electrical current, cable design specifications, and installation conditions. The model's key task is to determine if, for a given set of thermal conditions, the cable's maximum safe operating temperature of 90°C is exceeded. If this threshold is exceeded the insulation begins to degrade, potentially leading to failure.

Figure 2 2D cross-sectional view of a subsea power cable showing its layered construction with multiple protective layers and three cylindrical cores visible in the cross section.

Figure 3 The image displays a detailed electrical circuit diagram representing the thermal modeling of a cable system, with resistors (R1-R4) arranged in series along the top and multiple capacitors (C1-C12) connected below. Color-coded sections indicate different components of the cable structure including conductor (blue), dielectric (green), lead sheath (red), oversheath (black), filler & armour bedding (light green), armour (blue), and serving (orange), demonstrating how heat transfer in cables can be analyzed using electrical circuit analogies.

The challenge: unknown seabed properties While Ørsted had extensive knowledge about their cable designs and materials, they faced an important challenge regarding the thermal properties of the seabed materials surrounding their cables. These properties, such as thermal resistivity and heat capacity, directly impact how efficiently heat dissipates from the cable into the surrounding environment. The uncertainty stemmed from limited and coarse-grained measurements taken during the design phase, combined with the fact that these properties may change over time. This created an input-data uncertainty constraint in their thermal models. Without more accurate and localized values for seabed thermal properties, temperature predictions contained margins of uncertainty, potentially leading to either:

Overly conservative operation—sacrificing power capacity and revenue Excessive risk of overheating—jeopardizing expensive infrastructure

The challenge was clear: how could Ørsted improve the accuracy of thermal property measurements for seabed materials at specific locations along their vast submarine cable network, accounting for both spatial variation and temporal changes? The solution: combining IoT data with physics-based simulation To solve their undersea cable thermal modeling problem, Ørsted partnered with AWS to develop a solution combining real-world measurements with computational optimization. Here's how they approached it:

Ørsted compiled a comprehensive dataset by combining distributed optical sensing along the cables with additional survey information. The dataset includes:

Cable temperatures at strategic points Precise cable burial depths from geophysical surveys Geographical coordinates (organized using location identifiers called "TAZIDs" that segment the cable route into manageable sections) Ambient seawater temperatures from sensors and models

This integrated dataset was used to calibrate the thermal models.

With this data in hand, the team implemented an iterative optimization approach:

Initial estimates for thermal properties (e.g., thermal resistivity) at each location are provided from field investigations during the design process and existing soil models. Run the Thermal Electrical Equivalent Model (TEEM) to simulate expected temperatures Compare simulated results with actual measurements on the operational cable Adjust thermal properties to minimize the discrepancy Repeat until convergence

Mathematically, this process minimizes an objective function representing the error between measured and simulated temperatures:

Where represents the thermal parameters being optimized at each location, i is index for individual temperature measurements, and n is the total number of temperature measurements or data points used in the optimization process.

Beyond optimization, the team conducted rigorous statistical analysis to validate their results:

Grouped measurements by TAZID location identifiers Applied non-parametric statistical techniques to handle non-Gaussian distributions Implemented bootstrap sampling (5000 samples per location) to calculate robust confidence intervals Performed variogram analysis to assess spatial correlation between sampling points Used temporal models such as Autoregressive Integrated Moving Average (ARIMA) to perform hypothesis testing and confirm the existence of material properties that are changing over time

Figure 5 demonstrates how thermal resistivity varies significantly along the cable route, with error bars representing 1st/99th percentiles from bootstrap analysis of the median values. The analysis reveals that current thermal resistivity assessments are overly conservative, as they assume homogeneous soil properties within each TAZID. Statistical distribution fitting shows many locations exhibit multi-modal patterns consistent with soil property mixtures rather than uniform composition, indicating the current approach fails to capture the full complexity of soil characteristics within individual TAZIDs.

Figure 5 Demonstration of change in the median of each TAZID group vs distance. Error bars are based on 1/99 quantiles of 5000 sample bootstraps for the median. Medians are at the average distance for a group. The value of this optimization process extends beyond simply generating new thermal property values. By systematically comparing the optimized thermal resistivity values against the initial estimates derived from pre-construction field investigations and standard soil models, Ørsted gained insights into the accuracy of their existing modeling approaches. In many locations, the optimization revealed significant discrepancies between assumed and actual thermal properties—up to 30% difference in some seabed environments. These findings enabled Ørsted to recalibrate their baseline assumptions for future projects, refine their cable design safety margins with greater precision, and develop more sophisticated regional thermal property maps. Additionally, by identifying patterns in the discrepancies, engineering teams could investigate specific environmental factors that might explain why conventional models underestimated or overestimated thermal resistivity in particular conditions, ultimately improving the guidance provided by their existing predictive models.

Building on this foundation, the team extended their approach to simultaneously optimize multiple thermal properties—specifically thermal resistivity and volumetric heat capacity. This required implementing Simultaneous Perturbation Stochastic Approximation (SPSA), which efficiently estimates gradients in high-dimensional spaces:

is a random perturbation vector is a small positive number that decreases with iterations

The key advantage of SPSA is that it requires only two function evaluations to estimate gradients regardless of parameter dimensionality—vastly more efficient than the n+1 evaluations needed by standard finite difference methods. Since spatial statistical analysis demonstrated sufficient independence between sensor locations, the team was able to parallelize the gradient calculations across AWS infrastructure, dramatically accelerating the optimization process. Optimization results and operational impact The thermal property optimization delivered several significant analytical insights for Ørsted's undersea cable modeling:

Refined Thermal Property Understanding: The analysis revealed significant variations in thermal resistivity across different seabed locations (TAZID groups), with non-Gaussian distributions and notable outliers. This provided a more nuanced understanding of the thermal environment surrounding the cables than previously assumed values. Improved Model Accuracy: By using optimized thermal resistivity values rather than assumed constants, the TEEM simulations achieved substantially reduced error rates at specific cable distances, enabling more accurate temperature predictions. Temporal Variation Insights: ARIMA model analysis of the TEEM residuals suggested that thermal resistance may vary over time, indicating that the constant thermal resistance assumption in the current model may be limiting long-term simulation accuracy. Material Heterogeneity Confirmation: Statistical analysis revealed that observed uncertainty in thermal resistivity is primarily due to process variability rather than measurement or model error, confirming heterogeneity in seabed sediments. Computational Framework Development: A scalable computational approach was established using AWS resources, with optimal EC2 instance selection providing cost-effective execution of complex thermal simulations.

From static model to living digital twin With our established methodology now validated, the next evolution is to transform this historical-data physics model into a living digital twin. Having confirmed the presence of temporal components in seabed properties, we can now leverage the combination of sensors, TEEM modeling, and AWS's event-driven infrastructure to create an adaptive thermal model. This dynamic system continuously reassesses how soil thermal properties change over time, enabling Orsted to detect thermal anomalies before the cable temperature exceeds its limit and inform design decisions for more robust cable systems in future deployments. How AWS enabled the prototype solution The computational demands of this project were substantial. Each TEEM simulation processed millions of temperature data points, and thousands of simulations were needed for the optimization process. AWS provided the scalable computing infrastructure that made this analysis feasible.

To determine the most appropriate computing resources, the team used the CloudInstanceOptimizer tool to evaluate 114 different EC2 instance types, conducting systematic performance testing with replications to identify the optimal configuration. Figure 5 shows an example output from the optimizer. The analysis revealed that memory-optimized instances were most suitable for this workload, with each TEEM execution requiring approximately 8GB of RAM per vCPU.

Figure 6 : runtime (s) vs cost ($) for each EC2 type reviewed with the TEEM model. Each color is a different instance type. This figure can be filtered and analyzed in the streamlit app.

Each TEEM execution used Ray on AWS Batch, a distributed computing framework. Ray allows for precise control over RAM usage for each TEEM execution, which is crucial for proper utilization of the Ray cluster when scaling up calculations to review all available data as fast as possible.

For univariate analysis: 9 nodes, 1600 vCPUs, and approximately 13 terabytes of RAM For multivariate analysis: 17 nodes, 3264 vCPUs, and approximately 21 terabytes of RAM

Despite the large computational requirements, AWS's scalable infrastructure delivered results quickly:

The univariate optimization completed in less than 2 hours The more complex multivariate analysis finished in less than 3 hours

This rapid turnaround enabled the team to iterate on their approach, refine their methodologies, and conduct comprehensive statistical analysis within a reasonable timeframe. Conclusion This proof-of-concept collaboration between Ørsted and AWS demonstrates the power of combining domain expertise with cloud computing capabilities to solve complex engineering challenges. By successfully estimating the thermal properties of seabed materials, Ørsted gained valuable insights that will help them optimize their undersea cable operations, potentially extending cable lifespan and maximizing the economic value of their offshore wind projects. The project's success relied on several key elements:

Sophisticated physics-based modeling with TEEM Advanced optimization algorithms including gradient descent and SPSA Rigorous statistical analysis to quantify uncertainty Massive parallel processing enabled by AWS's scalable infrastructure

As renewable energy continues to grow, particularly offshore wind, the insights and methodologies developed in this project will help ensure that the critical undersea cable infrastructure connecting these resources to the grid operates safely, efficiently, and reliably—bringing us one step closer to a sustainable energy future. Note: The figures referenced in this blog are illustrative examples from the original project documentation. Contact Orsted for access to the complete analysis and results. References [1] Frank P. Incropera, David P. DeWitt, Theodore L. Bergman, and Adrienne S. Lavine. Fundamentals of Heat and Mass Transfer. 7th Edition, Wiley, 2011.

Ross Pivovar Ross has over 15 years of experience in a combination of numerical and statistical method development for both physics simulations and machine learning. Ross is a Senior Solutions Architect at AWS focusing on development of self-learning digital twins, multi-agent simulations, and physics ML surrogate modeling.

Cindy Xin Qi Cindy Xin Qi Lee: is a Digital Innovation Specialist at Ørsted with over 6 years of experience in strategic transformation, computational modelling, and automation across banking and renewable energy sectors. At Ørsted, she develops in-house tools for offshore wind projects, focusing on array cable layout optimization, harmonic analysis, cable thermal modelling, and structural load simulations.

Rafał Ołdziejewski Rafał Ołdziejewski: For over 11 years, Rafał has leveraged advanced mathematical and computational methods for quantum simulations in condensed matter physics and stochastic processes in the renewables sector. At Orsted, Rafał specializes in stochastic cable rating, cable thermal models, and stochastic financial planning and scheduling risk management.

Powering serverless multi-account analytics with AWS Lambda, Amazon Aurora, and Amazon Quick Suite

As a large Amazon Web Services (AWS) public sector customer, you might need information about which accounts contributed to this month's billing surge, which workloads generated the most security findings, or which services experienced the highest adoption rates. Although straightforward in single-account environments, finding such information becomes complex when organizations manage hundreds or thousands of AWS accounts. Without comprehensive visibility across multiple accounts, public sector organizations lack the information needed to make sound decisions about the costs, security, and performance of their cloud workloads. As a result, they experience adverse impacts on decision-making speed and quality. In this post, we will discuss a comprehensive analytics solution we designed, which provides public sector decision makers with the insights necessary to accelerate data-driven decision-making through a serverless, event-driven architecture. Understanding the multi-account analytics challenge Organizations rely on multi-account AWS strategies for security boundaries, cost management, and compliance across business units. However, many organizations lack full access to their payer account or centralized AWS Organizations account due to organizational constraints or partner management arrangements. This distributed approach creates visibility gaps, making comprehensive cloud infrastructure oversight difficult. The need for centralized analytics becomes critical in public sector environments where compliance and security standards demand continuous monitoring. This implementation addresses these challenges through a hub-and-spoke architecture that aggregates multi-account data into a centralized analytics solution, enabling unified visibility while maintaining the security benefits of account separation. The solution implements a serverless architecture built on proven AWS services. At its foundation, the solution uses AWS Lambda for data processing, Amazon Aurora PostgreSQL-Compatible Edition for data storage, and Amazon Quick Suite for visualization and analytics. This serverless approach is cost-effective and provides the scalability needed to handle data from hundreds of AWS accounts. The architecture consists of two primary account types. The Analytics Account serves as the central hub that receives, processes, and aggregates data from all sender accounts. The Sender Accounts act as spokes that collect and transmit their infrastructure data to the analytics account. Each sender account runs a lightweight Lambda function that gathers comprehensive data from over 20 AWS services and securely uploads it to the central Amazon Simple Storage Service (Amazon S3) bucket. The data flow follows a six-step process that begins with Amazon EventBridge triggering sender Lambda functions on a daily schedule. These functions retrieve data collection scripts from the analytics account, gather comprehensive infrastructure data, and upload daily and monthly JSON files to Amazon S3. Amazon S3 events then trigger receiver Lambda functions that process the JSON data and load it into Aurora PostgreSQL-Compatible, where it becomes available for analysis through Amazon Quick Suite dashboards. The following diagram illustrates this architecture.

Data collection capabilities This solution collects data from over 20 AWS services, delivering comprehensive cloud infrastructure visibility to organizations. The solution aggregates security and compliance data from AWS Security Hub, AWS Config, Amazon GuardDuty, AWS Identity and Access Management (IAM), and AWS CloudTrail. These services track user activity, evaluate resource configurations, and provide threat detection across AWS accounts. This solution leverages AWS Cost Explorer, AWS Systems Manager, and AWS Trusted Advisor for comprehensive cost and resource management—providing detailed analysis, usage metrics, and optimization guidance. For security posture assessment, it gathers security inventory data from AWS Key Management Service (AWS KMS), AWS WAF, AWS Secrets Manager, and AWS Certificate Manager. This data collection creates unified dashboards that deliver technical and business insights for decision-making. The solution aggregates data from multiple sources into a single view, addressing multi-account AWS environment challenges through integrated monitoring capabilities. Security architecture Multilayered security protection maintains data integrity and access control. The Amazon Aurora database operates in private subnets without internet access, following AWS best practices where database instances in private subnets can't access the internet directly. Virtual private cloud (VPC) endpoints deliver secure connectivity without internet gateways by creating private connections that don't traverse the public internet. The solution enforces least privilege access through service-specific IAM roles and automatic database credential rotation through AWS Secrets Manager. IAM policies assign permissions that determine who can manage Amazon Relational Database Service (Amazon RDS) resources, and database engine security features control database login access. Cross-account access operates through controlled S3 bucket policies that restrict data uploads to authorized sender accounts only. Security groups manage VPC and Amazon Quick Suite access, functioning as instance-level virtual firewalls alongside network access control lists (ACLs) that provide subnet-level protection. Network-level security works with application-level access controls to create defense-in-depth protection. This security model maintains strict security standards while delivering centralized analytics capabilities through multiple security control layers across all infrastructure components. The technical specifications use AWS serverless technologies, including Lambda functions running Python 3.13 with 10 GB memory allocation and 15-minute timeout configurations. The Amazon Aurora Serverless v2 database delivers Multi-AZ deployment with Data API enabled for performance and availability. The VPC architecture uses a 10.0.0.0/16 network with three private subnets distributed across Availability Zones for high availability and fault tolerance. Analytics and visualization through Amazon Quick Suite The solution uses the powerful analytics capabilities of Amazon Quick Suite to transform raw infrastructure data into actionable insights. The solution establishes secure VPC connectivity to Aurora PostgreSQL through VPC endpoints and therefore sensitive data never traverses the public internet. Amazon Quick Suite support for multiple data sources—including Amazon S3, Amazon Aurora, and traditional databases—provides flexibility for organizations with diverse data requirements. The integration with Amazon Quick Suite enables natural language querying capabilities. Users can ask questions about their infrastructure in plain English and receive immediate, data-driven answers. This democratization of analytics access means that both technical and business stakeholders can benefit from the solution's insights without requiring deep technical expertise. The solution features comprehensive dashboards that present critical insights across multiple domains of AWS infrastructure management. These dashboards visualize key metrics for account status, cost management, security assessments, configuration details, operational metrics, application health, and system resilience. With these analytics capabilities, teams can maintain continuous visibility of their compliance requirements, analyze cost trends, detect potential security issues, and fine-tune resource allocation throughout their AWS environment.

Implementation and deployment considerations Successful deployment requires careful planning and adherence to best practices. Organizations should begin with a phased rollout, starting with a pilot group of accounts before expanding to full deployment. By following this approach, teams can validate the data collection process, test dashboard functionality, and refine access controls before scaling to production environments. Sender account configuration remains minimal, requiring only the deployment of lightweight Amazon CloudFormation templates and the configuration of cross-account permissions. This streamlined approach makes it possible to add new accounts to the analytics solution with minimal technical expertise. For implementation details, refer to this GitHub repository. Best practices and operational excellence Organizations deploying this implementation must establish data governance policies that include data retention schedules and access control procedures. Continuous security monitoring and alerting protects the solution while monitoring the connected account security posture. Performance optimization through AWS Lambda monitoring and Amazon Aurora utilization tracking maintains the solution's performance when data volumes increase. Cost management requires implementing Amazon S3 lifecycle policies to control storage costs and optimizing resource usage based on utilization patterns. Organizations must establish backup strategies, implement CloudWatch alarms for critical components, and develop data validation processes for data quality and reliability. The architecture handles hundreds of sender accounts and processes large data volumes efficiently. The serverless architecture scales automatically with organizational growth, and multi-Region deployment capabilities support geographically distributed organizations. API integration plans deliver RESTful access to analytics data for programmatic integration with existing enterprise systems. Future enhancements include machine learning (ML) integration for predictive analytics and anomaly detection, extended data source support for additional AWS services and third-party integrations, and advanced visualization capabilities with custom widgets and dashboard functionality. The architecture follows AWS best practices by taking a data-driven approach to building high-performance systems. Conclusion In this post, we introduced a serverless multi-account analytics solution that provides public sector organizations with a single view across their AWS accounts, using AWS Lambda, Amazon Aurora PostgreSQL, and Amazon Quick Suite for comprehensive infrastructure insights. To learn more about implementing similar solutions in your organization, explore AWS best practices for multi-account strategies and consider how centralized analytics can enhance your cloud governance and compliance efforts. Ready to implement your own solution? Access the deployment template hosted in AWS Samples GitHub repositories today.

Arun PC Arun is a senior solutions architect at AWS based in Singapore, covering the public sector. He has 20 years of experience in pre-sales engineering, consulting, and solution architecture across ASEAN. He holds a bachelor's degree in engineering and is the holder of four US patents.

Akshat Sawhney Akshat is a senior technical account manager at AWS. He brings deep expertise in cloud infrastructure and serverless architecture, focusing on helping organizations implement comprehensive analytics solutions. Akshat is dedicated to enabling customers to make data-driven decisions through innovative AWS solutions.

Vignesh Pillai Vignesh, customer solutions manager at AWS, has over 14 years of product development and software engineering expertise in data and AI. He is passionate about helping customers build and implement scalable, resilient, and cost-effectives solutions that drive business value.

How Amazon Search increased ML training twofold using AWS Batch for Amazon SageMaker Training jobs

In this post, we show you how Amazon Search optimized GPU instance utilization by leveraging AWS Batch for SageMaker Training jobs. This managed solution enabled us to orchestrate machine learning (ML) training workloads on GPU-accelerated instance families like P5, P4, and others. We will also provide a step-by-step walkthrough of the use case implementation. Machine learning at Amazon Search At Amazon Search, we use hundreds of GPU-accelerated instances to train and evaluate ML models that help our customers discover products they love. Scientists typically train more than one model at a time to find the optimal set of features, model architecture, and hyperparameter settings that optimize the model's performance. We previously leveraged a first-in-first-out (FIFO) queue to coordinate model training and evaluation jobs. However, we needed to employ a more nuanced criteria to prioritize which jobs should run in what order. Production models needed to run with high priority, exploratory research as medium priority, and hyperparameter sweeps and batch inference as low priority. We also needed a system that could handle interruptions. Should a job fail, or a given instance type become saturated, we needed the job to run on other available compatible instance types while respecting the overall prioritization criteria. Finally, we wanted a managed solution so we could focus more on model development instead of managing infrastructure. After evaluating multiple options, we chose AWS Batch for Amazon SageMaker Training jobs because it best met our requirements. This solution seamlessly integrated AWS Batch with Amazon SageMaker and allowed us to run jobs per our prioritization criteria. This allows applied scientists to submit multiple concurrent jobs without manual resource management. By leveraging AWS Batch features such as advanced prioritization through fair-share scheduling, we increased peak utilization of GPU-accelerated instances from 40% to over 80%. Amazon Search: AWS Batch for SageMaker Training Job implementation We leveraged three AWS technologies to set up our job queue. We used Service Environments to configure the SageMaker AI parameters that AWS Batch uses to submit and manage SageMaker Training jobs. We used Share Identifiers to prioritize our workloads. Finally, we used Amazon CloudWatch to monitor and the provision of alerting capability for critical events or deviations from expected behavior. Let's dive deep into these constructs. Service environments. We set up service environments to represent the total GPU capacity available for each instance family, such as P5s and P4s. Each service environment was configured with fixed limits based on our team's reserved capacity in AWS Batch. Note that for teams using SageMaker Training Plans, these limits can be set to the number of reserved instances, making capacity planning more straightforward. By defining these boundaries, we established how the total GPU instance capacity within a service environment was distributed across different production jobs. Each production experiment was allocated a portion of this capacity through Share Identifiers. Figure 1 provides a real-world example of how we used AWS Batch's fair-share scheduling to divide 100 GPU instance between ShareIDs. We allocated 60 instances to ProdExp1, and 40 to ProdExp2. When ProdExp2 used only 25 GPU instances, the remaining 15 could be borrowed by ProdExp1, allowing it to scale up to 75 GPU instances. When ProdExp2 later needed its full 40 GPU instances, the scheduler preempted jobs from ProdExp1 to restore balance. This example used the P4 instance family, but the same approach could apply to any SageMaker-supported EC2 instance family. This ensured that production workloads have guaranteed access to their assigned capacity, while exploratory or ad-hoc experiments could still make use of any idle GPU instances. This design safeguarded critical workloads and improved overall instance utilization by ensuring that no reserved capacity went unused.

Share Identifiers. We used Share Identifiers to allocate fractions of a service environment's capacity to production experiments. Share Identifiers are string tags applied at job submission time. AWS Batch used these tags to track usage and enforce fair-share scheduling. For initiatives that required dedicated capacity, we defined preset Share Identifiers with quotas in AWS Batch. This reserved capacity for production tracks. These quotas acted as fairness targets rather than hard limits. Idle capacity could still be borrowed, but under contention, AWS Batch enforced fairness by preempting resources from overused identifiers and reassigned them to underused ones. Within each Share Identifier, job priorities ranging from 0 to 99 determined execution order, but priority-based preemption only triggered when the ShareIdentifier reached its allocated capacity limit. Figure 2 illustrates how we setup and used our share identifiers. ProdExp1 had 60 p4d instances and ran jobs at various priorities. Job A had a priority of 80, Job B was set to 50, Job C was set to at 30, and Job D had a priority 10. When all 60 instances were occupied and a new high-priority job (priority 90) requiring 15 instances was submitted, the system preempted the lowest priority running job (Job D) to make room, while maintaining the total of 60 instances for that Share Identifier.

Amazon CloudWatch. We used Amazon CloudWatch to instrument our SageMaker training jobs. SageMaker automatically publishes metrics on job progress and resource utilization, while AWS Batch provides detailed information on job scheduling and execution. With AWS Batch, we queried the status of each job through the AWS Batch APIs. This made it possible to track jobs as they transitioned through states such as SUBMITTED, PENDING, RUNNABLE, STARTING, RUNNING, SUCCEEDED, and FAILED. We published these metrics and job states to CloudWatch and configured dashboards and alarms to alert anytime we encountered extended wait times, unexpected failures, or underutilized resources. This built-in integration provided both real-time visibility and historical trend analysis, which helped our team maintain operational efficiency across GPU clusters without building custom monitoring systems. Operational impact on team performance By adopting AWS Batch for SageMaker Training jobs, we enabled experiments to run without concerns about resource availability or contention. Researchers could submit jobs without waiting for manual scheduling, which increased the number of experiments that could be run in parallel. This led to shorter queue times, higher GPU utilization, and faster turnaround of training results, directly improving both research throughput and delivery timelines. How to set up AWS Batch for SageMaker Training jobs To set up a similar environment, you can follow this tutorial, which shows you how to orchestrate multiple GPU large language model (LLM) fine-tuning jobs using multiple GPU-powered instances. The solution is also available on GitHub. Prerequisites To orchestrate multiple SageMaker Training jobs with AWS Batch, first you need to complete the following prerequisites: Clone the GitHub repository with the assets for this deployment. This repository consists of notebooks that reference assets:

Create AWS Batch resources To create the necessary resources to manage SageMaker Training job queues with AWS Batch, we provide utility functions in the example to automate the creation of the Service Environment, Scheduling Policy, and Job Queue. The service environment represents the Amazon SageMaker AI capacity limits available to schedule, expressed by maximum number of instances. The scheduling policy indicates how resource computes are allocated in a job queue between users or workloads. The job queue is the scheduler interface that researchers interact with to submit jobs and interrogate job status. AWS Batch provides two different queues we can operate with:

FIFO queues – Queues in which no scheduling policies are required Fair-share queues – Queues in which a scheduling policy Amazon Resource Name (ARN) is required to orchestrate the submitted jobs

We recommend creating dedicated service environments for each job queue in a 1:1 ratio. FIFO queues provide basic message delivery, while fair-share scheduling (FSS) queues provide more sophisticated scheduling, balancing utilization within a Share Identifier, share weights, and job priority. For customers who don't need multiple shares but would like the ability to assign a priority on job submission, we recommend creating an FSS queue and using a single share within it for all submissions.To create the resources, execute the following commands:

You can navigate the AWS Batch Dashboard, shown in the following screenshot, to explore the created resources.

ml-c5-xlarge-queue – A FIFO queue with priority 2 used for CPU workloads ml-g6-12xlarge-queue – A fair-share queue with priority 1 used for GPU workloads

The associated scheduling policy for the queue ml-g6-12xlarge-queue is with share attributes such as High priority (HIGHPRI), Medium priority (MIDPRI) and Low priority (LOWPRI) along with the queue weights. Users can submit jobs and assign them to one of three shares: HIGHPRI, MIDPRI, or LOWPRI and assign weights such as 1 for high priority and 3 for medium and 5 for low priority. Below is the screenshot showing the scheduling policy details:

For instructions on how to set up the service environment and a job queue, refer to the Getting started section in Introducing AWS Batch support for SageMaker Training Jobs blog. Run LLM fine-tuning jobs on SageMaker AI We run the notebook notebook.ipynb to start submitting SageMaker Training jobs with AWS Batch. The notebook contains the code to prepare the data used for the workload, upload on Amazon Simple Storage Service (Amazon S3), and define the hyperparameters required by the job to be executed. To run the fine-tuning workload using SageMaker Training jobs, this example uses the ModelTrainer class. The ModelTrainer class is a newer and more intuitive approach to model training that significantly enhances user experience. It supports distributed training, build your own container (BYOC), and recipes. For additional information about ModelTrainer, you can refer to Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK – Part 1: ModelTrainer. To set up the fine-tuning workload, complete the following steps:

Select the instance type, the container image for the training job, and define the checkpoint path where the model will be stored:

Create the ModelTrainer function to encapsulate the training setup. The ModelTrainer class simplifies the experience by encapsulating code and training setup. In this example:

SourceCode – The source code configuration. This is used to configure the source code for running the training job by using your local python scripts. Compute – The compute configuration. This is used to specify the compute resources for the training job.

from sagemaker.modules.configs import Compute, OutputDataConfig, SourceCode, StoppingCondition from sagemaker.modules.distributed import Torchrun from sagemaker.modules.train import ModelTrainer

# Define the script to be run source_code = SourceCode( source_dir="./scripts", requirements="requirements.txt", entry_script="train.py", )

# Define the compute compute_configs = Compute( instance_type=instance_type, instance_count=instance_count, keep_alive_period_in_seconds=0 )

# Define the ModelTrainer model_trainer = ModelTrainer( training_image=image_uri, source_code=source_code, base_job_name=job_name, compute=compute_configs, distributed=Torchrun(), stopping_condition=StoppingCondition(max_runtime_in_seconds=7200), hyperparameters={ "config": "/opt/ml/input/data/config/args.yaml" }, output_data_config=OutputDataConfig(s3_output_path=output_path), role=role, )Python

Set up the input channels for ModelTrainer by creating InputData objects from the provided S3 bucket paths for the training and validation datasets:

train_input = InputData( channel_name="train", data_source=train_dataset_s3_path, ) val_input = InputData( channel_name="val", data_source=val_dataset_s3_path, ) config_input = InputData( channel_name="config", data_source=train_config_s3_path, )

Queue SageMaker Training jobs This section and the following are intended to be used interactively so that you can explore how to use the Amazon SageMaker Python SDK to submit jobs to your Batch queues. Follow these steps:

In the next cell, submit two training jobs in the queue:

job_name_1 = job_name + "-low-pri" queued_job_1 = queue.submit( model_trainer, TRAINING_INPUTS, job_name_1, priority=5, share_identifier="LOWPRI" ) job_name_2 = job_name + "-mid-pri" queued_job_2 = queue.submit( model_trainer, TRAINING_INPUTS, job_name_2, priority=3, share_identifier="MIDPRI" )Code

Display the status of running and in queue jobs We can use the job queue list and job queue snapshot APIs to programmatically view a snapshot of the jobs that the queue will run next. For fair-share queues, this ordering is dynamic and occasionally needs to be refreshed because new jobs are submitted to the queue or as share usage changes over time.

The following screenshot shows the jobs submitted with low priority and medium priority in the Runnable State and in the queue.

You can also refer to the AWS Batch Dashboard, shown in the following screenshot, to analyze the status of the jobs.

As shown in the following screenshot, the first job executed with the SageMaker Training job is the MEDIUM PRIORITY one, by respecting the scheduling policy rules defined previously.

You can explore the running training job in the SageMaker AI console, as shown in the following screenshot.

Submit an additional job You can now submit an additional SageMaker Training job with HIGH PRIORITY to the queue:

job_name_3 = job_name + "-high-pri" queued_job_3 = queue.submit( model_trainer, TRAINING_INPUTS, job_name_3, priority=1, share_identifier="HIGHPRI" )Code

You can explore the status from the dashboard, as shown in the following screenshot.

The HIGH PRIORITY job, despite being submitted later in the queue, will be executed before the other runnable jobs by respecting the scheduling policy rules, as shown in the following screenshot.

As the scheduling policy in the screenshot shows, the LOWPRI share has a higher weight factor (5) than the MIDPRI share (3). Since a lower weight signifies higher priority, a LOWPRI job will be executed after a MIDPRI job, even if they are submitted at the same time.

Clean up To clean up your resources to avoid incurring future charges, follow these steps:

Verify that your training job isn't running anymore. To do so, on your SageMaker console, choose Training and check Training jobs. Delete AWS Batch resources by using the command python create_resources.py --clean from the GitHub example or by manually deleting them from the AWS Management Console.

Conclusion In this post, we demonstrated how Amazon Search used AWS Batch for SageMaker Training Jobs to optimize GPU resource utilization and training job management. The solution transformed their training infrastructure by implementing sophisticated queue management and fair share scheduling, increasing peak GPU utilization from 40% to over 80%.We recommend that organizations facing similar ML training infrastructure challenges explore AWS Batch integration with SageMaker, which provides built-in queue management capabilities and priority-based scheduling. The solution eliminates manual resource coordination while providing workloads with appropriate prioritization through configurable scheduling policies. To begin implementing AWS Batch with SageMaker Training jobs, you can access our sample code and implementation guide in the amazon-sagemaker-examples repository on GitHub. The example demonstrates how to set up AWS Identity and Access Management (IAM) permissions, create AWS Batch resources, and orchestrate multiple GPU-powered training jobs using ModelTrainer class.

The authors would like to thank Charles Thompson and Kanwaljit Khurmi for their collaboration. About the authors

Mona Mona Mona is a generative AI Specialist Solutions Architect at Amazon focusing. She is a published author of two books – Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide.

Mayank Jha Mayank is a Senior Machine Learning Engineer at Amazon Search working on the model training optimization. He is passionate about finding practical applications for complex problems at hand and aims to develop solutions that have a deep impact on how businesses and people thrive.

Bruno Pistone Bruno is a Senior generative AI and ML Specialist Solutions Architect for AWS based in Milan. He works with large customers helping them to deeply understand their technical needs and design AI and Machine Learning solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He enjoys spending time with his friends and exploring new places, as well as travelling to new destinations.

James Park James is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In his spare time he enjoys seeking out new cultures, new experiences, and staying up to date with the latest technology trends.

In today's security-conscious world, organizations are implementing strong authentication methods for applications running inside virtual desktop environments. A common challenge is enabling users to leverage their local security keys and biometric devices with applications running inside a remote desktop session. Amazon WorkSpaces now addresses this challenge with WebAuthn redirection, allowing users to seamlessly use their local FIDO2-compatible authentication devices with applications running inside their Windows and Linux WorkSpaces. This redirection capability bridges the gap between physical and virtual environments, enhancing security without compromising productivity. What is WebAuthn and why does it matter in virtual desktops? Web Authentication (WebAuthn) is an open standard that enables strong, passwordless authentication using hardware security keys, biometrics, and platform authenticators like Windows Hello. By using WebAuthn redirection in Amazon WorkSpaces, users can leverage the same authentication methods in their virtual desktop that they use on their local devices, providing a consistent experience across environments while enhancing security. WebAuthn redirection bridges the gap between your local authentication devices and remote applications. When an application inside your WorkSpace requests WebAuthn authentication, that request is securely redirected to your local device, allowing you to authenticate with your security key or biometric authenticator. Implementation Options Amazon WorkSpaces offers two approaches to WebAuthn integration: Standard WebAuthn (Windows and Linux WorkSpaces) uses browser extensions to facilitate WebAuthn redirection, ensuring compatibility across different platforms. This implementation works with popular browsers including Microsoft Edge, Google Chrome, and Chromium. Enhanced WebAuthn (Windows WorkSpaces) eliminates the need for browser extensions, providing a streamlined experience. Key benefits include:

No browser extension required Support for WebAuthn in both web browsers and native Windows applications Improved authentication performance Seamless experience across different application types.

Both approaches enable users to authenticate with a variety of FIDO2-compliant devices including YubiKeys, Windows Hello, and other biometric authenticators. Business Benefits WebAuthn Redirection for Amazon WorkSpaces helps organizations:

No USB redirection required – Use security keys and biometric authenticators without the complexity of USB device management Seamless authentication experience – Users authenticate to applications in their WorkSpace using their local devices without additional configuration Consistent security implementation – Apply the same authentication policies across physical workstations and virtual desktops Zero additional hardware cost – Leverage existing authentication devices without purchasing specialized equipment for virtual environments

Availability WebAuthn support is available today in all AWS Regions where Amazon WorkSpaces is offered. It works with:

Windows WorkSpaces bundles with DCV (both Enhanced and Standard WebAuthn) Linux WorkSpaces bundles (Standard WebAuthn) Amazon WorkSpaces clients for Windows, macOS, and Linux

There is no additional charge for using WebAuthn redirection with Amazon WorkSpaces. Getting Started To enable WebAuthn redirection in your WorkSpaces environment:

Ensure your WorkSpaces are running the latest version of the WSP host agent (version 2.0.0.1425 or higher on Windows and version 2.1.0.1923 or higher on Linux WorkSpaces) Verify your WorkSpaces client is updated to version 5.29.0 or higher Ensure WebAuthn is enabled through GPO on the WorkSpace. The feature is enabled by default.

Conclusion WebAuthn redirection in Amazon WorkSpaces eliminates the traditional barriers between local authentication devices and virtual desktop applications. By enabling your users to authenticate with their familiar FIDO2-compatible devices directly within their WorkSpaces, you can strengthen security while maintaining the productivity your organization depends on. Get started today with WebAuthn redirection and transform how your users authenticate in virtual desktop environments. For detailed implementation instructions, refer to the WebAuthn Redirection section of the WorkSpaces administrator guide.

. . Chirag is a Senior Product Manager for Amazon DCV, where he works with enterprise customers on remote work and virtualization solutions. He is passionate about building products that solve real business challenges while delivering intuitive user experiences.

Application authorization is a critical component of modern software systems, determining what actions users can perform on specific resources. Many organizations have adopted Open Policy Agent (OPA) with its Rego policy language to implement fine-grained authorization controls across their applications and infrastructure. While OPA has proven effective for policy-as-code implementations, organizations are increasingly looking for more performant and managed services that reduce operational overhead while maintaining the flexibility and power of policy-based authorization. Amazon Verified Permissions is a fully managed authorization service that uses the Cedar policy language to help you implement fine-grained permissions for your applications. Cedar is an open source policy language developed by AWS that provides many of the same capabilities as Rego while offering improved performance (42–60 times faster than Rego), straightforward policy authoring, and formal verification capabilities. By migrating from OPA to Verified Permissions, organizations can reduce the operational burden of managing authorization infrastructure while gaining access to a service designed specifically for scalable, secure authorization. This migration offers several key benefits: reduced infrastructure management overhead, improved policy performance and validation, enhanced security through the AWS managed service model, and seamless integration with other AWS services. Additionally, Cedar's syntax is designed to be more intuitive than Rego, reducing the effort needed to write, read, and maintain policies. In this post, we explore the process of migrating from OPA and Rego to Verified Permissions and Cedar, including policy translation strategies, software development and testing approaches, and deployment considerations. We walk through practical examples that demonstrate how to convert common Rego policies to Cedar policies and integrate Verified Permissions into your existing applications.

The migration from OPA to Verified Permissions represents a shift from self-managed authorization infrastructure to a fully managed service. In a typical OPA setup, customers have OPA servers running either as sidecars, standalone services, or embedded libraries that evaluate Rego policies against incoming authorization requests. These servers pull policy bundles from storage systems and maintain their own performance and availability. With Verified Permissions, AWS manages the entire authorization infrastructure. Applications make API calls to the Verified Permissions service which evaluates Cedar policies stored in managed policy stores. This removes the need to operate and maintain OPA servers, manage policy distribution, or handle service scaling and availability. This shift means that your team can concentrate on authorization logic rather than infrastructure management while gaining the benefits of the scale and reliability provided by AWS.

It's important to understand the fundamental differences between the Rego and Cedar policy languages before beginning your migration. These differences will shape how you approach translating your existing policies.

Rego policies are built around rules that can be evaluated to produce sets of results. Rego uses a logic programming approach where you define conditions that must be satisfied for a rule to be true. Policies often involve complex queries, loops, and comprehensions to examine data structures. Example Rego policy

# Rule 1: Allow users with the viewer role to read documents allow { input.action == "read" input.resource.type == "document" input.user.role == "viewer" } # Rule 2: Allow users with the editor role to write documents allow { input.action == "write" input.resource.type == "document" input.user.role == "editor" }

Cedar takes a more declarative approach with explicit permit and forbid statements. Each Cedar policy is a standalone authorization decision that clearly states what is being allowed or denied. Cedar policies are designed to be human-readable and straightforward to audit. Equivalent Cedar policies

// Policy 1: Allow principals with the viewer role to read documents permit ( principal in UserRole::"viewer", action == Action::"read", resource in ResourceType::"document" ); // Policy 2: Allow principals with the editor role to write documents permit ( principal in UserRole::"editor", action == Action::"write", resource in ResourceType::"document" );

One of the most significant differences between the two evaluation engines is how they handle data. Rego works with arbitrary JSON input data, giving users complete flexibility in how they structure authorization requests. Users can access any field in your input data using Rego's path notation. Cedar allows for the creation of a defined schema with typed entities. This means that users need to model authorization data as entities with specific types, attributes, and relationships. While this requires more upfront planning, it provides superior validation, runtime performance, and tooling support.

Rego and Cedar differ fundamentally in their approaches to policy evaluation. Rego uses a logic programming model and, as a result, policy evaluation functions much like a logic puzzle solver. It starts with a question and searches backward through linked rules to find an answer. This approach allows for flexible policy composition but can often be slower, less predictable, and more difficult to audit. Cedar, on the other hand, uses a simpler functional evaluation approach. It uses a straightforward evaluation model where each policy is checked independently against the authorization request. Policies use basic conditional logic to produce fast, deterministic allow or deny decisions. A policy either fully matches the authorization request (principal, action, resource, and all conditions), or it doesn't apply. This is essential for high-performance authorization scenarios where predictable evaluation time and clear audit trails are essential. Cedar policy evaluation follows four core principles:

Default deny for access not explicitly granted Forbid overrides permit for handling policy conflicts Order-independent evaluation to prevent bugs Deterministic outcomes for reliable results

Before you can begin migrating your authorization policies, you need to establish the foundational infrastructure in Verified Permissions.

To illustrate the migration process, you will use a fictional document management application that uses OPA and Rego for authorization. The first step in migrating to Verified Permissions is creating a policy store. A policy store is a container for your Cedar policies and schema. You can create multiple policy stores for different applications or environments. When creating a policy store, you choose between two validation modes:

STRICT mode: Requires a schema against which policies are validated OFF mode: Allows policies without a schema (useful for initial testing)

For production migrations, STRICT mode is recommended because it provides better validation compared to OFF mode and can enable optimizations that reduce the entity data needed for authorization requests. You can create a policy store through the AWS Management Console, AWS Command Line Interface (AWS CLI), or programmatically using AWS SDKs. The following example uses the AWS CLI:

aws verifiedpermissions create-policy-store \ --region us-east-1 \ --validation-settings mode=STRICT \ --description "Migration from OPA to Amazon Verified Permissions"

If the request is successful, you should see a JSON encoded response that looks like the following:

{ "policyStoreId": "PSEXAMPLEabcdefg012345", "arn": "arn:aws:verifiedpermissions:us-east-1:123456789012:policy-store/PSEXAMPLEabcdefg012345", "createdDate": "2025-09-15T10:30:45.123456+00:00", "lastUpdatedDate": "2025-09-15T10:30:45.123456+00:00" }

Make note of the policyStoreId from the response—you will need it for subsequent operations.

In STRICT mode, Verified Permissions requires a Cedar schema that defines the types of entities in an authorization system. This schema serves several important purposes, including validating policies at creation time, enabling entity slicing performance optimizations, enabling better tooling and IDE support, and documenting your authorization model. The schema should define:

Entity types: The kinds of objects in your system (for example, users, roles, documents, and so on.) Attributes: Properties that entities can have (for example, department, classification, and createdDate) Actions: Operations that can be performed (for example, read, write, and delete) Relationships: How entities relate to each other (for example, user belongs to role, document owned by user)

When designing a schema, you should consider how your current OPA input data maps to Cedar entities. For example, if your Rego policies access input.user.department, you will need a User entity type with a department attribute. The following is an example Cedar schema for your document management application:

{ "MyApp": { "entityTypes": { "User": { "shape": { "type": "Record", "attributes": { "department": {"type": "String"}, "jobLevel": {"type": "Long"}, "email": {"type": "String"} } } }, "Role": { "shape": { "type": "Record", "attributes": {"name": {"type": "String"}} } }, "Document": { "shape": { "type": "Record", "attributes": { "owner": {"type": "Entity", "name": "User"}, "classification": {"type": "String"}, "createdDate": {"type": "String"} } } } }, "actions": { "read": {"appliesTo": {"principalTypes": ["User"], "resourceTypes": ["Document"]}}, "write": {"appliesTo": {"principalTypes": ["User"], "resourceTypes": ["Document"]}}, "delete": {"appliesTo": {"principalTypes": ["User"], "resourceTypes": ["Document"]}} } } }

To apply this schema to the policy store you created earlier using the AWS CLI, you can run the following command:

aws verifiedpermissions put-schema \ --region us-east-1 \ --policy-store-id YOUR_POLICY_STORE_ID \ --definition file://schema.json

Ensure that you replace YOUR_POLICY_STORE_ID with the policyStoreId that was returned when you created your policy store. You can view the visualized policy schema (shown in Figure 1) in the Verified Permissions console by going to Policy Store and choosing Schema.

With your policy store and schema in place, you can now begin translating your Rego policies into Cedar policies, following common authorization patterns.

Role-based access control (RBAC) is one of the most used authorization patterns. In RBAC systems, users are assigned roles, and roles are granted permissions to perform actions on resources. In your current Rego implementation, you might check if a user has a specific role in their roles array, then allow certain actions based on that role. Your Rego policy might look something like the following:

allow if { input.user.roles[_] == "editor" input.action in ["read", "write"] }

allow if { input.user.roles[_] == "viewer" input.action == "read" }

When migrating to Cedar, you will model this using entity relationships where users belong to role entities.

// Admin users can perform any action on any resource permit ( principal in MyApp::Role::"admin", action, resource );

// Editor users can read and write on every resource permit ( principal in MyApp::Role::"editor", action in [MyApp::Action::"read", MyApp::Action::"write"], resource );

// Viewer users can only read on every resource permit ( principal in MyApp::Role::"viewer", action == MyApp::Action::"read", resource );

Migration approach To successfully migrate your RBAC policies from Rego to Cedar, follow these steps:

Define User and Role entity types in your schema Create permit policies for each role-action combination Use the Cedar in operator to check role membership Consider creating role hierarchies if you have nested roles

Key differences Understanding the fundamental differences between Rego and Cedar's approach to RBAC will help you design more effective policies:

Cedar uses entity relationships instead of checking array membership Each permission becomes a separate, explicit policy Role hierarchies are modeled through entity parent-child relationships

Attribute-based access control (ABAC) makes authorization decisions based on attributes of the user, resource, action, and environment. This is often more flexible than RBAC but can be more complex to implement. In Rego, you would access various attributes from the input data and use them in policy conditions:

default allow := false # Anyone can read public documents allow if { input.action == "read" input.resource.classification == "public" }

# Users can read internal documents from their department allow if { input.action == "read" input.resource.classification == "internal" input.user.department == input.resource.department }

# Users can write to documents they own allow if { input.action == "write" input.resource.owner == input.user.id }

Cedar handles this through entity attributes and policy conditions using the when and unless clauses.

// Anyone can read public documents. Blank 'principal' and 'resource' entities are wildcards that match everything permit ( principal, action == MyApp::Action::"read", resource ) when { resource.classification == "public" };

// Users can read internal documents from their department permit ( principal, action == MyApp::Action::"read", resource ) when { resource.classification == "internal" && principal.department == resource.department };

// Users can write to documents they own permit ( principal, action == MyApp::Action::"write", resource ) when { resource.owner == principal };

Migration approach Migrating ABAC policies requires careful mapping of attributes from your Rego input structure to Cedar's entity model:

Identify the attributes used in your current policies Map these attributes to entity attributes in your Cedar schema Use when clauses in Cedar policies to implement attribute-based conditions Consider using context for environment-specific attributes (time, IP address, and so on)

Key differences Cedar's schema-driven approach to attributes provides several advantages over Rego's dynamic attribute access:

Cedar requires attributes to be defined in the schema Cedar schema validation helps catch attribute access errors at policy creation time Complex attribute logic might need to be split across multiple policies

Relationship-based access control (ReBAC) grants permissions based on properties of the resource being accessed or relationships between the user and the resource (such as ownership). In Rego, this might be expressed as follows:

# Allow document owners to perform any action allow if { input.resource.type == "document" input.resource.owner_id == input.user.id }

# Alternative: checking ownership through a separate ownership data structure allow if { input.resource.type == "document" ownership := data.ownerships[input.resource.id] ownership.owner_id == input.user.id }

In the preceding example, ownership is checked by comparing the owner_id attribute on the resource with the user's ID. You might access this from the input data directly or from a separate data source. In Cedar, relationships are first-class concepts. The resource.owner == principal syntax directly checks if the principal is the owner entity referenced by the resource. This is more natural and type-safe than string comparisons:

permit ( principal, action, resource is MyApp::Document ) when { resource.owner == principal };

Migration approach Converting relationship-based policies requires modeling your data relationships as Cedar entity references:

Model resources as Cedar entities with relevant attributes Use resource attributes in policy conditions Model ownership and other relationships through entity references Use Cedar's attribute access syntax for resource properties

Many authorization systems need to consider contextual information such as time of day, user location, or request characteristics (IP address, user-agent, and so on). Expressing this in Rego would look like the following example:

default allow := false # Allow read access during business hours (9 AM to 5 PM UTC) allow if { input.action == "read" current_hour := time.clock([time.now_ns(), "UTC"])[0] current_hour >= 9 current_hour <= 17 }

In Cedar, the same policy logic can be expressed like the following:

// Allow read access during business hours (9 AM to 5 PM UTC) permit ( principal, action == MyApp::Action::"read", resource ) when { context.currentTime.hour >= 9 && context.currentTime.hour <= 17 };

Migration approach Context-based policies in Cedar use the context parameter passed with each authorization request:

Use Cedar's context feature for environment information Pass time-based information in the authorization request context Create policies with time-based conditions using context attributes Consider caching implications for time-sensitive policies

After migrating your policies to Cedar, you need to update your application code to integrate with Verified Permissions.

The most significant change in your application code will be replacing OPA API calls with Verified Permissions API calls. Understanding the differences between these systems will help you plan your integration work effectively. The sample code in this section is written in Python.

When calling OPA, you typically send a single JSON payload containing the authorization data. For example, your current OPA request might look like the following:

opa_request = { "input": { "user": { "id": "user123", "department": "engineering", "role": "editor" }, "resource": { "id": "doc456", "type": "document", "owner": "user123" }, "action": "read" } }

response = requests.post( " ", json=opa_request ) authorized = response.json()["result"]

Verified Permissions requires a more structured approach where principals, resources, and actions are explicitly typed entities.

import boto3 import json from typing import Dict, Any, List

class AuthorizationService: def __init__(self, policy_store_id: str, region: str = 'us-east-1'): self.client = boto3.client('verifiedpermissions', region_name=region) self.policy_store_id = policy_store_id

#Check if a principal is authorized to perform an action on a resource. def is_authorized(self, principal: Dict[str, Any], action: str, resource: Dict[str, Any], context: Dict[str, Any] = None) -> bool: try: # Convert to Cedar entity format principal_entity = self._to_cedar_entity(principal, "User") resource_entity = self._to_cedar_entity(resource, "Document") action_entity = {"actionType": "MyApp::Action", "actionId": action}

request = { 'policyStoreId': self.policy_store_id, 'principal': principal_entity, 'action': action_entity, 'resource': resource_entity }

response = self.client.is_authorized(**request) return response['decision'] == 'ALLOW' except Exception as e: print(f"Authorization error: {e}") return False

def _to_cedar_entity(self, entity_data: Dict[str, Any], entity_type: str) -> Dict[str, Any]: # Convert application data to Cedar entity format return { 'entityType': f'MyApp::{entity_type}', 'entityId': str(entity_data.get('id', '')), 'attributes': entity_data }

Entity type declarations: Each entity (principal, resource) must include an entityType that matches your Cedar schema Entity IDs: Every entity requires a unique entityId for identification Action format: Actions are specified with an actionType and actionId rather than as simple strings Separate context: Environmental information like time, IP address, or user agent is passed in a separate context parameter

OPA returns whatever your Rego policy outputs, which could be a Boolean, a set of allowed actions, or complex nested data structures. Regardless of the policy outputs, Verified Permissions returns a consistent authorization decision structure:

# Amazon Verified Permissions response structure { 'decision': 'ALLOW',# or 'DENY' 'determiningPolicies': [...],# Which policies determined the decision 'errors': [...]# Errors that occurred during evaluation }

Your application logic becomes simpler because you need to check for only ALLOW or DENY:

# Example principal (user) user = { 'id': 'user123', 'department': 'engineering', 'jobLevel': 5, 'email': [EMAIL] }

# Example resource (document) document = { 'id': 'doc456', 'owner': 'user123', 'classification': 'internal', 'department': 'engineering' }

# Example context context = { 'currentHour': 14,# 2 PM 'userAgent': 'MyApp/1.0' }

# Check authorization can_read = auth_service.is_authorized(user, 'read', document, context) can_write = auth_service.is_authorized(user, 'write', document, context)

print(f"User can read document: {can_read}") print(f"User can write document: {can_write}")

OPA errors typically relate to policy evaluation issues or server connectivity problems. With Verified Permissions, you'll encounter AWS-specific error types, as shown in the following example:

def is_authorized_with_error_handling(self, principal, action, resource, context=None): try: principal_entity = self._to_cedar_entity(principal, "User") resource_entity = self._to_cedar_entity(resource, "Document") action_entity = {"actionType": "MyApp::Action", "actionId": action}

response = self.client.is_authorized(**request) return response['decision'] == 'ALLOW' except ClientError as e: error_code = e.response['Error']['Code']

if error_code == 'ResourceNotFoundException': print(f"Policy store not found: {self.policy_store_id}") elif error_code == 'ValidationException': print(f"Invalid request: {e.response['Error']['Message']}") elif error_code == 'ThrottlingException': print("Request throttled - consider implementing exponential backoff") else: print(f"AWS error: {error_code}")

It's important to note that the AWS SDK provides built-in retry logic for transient failures. The following is an example of how you can enable this feature:

# Configure retry behavior config = Config( retries={ 'max_attempts': 3, 'mode': 'adaptive'# Automatically adjusts retry behavior }, connect_timeout=5, read_timeout=10 )

Your current authorization data needs to be transformed into Cedar's entity format. This transformation happens in the _to_cedar_entity method shown in the error handling changes example, but let's break down what's involved. Extracting entity information Identify which parts of your current OPA input represent the principal, resource, and action. In most OPA implementations, this mapping is straightforward:

# Current OPA structure opa_input = { "user": {...},# This becomes the principal "resource": {...},# This becomes the resource "action": "read"# This becomes the action }

# Map to Cedar structure principal = opa_input["user"] resource = opa_input["resource"] action = opa_input["action"]

Adding type information Cedar requires explicit type declarations for all entities. You'll need to determine the appropriate entity type based on your schema:

def _determine_entity_type(self, entity_data: Dict[str, Any]) -> str: # Determine the Cedar entity type based on entity data. This logic will be specific to your application. # Example: determine type based on entity structure or type field if 'role' in entity_data: return 'User' elif 'document_type' in entity_data: return 'Document' elif 'name' in entity_data and 'member_count' in entity_data: return 'Team' else: raise ValueError(f"Cannot determine entity type for: {entity_data}")

def _to_cedar_entity(self, entity_data: Dict[str, Any], entity_type: str = None) -> Dict[str, Any]: # Convert application data to Cedar entity format. if entity_type is None: entity_type = self._determine_entity_type(entity_data)

return { 'entityType': f'MyApp::{entity_type}', 'entityId': str(entity_data.get('id', '')), 'attributes': entity_data }

Structuring attributes Cedar attributes must match your schema definition, so you might need to transform attribute names or values. This is also a chance to iterate and improve on naming. The following example demonstrates a code pattern to convert attribute names and values in code.

def _prepare_attributes(self, entity_data: Dict[str, Any], entity_type: str) -> Dict[str, Any]: #Prepare entity attributes according to Cedar schema requirements. attributes = {}

if entity_type == 'User': # Map OPA field names to Cedar schema field names attributes = { 'department': entity_data.get('dept', entity_data.get('department')), 'jobLevel': int(entity_data.get('job_level', entity_data.get('jobLevel', 0))), 'email': entity_data.get('email', entity_data.get('email_address')) } elif entity_type == 'Document': attributes = { 'classification': entity_data.get('classification','internal'), 'department': entity_data.get('department'), 'owner': entity_data.get('owner', entity_data.get('owner_id')) }

# Remove None values return {k: v for k, v in attributes.items() if v is not None}

Handling context Separate environmental information from entity data. Context information should not be part of entity attributes.

# Entity data only includes intrinsic properties principal = { 'id': user_data['id'], 'department': user_data['department'], 'jobLevel': user_data['job_level'] }

resource = { 'id': resource_data['id'], 'classification': resource_data['classification'], 'owner': resource_data['owner'] }

# Context includes environmental and request-specific data context = {} if request_metadata: context = { 'currentHour': request_metadata.get('hour'), 'ipAddress': request_metadata.get('ip_address'), 'userAgent': request_metadata.get('user_agent'), 'requestTime': request_metadata.get('timestamp') } return self.is_authorized(principal, action, resource, context)

The most critical aspect of migration testing is verifying that you have correctly migrated your authorization logic from Rego to Cedar. This requires systematic testing with comprehensive test cases.

Inventory current policies: Document your current Rego policies, including their decision logic, input data requirements, and expected outcomes for key test scenarios Create test scenarios: Develop test cases covering all policy branches and edge cases Capture current behavior: Run your test cases against OPA to establish baseline results Test Cedar policies: Run the same test cases against your Cedar policies Analyze differences: Investigate mismatches and adjust policies accordingly

When testing your policies, start with basic, straightforward policies before tackling complex ones. Test both positive cases (should be allowed) and negative cases (should be denied) and include edge cases and boundary conditions. Additionally, test with real production data (anonymized if necessary) to verify that your policies will work effectively when implemented in production. It's also important to compare the performance characteristics of your OPA setup with Verified Permissions across several key metrics. These metrics should include average response time for authorization requests, throughput (requests per second), and error rates under normal and stress conditions. During testing, test from the actual deployment environment used by your application and account for network latency to AWS services. Finally, you should test the complete integration between your application and Verified Permissions across several critical areas. Your integration testing should cover authentication and AWS credential handling, request/response data transformation, error handling and fallback scenarios, connection pooling and resource management, and logging and monitoring integration to help ensure that the components work together seamlessly.

A successful migration from OPA to Verified Permissions requires careful planning and a risk-managed deployment approach that minimizes disruption to your production systems.

Rather than switching entirely to Verified Permissions in a single step, implement a phased migration to reduce risk.

Parallel deployment: Deploy Verified Permissions alongside your existing OPA infrastructure and route a small percentage of authorization requests to the new system. Log and compare results between both systems, focusing on non-critical operations initially to minimize risk during the transition process. Gradual traffic shift: Gradually increase the percentage of requests routed to Verified Permissions while monitoring system performance, error rates, and authorization accuracy. Implement circuit breaker patterns to fall back to OPA if needed and expand to more critical operations as your confidence grows in the reliability and performance of the new system. Full migration: Route all traffic to Verified Permissions but keep OPA infrastructure running temporarily. Monitor system behavior under full production load and decommission OPA infrastructure after stability is confirmed and you are confident in the performance of the new system.

Use feature flags to control the migration process through various flag types. These include percentage-based rollout to route a specific percentage of requests to the new system, user-based rollout to route specific users or user groups to the new system, operation-based rollout to route specific types of operations to the new system, and environment-based rollout to use different systems in different environments. Feature flags provide several benefits, including instant rollback capability if issues arise, granular control over migration scope, A/B testing of authorization decisions, and safe experimentation with new policies.

When migrating from Rego to Cedar, you might encounter several common issues. In this section, you'll find a troubleshooting guide.

Some Rego policies use complex logic that doesn't directly translate to Cedar. For example:

# Complex Rego policy with loops and comprehensions allow { some i # The i variable is used to iterate over the items in the input.user.permissions array input.user.permissions[i].resource == input.resource.id input.user.permissions[i].actions[_] == input.action # The wildcard _ is used to iterate over the items in the actions array }

In these scenarios, you should restructure your data model to work better with Cedar's entity-based approach. For example, Cedar provides the in operator for improved performance and readability, as shown in the following example:

permit ( principal, action, resource ) when { principal has permission && resource in principal.permission.resources && action in principal.permission.actions };

You can use the schema validation tools provided by Verified Permissions to triage these issues.

Adhering to the following recommendations and best practices will help you build a maintainable, secure, and performant authorization system with Verified Permissions.

Well-designed policies are the foundation of a reliable authorization system and directly impact maintainability and security:

Schema-first design: Start with a comprehensive schema design before writing policies. A well-designed schema makes policy authoring more maintainable. Basic, explicit policies: Favor multiple basic policies over complex monolithic ones. Cedar's explicit permit/forbid model works best with clear, straightforward policy statements. Meaningful naming: Use descriptive names for entity types, attributes, and policy descriptions. This improves understandability and maintainability of polices. Documentation: Document your authorization model, including entity relationships, policy intentions, and business rules.

Successfully migrating your authorization system requires balancing speed with safety through deliberate, incremental steps:

Incremental approach Don't attempt to migrate everything at once. Start with basic, low-risk policies and gradually move to more complex scenarios. Start in audit mode: Calculate and log the policy decisions for both systems. This will help you to compare results without impacting runtime authorization. Comprehensive testing: Invest heavily in testing during migration. The cost of thorough testing is much less than the cost of authorization failures in production. Parallel operations: Run both systems in parallel during migration to validate policy behavior and build confidence in the new system. Team training: Ensure your team understands Cedar's policy model and syntax. The conceptual differences from Rego require a learning investment.

Maintaining a production authorization system requires ongoing attention to operational concerns beyond the initial migration:

Version control: Treat policies as code with proper version control, code review, and deployment processes. Monitoring and alerting: Implement comprehensive monitoring from day one. Authorization issues can have significant business impact. Regular audits: Periodically review and audit policies to verify that they still meet business requirements and security standards. Performance optimization: Continuously monitor and optimize performance, particularly around caching strategies and policy efficiency.

Migrating from Open Policy Agent to Amazon Verified Permissions represents a significant step toward reducing operational overhead, improving runtime authorization performance and enhancing governance while maintaining robust authorization capabilities. The migration journey from OPA to Verified Permissions isn't only about changing technologies, it's an opportunity to improve your authorization architecture, enhance security practices, and build a more scalable foundation for your application's access control needs. Thank you for reading this post. If you have comments or questions about migrating from OPA to Verified Permissions, leave them in the comments section below.

The following links provide resources for further reading on the topics covered in this blog post:

Amazon Verified Permissions User Guide Cedar Policy Language Specification Simplify How You Manage Authorization in Your Applications with Amazon Verified Permissions Cedar Policy Language on GitHub Amazon Verified Permissions API Reference

If you have feedback about this post, submit comments in the Comments section below. If you have questions about this post, contact AWS Support.

Samuel Folkes Samuel is a Senior Security Solutions Architect at Amazon Web Services with more than 18 years of experience in software architecture, networking, and cybersecurity. Prior to AWS, he worked as a software engineer and led engineering teams across multiple industries. Samuel specializes in identity and access management and is passionate about using emerging technologies to drive business value.

Jupyter Deploy: Create a JupyterLab application with real-time collaboration in the cloud in minutes

Jupyter notebooks have become a popular tool for data scientists, researchers, educators and analysts who need to experiment with code, visualize data, and document their findings. Many users run Jupyter on their laptops. This creates limitations to collaborate with a distributed team because users cannot securely provide direct access to their local JupyterLab application over the internet. Similarly, users are limited by the compute power of their own device. If their workload requires more compute, for example GPU accelerators to fine-tune deep learning models, it requires a different setup. Large enterprises can afford teams of engineers to set up and maintain deployment frameworks or managed services that support distributed compute with secure and fast remote connections, but that is beyond the resources of small organizations such as startups or research teams. Introducing Jupyter Deploy Today, the AI/ML Open Source team at AWS is excited to share the availability of Jupyter Deploy: a new open source command line interface (CLI) to deploy Jupyter to the cloud in a few minutes. Jupyter Deploy gives you a dedicated and sharable application running on your own domain with encrypted HTTP (TLS), GitHub OAuth integration, real-time-collaboration, and high performance UV-based development environment for applying updates and further configure your application. The CLI leverages Infrastructure-as-code (IaC) to let you define and manage resources using configuration files instead of manual setup through web consoles. Like the rest of Jupyter, the Jupyter Deploy CLI is vendor neutral, although this blog will focus on deploying to AWS. GitHub OAuth enables your collaborators to log in with their existing GitHub accounts instead of creating and managing new credentials. With a few commands from the Jupyter Deploy CLI, you can set up an infrastructure-as-code project, configure it, and use it to create and manage the cloud resources. The first step consists of selecting a Jupyter Deploy template. Templates are pre-built infrastructure-as-code projects that handle the technical set up for you. Think of them as recipes that tell the system exactly what cloud resources to create and how to configure them. These templates are available as Python libraries and are automatically discovered by the Jupyter Deploy CLI after you install them in your Python environment. The Jupyter Deploy CLI ships with a base template with terraform infrastructure-as-code engine, AWS cloud provider, Amazon Elastic Compute Cloud (Amazon EC2) instance, and GitHub authentication. How Jupyter Deploy simplifies collaboration Once you have deployed your JupyterLab application to your new Amazon EC2 instance, you can access it from your web browser as you would if Jupyter was running locally on your laptop. But unlike a locally-run JupyterLab, you can also share its URL with others. When your collaborators paste the URL in their own web browser, they will be prompted to authenticate via GitHub. If their identity matches your allowlist, they will get access to your JupyterLab application, and you can work simultaneously thanks to the real-time-collaboration feature of Jupyter with the latest improvements provided by jupyter-server-documents. Need more compute? Run a couple commands from the directory of your Jupyter Deploy project to swap out your instance. Amazon EC2 offers a broad range of instance types: you can use compute optimized instances for CPU-intensive data science projects or GPU instances for deep-learning, for example. Another collaborator needs access your application? Another simple command takes care of it! Same goes for adding more storage; you can mount new Amazon Elastic Block Stores (Amazon EBS), scale the existing ones, or mount a new or existing Amazon Elastic File System (Amazon EFS) with a couple jupyter-deploy commands. How to use Jupyter Deploy The first-time set up process involves five main steps: configuring AWS, getting a domain, setting up GitHub authentication, installing the tool, and deploying your environment. You can reuse your domain, AWS account, GitHub apps for future deployments. Set up an AWS account and AWS credentials locally If you do not have an AWS account, you can take advantage of the AWS Free Tier program to set up your Jupyter Deploy project. Next, install the AWS Command Line Interface (AWS CLI) in your environment and configure the AWS CLI to access your own AWS account. Confirm your setup by running the following command that verifies that your AWS credentials are working correctly: aws sts get-caller-identity # you should see a response referencing your AWS account. Buy a domain on Amazon Route 53 Next, open the AWS console, log in with your email and password, and navigate to Amazon Route 53. Amazon Route 53 is the AWS service that manages Domain Name System (DNS) operations. Amazon Route 53 will allow you to associate DNS records – a global address such as amazon.com – to the IP address of the Amazon EC2 instance the base template will set up for you. To follow the steps in this post, you need to own a domain and have the ability to create DNS records with Amazon Route 53. If you do not own a domain, you can buy one from Amazon Route 53 console for less than $20 per year. Navigate to Amazon Route53 console. In the navigation pane, choose Domains, then Registered domains and choose Register domains.

We went ahead and purchased the domain. It automatically creates a Route 53 hosted zone in your AWS account. Per Amazon Route 53 documentation, a hosted zone is a container for records, which includes information about how you want to route traffic for a domain (such as example.com) and all of its subdomains (such as www.example.com, retail.example.com, and seattle.accounting.example.com). A hosted zone has the same name as the corresponding domain.

Set up your GitHub OAuth app The next step is to set up a GitHub OAuth app. Jupyter Deploy will configure your Amazon EC2 instance to redirect users to GitHub to authenticate by referencing this GitHub app in your GitHub account. If you do not have a GitHub OAuth app yet, follow this link to create one in your GitHub account. For more details, refer to the GitHub documentation. · Application name: type JupyterLab application (or any name) · Homepage URL: type your {your-domain}, in our case example.com · Application description: type jupyter.{your-domain} or leave empty (useful to remember the domain that this OAuth app refers to. · Authorization callback URL: type {your-domain}/oauth2/callback, in our case GitHub will use this callback URL to communicate the result of the authentication to your application. This URL MUST match exactly your domain or the authentication flow will fail. · Enable Device Flow: leave unset. Now that your GitHub OAuth app is configured, you should be able to see it on the developer settings page. When you open it, you'll see it has a Client ID and a Client Secret. Write down the Client ID, then generate the secret and save its value, we'll need both of them later. Create your Python environment and install Jupyter Deploy Next, let's start the jupyter-deploy project. We will create a Python virtual environment to separate the dependencies of your jupyter-deploy project from the rest of your system. We will use UV to do so. · Open a terminal, and navigate to your home directory cd ~ · Create a UV project: uv init jupyter-deploy-projects --bare · Install the Jupyter Deploy CLI: uv add jupyter-deploy . This command creates a virtual environment in your current directory, then downloads and install the CLI, the base template, and its dependencies. · Now activate this environment: source .venv/bin/activate · Then run jupyter-deploy --help You're all set! Time to create our first Jupyter Deploy project. Initialize your project Similarly to uv, the Jupyter Deploy CLI sets up a project for you: · In your terminal, create a new directory for your project: mkdir jupyterlab-on-ec2 · Change directory: cd jupyterlab-on-ec2 · Then run jupyter-deploy init . This generates the following project scaffolding: ~/jupyter-deploy-projects/jupyterlab-on-ec2 |_ engine |_ manifest.yaml |_ services |_ variables.yaml Configure your project with the interactive experience of Jupyter Deploy It's now time to configure your project. You'll need at hand: · your domain · your GitHub OAuth app Client ID · your GitHub OAuth app Client secret · your GitHub username Run jupyter-deploy config -s to verify your installation and prompt you to install required tools such as terraform, AWS CLI, jq and possibly other tools your specific template needs. Follow these instructions to install the dependencies, then run the command again: jupyter-deploy config -s The template uses Lets Encrypt to generate the TLS certificates so that the traffic to your JupyterLab application is encrypted end-to-end. You will now enter the interactive experience. The CLI will prompt you to select values for each of the required variables for this template. · var.domain, type your domain (just the domain, without in our case example.com · var.letsencrypt email, type your email. Letsencrypt uses it to communicate with your upcoming TLS certificates expiry. You do not have to confirm this email, in which case you will not receive any communication from letsencrypt but your JupyterLab application will work fine. · var.oauth_allowed_org leave blank · var.oauth_allowed_teams type: [] · var.oauth_allowed_usernames type ["your-user-name"] · var.oauth_app_client_id type the Client ID · var.oauth_app_client_secret type the Client secret · var.subdomains type jupyter If you made a mistake, run jupyter-deploy config -- VAR_VALUE For example jupyter-deploy config --domain example.com will change the domain variable while keeping the rest of your configuration in place. Create the cloud resources Now it's time to actually create your cloud resources. Run the following command jupyter-deploy up Here terraform takes over, and prepares the deployment for you. Infrastructure-as-code is a technology that enables you to create and manage multiple inter-dependent cloud resources based on declarative files. Terraform is a popular software tool for infrastructure as code. It is not affiliated with AWS, and can be used to deploy other cloud provider resources. Terraform prepares the plan, and displays the resources it will create in your AWS account using your AWS credentials. Type yes when prompted. Open your application Now, your infrastructure is ready, and is serving your application at the sub-domain.domain that we specified. Access it with jupyter-deploy open Your terminal might display a pop up asking you to confirm whether you trust this domain. Since you own it, you can trust it so answer yes. Now you will be redirected to the authentication page

You will then be redirected to the familiar JupyterLab launcher page.

From here, you can open and run notebooks, install new packages, or interact with the terminal. Add a collaborator and share your notebook URL Next, let's start collaborating on this application. Ask the person you want to collaborate with for their GitHub username. Then run the following command from your jupyter-deploy project directory: jupyter-deploy users list You should see only your own GitHub username. Now add your collaborator's username with: jupyter-deploy users add Verify the application was updated by running again jupyter-deploy users list Then provide your application URL to your collaborator (by email, text message, whichever way your prefer). Only your collaborator can access your application so you do not need to take steps to secure this URL. If you do not remember the URL, run jupyter-deploy open again. Now you can both access the JupyterLab application from your respective web browser, and you will see each other's changes in real time. Stop your instance to save on Cloud bill At the end of your workday, you may want to turn off your Amazon EC2 instance and save on your AWS bill or free-tier allowance. You can do so with a simple jupyter-deploy command for your jupyter-deploy project. jupyter-deploy host stop That's it, your Amazon EC2 instance state and file system will be preserved and you can resume your work where you left it when you reconnect. Things to know Upgrade your instance The base template uses t3.medium as the default Amazon EC2 instance type. That is the recommended instance for a simple workload, however you may run into capacity limits if your workload needs more CPU or more memory. This is where infrastructure-as-code really helps; by updating one string in the declaration files, we can re-apply the project and mutate the existing infrastructure. jupyter-deploy makes this even easier, you can just run: jupyter-deploy config --instance-type t3.2xlarge then: jupyter-deploy up Share access to your JupyterLab application with a GitHub organization and GitHub teams If you have a small team, you can also manage access to your JupyterLab application at the GitHub organization and GitHub team levels. The jupyter-deploy CLI makes this easy by providing jupyter-deploy teams and jupyter-deploy organization commands. The base template only supports GitHub, but there are many other identity providers that support the OAuth2 protocol. Future work includes extending support to other such identities in the Amazon EC2 base template, or with new templates. Mount additional Amazon EBS volumes or Amazon EFS file systems You can easily create or associate existing storage volumes or file systems to the Amazon EC2 instance serving your JupyterLab application. Use the —additional-ebs-mounts of the config command to create or reuse Amazon EBS volume, and mount them under the home directory of your JupyterLab application. You can find additional configuration options by running jupyter-deploy config --help. Call to collaborate and contribute This blog walked you through an end-to-end deployment of a JupyterLab application to Amazon EC2, using GitHub as OAuth provider. There are many other ways to deploy Jupyter, and the jupyter-deploy CLI provides an extensible framework to do so. Our team will be working on adding another template targeting Kubernetes on Amazon Elastic Kubernetes Service (Amazon EKS). It will support running multiple applications on the cluster, and allow users to dynamically create, share and delete their own applications. Running an application on remote compute, including on Kubernetes, is not limited to AWS. The project welcomes contributions that add new functionalities such as other OAuth providers, other TLS certificate providers, and other Cloud providers. Users would install the templates from pypi , GitHub or local using the package manager of their choice (pip, or uv as in this blog), then use the options of the init command to select it. We encourage open source contributions to update the AWS EC2 base template, or to improve the CLI itself. Right now, the CLI and the base template are source-controlled from the same GitHub repository. To learn more and get involved, here you can find: · Project repository: · PyPI page of the CLI: · PyPI page of the base template:

Jonathan Guinegagne Jonathan Guinegagne is a Senior Software Engineer with Amazon SageMaker JumpStart at AWS. He got his master's degree from Columbia University. His interests span machine learning, distributed systems, and cloud computing, as well as democratizing the use of AI. Jonathan is originally from France and now lives in New York City.

Danilo Poccia Danilo works with startups and companies of any size to support their innovation. In his role as Chief Evangelist (EMEA) at Amazon Web Services, he leverages his experience to help people bring their ideas to life, focusing on serverless architectures and event-driven programming, and on the technical and business impact of machine learning and edge computing. He is the author of AWS Lambda in Action from Manning.

Amazon Connect is an AI-powered customer experience solution that enables superior outcomes at a lower cost. Since its 2017 public launch, Amazon Connect has become an AI leader, transforming how organizations of all types interact with their customers. In the Q3 2025 earnings report last week, Amazon announced a significant milestone: Amazon Connect achieved a $1 billion annualized revenue run rate, with AI optimizing over 12 billion minutes of customer interactions in the previous year. Even with these signals of success, Amazon Connect remains mission-driven, measuring results through satisfied end-customers, fulfilled agents, and delighted business leaders – just as it has since its inception. Let's explore the Amazon Connect story – from internal solution to AI pioneer. The origin story Like Amazon Web Services (AWS), Amazon Connect exemplifies Amazon's ability to transform internal solutions into industry-leading services. The journey began in 2007 when the internal customer service team decided to propose building a new unified solution from scratch to replace three contact center vendors. The vendors were requiring a $3M up-front investment for a hardware upgrade, plus recurring license and maintenance fees. The proposed internal solution was not only less expensive but also supported Amazon's stated mission of being the Earth's most customer-centric company. "We looked around at the time for all of the other products that could work. Nothing was very compelling," said Jon Jay, who founded the original customer service team and remains one of Connect's most senior engineering leaders. "None of them had high volume capacity, so we would have needed to manage over a dozen instances of these contact center solutions. They were also extremely expensive and didn't solve any of the problems we hoped to address in delighting customers." After receiving approval from Jeff Wilke, former CEO of Amazon Worldwide Consumer, the team successfully deployed their first pilot in 2007, followed by full deployment in 2008. What started as an internal efficiency project would go on to serve Amazon's various divisions for years – from customer service to HR and transportation – generating estimated annual savings of $60 million compared to competitive solutions. New acquisitions such as Audible and Zappos also enthusiastically embraced this solution and proved that there was appetite in the market for a uniquely Amazon approach to customer experience. "When we showed other Amazon teams what we built, it spread like wildfire. Zappos, Audible – they all had the same headaches with traditional contact centers. We'd show them our solution and they'd say 'Wait – you solved all of our biggest problems,'" said Jay. Product launch and early success The decision to make Amazon Connect available to the public came in Q3 2015, with approval from then-AWS CEO Andy Jassy and led by Pasquale DeMaio. "Talking to potential external customers, it was clear that they faced the same challenges as Amazon," added Jay. "We saw an opportunity to disrupt a legacy industry with a cloud-based solution that was easy to implement, offered unrivaled velocity of innovation, and could scale with the reliability and security that the largest enterprises require. That's when we knew we weren't fixing just Amazon's problems – we were solving issues the whole industry had been stuck with for decades." After just over a year in development as an external service, Amazon Connect made its public debut at Enterprise Connect 2017, quickly gaining traction among large enterprises facing significant scaling challenges. The deep integration with AWS services enabled seamless scaling and rapid feature development, while early adoption of Alexa AI technology through Lex delivered natural language interactive voice response (IVR) capabilities that set Amazon Connect apart. Early adopters like Capital One, Hilton, and GE were drawn to Connect's unique value proposition: a cloud-native architecture that eliminated the need for traditional telephony infrastructure. This innovative approach transformed what was traditionally a year-long buildout process into a weekend project for many organizations, dramatically reducing both time-to-market and operational complexity. "From the very beginning, organizations that wanted our speed of innovation found success partnering with Amazon Connect," said VP of Amazon Connect, Pasquale DeMaio. "Organizations know that we understand customer experience in a very personal way. It's one thing to claim to be customer-obsessed. At Amazon, we live it everyday." When the COVID-19 pandemic struck, Amazon Connect's cloud-native design proved prescient. Its self-service setup and native work-from-home agent support became crucial advantages as organizations scrambled to maintain customer service operations with remote workforces. The ability to function with just standard internet connections and headsets, eliminating the need for specialized phone equipment, made it an ideal solution for the sudden shift to remote work. By the end of the pandemic, Amazon Connect had tens of thousands of customers. Leading the AI revolution in customer experience Amazon Connect's evolution took a significant leap forward in 2019 with the launch of AI-powered conversational analytics, sentiment analysis, and more, without the complex technical requirements typical of competitors. Customers only needed to select a checkbox to enable these AI features, versus weeks of deployment with other solutions. In 2023, Amazon Connect achieved a leadership position for the first time in two key industry reports: the Forrester Wave for Contact Center as a Service and Gartner's Contact Center as a Service Magic Quadrant. Amazon Connect has maintained these leadership positions in subsequent reports through present day. Additional AI features were rapidly delivered across the customer journey. With the advent of generative AI, the team pivoted their roadmap to embrace Large Language Model (LLM) technologies, enabling features like automated agent wrap-up, call summarization, and LLM-based self-service experiences. In December 2024, Amazon Connect reported that 6 billion minutes of customer interactions were optimized by AI, showing the scale at which customers were putting AI to work in real scenarios. When Amazon Connect unveiled its "next generation" in March 2025, featuring AI enabled throughout the customer journey, customer response was overwhelmingly positive. Now, Amazon Connect is optimizing 12 billion minutes of customer interactions with AI—double what was announced less than a year ago. Amazon Connect is delivering innovative and integrated AI solutions at enterprise scale. Recently, multiple global brands have selected Amazon Connect over other providers and emerging AI-native players. In these evaluations, Amazon Connect demonstrated superior performance in critical areas such as intent detection accuracy, AI agent safety, and human-AI collaboration capabilities. These wins demonstrate Connect's ability to deliver best-in-class AI solutions beyond just a great demo, in a way that enables mission-critical use cases based on technical merit and reliability. Looking ahead Amazon Connect's achievement of a $1 billion revenue run rate milestone positions it as the only usage-based customer experience solution to reach this scale. This pay-as-you-go approach also uniquely positions Amazon Connect for the AI and agentic future. "We've had no shortage of inflection points: first, being a key driver of making cloud-based contact centers the new normal, then enabling businesses to navigate remote work while managing massive swings in demand during COVID, and finally delivering real-world results with Gen AI," said DeMaio. "Now we face two more: delivering safe and ethical agentic AI at enterprise scale and evolving from reactive to proactive customer engagement—all while expanding into new areas of opportunity to solve customer problems." From an internal project to solve Amazon's own customer service challenges, Amazon Connect evolved into a global service trusted by tens of thousands of customers in 8 years. And, it's still Day 1. With the same curiosity that sparked its creation and a mission to transform experiences, the team continues to pioneer AI-powered solutions aimed at solving problems and delighting customers. To learn more about Amazon Connect, visit the Amazon Connect page. Ready to transform your customer service experience with Amazon Connect? Contact us. About the Author Heidi Elmore is a serial technologist passionate about enterprise communications. She has launched UCaaS, CPaaS, and CCaaS products and led teams across multiple functions. When she's not obsessing over enterprise software, she rides and shows dressage horses.

Tonic AI: Synthetic data solutions for software and AI development Enterprises use the Tonic.ai solution to transform sensitive data into secure, high-fidelity synthetic datasets for AI and analytics workloads. Through a new notebook-based workflow, AWS customers can now use Tonic Textual on data stored in their lakehouse architecture of Amazon SageMaker to transform high-risk, unstructured data into safe, high-quality assets. This de-identified data is essential for a wide range of generative AI use cases, including fine-tuning large language models (LLMs), building knowledge bases for Retrieval Augmented Generation (RAG) workflows, creating evaluation datasets, and testing generative AI applications. To see the full workflow in action, readers can find a detailed, step-by-step guide on the Tonic.ai blog. "The SageMaker next generation incubator program put our mission on the fast track: bringing Tonic Textual's advanced technology directly to enterprises building on SageMaker. It has been pivotal in helping them replace sensitive information with high-quality synthetic data, dramatically shortening the path from raw data to impactful, production-ready AI models." — Tomer Benami, VP of Business Development Activeloop: Unlocking AI Data Analysis Activeloop, a pioneering AI infrastructure company, specializes in developing solutions that optimize AI data management and processing workflows. Their flagship product, Deep Lake, serves as a database specifically designed for AI applications, enabling efficient handling of multimodal data, including text, images, signals, and literature, by delivering impressive speeds in Apache Spark workloads of up to 3.9 times faster, compared to open source alternatives. Enhanced by their integration with the next generation of Amazon SageMaker and its lakehouse architecture approach, Activeloop combines Deep Lake's ultra-fast, multimodal AI data retrieval capabilities with the enterprise-grade scalability and versatility of SageMaker. This integration enables direct access to complex AI data from object storage, streamlining analytics and artificial intelligence and machine learning (AI/ML) model development. By unifying multimodal data with Deep Lake's advanced indexing capabilities, organizations can seamlessly connect textual, numerical, and visual information, accelerating insights and innovation across various domains, from healthcare to drug discovery, transforming their data assets into competitive advantages while maintaining efficient, scalable, and cost-effective AI operations. Read more about the solution on the Activeloop blog. "Through the incubator, we've gained invaluable technical enablement and marketing support, allowing us to scale our Deep Lake service capabilities to meet enterprise demand." — Davit Buniatyan, CEO Weaviate: For AI engineers who think big Weaviate is a cloud-centered open source vector database that simplifies development and deployment of AI applications. It offers vector search, keyword, and hybrid search capabilities through a scalable, flexible platform with pluggable architecture to connect with ML models. Production deployments benefit from built-in multi-tenancy, replication, role-based access control (RBAC) authorization, zero-downtime backups, advanced filtering, vector compression, and out-of-the-box RAG capabilities. Weaviate is available as a hosted service, self-managed instance, or as fully managed Weaviate Cloud. By handling the infrastructure and operational details, Weaviate Cloud frees developers to focus on innovation while providing enterprise-ready solutions through various hosting options including serverless cloud, enterprise cloud, and bring your own cloud. Weaviate has developed a comprehensive SageMaker Unified Studio notebook solution that bridges the gap between traditional data storage and AI-powered applications by using existing lakehouse data. This solution seamlessly transforms your structured or unstructured data stored in your lakehouse into a vectorized format within Weaviate's vector database, enabling sophisticated semantic search capabilities and RAG queries for enhanced data utilization. Read about the solution on Weaviate blog. "The SageMaker Incubator provided us the guidance and support we needed to bridge the gap between open source innovation and enterprise readiness, making it easier for our customers to deploy Weaviate at scale." — Tony Le, Sr. Director of Partners Snowplow: Turn customer behavior into competitive advantage Snowplow provides a customer data infrastructure that organizations can use to collect and stream clickstream data in real time. Organizations can capture complete, high-fidelity event data across every digital touchpoint—governed, modeled, and ready for downstream analytics and AI-driven use cases. Through the SageMaker Startup Partner Incubator program, Snowplow has built a solution to collect your data in real time and ingest it into your lakehouse on AWS. Once ingested, teams can use this lakehouse data to drive analytics use cases. You can run extract, transform, and load (ETL) capabilities available within SageMaker Unified Studio or use the SageMaker integration with Amazon Quick Sight to generate dashboards for enterprise reporting. You can further power ML use cases like recommendations and next best action. The joint solution provides a seamless path from a trusted behavioral data foundation to real-time AI-driven optimization at scale. Learn more about our partnership and joint solution at Snowplow for Amazon SageMaker. "The SageMaker Incubator program has accelerated our ability to deliver real-time, AI-ready clickstream data directly into customers' SageMaker environments. With Snowplow's solution deployable in the customer's tenant, and Snowplow Signals enabling easy real-time feature computation and serving, teams can securely build and operationalize ML and agentic models with rich behavioral data without sacrificing control or compliance." — Yali Sasson, Snowplow Co-founder and CTO SuperAnnotate: Streamlining AI development SuperAnnotate helps efficiently curate, label, and validate multimodal data for AI training and evaluation. Its robust annotation environment and connected data pipelines enable enterprises to accelerate time-to-production by up to 72%, powered by a seamless human and AI feedback loop that ensures consistent, high-quality data. SuperAnnotate's integration with Amazon SageMaker streamlines the entire machine learning workflow from raw data to AI deployment. Annotated datasets from SuperAnnotate are stored in standardized formats and are directly accessible in Amazon Simple Storage Service (Amazon S3) within the SageMaker Unified Studio environment. This gives teams simple, consistent access to labeled data in SageMaker Unified Studio to use in any stage of model development – from training, fine-tuning, and evaluation – without complex data handling. Together, SuperAnnotate and SageMaker create human-in-the-loop active learning cycles to continuously retrain and improve your models. "The SageMaker Incubator gave us exactly what we needed to move fast – deep technical support and tight collaboration, helping us deliver a truly integrated solution at scale. It's been an incredible experience building alongside AWS to deliver a unified workflow that brings greater value and speed to our mutual customers." – Vahan Petrosyan, Cofounder and CEO Conclusion In this post, we showed how innovative startups such as Tonic.ai, Activeloop, Weaviate, Snowplow, and SuperAnnotate are harnessing Amazon SageMaker to deliver enterprise-grade data and AI solutions. Partners in the program benefit from the robust capabilities of Amazon SageMaker. The unified environment enables comprehensive data management across data lakes and warehouses and provides AI-assisted development support and built-in governance and security controls. Partners can use seamless data sharing and ETL capabilities so they can focus on innovation rather than infrastructure management. For startups that want to scale their data and AI solutions for enterprises, the Amazon SageMaker Startup Partner Incubator provides a proven path to success. The program offers comprehensive support, including technical enablement, marketing assistance, and accelerated go-to-market strategies. Through this support, partners can rapidly develop and deploy enterprise-grade solutions that solve real business challenges while maintaining the highest standards of security and compliance. Ready to transform your startup's potential into enterprise value? Join the Amazon SageMaker Startup Partner Incubator today by visiting our application page at Amazon SageMaker Startup Partner Incubator.

TAGS: Amazon SageMaker, Amazon SageMaker Studio, Announcements, Lakehouse, Partner solutions

How Indeed scaled Governance across 1,000+ AWS accounts with AWS Trusted Advisor

Indeed is the #1 job site¹ in the world. With 615 million Job Seeker Profiles², people in more than 60 countries across 28 languages come to Indeed to search for jobs, post resumes, and research companies. Over 3.3 million employers use Indeed to find and hire new employees. Supporting this massive scale requires resilient, well-architected cloud infrastructure. Indeed prioritizes operational excellence by following AWS Well-Architected framework recommendations, but they also embrace AWS with a decentralized approach to foster velocity, accuracy, and accountability. To balance these concerns, they developed an approach to score maturity and adapt to ongoing challenges. The challenges of their decentralized approach included teams encountering service limits as they rapidly adopted AWS services. This led to reactive management that threatened the availability of scaling-dependent services. Additionally, they discovered inefficient resource utilization, such as underutilized Amazon Relational Database Service (RDS) instances consuming significant budgets across multiple AWS accounts without centralized oversight. These issues, among others, highlighted the need for a more strategic approach to organization-wide visibility and governance as they continued to grow and scale their operations. They recognized the need for a unified framework to enable meaningful inter-team comparisons and prioritize business outcomes. To achieve this, they leveraged AWS Trusted Advisor data through a data lake with robust reporting and visualization capabilities. This approach significantly improved their cloud governance, fostering continuous enhancement and optimization across they organization. The Dual Impact of Decentralization: Accelerated Progress and Dispersed Insights Indeed's engineering culture prioritizes product team autonomy and data-driven results. This philosophy has served well and enabled rapid innovation and the ability to respond quickly to market demands in the competitive job search landscape. To meet demands, they scaled for growth within AWS to encompass over 1,000 accounts across product teams and inevitably encountered a fundamental tension: decentralized progress limiting overall organizational observability. They had limited visibility into our overall cloud health posture. Cost optimization opportunities were distributed across accounts without a systematic way to identify the highest-impact improvements. Additionally, they needed mechanisms to benchmark team performance or share best practices organization-wide. With over 1,000 AWS accounts generating thousands of Trusted Advisor recommendations across six key areas—cost optimization, performance, security, fault tolerance, service limits, and operational excellence—manual approaches couldn't provide the comprehensive oversight and accountability they needed to meet for resilience with organization-wide initiatives such as improving high availability and having a disaster recovery plan. Championing Trust: Education and Enablement as Pillars of Progress Indeed began by creating cross-account IAM roles to enable centralized data collection. This enabled a centralized service account and a daily collection process to assume "trusted-advisor-data-collection" roles across all AWS accounts. The daily collection process, executed in Python, systematically collected Trusted Advisor data from all accounts, normalized the results, and subsequently uploaded them to a central Amazon S3 bucket. This bucket was partitioned by day to facilitate long-term retention. The collected data was then integrated into Indeed's existing data warehouse, where it could be correlated with organizational ownership information to ensure comprehensive visibility. This crucial step linked each finding directly to the responsible team, ensuring comprehensive visibility across the organization.

Figure 1: Account architecture for centralized aggregation of Trusted Advisor findings

Once they addressed implementation, they drove adoption across teams by educating them to review their scores across the six pillars through self-service in their own environments pertaining to their AWS account. And addressed common resistance points by maintaining: ● Transparency First: Rather than implementing top-down mandates, they made all data openly accessible, enabling teams to derive their own insights. This approach fostered trust and demonstrated value prior to requesting behavioral modifications. ● Early Win Showcases: They identified immediate, high-impact improvements that teams could implement, thereby creating positive associations with the new system. They present two specific examples where this implementation helped product teams. 1. Use Trusted Advisor data to identify idle RDS instances by platform teams A company-wide report detailing teams with the highest incidence of idle databases enabled platform teams to focus optimization efforts where they would achieve the most significant cost reductions. With 44% of RDS instances identified as idle, this singular initiative yielded immediate returns on investment and illustrated to teams how centralized data could augment existing optimization endeavors.

2. Proactive service limit management Previously, teams discovered service limits reactively when they were impacted —such as throttled Amazon DynamoDB operations. With centralized Trusted Advisor monitoring, teams began receiving early warnings when service limits moved from "OK" to "Warning" status across their AWS Accounts. In one instance, the early alert system identified that one of 17 DynamoDB tables was approaching its read capacity service limit in a production account. This early warning allowed the team to proactively request a quota increase before customers were affected. Building on this momentum, Indeed focused on scaling these benefits across the organization by democratizing cloud health data. The integrated data warehouse combines Trusted Advisor metrics with Indeed's ownership information, powering dashboards that allow teams to visualize cloud health, benchmark performance, and identify optimizations. This approach not only improved cloud stewardship across the organization but also enhanced developer productivity by providing clear visibility into how resource allocation choices affect cost optimization. Excellence as Competitive Advantage At Indeed, the evolution of cloud governance, achieved through centralized Trusted Advisor data, transcends mere operational enhancement; it has emerged as a strategic differentiator directly influencing their capacity to effectively serve job seekers and employers. By correlating cloud health metrics with business outcomes, they established links between technical excellence and competitive advantage. Teams that proactively manage their service limits encounter fewer outages, optimized resource utilization translates into cost reductions suitable for reinvestment, and an elevated security posture fosters trust. Their vision extends beyond current capabilities as they develop integration between Trusted Advisor data and existing service availability metrics within their data warehouse. Then they can answer critical questions: Which recommendations provide the greatest availability improvements? How do optimizations impact customer experience? Which teams' cloud health improvements correlate most strongly with business outcomes? Conclusion Not all organizations have access to data lakes with ownership information or service health metrics. For those customers, several excellent starting points are available: AWS Trusted Advisor Organizational View provides centralized visibility into all Trusted Advisor findings across AWS accounts, while the Trusted Advisor dashboard from Cloud Intelligence Dashboards on AWS collection offers rich visualization and analytics capabilities built on Amazon QuickSight. These Cloud Intelligence Dashboards provide pre-built, customizable reporting that can jumpstart your Trusted Advisor optimization program without requiring extensive data engineering resources. Success comes from combining the right tooling—whether a custom data lake solution, Organizational View, or Cloud Intelligence Dashboards—with creating a culture that actively adopts and acts on Trusted Advisor recommendations. Organizations need comprehensive visibility across complex environments, democratized access that empowers every team member, and cultural alignment that treats cloud stewardship as core to organizational mission rather than overhead to be minimized. ¹ Comscore, Total Visits, March 2025 ² Indeed data (worldwide), job seeker accounts that have a unique, verified email address About the authors

Susan Calland Susan Calland, a Senior Cloud Engineer at Indeed, finds satisfaction in both the business and technical aspects of her role. She enjoys coding solutions, budgeting, collaborating effectively with partners, creating visualizations to effectively communicate insights, and leveraging AI to optimize cloud infrastructure. Susan finds it particularly gratifying to demonstrate how enhanced security can lead to business savings.

Gayathri Veale Gayathri Veale, a Site Reliability Engineer at Indeed, boasts over 20 years in platform engineering and infrastructure. As an individual contributor on the Engineering Platform team, she ensures Indeed's data-driven culture is supported by a resilient, reliable, and scalable backbone enabling developers and internal users to address business initiatives.

Brett Seib Brett Seib is a Senior Solutions Architect based in Austin, Texas, with over a decade of professional experience architecting products and guiding AWS customers. He is passionate about innovating and leveraging technology to solve complex business challenges. Brett specializes in enterprise solutions, Artificial Intelligence (AI), and data analytics, helping organizations accelerate their business outcomes and drive digital transformation.

Alak Eswaradass Alak Eswaradass is a Principal Solutions Architect at AWS based in Chicago, IL. She focuses on empowering AWS customers to design innovative cloud architectures that solve complex business challenges. She is passionate about using artificial intelligence to advance accessibility, particularly in helping customers develop generative AI-powered assistive technologies that make digital experiences more inclusive. When she's not working, Alak enjoys spending time with her daughters and exploring the outdoors with her dogs.

Raj Athavale Raj Athavale is a Technical Account Manager at AWS, where he partners with enterprise customers to implement scalable cloud governance frameworks and multi-account optimization strategies. Passionate about containerization with Amazon EKS and AI solutions architecture, Raj specializes in solving complex technical challenges that enable organizations to maintain innovation velocity while achieving operational excellence at scale.

Wasif Abdul Rauf Wasif is a Senior Technical Account Manager based out of Houston TX. He loves to dive deep into his customer's technical and operational challenges, working closely to uncover solutions. With a strong interest in Generative AI, machine learning, and data analytics, Wasif is driven by a passion for innovation.

How Omnissa saved millions by migrating to Amazon RDS and Amazon EC2

Omnissa, formerly the VMware End-User Computing business, is a digital workspace technology leader that delivers smart, seamless, and secure digital work experiences for organizations worldwide. It serves 26,000 customers, including the top seven of the Fortune 500 companies. The company's mission is to simplify and secure the digital work environment, empowering employees to work productively from anywhere, on any device, without sacrificing IT control or user experience. Omnissa Workspace ONE®, Unified Endpoint Management (UEM) , helps its customers secure and manage all devices that connect to a network, both on premises and in the cloud. Omnissa Horizon® provides solutions for virtual desktops and apps, digital employee experience, and robust security—all underpinned by AI-driven automation. With AWS, Omnissa was able to successfully migrate its mission-critical workloads from VMware Cloud on AWS (VMC-A) to native AWS services, including Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) for SQL Server. This migration exemplifies the strategic impact of Omnissa's cloud transformation journey. Using these services, it not only achieved a substantial cost reduction of 39%, saving millions, but also significantly enhanced operational efficiency. Omnissa used this migration as an opportunity to start a transformation initiative, which included refactoring its applications and databases as part of a multi-year modernization strategy. This approach aims to further optimize costs and transfer the resulting savings to its customers. Josh Broch, VP of Engineering at Omnissa, commented on the success of the migration:

"The transition from VMC-A to Amazon RDS for SQL Server and Amazon EC2 was a critical strategic move for us. The AWS team's expertise, proactive approach, and unwavering support were instrumental in making this complex migration a success. Not only have we achieved significant cost savings, but we've also gained a more flexible, scalable infrastructure that allows us to better serve our customers and drive innovation."

Anita Singh Anita is a Principal Specialist Solutions Architect with over 20 years of experience architecting solutions focused on database technologies. She guides customers in designing and building highly scalable, available, and performant applications, while specializing in database migrations and modernization initiatives to AWS Cloud. Her expertise extends to implementing resilient database architectures and integrating operational data with analytics and AI services, helping organizations maximize their cloud infrastructure investments.

Satya Pattanaik Satya is a Senior Solutions Architect at AWS, where he helps Independent Software Vendors (ISVs) build scalable and resilient applications on the AWS Cloud. Before joining AWS, he played significant role in driving growth and success for many organizations. Outside of work, he pursues his passion for BBQ, continuously learning new techniques to create flavorful dishes and experimenting with various recipes.

Scott Webber Scott is a Principal Customer Solutions Manager at AWS, leading digital transformation and Gen AI initiatives for large ISV customers. He has 25+ years of technology industry experience, including leadership roles at AWS, Informatica, Infor, and NTT DATA, Scott brings extensive expertise in cloud adoption, digital innovation, and business value optimization.

Neha Sharma Neha is a Senior Technical Account Manager at AWS and based in Bay Area, California. Neha provides strategic technical guidance for Independent Software Vendors (ISVs) to innovate and operate their workloads efficiently on AWS cloud.

Cees "Case" Molenaar Cees is a Sr. Databases Account Manager with over 3 ½ years covering managed database services at AWS. His customers are primarily large, international ISVs. Cees specializes in creating data strategies and helping his customers with database migrations and modernization initiatives. By aligning the right technical resources and programs, he simplifies and accelerates the process and within budget. Outside work, he enjoys family, friends, sports, travel, history and comedy.

Camilo Leon Camilo is a Principal Solutions Architect at AWS specialized in databases and based in San Francisco, California. He works with AWS customers, providing architectural guidance and technical support for the design, deployment, and management of their AWS relational database workloads and business applications. In his spare time, he enjoys mountain biking, photography, and movies.

Introducing cross-account support for Amazon CloudFront Virtual Private Cloud (VPC) origins

In November 2024, Amazon CloudFront introduced CloudFront Virtual Private Cloud (VPC) origins, a security feature that allowed customers to deliver content from applications hosted in private subnets. In addition, we are now introducing cross-account support for Amazon CloudFront VPC origins, enabling network traffic flow between Amazon CloudFront and Application Load Balancers (ALBs), Network Load Balancers (NLBs), or Amazon Elastic Compute Cloud (Amazon EC2) instances deployed within private subnets across different Amazon Web Services (AWS) accounts. This enhancement allows you to keep your Amazon VPC origins and CloudFront distributions in separate AWS accounts, enabling organizations with multi-account strategies to use VPC origins while maintaining their desired account structure. Many AWS customers set up multiple AWS accounts to isolate ownership and development processes across different business functions. This approach, following modern cloud security architecture principles, typically involves creating multiple AWS Organizational Units (OUs) with dedicated network and security accounts. While CloudFront distributions, serving as global entry points for customer applications, are typically deployed in network accounts, customers have been able to link public origins from multiple accounts to a single distribution. However, organizations wanting to enhance their security posture with VPC origins have faced limitations, as VPC origins and CloudFront distributions had to reside in the same account. This meant customers who had their origins in multiple AWS accounts, had to keep their accounts in public subnets to get the scale and performance benefits of CloudFront. Customers then had to maintain additional security controls, such as access control lists (ACL), at both the edge and within regions, rather than benefiting from the inherent security of VPC origins. Until now, these customers were unable to take advantage of the security benefits of VPC origins while maintaining their preferred multi-account architecture. Architecture diagram In this section we examine the key components of sharing cross-account VPC origins for CloudFront distributions, as shown in the following diagram.

Figure 1: VPC origins shared using AWS Resource Access Manager (AWS RAM) with a separate Resource Consumer account where the CloudFront distribution is located The cross-account sharing process involves two main entities: (1) the Resource Owner (the AWS account containing the VPC origin resource and the underlying infrastructure) and (2) the Resource Consumer (the AWS account hosting the CloudFront distribution). To share a VPC origin across two accounts, the Resource Owner first creates the (3) VPC origin in their own account. For HTTPS only, the Resource Owner may configure a custom domain with a valid (4) certificate. This is an AWS Certificate Manager (ACM) certificate for ALB/NLB. For HTTP only deployments, ACM certificates are not necessary. Then, using (5) AWS Resource Access Manager (AWS RAM), they can share the VPC origin with the desired Resource Consumer account. (6) At this stage, Resource Consumers are able to use the shared VPC origin in their CloudFront distribution, specifying the custom domain for HTTPS communication. Prerequisites The following instructions are for HTTPS only VPC origins setup. Before proceeding with this walkthrough, ensure you have the following resources already deployed in your Resource Owner Account: EC2 instances running your application (in our example, Apache web servers), an ALB fronting these instances, proper VPC and subnet configuration with instances deployed, a public-facing custom domain (for example api.example.com), and a certificate registered in ACM for that domain. This allows the VPC Origin to have HTTPS communication between CloudFront and the origin. These resources are necessary in order to proceed with the VPC origin setup, shown below.

Figure 2: Workflow for VPC origins sharing using AWS RAM, enabling setup with a cross-account CloudFront distribution Getting started This section explains the process, starting with how to set up VPC origin in the Resource Owner Account containing an ALB fronting Apache web servers on EC2 instances. Once completed, we'll detail how to share this VPC origin with the Resource Consumer Account using AWS RAM. Next, you create a CloudFront distribution in the Resource Consumer Account using the shared origin. Finally, you will learn how to verify the setup by accessing the web content through CloudFront's domain name. Navigate to the Resource Owner Account.

Create a VPC origin exclusively in the Resource Owner Account. For our example, we're using an ALB, so we input the ALB's ARN as demonstrated in the following image. Under Protocol, we choose HTTPS only to make sure of a secure communication between CloudFront and the origin.

When the VPC origin status shows Deployed, choose the VPC origin and choose the Share VPC origin button.

On the next screen, you can either choose an existing Resource share or choose the Create resource share button. For this example, create a new Resource share.

On the next screen, specify the Principal type and corresponding details (either account number, Organization ARN, or Organization unit ARN). For this example, we use AWS account as the Principal type and enter an AWS account ID. For this example, the AWS account ID is for Resource Consumer Account.

After completion, return to the VPC origin screen where a green banner confirms the successful creation of the resource share.

Although we created the resource share for VPC origin through the CloudFront console, you can also create it from the AWS RAM console. If you want to observe the share that was created, then you can navigate to the AWS RAM console.

If you are using an already existing resource share, then you can skip this step. Otherwise, when you are in the Resource Consumer Account, in the AWS RAM console, go to Shared with me, choose Resource shares, choose the name of the resource share, and choose Accept resource share.

On the next screen, choose VPC origin under Origin type. In the Origin section, choose Browse VPC origins to open a popup window. Choose the origin and select Choose. Under VPC origin endpoint, enter your custom domain (api.example.com is used as an example, but this should be your own public domain name that you own) because we are using HTTPS only and need the domain that matches your ACM certificate. Then, choose Next.

On the next screen, select Do not enable security protections and choose Next. Although we have decided not to use AWS WAF for this specific example, we recommend implementing it to secure your distribution.

On the next screen, review the details, scroll down, and choose Create distribution. You are taken to a screen where you should see the distribution in the Deploying state, which takes a few minutes to complete.

After the distribution is created, copy its Distribution domain name.

Access the copied domain name using a web browser. You should see that you can successfully reach the shared VPC origin from the Resource Owner Account through the CloudFront distribution in the Resource Consumer Account.

Figure 14: Access the URL for the CloudFront distribution created using shared VPC origin This example demonstrates how you can maintain a secure and organized infrastructure within your organization by keeping the VPC origin in one account (Resource Owner Account) while hosting the CloudFront distribution in another account (Resource Consumer Account). This setup allows for centralized management of VPC resources while enabling other accounts to securely access and distribute content through CloudFront. Considerations

If the Resource Owner Account stops sharing the VPC origin with the Resource Consumer Account, then the existing traffic continues to function. Shared VPC origin deletion needs a specific sequence:

Disable CloudFront distribution in Resource Consumer Account Unshare Shared VPC origin in Resource Owner Account Delete Shared VPC origin in Resource Owner Account

Directly from the CloudFront console Through the AWS RAM console in the use-east-1 (N. Virginia) AWS Region Both methods achieve the same result, providing flexibility in management.

To view and accept the resource share for the VPC origin in the Resource Consumer account, set your AWS Region to us-east-1 (N. Virginia), because global resources such as VPC origins are visible only in this Region.

VPC origins are marked as "(shared)" in Resource Consumer Account Resource Consumer Account cannot re-share received VPC origins One-way sharing: Resource Owner Account → Resource Consumer Account only

AWS RAM uses the managed permission: AWSRAMDefaultPermissionCloudfrontVpcOrigin This permission specifically allows sharing of the Resource type: cloudfront:Vpcorigin No additional permission configurations needed for basic sharing functionality

The Resource Owner Account can configure a custom domain (for example api.example.com) with a valid ACM certificate for their origin. The Resource Consumer Account must use this custom domain as the VPC origin endpoint in their CloudFront distribution configuration. This makes sure of proper SSL/TLS certificate validation between CloudFront and the origin.

Conclusion Cross-account support for Amazon CloudFront VPC origins eliminates the previous constraints that required origins and distributions reside in the same account. Using AWS RAM, organizations can now share VPC origins across account boundaries while maintaining their multi-account structures. With this innovation, development teams are now able to manage applications in private VPC subnets within their own accounts, while content delivery teams can reference these origins from CloudFront distributions in separate accounts. Get started today by using CloudFront VPC origins with cross account support. About the Authors

Salman Ahmed Salman is a Senior Technical Account Manager in AWS Enterprise Support. He specializes in guiding customers through the design, implementation, and support of AWS solutions. Combining his networking expertise with a drive to explore new technologies, he helps organizations successfully navigate their cloud journey. Outside of work, he enjoys photography, traveling, and watching his favorite sports teams.

Ankush Goyal Ankush is a Senior Technical Account Manager at AWS Enterprise Support, specializing in helping customers in the travel and hospitality industries optimize their cloud infrastructure. With over 20 years of IT experience, he focuses on leveraging AWS networking services to drive operational efficiency and cloud adoption. Ankush is passionate about delivering impactful solutions and enabling clients to streamline their cloud operations.

Kunj Thacker Kunj is a Technical Account Manager at AWS and is based out of Vancouver, Canada. He has an extensive background in Network and Infrastructure engineering prior to this role. He is passionate about new technologies and enjoys helping customers build, implement, and optimize their cloud infrastructure on AWS.

Configuring the AWS WAF Anti-DDoS managed rule group for your resources and clients

Users wanting to protect themselves from Layer 7 (HTTP) DDoS threats can use the AWS WAF L7 Anti-DDoS managed rule group to detect and mitigate DDoS events in single digit seconds. The Anti-DDoS managed rule group has a default configuration that is appropriate for many applications and clients. However, there are clients that need special attention. Browser-based Single Page Applications (SPAs) making fetch requests, and native mobile applications making API requests, do not support all mitigations and may have their requests inadvertently stopped during a detected event. In this post we provide guidance on how to tune your web access control list (web ACL) for different kinds of clients and resources to optimize your DDoS protection and user experience. How AWS WAF Anti-DDoS managed rule group trades off mitigation effectiveness with end-user impact When the Anti-DDoS managed rule group is added to your AWS WAF web ACL configuration, it quickly learns your traffic patterns and establishes baselines for each protected resource. It identifies anomalies by comparing current traffic to these baselines, assigning suspicion scores to requests for use in mitigations. By default, requests designated low and medium suspicion have a soft mitigation in the form of a silent JavaScript-based browser challenge. This challenge is imperceptible to legitimate users but significantly increasing resource costs for threat actors. Requests designated as high suspicion of contributing to DDoS attacks have a hard mitigation in the form of being blocked even if they have completed a challenge. The Anti-DDoS managed rule group defines challengeable requests as those using the GET method for URIs that do not match the Exempt URI regular expression (URIs that can't handle a silent JavaScript challenge). These requests are assigned the label awswaf:managed:aws:anti-ddos:challengeable-request. All other requests are non-challengeable. The AWS WAF Challenge action is the soft mitigation for challengeable requests. By default, challengeable requests with the header Accept: text/html receive a JavaScript payload that completes a task and issues a token/cookie. Then, the user is redirected back to their original page. Figure 1 shows this as a sequence diagram, and subsequent requests automatically include this cookie. Challengeable requests for other content types receive an HTTP 202 Request Accepted status and cannot proceed because the challenge is not completed. Further details on how the challenge works can be found in this AWS Networking and Content Delivery post.

Figure 1: Sequence diagram of the interstitial challenge interaction between a user, their browser, and AWS WAF protecting a resource

The use of soft and hard mitigations helps balance the effectiveness of DDoS mitigation with the impact to legitimate users during DDoS events. You may have considered minimizing the impact to legitimate users using clients that do not support this sequence by disabling the challenge or reducing the sensitivity of the block action as shown in Figure 2. This introduces the risk that your resource may be inadequately protected against DDoS attacks, or legitimate users are finding their falsely identified requests blocked due to increased sensitivity.

Figure 2: AWS WAF Anti-DDoS managed rule group configuration to disable the challenge

There are a number of rules and configuration options in the Anti-DDoS managed rule group that enable you to find the right trade-off between resource availability during a DDoS attack and the experience of your legitimate users who may or may not support the challenge soft mitigation. The flow chart in Figure 3 shows how the Anti-DDoS managed rule group works based on the incoming request and rule group configuration. The following section details how you can change the rule group configuration in a way that best makes the trade-off between your user experience and resource sensitivity during a DDoS attack.

Figure 3: Flow diagram for how the AWS WAF Anti-DDoS managed rule group evaluates each rule

Scenario 1: your clients or request types don't support the challenge The default Anti-DDoS managed rule group configuration sets the ChallengeAllDuringEvent rule to the challenge. Legitimate users experience disruption if they are making challengeable requests from unsupported clients during a DDoS event. The solution that offers the best experience for your users is to allow their clients to complete the challenge. The AWS WAF client integrations proactively complete a challenge and acquire a token as part of loading your resource outside of DDoS events. These SDKs need the AWS WAF Targeted Bot Control AMR to be deployed in the web ACL. All requests can now be treated as challengeable by the Anti-DDoS managed rule group. There are two options: JavaScript SDK: This SDK is suited for browser-based applications. This is achieved by embedding JavaScript in the HTML head of your application so that the challenge completes during page load. The cookie is automatically sent in subsequent requests. Mobile SDK: This SDK is available for both Android and iOS. The underlying workflow is similar to the JavaScript SDK as described in the AWS Developer Guide. If you have other types of clients that cannot use these SDKs (for example, machine-to-machine communication), then you cannot treat all requests as challengeable and must review the next scenarios for handling non-challengeable requests. If you operate across domains (for example an SPA www.company.com makes fetch requests to api.company.com), then you also need to configure the token domain correctly. This post covers different scenarios for cross domain configurations. The solution that gives your resource the most protection from DDoS requests is to make all requests challengeable. Figure 4 shows relaxing the Exempt URI regular expressions to \x00 to match no requests. The challenge no longer breaks API requests and static assets such as images, because all requests include the token when using SDK.

Figure 4: AWS WAF Anti-DDoS managed rule group configuration to challenge all requests

Non-GET (for example POST/PUT/DELETE) requests continue to be defined as non-challengeable by the Anti-DDoS managed rule group. However, the client integrations now include the token, which means they pass a challenge. You can make sure that clients making these requests have completed a challenge by adding the following rule after the Anti-DDoS managed rule group to challenge requests matching the DDoS event label (awswaf:managed:aws:anti-ddos:event-detected) for non-GET HTTP methods. { "Name": "ChallengeNonGetDuringDDoSEvent", "Priority": 100, "Action": { "Challenge": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "ChallengeNonGetDuringDDoSEvent" }, "Statement": { "AndStatement": { "Statements": [ { "NotStatement": { "Statement": { "ByteMatchStatement": { "FieldToMatch": { "Method": {} }, "PositionalConstraint": "EXACTLY", "SearchString": "GET", "TextTransformations": [ { "Type": "NONE", "Priority": 0 } ] } } } }, { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:event-detected" } } ] } } } JSON Scenario 2: Our clients cannot use the AWS WAF client integrations In this scenario, you with the trade-off of either impacting legitimate users using clients that cannot support the challenge or stopping DDoS requests that may impact the availability of your resource. Depending on the extent to which you are willing to impact legitimate users, you can start by changing the default rule group configuration. The following configuration options are ordered from highest to lowest in terms of providing your application with the most protection against DDoS attacks by relaxing the challenge soft mitigation as little as possible:

Permitting non-suspicious requests without challenge during an event: Changing the ChallengeAllDuringEvent rule to Count means that your legitimate users' non-suspicious requests are no longer challenged during a DDoS event. All suspicious requests are still challenged by the ChallengeDDoSRequests rule. Legitimate users making non-suspicious requests from native mobile applications and SPA fetch requests are no longer impacted. This trades off inconvenience for legitimate users falsely identified as making suspicious requests in exchange for stronger protection for your protected resource. Permitting more suspicious requests without challenge: If you are still concerned about legitimate users being falsely identified as sending low suspicion requests, then you can change the Challenge sensitivity to Medium. Requests from users with low suspicion are permitted so that your legitimate users are less likely to be impacted. Requests with medium suspicion are still challenged. This may be an acceptable trade-off for mobile clients that cannot support challenge and if you are concerned about low suspicion requests impacting legitimate users. Not challenging more endpoints: If you are unable to support the challenge, then add the minimal number of endpoints to this expression to permit your legitimate users through, while still using a soft mitigation for other endpoints that support the challenge. Consequently, you may find that a large volume of non-challengeable requests overwhelms your sensitive protected resource.

These options improve the user experience because users are less likely to be impacted by the challenge. At a certain scale this may mean either too few DDoS requests are blocked, or too many legitimate users are impacted by a challenge that they cannot complete. The following scenario offers a solution for this. Scenario 3: Non-challengeable requests overwhelm your resource before they're blocked You may still have a certain volume of non-challengeable requests, especially if the challenge mitigation is disabled or when you have limited ability to use the AWS WAF client integrations. Non-challengeable requests are always permitted until they reach the DDoS block sensitivity (high by default). This means that sensitive endpoints may be overwhelmed with low or medium suspicion requests because the soft mitigation isn't applied. The following common use-cases need special consideration:

POST requests for form submissions (such as from HTML pages) Single Page Apps (for example React) making fetch requests to API endpoints, particularly for endpoints that put load on your server Native mobile applications making requests to API endpoints Machine-to-Machine requests to API endpoints Requests for images or other static assets that overwhelm your resource

GET requests are made non-challengeable by the Exempt URI regular expressions than by default excludes /api and static assets. We recommend that you reduce the scope of this expression to make sure that as many requests can be challenged as possible. You have two options if this isn't possible:

Update the DDoS block sensitivity from low to medium (or high): Medium and high suspicion requests are blocked, ideally before impacting your resource negatively. Setting the block sensitivity to high also blocks low suspicion requests. This limits accessibility for legitimate users incorrectly identified with medium (or low) suspicion DDoS requests but maintains broader availability. Create an alternative soft limit by limiting the volume of non-challengeable requests during an event: Rather than negatively impacting your potentially legitimate users by blocking low/medium suspicion DDoS requests, you can create a rate-based rule that limits the number of non-challengeable suspicious requests allowed during a DDoS event. You can use an aggregation key such as IP, users (for example using the Authorization header), or a specific sensitive endpoint URI (for example an endpoint that completes an expensive database query).The following rule added after the Anti-DDoS managed rule group limits suspicious non-challengeable requests from a user (based on the Authorization header) to 10 requests per minute. Legitimate users with low/medium suspicion requests can still interact with your resource, while still limiting the volume of suspicious requests reaching your protected resource. { "Name": "RateLimitSuspiciousUserRequests", "Priority": 100, "Action": { "Block": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "RateLimitSuspiciousUserRequests" }, "Statement": { "RateBasedStatement": { "Limit": "10", "AggregateKeyType": "CUSTOM_KEYS", "EvaluationWindowSec": 60, "CustomKeys": [ { "Header": { "Name": "Authorization", "TextTransformations": [ { "Type": "NONE", "Priority": 0 } ] } } ], "ScopeDownStatement": { "AndStatement": { "Statements": [ { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:ddos-request" } }, { "NotStatement": { "Statement": { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:challengeable-request" } } } } ] } } } } } JSON Scenario 4: Challengeable requests overwhelm your resource before DDoS requests are blocked A threat actor may be able to overcome challenges, leading to low and medium suspicion requests overwhelming your server before they can blocked as high suspicion requests. This may be an issue if a GET request for HTML pages puts sufficient load on your server (for example a listing page that undertakes an expensive database query). You have three options:

Update the DDoS block sensitivity from low to medium (or high): Medium and high suspicion requests are blocked, ideally before it negatively impacts your resource. Setting block sensitivity to high will also block low suspicion requests. This limits accessibility for legitimate users incorrectly identified with medium (or low) suspicion DDoS requests but maintains broader availability. Create an alternative soft mitigation by blocking suspicious DDoS requests to sensitive endpoints: Rather than blanket blocking challengeable low/medium DDoS requests, you can create a rule that matches the awswaf:managed:aws:anti-ddos:ddos-request label and blocks these known requests from targeting specific sensitive endpoints. The following rule applied after the Anti-DDoS managed rule group blocks identified DDoS requests that are for the sensitive URI endpoint /products/list. { "Name": "BlockSensitiveEndpointsDDoS", "Priority": 100, "Action": { "Block": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "BlockSensitiveEndpointsDDoS" }, "Statement": { "AndStatement": { "Statements": [ { "ByteMatchStatement": { "FieldToMatch": { "UriPath": {} }, "PositionalConstraint": "EXACTLY", "SearchString": "/products/list", "TextTransformations": [ { "Type": "NONE", "Priority": 0 } ] } }, { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:ddos-request" } } ] } } } JSON Require CAPTCHA completion during an event: If your resource is particularly sensitive to DDoS attacks, or you have no tolerance for negatively impacting legitimate users, then you can demand a CAPTCHA be completed. This CAPTCHA significantly increases the resource costs for a threat actor. The following rule added after the Anti-DDoS managed rule group forces users to complete a CAPTCHA for challengeable requests that match the event detected label (awswaf:managed:aws:anti-ddos:event-detected). Legitimate users can still engage with your resource. CAPTCHA attempts are charged at the standard rate. Further details on CAPTCHA and how to integrate can be found in this AWS Networking and Content Delivery post. { "Name": "CAPTCHADDoSEvent", "Priority": 8, "Action": { "Captcha": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "CAPTCHADDoSEvent" }, "Statement": { "AndStatement": { "Statements": [ { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:event-detected" } }, { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:challengeable-request" } } ] } } }

JSON Conclusion In this post we described how the new AWS WAF Anti-DDoS managed rule group uses soft and hard mitigations to achieve a balance between the availability of a resource and the user experience of legitimate users. We described situations commonly encountered when using these mitigations with different clients, and explained how to effectively trade off user experience with mitigation effectiveness even if clients cannot support challenges. If you still have concerns about your ability to respond to sophisticated DDoS threats, then consider AWS Shield Advanced. Shield Advanced provides 24/7 access to the Shield Response Team (SRT) working with you to deploy custom mitigations to specific DDoS threats. About the authors

David MacDonald David is a Senior Solutions Architect focused on helping New Zealand startups build secure and scalable solutions. He has spent most of his career building and operating SaaS products that serve a variety of industries.

Joanna Knox Joanna is a Senior Cloud Support Engineer at AWS in Sydney Australia. With a background in network engineering, application delivery and security, she focuses on helping customers architect themselves to be resilient against DDoS and other security threats, as well as helping customers actively under DDoS. Outside of work she likes to DJ and cook.

Introducing the Overview of the AWS European Sovereign Cloud whitepaper

Amazon Web Services (AWS) recently released a new whitepaper, Overview of the AWS European Sovereign Cloud, available in English, German, and French, detailing the planned design and goals of this new infrastructure. The AWS European Sovereign Cloud is a new, independent cloud for Europe, designed to help public sector organizations and customers in highly regulated industries meet their evolving sovereignty and compliance needs. This effort, backed by a €7.8 billion investment in infrastructure, jobs creation, and skills development, will launch its first AWS Region in the State of Brandenburg, Germany by the end of 2025. This whitepaper provides a broad overview of the AWS European Sovereign Cloud highlighting how AWS is helping customers achieve their sovereignty requirements while benefitting from access to the full power of AWS. Key aspects covered in the whitepaper include:

Infrastructure – Dedicated physical infrastructure with multiple Availability Zones, following the established AWS Regional model approach Logical isolation – Logical separation from existing AWS Regions, with independent billing, account, and identity systems Operational control – Measures to help assure independent operation of the AWS European Sovereign Cloud, including staffing requirements Data sovereignty – Design that helps make sure customer content and customer-created metadata remain within EU boundaries unless customers choose otherwise Corporate governance – A distinct corporate structure under EU law, with EU nationals serving as managing directors and an independent advisory board Approach to law enforcement requests – The technical, operational, and legal measures implemented to help protect customer data and manage law enforcement requests

The whitepaper describes how these elements work together to deliver sovereign control and operational autonomy of our expansive service portfolio to meet Europe's digital sovereignty needs. The AWS European Sovereign Cloud will be the only fully featured, independently operated sovereign cloud backed by strong technical controls, sovereign assurances, and legal protections designed to meet the needs of European governments and enterprises. Customers and partners using the AWS European Sovereign Cloud will benefit from the full power of AWS including the same service portfolio, security, availability, performance, architecture, APIs, and innovations such as the AWS Nitro System. We have already made—and will continue to make—new investments in the design, development, and operation of the AWS European Sovereign Cloud. We are building on the strong foundation that has underpinned AWS services for years, including our long standing commitment to customer control over data residency, our design principal of strong regional isolation, our deep European engineering roots, and our more than a decade of experience operating multiple independent clouds for the most critical and restricted workloads. For more information about the AWS European Sovereign Cloud visit AWS European Sovereign Cloud.

J.D. Bean J.D. is Principal Architect of the AWS European Sovereign Cloud. His interests include security, privacy, and compliance. He is passionate about his work enabling AWS customers' successful cloud journeys. J.D. holds a Bachelor of Arts from The George Washington University and a Juris Doctor from New York University School of Law.

TAGS: data sovereignty, Digital Sovereignty, EU Data Protection, European Union, Public Sector, Security Blog

Cómo Enviar SMS Internacionales con Números Gratuitos de EE.UU. Usando AWS End User Messaging

AWS End User Messaging ahora admite capacidades de SMS internacional para Números Gratuitos de EE.UU. (TFN). Esta nueva función permite a las empresas usar un solo TFN de EE.UU. para enviar mensajes SMS a más de 150 países, simplificando el alcance global. Beneficia principalmente a clientes que necesitan enviar alertas transaccionales unidireccionales—como contraseñas de un solo uso (OTP) o notificaciones de envío—y empresas que quieren crear prototipos rápidamente y probar su estrategia de mensajería en nuevos mercados internacionales sin la complejidad de adquirir números específicos por país. Esta guía te mostrará los pros y contras de esta función y cómo habilitarla y cuándo usarla versus métodos tradicionales de envío específicos por país. ¿Qué Son los Números Gratuitos Internacionales de EE.UU.? Un número gratuito internacional de EE.UU. es un TFN (toll-free number) estándar de EE.UU. que ha sido habilitado con la capacidad de enviar mensajes SMS a destinos fuera de Estados Unidos. Esta función es compatible con versiones anteriores, lo que significa que puedes habilitarla en cualquier TFN de EE.UU. nuevo o existente en tu cuenta. Cómo Habilitar el Envío Internacional Hay tres formas principales de habilitar esta función para tus Números Gratuitos de EE.UU.:

Habilitar el envío internacional al registrar un nuevo número en la consola. Habilitar el envío internacional para un número existente en la consola. Habilitar el envío internacional para un número existente a través del AWS CLI.

1. Habilitar Al Registrar un Nuevo Número Gratuito de EE.UU. (Consola)

Desde la consola de AWS End User Messaging, navega a Administrar SMS

Desde la consola de AWS End User Messaging, navega a Configuraciones > Números de teléfono > y selecciona Originador de la solicitud

Paso 1: Seleccione un país, selecciona Estados Unidos (US) como tu país de destino

En Paso 2: Defina el caso de uso, configura las diversas opciones listadas para tu Caso de uso de mensajería previsto, y selecciona Sí para habilitar el envío Internacional, antes de hacer clic en Siguiente

Para Paso 3: Seleccionar tipo de originador, selecciona Gratuito, valida tus opciones de Política de recursos, selecciona Siguiente

En Paso 4: Revisar y solicitar: Verifica que la información que ingresaste sea correcta y selecciona Solicitar. Nota: Las solicitudes de registro de números gratuitos de EE.UU. pueden tomar aproximadamente 15 días hábiles para ser aprobadas.

Para más información, consulta Solicitar un número de teléfono en AWS End User Messaging SMS 2. Habilitar para un Número Gratuito de EE.UU. Existente (Consola o CLI) Si ya has adquirido un TFN, puedes habilitar la función de envío internacional en cualquier momento. Usando la Consola de Administración de AWS:

Navega a Configuraciones > Números de teléfono > y selecciona un número Gratuito existente

Marca la casilla Habilitar envío internacional en los detalles de tu número de teléfono

Usando el AWS CLI El comando update-phone-number te permite modificar las capacidades de un número de teléfono, mientras que el comando describe-phone-numbers te permite verificar su estado. 1. Para Habilitar el Envío Internacional: Usa el parámetro --international-sending-enabled aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --international-sending-enabled \ --region us-east-1 Bash Nota: Reemplaza "phone-a1b2c3d4e5f67890" con el ID real de tu número de teléfono 2. Para Deshabilitar el Envío Internacional: Usa el parámetro --no-international-sending-enabled aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --no-international-sending-enabled \ --region us-east-1Bash Respuesta Esperada (para update-phone-number): Un comando exitoso devuelve el objeto JSON completo para el número de teléfono. Confirma el cambio verificando que el valor InternationalSendingEnabled sea true { "PhoneNumberArn": "arn:aws:sms-voice:us-east-1:111122223333:phone-number/phone-a1b2c3d4e5f67890", "PhoneNumberId": "phone-a1b2c3d4e5f67890", "PhoneNumber": "+18005550199", "Status": "ACTIVE", "IsoCountryCode": "US", "MessageType": "TRANSACTIONAL", "NumberCapabilities": [ "SMS" ], "NumberType": "TOLL_FREE", "MonthlyLeasingPrice": "2.00", "TwoWayEnabled": true, "InternationalSendingEnabled": true, "CreatedTimestamp": "2025-08-15T10:30:00.123Z" } JSON 3. Para Verificar el Estado Actual: Usa el comando describe-phone-numbers con tu ID de Número de Teléfono para verificar su configuración actual en cualquier momento. aws pinpoint-sms-voice-v2 describe-phone-numbers \ --phone-number-ids "phone-a1b2c3d4e5f67890" \ --region us-east-1Bash Beneficios y Limitaciones Esta función ofrece una nueva forma poderosa de llegar a una audiencia global, pero es importante entender dónde destaca y cuáles son sus limitaciones. Beneficios (Ventajas)

Alcance Global con un Solo Número: Envía SMS a más de 150 países usando un solo TFN de EE.UU. existente. Gestión Simplificada: Evita la complejidad operacional y el costo de comprar y gestionar una flota de números de teléfono específicos por país. Prototipado y Pruebas Rápidas: Prueba rápidamente campañas de mensajería en nuevos mercados internacionales antes de comprometerte con el enfoque de mejores prácticas de adquirir números dedicados en el país. Optimización de Costos para Alertas Unidireccionales: Proporciona un método rentable para enviar mensajes transaccionales unidireccionales de alto volumen como OTP, recordatorios de citas y notificaciones de envío globalmente.

SMS Bidireccional Limitado a EE.UU. y Canadá: Las conversaciones SMS bidireccionales confiables solo son compatibles para destinatarios en Estados Unidos y Canadá. Solo Unidireccional para Todos los Otros Países: Para todos los otros destinos, esto es solo unidireccional. Entrega con máximo esfuerzo (no garantizado): El envío fuera de EE.UU. y Canadá es con máximo esfuerzo (no garantizado). El número de teléfono que aparece en el dispositivo del destinatario puede ser reemplazado con un número local o ID de Remitente, por lo que la mensajería bidireccional no funcionará para estos destinos. Para más detalles sobre maximizar la entrega, lee Una Guía para Optimizar la Entrega de SMS y Mejores Prácticas (Inglés). La Exclusión Gestionada No Está Garantizada Internacionalmente: La funcionalidad automática de respuesta STOP no funciona para destinos fuera de EE.UU. y Canadá. Para destinatarios internacionales, debes proporcionar un método alternativo de exclusión. Rendimiento Estándar (3 MPS): Los TFN internacionales tienen un rendimiento predeterminado de 3 Partes de Mensaje Por Segundo (MPS). Para campañas de alto volumen y alto rendimiento, los números específicos por país dedicados (como códigos cortos) son la mejor práctica recomendada.

Entendiendo el Costo El precio para esta función es directo:

Sin Tarifas Mensuales Adicionales: No hay cargo extra por habilitar la capacidad de envío internacional en tu TFN de EE.UU. Solo pagas el arrendamiento mensual estándar por el número mismo. Mensajería de Pago por Uso: Se te factura por cada mensaje SMS saliente a la tarifa estándar por mensaje para el país de destino.

Para una lista completa y actualizada de precios por país, visita la página de Precios de AWS End User Messaging. Cuándo Usar TFN Internacional vs. Números Específicos por País Elegir la herramienta correcta depende de tu caso de uso. Aquí hay una comparación simple:

Caso de Uso ¿Usar TFN Internacional? ¿Usar Número Específico por País (Mejor Práctica)?

Probar rápidamente un nuevo mercado Sí. Es la forma más rápida de comenzar. No, este enfoque toma más tiempo para configurar.

Enviar alertas unidireccionales (OTP, notificaciones) Sí. Es una solución simple y rentable. Sí, pero es más complejo si necesitas enviar a muchos países.

Requerir conversaciones bidireccionales Solo para EE.UU. y Canadá. Sí. Este es el enfoque requerido para SMS bidireccional confiable en un país específico.

Garantizar que tu marca/número aparezca consistentemente No, planifica que el ID del Remitente no se preserve, ya que esto no está garantizado internacionalmente. Sí. Esta es la razón principal para usar un número dedicado en el país.

Maximizar la entregabilidad para campañas críticas No, la entrega es "con máximo esfuerzo". Sí. Un número local proporciona la mayor probabilidad de entrega exitosa.

Consideraciones y Próximos Pasos Una vez que hayas habilitado tu envío internacional sobre Números Gratuitos de EE.UU., puedes mejorar tu estrategia de mensajería considerando resistencia, monitoreo y escalabilidad. Los siguientes recursos proporcionan mejores prácticas para mejorar tu envío.

Monitoreo de Entrega: Para monitorear tasas de entrega y patrones por país, puedes usar Conjuntos de Configuración para crear destinos de eventos. Esto te permite transmitir eventos SMS (como DELIVERED o FAILED) a servicios como Amazon CloudWatch o Amazon Data Firehose para análisis. Construyendo Resistencia: Para implementar entrega robusta, incluyendo estrategias de reintento automático para mensajes fallidos, recomendamos leer nuestra guía: Cómo construir entrega de SMS resistente con AWS End User Messaging (Inglés). Estrategia Global Más Amplia: Para una mirada más profunda a los elementos estratégicos de un programa SMS global, nuestro post sobre Cómo Gestionar el Envío Global de SMS con AWS End User Messaging (Inglés) proporciona información valiosa e incluye una plantilla para organizar casos de uso y seleccionar originadores.

Conclusión El SMS Internacional para Números Gratuitos de EE.UU. es una herramienta estratégica poderosa para empresas que buscan simplificar su mensajería global. Destaca en permitir pruebas rápidas en nuevos mercados y entregar eficientemente alertas transaccionales unidireccionales en todo el mundo desde un solo número. Sin embargo, no es un reemplazo para la mejor práctica de usar números de teléfono dedicados en el país cuando conversaciones bidireccionales confiables y marca garantizada son críticas para el éxito de tu campaña. Al entender sus beneficios y limitaciones, puedes usar estratégicamente esta función para comenzar rápidamente mientras planificas un movimiento a largo plazo hacia códigos específicos por país para tus mercados más importantes.

Taking a comprehensive perspective to mainframe application modernization with a disposition strategy

Introduction Mainframe customers face a myriad of options to modernize. Organizations now face urgent imperatives to modernize due to talent shortages, high and often rising costs, and restrictions on business agility imposed by the legacy environment. Customers also find themselves navigating a multitude of modernization patterns, tools, and strategies. The disposition strategy includes guiding principles that help you address complex mainframe monolithic applications and large code bases. Customers decompose legacy applications into manageable chunks, develop a target migration pattern, sequence the migrations, and build integrations to remaining mainframe workloads. This is iterated on until the application is fully decoupled. A disposition strategy is a multi-pattern approach to mainframe modernization. The right pattern is selected for the right workload, based on prioritization of business and IT objectives and workloads characteristics. Rather than view mainframe modernization as the collection of individual projects and disjointed migration phases, the disposition strategy advocates for the creation of a comprehensive perspective. This includes a roadmap that defines the entire journey from mainframe to cloud native from the beginning. This approach serves to accelerate migrations, reduce risk, and help customers achieve business and technology objectives in acceptable timeframes. Define a North Star Mainframe estates are varied across geographies, industries, and customers, with no two mainframe applications truly being alike. There is great diversity in legacy technologies, business objectives, future state requirements, and risk appetites across customers. Many large enterprises have multiple lines of business supported by applications running on the mainframe. These business lines also have a wide number of business leaders, application owners, and stakeholders across the organization dependent on mainframe processing. This dynamic often results in a scenario where organizations lack a clear strategy for the future state of the mainframe. We often see customers early in their modernization journey who have a mix of strategies actively being implemented or planned by different mainframe stakeholders. This results in a disjointed approach to modernization. The modernization may not have a north star vision articulated that guides the organization on their modernization journey. The first step for any successful mainframe modernization program is to define a north star. This north star is shared by the C-Suite and often the Board of Directors of an organization. Customers recognize the growing risks, costs, and competitive disadvantages associated with remaining on the mainframe. With executive leadership guidance for modernizing legacy applications, we see customers operate with greater speed, urgency, and successful delivery. Without a clear mission, we see that customers engage in a series of disjointed, tactical modernization programs. Disjointed programs may succeed in moving workloads of the mainframe platform yet struggle to deliver the full benefits from the modernization. In some cases, this may even result in MIPS usage increasing due to constraints put on the mainframe. To avoid this situation, we advise customers to define a north star by answering three primary questions:

Why are we modernizing? Where are we going? When will it happen?

Answering these questions sets the foundation for a successful mainframe migration program and helps the organization define common business and technical goals shared by the organization. Business Criteria for Modernization: Balancing business requirements with technical realities Business objectives can vary dramatically across divisions within organizations. Some business units have urgent requirements to change the functionality of their legacy applications, while others resist modification to established processes or user experiences. Broadly, we see priorities from the business falling into two categories: Category 1: Business functionality remains the same, but technology modernization is essential for business agility. No desire for functional change. In this scenario, the business users are quite happy with the existing business functions supported by the legacy applications. The business functions and workflows that exist today do not need to be altered, and the business users are opposed to the idea of functional changes. This can even be true for customers who have users interacting with greenscreens. Users that have been using greenscreens may be operating at such a high efficiency level that replacing this with a modern UX could result in productivity declines. Customers should generally expect that the majority of their mainframe workloads will fall into this category. Mainframe applications have persisted in organizations for decades. These applications may be well-suited to the business with years of custom business logic. They could be tailor-made for large enterprises and their differentiated way of doing business in their respective industries. Not every application requires functional transformation. For systems where stability and predictability are paramount, it is important to consider:

Refactor options that convert the legacy applications to a modern programming language and relational database. For example, using AWS Transform for mainframe, customers can refactor COBOL applications to Java on AWS using specialized AI agents from AWS. Re-platforming options that maintain existing functionality and preserve legacy technology stacks while moving to more modern infrastructure. For example, AWS Mainframe Modernization capabilities provide options to re-platform to mainframe compatible runtimes in the cloud.

For these applications, focus on communicating the non-functional benefits of modernization: resiliency, reduced impact radius, availability, agility. Category 2: Business functionality requires changes to remove technical debt, add new features, and decompose monoliths for product alignment. Requirements for functional enhancements. This is where the business requires functional enhancements to be made to the applications. Generally, customers expect only a portion of their mainframe workloads to fall into this category. In this situation, the business may have desires for modern UIs, real time capabilities, or faster batch processing. Customers may also have goals to break legacy monoliths into product aligned business functions. This approach creates a microservices architecture for loose coupling, which facilitates agility and innovation. Customers are increasingly met with growing end-customer expectations for near real-time capabilities. These are challenging to introduce to mainframe monoliths. Additionally, we meet with customers who have ambitions to expand into new markets, industries, or otherwise grow their customer base. Often, customers with aggressive growth objectives find that maintaining mainframe applications hinders their ability to grow and win new business.

Growth enablement: Which applications, if modernized, would unlock new revenue streams or support business expansion? Customer experience impact: Which applications directly affect customer interactions and satisfaction? Market responsiveness: Which systems currently limit your ability to respond to market changes? Innovation potential: Which applications would benefit most from modern development practices and integration with cutting-edge technologies?

Business units with clearly articulated functional requirements for enhancement should typically receive higher prioritization. Their specific needs, whether for new features, improved user experiences, or integration capabilities, provide concrete objectives that can drive modernization efforts and demonstrate tangible value.

Reimagine pattern is defined as the rewriting and rearchitecting of a mainframe application to a modern technology stack. The goal of this modernization is to introduce functional changes to the application. When the business requires new functionality, the reimagine pattern is the preferred approach.

Technical Criteria for Modernization The disposition strategy for mainframe modernization should also incorporate technical criteria evaluated at both the organizational and application levels. Evaluating Technical considerations at the organization level: Data center exit strategy or urgent migration deadlines Organizations facing mandates for a data center exit have urgent migration deadlines that must prioritize speed and risk mitigation. The rehost pattern, meaning moving mainframe workloads to a partner-managed data center, with no code changes, might be a pragmatic first step. The rehost approach can often be completed in months, bypassing the lengthy cycles of alternative migration patterns. Rehosting can provide business continuity. It also lays the foundation for future modernization and helps organizations incrementally adopt more advanced patterns. These patterns can address modernization needs like refactor or reimagine within time and resource constraints. Niche Mainframe technologies: programming language, transactional monitor, database Specific programming languages, transactional monitors, and database technologies used by existing mainframe applications can have a significant impact on the feasibility and complexity of modernization efforts. This is an important technical consideration when evaluating a mainframe modernization disposition strategy. Certain mainframe technologies, like Natural/Adabas, IDMS, and others may not be directly supported or fully supported by some modernization patterns like re-platforming or refactoring. The availability and maintainability of skills for these legacy mainframe technologies is also a factor. This can limit the modernization options available, and patterns like replace or reimagine might be the only viable choices. Organizations must carefully assess the mainframe technology stack being used and how that aligns with the feasibility and complexity of different modernization approaches. This technical assessment is a key input into determining the right disposition strategy. Vendor renewal timelines Vendor renewal timelines can be an important technical consideration when evaluating a mainframe modernization disposition strategy. Organizations often have various mainframe vendors with different software licensing contracts. Renewal timelines raise risks when commercial terms are being evaluated. In this case, some customers determine the best value outcome includes an exit from those vendor technologies as quickly as possible. This timeline velocity can also influence the modernization approach to choose. For example, if there is an urgent deadline to exit a license agreement, a refactoring pattern may be more suitable than a replacement or reimagine approach. Refactoring helps the organization to modernize the application while preserving the core functionality, which can often be done more quickly than a full rewrite or reimplementation. However, it's important to note that not all refactoring solutions support all mainframe technologies. An assessment of the right refactoring solution for the specific technologies in use must be completed. In some cases, there may not be an obvious or proven refactoring solution available. A reimagination or replacement approach may be the only viable option to exit the vendor technology by the required deadline. It is important to evaluate mainframe vendor contracts and renewal timelines as part of the overall technical assessment when determining the best modernization strategy. This helps align the selected approach with the urgency to exit particular vendor technologies. Available mainframe skillsets When companies face a resource talent risk due to constrained mainframe skillsets, it is important to choose mainframe modernization options that have less reliance on those skills. In such cases, strategies like refactoring and reimagining the mainframe applications can be effective approaches. Conversely, when companies have a capable mainframe talent pool within their organization, a re-platforming approach can be a suitable strategy for modernization. They can use existing expertise to migrate the workloads to a more modern platform. Evaluating Technical Complexity and Dependencies of the application/workload The selection of an appropriate pattern should be based on both business considerations and technical requirements. It should consider the specific characteristics of each workload or application. It is critical to conduct a thorough technical assessment of the various applications and workloads to determine the best modernization approach for each one. During this assessment phase, consider the following factors:

Source technologies: Evaluate the programming languages and the volume of the existing source code. Some languages and frameworks are more amenable to automated transformation and modernization than others. This can impact the feasibility and complexity of certain modernization patterns. Data considerations: Assess the data store technologies used on the mainframe (Db2, IMS DB, VSAM, etc.). Evaluate the volumes of data, the complexity of the data structures, and the relationships between data entities. The nature and complexity of the data can influence the appropriate modernization approach. Degree of coupling: Identify the level of coupling between the different applications and workloads. For example, an application that includes transactional context propagation likely has tight coupling. In this case, it would pose more modernization challenges than those with loose coupling or clear service boundaries. This is because the interdependencies of the tightly coupled functions must be addressed in order and be specifically managed in the modernization journey. Integration complexity and dependencies: Assess the various integration points between the applications and workloads. Identify shared resources, data dependencies, and the overall complexity of the integration landscape. This can help determine the appropriate modernization pattern that can preserve the existing integrations or help to provide a transition with less risk. External interfaces: Depending on the chosen modernization pattern, some client applications running outside of the mainframe but accessing it through external interfaces may also be changed. Verify that the selected pattern supports the required interfaces for all external connection points, API operations, and data exchange mechanisms with external systems.

Grouping applications that access the same data in read/write mode together and choosing the same patterns for those groups. Selecting the same pattern for workloads with a high degree of coupling Considering the impact of the source programming languages on the feasibility of different patterns Selecting a pattern that will minimize changes to external interfaces and integrations where possible.

An application and workload analysis is a key input into the overall disposition strategy. It can add the right modernization patterns and solutions for workloads based on their unique technical characteristics and dependencies. Strategy Development Building a Business-Outcome-Driven Program Rather than treating modernization as a purely technical exercise, develop a program that:

Works backwards from the organizations north star: As mentioned at the start, customers need an organizational strategy and approach for their mainframe estate. Successful mainframe migration projects operate within the parameters set by the company's leadership. Why are they modernizing, where are they going, and when will they achieve it. Aligns with strategic business objectives: Modernization should support specific business outcomes such as increased agility, improved customer experience, or new capabilities. Considers the entire portfolio from the start: Even if modernization is defined as a phased approach, planning should cover the complete application landscape to avoid creating new technology silos. Balances tactical wins with strategic goals: Design the program to deliver incremental value while working toward comprehensive modernization. Establishes clear metrics for success: Define how you'll measure progress in both business and technical dimensions.

Without a strategy set at the C-suite level, individual teams may adopt divergent approaches or fall into a "wait and see" mindset. This could delay modernization and create additional complexity. Avoid the "rebuild everything" pitfall Our experience shows that the 80/20 principle generally applies to mainframe estates: roughly ~80% of the mainframe applications do not require functional changes, and ~20% of the applications must be reimagined. We advise customers to consider modernization approaches that include substantial mainframe exits. Customers like Transamerica and Goldman Sachs have successfully used refactor and re-platform patterns to move mission-critical mainframe workloads to AWS. Taking individual application-by-application approaches may be too slow to meet business imperatives. Consider incorporating multiple modernization patterns, based on business and technological objectives.

Refactoring at scale: AWS Transform for mainframe provides refactor capabilities that can help you to modernize the legacy application to modern, Java frameworks. This pattern can be used when you want to reduce dependency on legacy technologies while benefiting from the accelerated migration timeline offered by deterministic tooling. Re-platforming: Re-platforming uses emulation technology to achieve a like-for-like migration of mainframe applications. This is often referred to as a "COBOL to COBOL migration". In this case, the re-platforming pattern can address situations with COBOL talent shortages, and accelerate mainframe exits.

Mass modernization approaches combined with strategic reimagining provide customers the opportunity to align technical and business outcomes while still driving towards a platform exit. Customers who consider multiple patterns in their strategy are able to address more varied objectives within their organization. This is done while delivering operational cost reductions goals within the same time period. Conclusion: The Time for Action is Now Today, the imperative for mainframe application modernization is strong. Beyond the commonly cited challenges of talent shortages, increasing software costs, and organizational inefficiencies, a new driver has emerged: the growing impact of generative AI on software development. As generative AI coding assistants revolutionize productivity for modern languages, the development speed and productivity gap between modern and mainframe technologies will compound. Organizations with applications in COBOL, Assembler, or PL/1 languages face a growing competitive disadvantage regarding the speed to value. Their peers may operate core systems written in modern technologies that can move at ever increasing development speeds. There is no "silver bullet" for mainframe modernization. Success requires a business-driven, multi-pattern approach that aligns IT and business objectives around concrete outcomes. By using automation and iterating incrementally, you can focus on the value beyond cost savings. The disposition strategy provides a framework for this journey, one that recognizes the nuances of each application portfolio. By modernizing mainframe applications with this approach, organizations can preserve the valuable business logic built over decades while positioning themselves for future innovation demands.

Tim Gray Tim is the worldwide go-to-market lead for AWS Transform for mainframe. Tim focuses on the go-to-market strategy to help customers leverage AWS Transform to reimagine and modernize their core systems on AWS. Today, Tim focuses on building repeatable patterns to deploy generative and agentic AI services that accelerate large-scale modernization programs in unprecedented timeframes.

Sunil Divvela Sunil Divvela is a Worldwide Specialist Solutions Architect for Mainframe Modernization at AWS. He works closely with customers and partners to innovate and accelerate their mainframe modernization journeys, leading initiatives from portfolio assessment through post-migration support using Generative AI and Agentic AI. Before joining AWS, Sunil served as a Senior Technology Architect for Infosys, where he led multiple mainframe transformation projects.

Yann Kindelberger Yann Kindelberger is a Principal Solution Architect at Amazon Web Services. Yann has worked with Mainframes for 23+, being a Mainframe Architect at IBM for more than 20 years. He is part of a WW team working on migration and modernization of Mainframe to the AWS Cloud. He joined AWS in 2021 and his role as a Solution Architect is to help, advise and support customers to migrate and modernize their Mainframe.

When your 20-year-old monolithic application needs new features and you can't afford downtime or risk breaking existing functionality, what do you do? This challenge faces countless organizations running mission-critical legacy systems that power their business but struggle to keep pace with modern requirements. At AWS, we often hear from customers who need to modernize their legacy applications while maintaining business continuity. In this blog post, we will explore how to use event-driven architecture (EDA) with the leave-and-layer pattern to modernize your applications incrementally. The "leave-and-layer" architectural pattern, combined with using an event bus like Amazon EventBridge, enables you to add new capabilities to existing applications without the complexity and risk of traditional modernization approaches. The Challenge: Innovation vs. Stability Consider a scenario: a developer with extensive experience in JavaScript and .NET joins a company running a Java-based e-commerce application. The application has been successful for two decades, but the company is losing market share because they can't innovate fast enough. The new developer's lack of domain knowledge of the application and limited Java skills create a challenge. The business needs to integrate with a third-party customer support system to notify teams whenever new customers sign up. The challenges are significant: the developer doesn't know Java, the application can't have downtime, and leadership mandates all new development must be cloud-native. This scenario illustrates a common dilemma: how do you extend existing applications, with new modern components, while maintaining business continuity and reducing risk? Two Paths: Strangler Fig vs. Leave-and-Layer The Strangler Fig Pattern The strangler fig pattern involves gradually replacing parts of a legacy application by extracting functionality from the monolith, implementing it as microservices, routing traffic to new services, and eventually "strangling" the old system. While effective, this approach presents several challenges. Teams need deep knowledge of the existing codebase to safely extract functionality. Database splitting becomes complex when multiple services share data stores. Converting synchronous transactions to asynchronous patterns requires careful orchestration. Application Programming Interface (API) design and versioning add another layer of complexity as you manage contracts between old and new systems. The Leave-and-Layer Pattern The leave-and-layer pattern takes a fundamentally different approach. You leave the existing application completely unchanged while layering new functionality alongside it. This approach uses loose coupling for integration and allows you to build new capabilities in your preferred technology stack. This pattern is ideal when you need to add functionality without affecting existing systems, work with unfamiliar legacy technology, deliver quickly with minimal risk, or build cloud-native extensions to traditional applications.

Building Asynchronous Integration with Amazon EventBridge The key to a successful leave-and-layer implementation lies in asynchronous communication. Modern systems benefit from asynchronous patterns where actions trigger responses without requiring immediate acknowledgment. Amazon EventBridge provides foundation for event-driven architecture through three core components. Event producers are systems that generate events, such as when a new customer signs up. Event consumers are systems that react to these events, like sending notifications to support teams. The event broker, which is EventBridge itself, acts as the intelligent routing layer between producers and consumers.

EventBridge offers three types of event buses to handle various integration scenarios. The default event bus works with events coming from AWS services. Custom event buses handle events from your applications. Software as a Service (SaaS) event buses integrate with upstream SaaS platforms that support EventBridge as a destination. Implementation: Minimal Code, Maximum Impact The integration challenge is solved with just a few lines of code added to the existing application. There are three steps: enabling event production in the legacy application, configuring EventBridge rules for routing and transformation, and setting up API Destinations for third-party integration. The Complete Solution Architecture The final architecture demonstrates the elegance of the leave-and-layer pattern. The legacy application continues running unchanged, serving existing customers without any disruption. A minimal event publishing layer publishes customer signup events to EventBridge. EventBridge rules provide the integration intelligence, matching event patterns and transforming data as needed. API Destinations handle connections to external systems, providing reliable delivery with built-in retry mechanisms. The entire integration happens asynchronously, ensuring that third-party system availability doesn't impact the core application.

Step 1: Enable Event Production To add event publishing to your legacy application, create an abstraction layer that hides the EventBridge implementation details. This approach lets you switch between event buses and publish events both synchronously and asynchronously. The following examples use the AWS SDK for Java v2 EventBridge client. Read, What is the AWS SDK for Java 2.x for complete SDK documentation. Create the Event Publishing Interface First, define the PostEvents interface that specifies how to publish events: import java.util.concurrent.CompletableFuture;

public interface PostEvents { void publishEvent(String source, String detailType, Object eventData); CompletableFuture publishEventAsync(String source, String detailType, Object eventData); }Java Implement the EventBridge Publisher Next, create the EventBridgeEventPublisher class that implements the PostEvents interface: import software.amazon.awssdk.services.eventbridge.EventBridgeClient; import software.amazon.awssdk.services.eventbridge.model.PutEventsRequest; import software.amazon.awssdk.services.eventbridge.model.PutEventsRequestEntry; import com.fasterxml.jackson.databind.ObjectMapper; import java.util.concurrent.CompletableFuture; import org.springframework.stereotype.Component;

@Component public class EventBridgeEventPublisher implements PostEvents { private final EventBridgeClient eventBridgeClient = EventBridgeClient.create(); private final ObjectMapper objectMapper = new ObjectMapper();

@Override public void publishEvent(String source, String detailType, Object eventData) { try { PutEventsRequestEntry event = PutEventsRequestEntry.builder() .source(source) .detailType(detailType) .detail(objectMapper.writeValueAsString(eventData)) .eventBusName("unicorn-shop-events") .build();

eventBridgeClient.putEvents(PutEventsRequest.builder() .entries(event) .build()); } catch (Exception e) { // Handle serialization/publishing errors throw new RuntimeException("Failed to publish event", e); } }

@Override public CompletableFuture publishEventAsync(String source, String detailType, Object eventData) { return CompletableFuture.runAsync(() -> publishEvent(source, detailType, eventData)); } }Java Update Your Controller Finally, modify your controller to use the new event publishing abstraction: import org.springframework.web.bind.annotation.RestController; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.http.ResponseEntity;

@PostMapping("/signup") public ResponseEntity createUser(@RequestBody User user) { // Existing user creation logic remains unchanged...

// Publish event using abstraction eventPublisher.publishEvent("unicorn.shop", "New Customer Signup", user);

return ResponseEntity.ok("User created successfully"); } }Java This approach maintains your existing user creation logic while adding event publishing capabilities. This abstraction provides several benefits: the EventBridge implementation details are hidden from business logic, and both synchronous and asynchronous publishing patterns are supported based on use case requirements. Step 2: Configure EventBridge Rules In Step 1, you configured your application to publish events with the source 'unicorn.shop' and detail type 'New Customer Signup'. Now you'll create rules to handle these specific events. EventBridge rules provide the intelligence layer that determines how events flow through your system. These rules match specific event patterns, transform data for downstream systems, and route events to appropriate targets, as shown in Figure 4. The rule includes pattern matching that ensures only events containing the required fields get processed. Any events that don't match this pattern are automatically rejected. You can create EventBridge rules using the AWS Command Line Interface (CLI), AWS CloudFormation, or the AWS Management Console. Here's how to set up the rule using AWS CLI: # First, create the custom event bus aws events create-event-bus --name customer-events

# Create the EventBridge rule aws events put-rule \ --event-bus-name customer-events \ --name customer-signup-rule \ --event-pattern '{ "source": ["ecommerce.application"], "detail-type": ["New Customer Signup"], "detail": { "firstName": [{"exists": true}], "lastName": [{"exists": true}], "email": [{"exists": true}] } }' \ --description "Route new customer signup events to external systems"Bash

Step 3: Use API Destinations EventBridge API Destinations eliminate the need for custom integration code when connecting to third-party systems. This feature provides built-in retry logic, error handling, and authentication support including API keys and OAuth. You configure an API Destination pointing to your third-party system's webhook URL. The rule includes an input transformer that converts the EventBridge event format into the JSON structure that the external system expects. This transformation happens entirely within EventBridge, requiring no additional compute resources or custom code.

Create the Authentication Connection First, set up authentication for your third-party system: # Create a connection for authentication aws events create-connection \ --name slack-connection \ --description "Connection to Slack webhook" \ --authorization-type API_KEY \ --auth-parameters '{ "ApiKeyAuthParameters": { "ApiKeyName": "Authorization", "ApiKeyValue": "Bearer xoxb-your-slack-bot-token" } }'Bash Create the API Destination Next, configure the destination endpoint: # Create the API destination aws events create-destination \ --name slack-customer-notifications \ --description "Send customer notifications to Slack" \ --connection-arn arn:aws:events:us-east-1:123456789012:connection/slack-connection \ --http-method POST \ --invocation-endpoint Add the Target to Your Rule Finally, connect the API destination to your EventBridge rule: # Add the API destination as a target to the rule aws events put-targets \ --event-bus-name customer-events \ --rule customer-signup-rule \ --targets '[ { "Id": "1", "Arn": "arn:aws:events:us-east-1:123456789012:destination/slack-customer-notifications", "HttpParameters": { "HeaderParameters": { "Content-Type": "application/json" } } } ]'Bash Benefits Realized This approach delivers immediate and measurable value to organizations. Existing applications experience limited downtime during implementation and continue to serve customers with no performance impact on the legacy application. Development teams can deliver new features within single sprints, demonstrating the speed advantage of the leave-and-layer approach. Developers can implement solutions using familiar AWS services rather than spending weeks learning legacy frameworks and patterns. The cloud-native foundation built with EventBridge provides a platform for future integrations without requiring additional changes to legacy applications. When used with the strangler fig pattern, this can accelerate the decomposition of the monolith by providing loose coupling. Perhaps most importantly, the solution proves its resilience in production. When external systems experience temporary outages, EventBridge automatically retries failed deliveries, ensuring no notifications are lost. Key Takeaways Pattern selection impacts project success. Choose leave-and-layer pattern when you need to extend rather than replace functionality, especially when working with unfamiliar technology stacks or tight delivery timelines. Asynchronous architecture provides inherent resilience. Event-driven systems handle third-party unavailability gracefully, automatically retrying failed operations and maintaining system stability even when dependencies experience issues. Minimal code changes can enable significant new capabilities. With a few lines of code to unlock integration possibilities that would require extensive custom development using traditional approaches. EventBridge simplifies complex integrations through managed services. API Destinations eliminate the need for custom integration code, while built-in transformation capabilities handle data format differences between systems. Call to Action Ready to implement the leave-and-layer pattern in your own legacy modernization project? Start by exploring the EventBridge documentation to learn more about event-driven architecture patterns. For hands on work, try out Building Event-Driven Applications With Amazon EventBridge on Skill Builder or Building event-driven architectures on AWS Workshop Studio.

Jeff Escott Jeff Escott is a Principal Architect at AWS with over 30 years of technology experience. He works with customers to define modernization strategies and adopt cloud-first approaches, focusing on helping customers transform across people, process, and technology. Additionally, Jeff has been a long time advocate for advancing the practice of architecture and growing future architects. Jeff holds a bachelor's degree in Actuarial Science from Worcester Polytechnic Institute and is based in Connecticut, USA.

Jorge Alvarez Jorge Alvarez is a Solutions Architect Specialist in Migration and Modernization at AWS. In his role, he provides guidance to customers in their Migration and Modernization journey in EMEA. Prior to becoming a solutions architect at AWS, Jorge was leading teams in the cloud space, helping customers in their journey and modernization efforts.

How Strands Agents brings geospatial generative AI to the public sector

Government agencies manage vast amounts of location-based information daily, tracking where incidents occur, coordinating nearby crews, and optimizing resource routing. Yet despite this spatial data representing roughly 80% of all government information, accessing these insights often requires specialized geographic information system (GIS) expertise and complex software that creates barriers for staff members. This post explores how AI agents powered by Strands Agents on Amazon Web Services (AWS) can transform geospatial workflows, making spatial data accessible to any government worker through natural language interactions. We'll examine the technical architecture, implementation details, and demonstrate the approach using Brussels' FixMyStreet open data API, an open data API available for the city of Brussels, which provides real-time nonemergency incident reports including graffiti, road maintenance, and public infrastructure repairs across the city. The challenge: Inaccessible location data Although approximately 80% of government data contains spatial components that could drive better decision-making across numerous areas, including infrastructure maintenance, emergency response, urban planning, and resource allocation, two critical barriers prevent broader access to this information. The first is bottlenecks due to GIS expertise. Traditional tools require training in spatial concepts, coordinate systems, and complex interfaces. Second, some interfaces are inaccessible. Without intuitive tools, valuable spatial insights remain locked away from the staff who could benefit most from location-based decision-making in their daily operations. The solution lies in intelligent agents that can bridge this gap, handling the technical complexity and allowing staff to interact with spatial data through simple, conversational requests. Solution overview This agent-based approach processes natural language queries and presents geospatial insights through an interactive map visualization. The AI agents are built with Strands Agents, a lightweight framework designed for creating task-oriented AI agents that can integrate multiple tools and data sources. Strands provides the structure needed to integrate natural language understanding—using models such as Anthropic's Claude Haiku 3.5 in Amazon Bedrock—with specialized geospatial tools while maintaining control over how these components work together. By combining these capabilities, the solution eliminates the technical barriers that prevent most government staff from accessing location-based insights while maintaining the accuracy and reliability required for public sector operations. The following real-world example demonstrates how this works in practice, using publicly available Brussels nonemergency incident data. Here's how a government employee might interact with the system using publicly available Brussels nonemergency data. User query: "Show me all graffiti reports in Grand Place that are ready to get processed"

Figure 1: User asking the agent "Show me all graffiti reports in Grand Place that are ready to get processed"

Agent response: "I've found 10 graffiti incidents in the Grand Place area that are currently in the PROCESSING status. Notably, 9 out of the 10 incidents have been open for more than 14 days, which might indicate a backlog in graffiti removal. Would you like me to help you create a route to address these incidents or provide more details about any specific incident?" The results appear immediately on an interactive map, while the agent seamlessly handles the technical complexity behind the scenes, as shown in the following screenshot.

Figure 2: The result of the query, showing the points from the search results for incident reports in "Grand Place" that are in "PROCESSING" status, as well as a short summary

Geocoding "Grand Place" to coordinates (4.35178, 50.84623; 4.35329, 50.84721) Filtering for "graffiti" category and "PROCESSING" status Executing a spatial query within the Grand Place bounding box Returning formatted results with summary statistics

The user can then ask a follow-up query for route optimization and assignment. For follow-up queries, the user can interact with the visualized results. When they select the bounding box from the previous response, it becomes highlighted and provides context for the next request, as shown in the following screenshot. User Query: "Create a route starting and ending at Avenue Des Arts 27 and assign it to the relevant team"

Figure 3: User asking the agent the follow-up query, "Create a route starting and ending at Avenue Des Arts 27 and assign it to the relevant team"

Agent Response: "I've created an optimized route starting and ending at Avenue Des Arts 27, which covers 10 graffiti incidents in the Grand Place area. The route has been assigned to the graffiti team with high priority, with special notes about the long-standing incidents. The team will receive full details about the route and incidents to address. Is there anything else you would like me to do?" The optimized route is displayed on the map with turn-by-turn directions, and team members receive instant notifications containing complete route details, the priority level and special notes, a direct link to the navigation interface, and a list of incidents to address, as shown in the following screenshot.

Figure 4: The result of the query, showing the route through the incident reports from the previous search result and assigning it to the relevant team for collection

This complete workflow, from natural language query to actionable route assignment, demonstrates how the solution transforms geospatial operations into straightforward, conversational interactions that still offer the precision and reliability that government operations require. Architecture overview The following architecture demonstrates how these concepts work together in practice, integrating AI processing, geospatial tools, and data management to deliver the solution's conversational geospatial capabilities. The solution architecture integrates several components to deliver seamless geospatial analysis:

User interface layer – A web-based interface that accepts natural language queries and displays interactive maps with search results. Tool layer – AWS Lambda functions handle geocoding, route creation, and route assignment. These are decoupled from the agent for modularity, whereas tools that generate SQL remain within the agent Lambda in the isolated subnet in the virtual private cloud (VPC) for secure PostGIS access. AI processing layer – Amazon Bedrock with Claude Haiku 3.5 interprets user intent and orchestrates the appropriate geospatial operations. Data layer – Amazon Relational Database Service (Amazon RDS) for PostgreSQL with PostGIS extensions provides spatial data storage and querying capabilities. Ingestion layer – A Lambda function updates nonemergency incidents from the Brussels FixMyStreet API every minute using Amazon EventBridge Scheduler.

Although the architecture provides the foundation for the solution, the real intelligence comes from how the agent processes and responds to user queries. Let's examine the systematic workflow, or Agent Loop, that enables natural language interactions with this infrastructure. Agent Loop The agent uses Strands Agents to process user queries through an iterative workflow that seamlessly orchestrates specialized geospatial tools. This Agent Loop operates through a systematic sequence:

Query analysis – The agent interprets natural language input to understand spatial intent and requirements. Tool selection – Based on current context and conversation history, the agent chooses the most appropriate geospatial tool (this decision can be revisited after each tool execution). Parameter extraction – The agent pulls the inputs it needs immediately—such as addresses, coordinates, and filters—and uses any new data gathered so far from previous tool execution and conversation history. Tool execution – The agent invokes the chosen tool, analyzes the output, and determines whether to continue the loop with additional tools or proceed to finalization. Result processing – After the iterative loop concludes, the agent formats spatial outputs for clear visualization. Response generation – The agent delivers natural language explanations alongside interactive map displays.

To be able to achieve its goals, the agent has some tools available:

Geocode Address – Converts addresses and place names to single coordinates or bounding-boxes Get Incidents In Area – Generates and executes PostGIS point-in-polygon queries using ST_Within() for spatial containment analysis Get Incidents Near Location – Generates and executes PostGIS proximity searches with ST_DWithin() to find incidents within specified radii Get Optimized Route – Calculates efficient routes for field teams and service vehicles Assign Route To Team – Assigns the optimized routes to relevant field teams with priority levels and contextual notes Get Current Time – Retrieves current date and time for temporal context

The following diagram illustrates the Agent Loop for the geospatial application, highlighting the available tools.

Figure 6: The agent loop of the geospatial agent, with the different tools available to the agent

This iterative approach brakes down complex geospatial queries into manageable steps, with each tool building upon previous results to deliver comprehensive, actionable insights. Tool-based query building Rather than allowing AI agents to generate arbitrary SQL queries (text-to-SQL), we've implemented a controlled, tool-based approach that balances flexibility with security and reliability. Direct text-to-SQL approaches let an LLM write raw SQL for every prompt. That works for quick prototypes, but in public sector environments, it introduces unpredictable queries, hard-to-trace errors, and wide security exposure. Our tool-based architecture addresses these concerns:

Controlled execution – The AI selects from preapproved tools rather than generating arbitrary SQL queries Parameterized queries – All database interactions use tested and secure code with user inputs is safely handled as parameters Consistent performance – Query patterns are optimized and predictable, enabling reliable system behavior Full auditability – The observability capabilities of Strands Agents provide complete logging of which tools were executed and with what parameters

Our approach begins by encoding existing field team workflows into dedicated tools, then expanding the catalog as new use cases emerge. Each new tool inherits the same security guardrails and operational standards. Because every query executes through owned, versioned tools, we can thoroughly test changes before deployment and reuse audited logic across teams—so that operations staff receive consistently trustworthy results. Radius search example The Strands framework enables reliable spatial analysis through validated tools. Here's an example of a proximity search tool that generates PostGIS queries for area-based incident searches:

base_query += " AND (" + " OR ".join(["LOWER(status) = LOWER(%s)"] * len(statuses)) + ")"

This tool definition uses the Strands Agents @tool decorator, which automatically generates a schema from the function signature and type constraints. This means that the agent's parameter choices are restricted to valid, domain-specific values. The status parameter demonstrates this constraint-based approach: IncidentStatus = Literal["CREATED", "PROCESSING", "CLOSED"] By restricting the agent to the three specific status values, "CREATED", "PROCESSING", and "CLOSED", the system prevents errors from invalid status strings but provides consistent data handling across all queries. This structured approach transforms potentially risky, open-ended SQL generation into a more secure and predictable system that maintains the flexibility government operations require while delivering the reliability they demand. Conclusion This solution shows how government agencies can democratize access to geospatial data by pairing the natural-language tooling of Strands Agents with purpose-built geospatial capabilities. The approach maintains the accuracy and reliability essential for public sector operations while dramatically expanding who can access spatial insights. The versatility of this agent-coordinated, tool-based approach extends well beyond incident response across numerous public sector applications such as urban planning, transportation, and environmental monitoring. Urban planning teams can surface permit backlogs, identify zoning conflicts, visualize hotspots, and drill into specific parcels through natural language queries displayed on interactive maps. Transportation departments can analyze traffic congestion patterns, optimize route planning, and seamlessly share navigation paths with field crews. Units can query air quality readings, overlay trend analysis, and proactively identify compliance risks across geographic regions. By using trusted AI models, validated tools, and shared map visualization to standardize queries, this architecture provides agencies with a practical, cost-effective alternative to traditional GIS software. The result is a scalable solution that puts sophisticated geospatial insights into the hands of far more government workers, enabling data-driven decision-making across departments without requiring specialized technical expertise.

TAGS: Artificial Intelligence, AWS Public Sector, geospatial data, government, technical how-to

Andreas Holt Andreas is a solutions architect intern at AWS. He is completing his master's in computer science at Aalborg University. His areas of interest are mainly in distributed systems and cloud computing.

Angel Zarramera Angel is a solutions architect at AWS working with public sector customers. His areas of interest are cloud governance, networking, and security.

We're excited to share that the Swift AWS Lambda Runtime project has officially moved to the AWS Labs organization. You can now find it here: 👉 This move marks a new chapter for the project, while maintaining full continuity with its roots. A thank you to the Swift community The Swift AWS Lambda Runtime was originally created seven years ago by the Swift community (the initial commit was made the 1st of November 2018), as part of a broader effort to make Swift a powerful choice for server-side development. Since then, the project has been an incubating project as part as the Swift Server Workgroup (SSWG). We want to thank the 58 contributors who have helped shape this project through their code, feedback, and support. Their collective work established a solid foundation that has allowed Swift developers to confidently run their code on AWS Lambda. Why choose Swift for your server-side workload Swift has evolved far beyond its origins as a language for Apple platforms — it's now a robust, high-performance choice for building modern server-side systems. Swift's combination of type safety, predictable performance, and developer-friendly syntax makes it an appealing option for backend services where reliability, maintainability, and scalability matter most. Over the past few years, several organizations have successfully deployed Swift in production on the server. According to the Swift.org ecosystem overview, companies migrating from other languages have achieved impressive results: one reported a 4× performance increase after switching to Swift, while another saw 40 % more throughput, 50 % less hardware usage, and 90 % lower memory consumption compared to their Java-based implementation. Web frameworks such as Vapor and Hummingbird have made Swift a first-class option for developing REST APIs, microservices, and event-driven systems. Whether you're building a traditional web service, a distributed system, or serverless functions, Swift provides the safety and speed developers expect — without compromising on readability or productivity. Why choose Swift for your Lambda functions The Swift AWS Lambda Runtime allows you to write Lambda functions using the Swift programming language, bringing the language's safety, performance, and expressiveness to the world of serverless computing. In an event-driven architecture like AWS Lambda — where functions start and stop on demand and billing is measured in milliseconds — every millisecond and megabyte matters. Swift's design makes it especially well-suited for this environment: it's fast to start, efficient in memory use, and helps developers produce safe, predictable code that performs consistently under varying loads. Here are a few of the advantages that make Swift a great fit for Lambda workloads:

Performance – Swift is fast, leveraging LLVM optimizations and native execution on Amazon Linux environments. Memory efficiency – Swift's small memory footprint and automatic memory management are crucial in Lambda's constrained runtime environment, directly impacting both performance and cost. Type safety – Swift's strong typing and compiler checks help catch errors at build time, reducing runtime issues. Concurrency safety – Swift's strict concurrency checking at compile time helps prevent data races and concurrency bugs before deployment. Modern developer experience – Swift's concise syntax, rich type system, and expressive APIs make it easy to write, test, and maintain Lambda functions efficiently.

Whether you're building APIs, event-driven microservices, or data processing functions, Swift offers a powerful, modern option for your serverless workloads. What changes — and what stays the same Going forward, governance of the project will transition to AWS, under the AWS Labs GitHub organization. There will be no major changes in how the project works today. The same codebase, APIs, and open source spirit remain intact. The project remains an incubation project as part of the Swift Server Workgroup. Our goal with this move is to increase visibility for the runtime and make it easier for Swift developers — and the broader serverless community — to discover, use, and contribute to it. By hosting it under AWS Labs, we hope to strengthen the collaboration between the Swift open source communities and AWS teams, and to encourage more experimentation and innovation in this space. Open source Lambda runtimes Swift joins a growing family of open source Lambda runtimes maintained by the community and AWS. If you're working with other languages, you might be interested in exploring all the open source Lambda runtimes in the official documentation. Get involved The Swift AWS Lambda Runtime remains an open source project, and community contributions are at its heart. We warmly welcome issues and pull requests from developers who want to help improve the runtime or extend its capabilities. Visit the repository, explore the documentation, and join us in building the next generation of Swift serverless applications: 👉

Sébastien Stormacq Seb has been writing code since he first touched a Commodore 64 in the mid-eighties. He inspires builders to unlock the value of the AWS cloud, using his secret blend of passion, enthusiasm, customer advocacy, curiosity and creativity. His interests are software architecture, developer tools and mobile computing. If you want to sell him something, be sure it has an API. Follow @sebsto on Bluesky, X, Mastodon, and others.

In today's competitive gaming landscape, machine learning (ML) has become essential for delivering personalized experiences, optimizing game mechanics, and driving business outcomes. However, traditional approaches to building and deploying ML systems often require extensive DevOps expertise, manual pipeline configuration, and complex infrastructure management that can slow down innovation and time-to-market. Game studios need agile, automated solutions that can rapidly iterate on ML models, while maintaining production reliability and scalability across diverse gaming use cases. Amazon SageMaker AI and MLOps Amazon SageMaker AI provides powerful MLOps capabilities. However, orchestrating the complete continuous integration and continuous delivery (CI/CD) pipeline—from model development to production deployment—typically involves navigating multiple Amazon Web Services (AWS) services. These include managing intricate dependencies and coordinating approval workflows. This complexity can create barriers for game studios, or game analytics teams, who want to focus on building great predictive models rather than wrestling with infrastructure. We will demonstrate how to leverage Amazon Bedrock Agents to create an intelligent MLOps assistant that streamlines the entire CI/CD pipeline construction and management process. We will combine the conversational capabilities of Amazon Bedrock with the robust MLOps features of Amazon SageMaker AI. With this solution game teams can create, manage, and deploy gaming prediction models using natural language commands. Our solution addresses common pain points in gaming machine learning model build, train, and deploy pipelines:

Rapid experimentation: Quickly spin up new prediction experiments without infrastructure overhead Automated workflows: Streamline the path from model training to production deployment Approval management: Handle model approvals through conversational interfaces Multi-project coordination: Manage multiple game titles and their respective models from a single interface

By the end of this walkthrough we will have created a fully functional MLOps agent, capable of managing complex machine learning workflows for gaming analytics. Your team can then deploy the gaming prediction solution with conversational commands such as, "Create a player churn CI/CD pipeline for my mobile puzzle game," or "Show status of build pipeline execution". Prerequisites Before starting you will need to make certain you have done or have the following:

A GitHub account with remote access The AWS Command Line Interface installed An Amazon SageMaker AI domain and user

Create and configure an MLOps management agent Set up the foundation infrastructure Before creating the Amazon Bedrock Agent, establish the core AWS infrastructure that will support the MLOps workflows. The infrastructure includes two AWS Identity and Access Management (IAM) roles:

Trust relationships and policies for each role have been provided and referenced in the create role steps. Roles First, create an mlops-agent-role with attached inline policies to enable the Amazon Bedrock Agent to access required AWS services that support an MLOps pipeline.

Create an IAM role, mlops-agent-role with Trusted entity type AWS service and use case Lambda Select the Trust relationships tab, Edit the trust policy and paste the trust relationship policy in the trusted entities editor Add permissions with Create inline policy and paste the mlops-agent-policy in the policy editor box Create a second inline policy for AWS Lambda invocation access, and paste the lambda-invoke-access policy in the policy editor Replace ACCOUNT_ID with your AWS account ID in the policy document

Next, create an IAM role that allows the AWS Lambda action invocation to access required AWS services.

Create an IAM role called lambda-agent-role with trusted entity type AWS service and use case Lambda Search for the AWSLambdaBasicExecutionRole managed policy and add Select the Trust relationships tab, Edit the trust policy and paste the trust relationship policy in the trusted entities editor Next, add permissions with Create inline policy and add a lambda-agent-policy

MLOps AWS Lambda function The AWS Lambda function serves as the backend engine for the Amazon Bedrock Agent, handling all MLOps operations through an API. The Amazon Bedrock action group invocation calls the AWS Lambda function using an action group schema that maps endpoints to actions. Use the following steps to create the AWS Lambda function:

In the console, select AWS Lambda Select Author from scratch Enter a Function name, we used: mlops-project-management Choose a Python 3.1x Runtime Select x86_64 Change the default execution role to use the existing lambda-agent-role previously created Select the Create function button

Download the AWS Lambda function file or clone the function from our GitHub AWS Samples repository Copy and paste the function code into the Lambda code window

Select the Configuration tab to access the Function and General configuration Choose the Edit button and update the function Timeout value to 15 minutes On the Configuration tab, select Permissions and Add permissions for Resource-based policy statements:

Choose AWS Service For Service, select other Statement ID: bedrock-agent-invoke Principal: amazonaws.com Source ARN: arn:aws:bedrock: : :agent/* Action: lambda:InvokeFunction and Save

The function is now ready to act as an Amazon Bedrock Agent and support the following actions:

/configure-code-connection – Set up AWS CodeConnections connection for GitHub integration /create-mlops-project – Create a new SageMaker MLOps project with GitHub integration /create-feature-store-group – Create SageMaker Feature Store Feature Group /create-model-group – Create SageMaker Model Package Group /create-mlflow-server – Create Amazon SageMaker AI MLflow Tracking Server /build-cicd-pipeline – Build CI/CD pipeline using seed code from GitHub /manage-model-approval – Manage model package approval in Amazon SageMaker AI Model Registry /manage-staging-approval – List models in staging ready for manual approval /manage-project-lifecycle – Handle project updates and lifecycle management /list-mlops-templates – List available MLOps AWS Service Catalog templates

Repository seed code population from GitHub Dynamic AWS CodeBuild build script generation with project-specific parameters Pipeline parameter injection and configuration Multi-stage approval workflow management Error handling and detailed logging for troubleshooting

Amazon Bedrock MLOps Agent Use the following steps to create the agent:

In the console, Navigate to Amazon Bedrock Select Agents from the left navigation panel under Build Choose Create Agent Configure the agent with these settings:

Agent Name: MLOpsOrchestrator Description: Intelligent assistant for gaming MLOps CI/CD pipeline management Foundation Model: US Anthropic Claude 3.7 Sonnet Use the existing service role, mlops-agent-role for the agent resource role

Configure the agent Instructions—provide instructions that establish the agent's identity and capabilities by using the following:

You are an expert MLOps engineer specializing in SageMaker pipeline orchestration. Help users create, manage, and deploy ML models through automated CI/CD pipelines. Always follow AWS best practices and provide clear status updates. Available actions include: - Creating CodeConnections for GitHub integration - Setting up MLOps projects and CI/CD pipelines - Managing feature stores and MLflow tracking - Handling model and deployment approvals

Amazon Bedrock Agent action groups Use the following steps to create action groups:

Enter the group name: ProjectManagement Enter the following description: Actions for managing SageMaker MLOps projects and GitHub integration Select the Action group type: Define with API schemas Under Action group invocation, make certain to select: Select an existing Lambda function Under Select Lambda function, select mlops-project-management, with Function version as $LATEST

Under Action group schema, select Define via in-line schema editor Download the MLOps agent OpenAPI schema from the GitHub AWS Samples repository Select JSON from the drop-down and paste the provided OpenAPI schema in the editor Choose Save and exit

Use the MLOps agent With the agent created and configured, it's ready to use for launching AWS resources to support an MLOps CI/CD pipeline. As a foundation of the pipeline, an AWS Service Catalog template defines AWS CodeBuild projects, AWS CodePipeline pipelines, and SageMaker AI inference endpoints for staging and production. Creating an Amazon SageMaker AI project launches these resources with configuration specified with the MLOps agent. Before creating an Amazon SageMaker AI project with the AWS Service Catalog template, you'll need to set up several prerequisites. These prerequisites include:

An AWS CodeConnection to access GitHub A managed MLflow tracking server A feature store with sample features for the MLOps template that handles model building, training, and deployment with third-party Git repositories

The Feature Store Group features are based on a synthetic player churn data set

For the AWS Service Catalog template, create two empty private GitHub repositories:

Select Test and Prepare in the Amazon Bedrock Agents console Enter a prompt to create resources using natural language

Start with what MLOps tasks can you perform? When using provided, example prompts, note created resources values and replace where appropriate

Use the agent to create a Feature Store group for gaming analytics by using the following prompt:

Create Feature Store group named "player-churn-features" with feature description "player_id as string identifier, player_lifetime as number, player_churn as integer, time_of_day features as floats, cohort_id features as binary flags, event_time as event time feature" and description "Feature group for player churn prediction model containing player behavior and engagement metrics"

Next, Create an Amazon SageMaker AI managed MLflow tracking server by entering the following prompt. Use your account ID where indicated:

Create MLflow tracking server named "player-churn-tracking-server" with artifact store "s3://game-ml-artifacts-ACCOUNT_ID/mlflow/" and size "Medium" and role_arn "arn:aws:iam::ACCOUNT_ID:role/mlops-agent-role"

To complete the AWS CodeConnection setup, select the created connection in the console and choose Update pending connection. Select Install a new app. You will be redirected to GitHub to authenticate and select repository access. Choose Connect and the connection status will change from Pending to Available.

With supporting MLOps infrastructure created, navigate to the MLOpsOrchestrator agent in the AWS console.

Create an Amazon SageMaker AI MLOps project by using the following prompt:

Create an MLOps project named "mlops-player-churn" with GitHub username "your Github username", build repository "player-churn-model-build", deploy repository "player-churn-model-deploy", using connection ARN "your connection arn"

The MLOps project creates and executes an Amazon SageMaker AI pipeline. Copy the model package group name from the prompt response. The pipeline execution adds a model to the Amazon SageMaker AI Model Registry in Pending manual approval Approve the model by using the following prompt:

Approve model in model package group "your model package group name"

Build a CI/CD pipeline for project "mlops-player-churn" with model build repository "gitUserName/player-churn-model-build", deploy repository "gitUserName/player-churn-model-deploy", connection ARN "your connection arn", feature group "player-churn-features", S3 bucket "game-ml-artifacts-ACCOUNT_ID", MLflow server "your-mlflow-arn", and pipeline "player-churn-training-pipeline"

To verify and visualize the pipeline, in the console, navigate to AWS CodePipeline.

There will be two pipelines, one for build and another for deploy.

Select the link of the build project to view pipeline steps.

To deploy a production inference endpoint, navigate to the AWS CodePipelines deploy pipeline.

To trigger CI/CD pipeline execution, push any changed code to the model-build repository.

Using an Amazon Bedrock Agent, a complete MLOps model build, and deployment of a CI/CD pipeline, has been created. Try out additional agent prompts to experiment with the flexibility and function of the agent. Amazon SageMaker Canvas can be used to connect to data sources (such as transactional databases, data warehouses, Amazon Simple Storage Service (Amazon S3)) or over 50 other data providers. SageMaker Canvas can be used to feature engineer data and as a data source for the MLOps model build and deploy pipeline. Cleanup To avoid ongoing charges, navigate to these AWS services in the console and terminate those resources. A command line automated cleanup script is available to delete resources as well. The script uses resource tags to safely identify and remove all MLOps deployed resources. The script automatically removes all MLOps resources tagged with CreatedBy=MLOpsAgent. Run cleanup-by-tags.sh to terminate resources. Conclusion Building an intelligent MLOps CI/CD pipeline management system using Amazon Bedrock Agents represents an advancement in how gaming teams can approach machine learning operations. Throughout this walkthrough, we've demonstrated how to transform complex, multi-service MLOps workflows into streamlined, conversational interactions that reduce the barrier to entry for gaming analytics. Contact an AWS Representative to find out how we can help accelerate your business. Further reading

Amazon SageMaker AI workflows Amazon SageMaker Pipelines Amazon SageMaker for MLOps MLOps Amazon SageMaker AI notebook samples Operationalize Machine Learning with Amazon SageMaker MLOps and MLFlow workshop

Steve Phillips Steve Phillips is a senior technical account manager at AWS in the North America region. Steve has worked with games customers for eight years and currently focuses on data warehouse architectural design, data lakes, data ingestion pipelines, and cloud distributed architectures.

Scott Selinger Scott Selinger is a Senior Solutions Architect in North America who works with Games customers. With his wealth of experience in the GameTech industry, he skillfully guides customers in leveraging AWS services to unlock the full potential of their endeavors.

Scott Flaster Scott Flaster is a Senior Technical Account Manager who works with Games customers. He is passionate about building large-scale distributed applications to solve business problems using his knowledge in AI/ML, Security, and Infrastucture.

Migrating Amazon DocumentDB Cluster across Regions: A step by step guide

As businesses expand globally and AWS continues to grow its regional footprint, some organizations need their database infrastructure to span multiple Regions. Organizations span Amazon DocumentDB (with MongoDB compatibility) clusters across AWS Regions to reduce latency by serving customers from the nearest AWS Regions, or to meet regulatory compliance requirements. You can also use Amazon DocumentDB Global Clusters to do cross-region migration with minimal downtime. On July 14, 2025, Amazon DocumentDB expanded Global Clusters support from 5 to 10 secondary Regions. This enhancement allows organizations to deploy their document compatible databases across more regions worldwide, improving performance for globally distributed applications and providing greater flexibility for compliance and Disaster Recovery (DR) strategies. Amazon DocumentDB is a serverless, fully managed, MongoDB API-compatible document database service that makes it easy and cost effective to operate critical document workloads at virtually any scale without managing infrastructure. Amazon DocumentDB serves tens of thousands of customers globally across all industries. You can enhance your applications with GenAI and machine learning (ML) capabilities using vector search for Amazon DocumentDB and integration with Amazon SageMaker Canvas. Solution overview In this post, you will learn to migrate a regional Amazon DocumentDB cluster from one AWS Region to another using the Global Cluster feature with low downtime and without performance impact. This solution will incur Global Cluster costs for replicated write I/O, compute instances, and storage across regions based on your configuration. You can use the AWS Pricing Calculator to estimate the cost based on your configuration. Below are the high-level steps to perform this migration:

Convert existing regional cluster into a global cluster by adding a Region Perform the switchover and update the application configuration Remove the global cluster setup

Prerequisites Let's consider an example scenario where we demonstrate how to migrate a regional Amazon DocumentDB cluster from ap-southeast-1 (Singapore) Region to ap-south-1 (Mumbai) using Global cluster setup through the AWS console

Amazon DocumentDB 5.0 instance-based cluster in the source Region ap-southeast-1 . You can use an existing cluster or create a new one. VPC/subnet setup in the target Region ap-south-1 Take a manual snapshot of cluster on source ap-southeast-1 Region Application is deployed in target Region ap-south-1

Convert the regional cluster into a Global Cluster We will create a global cluster by converting the source regional cluster docdb-sgp in ap-southeast-1 into a global cluster by adding ap-south-1 as secondary Region.

From the Amazon DocumentDB console in ap-southeast-1, select the docdb-sgp cluster and from the Actions menu, select Add Region. Choose the Secondary region as ap-south-1 and provide the global cluster identifier and secondary cluster identifier. For example, Global cluster identifier: docdb-migration-demo and Secondary cluster identifier: docdb-mum Choose the DB instance class and Number of replica instances for your secondary cluster. We recommend selecting the same number of instances and instance class for your secondary cluster to ensure you have the same capacity available for your application in the new Region. Enable Show advanced settings to select the network settings for your secondary Region cluster, including VPC details and subnet group. Ensure other settings match your primary cluster, such as encryption at rest settings. Note: You cannot change VPC, subnet groups and encryption settings after the cluster is created. Select secondary cluster port number and cluster parameter group. Make sure your cluster parameter group have similar settings as source Region cluster. Rest of the settings can be left to default and choose create cluster button

Wait for secondary Region to become active The secondary Region will need some time to become available. Once ready, the docdb-migration-demo will have its primary cluster in the ap-southeast-1 Region and a secondary cluster in the ap-south-1 Region.

Data Replication happens from primary Region cluster ap-south-east-1 to secondary Region cluster in ap-south-1. It is important to confirm the replication lag by monitoring the GlobalClusterReplicationLag CloudWatch metric on secondary cluster doc-mum. This metric should be minimum (in milliseconds)

Navigate Amazon DocumentDB console on the secondary ap-south-1 Region. On the doc-mum (secondary region cluster), select Monitoring tab to find GlobalClusterReplicationLag and choose view in metrics

Note: GlobalClusterReplicationLag metric is available on secondary clusters only Perform the switchover Once replication is synced (replica lag in millisecond range), we are ready to promote the secondary cluster in ap-south-1 to primary. As both the Regions are healthy, we will perform a Switchover to complete our migration.

Stop application writes to the source Region cluster. Using AWS DocumentDB console in any Region, select the global cluster docdb-migration-demo and from the Actions menu choose Switchover or Failover. Select the switchover and new primary Region cluster ap-south-1 Region cluster and hit the Confirm button. The switchover process takes few minutes to complete to prevent data loss. During this period, the application cannot perform any operation on the database (downtime).

Wait for status to become available for primary and secondary clusters. After switchover step completion, cluster is ap-south-1 Region becomes the new primary and the cluster in ap-southeast-1 Region becomes secondary.

Locate connection string for the ap-south-1 cluster docdb-mum, for more information see Finding a cluster's endpoints. Update the application connection configuration to use the docdb-mum cluster endpoint and start the application in ap-south-1 Region. Don't use global cluster endpoint. Make sure application is working fine and all the checkouts are good. Now you have successfully migrated to ap-south-1 Region. The secondary Region cluster in ap-southeast-1 is now unused. But, before deleting the secondary Region, we recommend keeping this setup running for a few days in case you need to switch back to the old region ap-southeast-1

To roll back to the previous primary Region ap-southeast-1, we can initiate a switchover from ap-south-1 to ap-southeast-1 and update the application endpoint, provided the ap-southeast-1 cluster or the global cluster configuration exists. Remove Global Cluster Setup We can remove the global cluster setup using the following steps

From the Amazon DocumentDB Console in ap-southeast-1 Region. Select the current secondary Region cluster docdb-sgp and from the Actions choose Remove from global and select Remove and Promote. This step removes docdb-sgp cluster from Global cluster setup and promotes it as standalone cluster.

Once docdb-sgp is removed from the Global cluster, only the docdb-mum cluster will remain in the global cluster with the role of primary cluster.

Remove the primary cluster in docdb-mum from Global cluster setup. Choose the current primary cluster docdb-mum and from Actions – Select Remove and Promote

Delete the Global Cluster docdb-migration-demo, which has 0 Regions Navigate to the ap-southeast-1 Region and delete the old cluster docdb-sgp

Note: You may need to disable the Deletion protection if it is enabled. Conclusion You can use Amazon DocumentDB Global Clusters to migrate your database between AWS Regions with minimum downtime and operational complexity. Whether you're moving for performance, compliance, or disaster recovery, this architecture supports scalable, cross-regional Amazon DocumentDB deployments with confidence. For more information about recent launches and blog posts, see Amazon DocumentDB resources.

Anshu Vajpayee Anshu is a Senior Amazon DocumentDB Specialist Solutions Architect at AWS, helping customers adopt NoSQL databases and modernize applications using Amazon DocumentDB. Before joining AWS, he worked extensively with relational and NoSQL databases.

Kaarthiik Thota Kaarthiik is a Senior NoSQL Specialist Solutions Architect at AWS. He is passionate about database technologies and enjoys helping customers solve problems and modernize applications using NoSQL databases. Before joining AWS, he worked extensively with Relational databases, NoSQL databases, and Analytics platforms for over 15 years.

This blog walks you through how to use the WorkSpaces applications Simple Pricing Tool for Always-On and On-Demand fleets with visualization of usage patterns, common pricing examples, and education licensing. To learn about Elastic fleets, read AWS announces Availability of Elastic fleets with Amazon WorkSpaces applications and refer to the WorkSpaces applications pricing page. You can also use the AWS Pricing Calculator with WorkSpaces applications to create an estimation of costs as part of a broader AWS service deployment. Amazon WorkSpaces applications is a fully managed application streaming service that enables you to centrally manage your desktop applications and securely deliver them to any computer. With WorkSpaces applications, you can easily scale to any number of users across the globe without acquiring, provisioning, and operating hardware or infrastructure. Software vendors are using WorkSpaces applications to stream GPU-intensive 3D design and engineering applications to users, while enterprises are replacing their application streaming technologies with WorkSpaces applications. Customers really like the WorkSpaces applications pricing model of paying only for the instances they provisioned to meet their users' streaming needs, but it is different than their existing application streaming environments that are based on provisioning for peak capacity, even during nights, weekends, and holidays when load would be lower. To help customers estimate their WorkSpaces applications price, we developed a simple, easy to use pricing tool. Simply provide your total number of users, actual concurrent usage per hour, instance type, and fleet utilization, and the pricing tool will estimate your per user price. It will also show you the estimated price savings when you use an On-Demand fleet instead of an Always-On fleet. Both fleet types use scaling policies to ensure there are enough streaming instances available when your users want to stream, while letting you (the admin) choose between application launch time, and cost. When your user launches their application on an Always-On streaming instance, the application starts loading almost immediately, while an On-Demand streaming instance has a brief wait (up to 90 seconds) while the streaming instance is powered on and made available. With Always-On streaming instances, you pay the hourly fee for the instance even when no user is streaming. With On-Demand streaming instances, you pay a lower stopped instance fee when the user isn't streaming, and the hourly fee when they are. The Amazon WorkSpaces applications Simple Pricing Tool provides only an estimate of your AWS fees related to your usage of WorkSpaces applications and doesn't include any taxes that might apply. Your actual fees depend on a variety of factors, including your actual usage of AWS services. Prerequisites

Download the WorkSpaces applications Pricing Tool: Microsoft Excel File Collect your actual or estimated concurrency usage by hour

How to use the WorkSpaces applications Pricing Tool The WorkSpaces applications Pricing Tool is provided as a Microsoft Excel or OpenOffice Calc spreadsheet that enables you to enter in basic information about your usage, then provides a cost estimate for the WorkSpaces applications environment. The spreadsheet has a Price Estimator worksheet, and a Usage Pattern worksheet. Within the spreadsheet, the cells that require your input are denoted by a light blue background. The cells with a gray background are informational or aggregations, while the cells with green backgrounds are calculated cost estimates. To get started, open the WorkSpaces applications Pricing Tool using Microsoft Office or Open Office. On the Price Estimator worksheet enter the following information:

Cell B8: The total number of unique users that will stream in a given month. You will fill out expected concurrency per hour later. Cell B9: Maximum number of user sessions to be provisioned per instance. This must be set > 1 if you are using a multi-session fleet. Cell B10: The Operating System to be used Cell B11: The AWS Region that will be used Cell B12: The instance type and size that will be used by your users Cell B13: The RDS SAL type – you can select between Commercial License Included, Bring Your Own License (BYOL) if you have RDS CALs that can be ported, or Academic License Included for qualified schools, universities, and public institutions. RDS SAL types are only applicable for Microsoft Windows operating system. For Amazon Linux 2, there is no RDS SAL type and you must set it to "Not Applicable". Cell B14: Enter in a percent of concurrency that you want to maintain as buffer capacity to ensure new users are able to start streaming without waiting for new instances to be provisioned. Note: Each cell has text that appears when you mouse over it providing additional details and instructions.

Once you have entered all of the information on the Price Estimator worksheet, switch to the Usage Pattern worksheet. On the Usage Pattern worksheet, enter the following:

Cell C2: Enter the number of weeks per month. A default of 4 weeks per month has been entered. Cell C5: Enter in the number of days per work week. A default of 5 days per work week has been entered. Cell C7-C30: Fill in your actual or estimated concurrency per hour during a week day. If you expect no usage in specific hours, enter 0. Column D will be automatically calculated based on the buffer capacity entered on the Price Estimator worksheet. Cell C35: Enter in the number of days per weekend. A default of 2 days per weekend has been entered. Cell C37-C61: Fill in your actual or estimated concurrency per hour during a weekend day. If you expect no usage in specific hours, enter 0. Column D will be automatically calculated based on the buffer capacity entered on the Price Estimator worksheet.

The WorkSpaces applications Pricing Tool will automatically calculate the number of instances and buffer instances based on what was entered in column C, Max number of sessions per instance, and the buffer capacity percent specified on the Price Estimator worksheet.

After you have filled in your per-hour usage patterns on the Usage Pattern worksheet, switch back to the Price Estimator worksheet. The WorkSpaces applications Pricing Tool calculates the effective monthly used and buffer hours, total monthly cost estimate, annualized cost estimate, and effective monthly cost per user estimate. The WorkSpaces applications Pricing Tool also calculates the cost savings by using On-Demand fleet type versus the Always-On fleet type, allowing you to choose between application launch speed and cost to deliver applications.

You can use the effectively monthly hours per user calculation to spot check whether the values entered in the usage pattern match your expectations of user usage. If they don't, switch to the Usage Pattern worksheet, and adjust the per-hour usage pattern. Now let's walk through some examples of using the WorkSpaces applications Pricing Tool. Example scenario streaming line of business applications Suppose you are an enterprise that uses an application streaming technology to stream the SAP GUI to your users, or you're a financial ISV that wants to stream your accounting software to your customers as a service and need to estimate the price of using WorkSpaces applications. Your infrastructure is built in the Oregon (us-west-2) AWS Region, and you have 2,000 total users that will stream during the month. The application runs perfectly using the stream.standard.medium instance type and size. Your usage is fairly predictable during the weekday, so you maintain a 20% buffer, with sporadic usage over the weekend. Your enterprise has Microsoft License Mobility, enabling you to port your RDS CALs to AWS for use with WorkSpaces applications. For users covered by your RDS CALs, you won't incur monthly user fees. Your inputs on the Price Estimator worksheet will look like this:

Based on this usage pattern, and the environment details, the WorkSpaces applications Pricing Tool estimates that the effective monthly cost per user would be $7.49 for Always-On, or $6.56 for On-Demand. You can find that at the bottom of the Price Estimator worksheet:

Example scenario streaming graphics apps Now suppose you are an industrial engineering company that wants to stream GPU-intensive applications like SOLIDWORKS or Siemens NX to your design engineers who are located in France, Germany, and Italy. Currently you are deploying engineering workstations, but your engineers want portability in how and where they work by using ultrabooks. You want to estimate the cost of using WorkSpaces applications GPU-backed instances to augment providing your design engineers with ultrabooks. Your product lifecycle management and product data management infrastructure are in the Frankfurt (eu-central-1) AWS Region, and you have 3,000 design engineers who will stream sporadically throughout the day. You are going to use the stream.graphics.g4dn.2xlarge for your design engineers to ensure they have sufficient CPU, memory, and GPU for them. You don't have existing RDS CALs to port, so you will be buying them from AWS. You're not too sure on your usage pattern, and so you want to maintain extra buffer capacity of 40% during the onboarding phase, then adjust after usage is identified. On weekends, you don't expect any usage, but want to maintain some capacity for the design engineers who may need it. On the Price Estimator worksheet, you would enter:

Based on this usage pattern and the environment details, the WorkSpaces applications Pricing Tool estimates that the effective monthly cost per user per month would be $109.63 for an Always-On fleet, or $80.06 for an On-Demand fleet. In this scenario, you can save 27.0% with your users waiting up to 90 seconds for their application to launch with an On-Demand fleet. You can find this at the bottom of the Price Estimator worksheet:

Example scenario streaming graphics apps using multi-session fleets Now let us revisit the previous scenario, you realized that when your users are streaming their graphics applications using WorkSpaces applications stream.graphics.g4dn.2xlarge instances the underlying resource utilization is not high and you would like to squeeze in more users on the same instance. With multi-session fleets you can achieve that. Let us assume we provision 3 user sessions on a single instance and take a more power instance stream.graphics.g4dn.4xlarge. On the Price Estimator worksheet, you would enter:

And for the usage pattern, you expect (Observe number of instances required are less than the number of users):

Based on this usage pattern and the multiple user session on a single instance, the WorkSpaces applications Pricing Tool estimates that the effective monthly cost per user per month would be $67.95 for an Always-On fleet, or $50.42 for an On-Demand fleet. In this scenario, using multi-fleet with same number of users and usage pattern you can save ~39% of your effective monthly cost/user.

Other costs to consider The WorkSpaces applications Pricing Tool helps you with estimating your streaming instances costs. Other costs that are common to WorkSpaces applications environments are storage. For example, you will pay Amazon S3 storage fees if you enable WorkSpaces applications's Home Folder or Application Settings Persistence features for your users. Or, if you create an Amazon FSx file share for your users to save their data to. Conclusion WorkSpaces applications enables you to migrate away from your designed-for-peak-capacity application streaming environment to a dynamic, scalable, managed service where you only pay for what you use, when you use it. And with the WorkSpaces applications Pricing Tool, you can quickly estimate the cost per user to compare against your alternative options.

TAGS: Amazon End User Computing, Amazon WorkSpaces, Cost Optimization, desktop and application streaming

Recently, Equinix introduced the Equinix Fabric Cloud Router (FCR), a virtual routing service that enhances multicloud networking while improving performance, security and cost-efficiency. Equinix FCR with AWS Direct Connect is available in AWS Marketplace, allowing organizations to establish private, dedicated connections across their cloud infrastructure. By procuring Equinix FCR in AWS Marketplace, organizations benefit from streamlined procurement processes, consolidated billing with their existing Amazon Web Services (AWS) account, and seamless integration that reduces deployment complexity while enabling faster time-to-value for multicloud connectivity solutions.

According to a TechTarget Enterprise Strategy Group survey, 94% of organizations work with multiple cloud providers—some choosing different providers for specific capabilities, while others make these choices based on merger and acquisition considerations. For organizations building their workloads across different cloud environments, secure, reliable connectivity is a major benefit. Equinix FCR, working with Direct Connect, is designed to provide low-latency connectivity between AWS and other cloud infrastructure at various speeds in a reliable manner. This post explains how FCR enables cloud connectivity, examines key benefits and use cases, and helps you to get started with deploying this solution. Key benefits: Rapid multicloud connectivity at scale FCR using Direct Connect in AWS Marketplace provides:

Third-party quick network deployment – Deploy private connectivity between clouds in under an hour without colocation, hardware, or licensing Proximity and connectivity – Access enterprise-grade service-level agreements (SLAs) and support for interconnections Security and reliability – Direct routing over a private network with built-in resiliency Flexible multicloud connectivity – Scale from 200 Mbps to 25 Gbps with pay-as-you-go options Unlimited connections – Create unlimited connections per cloud router

FCR provides Layer 3 connectivity at configurable speeds when merging systems across different clouds. This helps in scenarios involving mergers and acquisitions. Using Direct Connect, FCR deploys without hardware management overhead. For applications that need to access data in different clouds, FCR provides consistent performance and low-latency private connectivity across cloud providers. The service reduces data transfer costs while simplifying infrastructure management. When working with artificial intelligence and machine learning (AI/ML) models across clouds, FCR enables high-performance data transfer through Equinix's private network. This helps optimize costs while maintaining training schedules. For petabyte-scale migrations between clouds, Equinix FCR provides secure private connectivity through AWS Marketplace deployment. The service works with AWS DataSync to reduce transfer costs through compression and optimized data movement, benefiting from the lower data egress charges offered by Direct Connect. Prerequisites To set up and provision a new Fabric Cloud Router, you need to have the following prerequisites:

Solution walkthrough: Enhancing multicloud networking connectivity with AWS Direct Connect and Equinix This section walks through the steps to sign up for an AWS account, an Equinix free trial in AWS Marketplace, and to set up and provision a new Fabric Cloud Router through the Equinix Customer Portal. You can sign up for a public AWS Marketplace Free Trial, which allows you to try out a Fabric Cloud Router at the Standard service tier level at no cost during the 7-day trial period. You can provision up to 2 x 200 Mbps Fabric Virtual Circuits (VCs).

Sign in to AWS Marketplace. To create an AWS account, visit Start building on AWS today and choose Create an AWS Account. Enter Equinix in the AWS Marketplace search field or go to Equinix Fabric Cloud Router with AWS Direct Connect. In the list of search results, choose Fabric Cloud Router (Free Trial). Choose Try for free as shown in the following figure.

In the Offer type window, choose Subscribe to free trial as shown in the following figure.

After subscribing, complete the following to set up your account:

In the success window, choose Set up your account. In the registration form, enter your company, name, email address, and other information. Choose Complete Registration. You will receive two emails within 10–15 minutes after your registration is complete that will include

A welcome email with your subscription key. An email with the link for registering your account.

Check your junk or spam folder if you don't receive the emails after 15 minutes. In the second email, choose the link to Verify Email Address. Set your password and choose Complete Registration to set up your Customer Portal account. Using your subscription key from the first email, create and provision a Fabric Cloud Router or new VCs through the Customer Portal.

Creating a Fabric Cloud Router To create a new Cloud Router, follow the instructions at Creating a Fabric Cloud Router in the Equinix Fabric Cloud Router documentation. After you set up a Fabric Cloud Router, you can view and manage its connections as described in the Fabric Cloud Router documentation. To create a Fabric Cloud Router connection, use the subscription keys from your welcome email. Creating a Fabric Cloud Router connection using a subscription key The following steps show you how to establish connectivity between Direct Connect and your Fabric Cloud Router using a subscription key. Using a cloud router connection, you can connect to AWS services through a private network connection.

Sign in to the Equinix Customer Portal and navigate to Fabric. On the Connections tab, choose Create Connection, as shown in the following figure.

3. On the A Service Provider card, choose Connect to a Service Provider and search for Amazon Web Services.

4. On the AWS Amazon Web Services card, choose Select Service, then choose Create Connection. 5. In Enter Your AWS Information, choose a connection type as shown in the following figure:

Redundant – Create a redundant pair of connections. If you're using the free trial, don't set up redundant connections because this will use both Virtual Circuits you have access to on the connection to one cloud. For production networks, redundant links are recommended. Primary – Create a single, primary connection. Secondary – Create a single, secondary connection.

Figure 5: Enter your AWS information and choose a Connection Type

6. Enter your AWS Account ID and choose Next. 7. To retrieve your AWS account identifier, sign in to the AWS Management Console and choose your username in the top right corner. When you copy your account ID into the field, make sure to remove the hyphens, as shown in the following figure.

From the AWS Destination dropdown list, choose your metro. Make sure to use the same destination for both Connections selected and choose Next.

In Configure Your Connection on the Origin Asset Type drop-down list, choose Cloud Router. Choose +Select Cloud Router. In the form that displays on the right, choose a cloud router instance and choose Select Cloud Router. In the Primary Connection section

Enter the primary Connection Name. The name must contain 3–24 characters. Choose a Bandwidth from the dropdown list. The bandwidth value that you choose also applies to the secondary connection.

In the Secondary Connection section, enter your secondary Connection Name, and choose Next. In Review Order and Additional Information, review your connections. To view a summary of your order details, choose Show Order Summary in the lower right. To download a PDF summary, choose Download Order Summary as shown in Figure 7.

After completing these steps, the configuration establishes a Fabric Cloud Router instance with Direct Connect connectivity. The configuration provides private, dedicated connections that support flexible bandwidth options based on the selected tier. Monitor and manage your connections through the Equinix Customer Portal. Cleanup

If you don't plan to go ahead with the free trial for your AWS Marketplace subscription, please go to the Manage subscriptions page, choose Manage next to the software subscription and cancel the subscription. You can learn more about cancelling a free trial here. Similarly, on the Equinix console, please ensure you cleanup and delete any resources that were created as part of this getting started guide if you don't plan to use them.

Conclusion Equinix Fabric Cloud Router, a third-party solution available in AWS Marketplace with AWS Direct Connect, provides a streamlined approach to multicloud networking. In AWS Marketplace, organizations can establish secure, private connections between cloud environments without managing physical infrastructure. The service supports various use cases—from post-merger system integration to large-scale data migration—while helping optimize performance and costs. To explore how FCR can support your cloud connectivity requirements, try the 7-day trial in AWS Marketplace. Learn more about Equinix FCR with Direct Connect. Connect FCR to AWS.

Dhanil Parwani Dhanil is a Senior Partner Solutions Architect at AWS. He works closely with networking, security, and AI partners to build solutions and capabilities to enable and simplify their migrations and operations in the cloud. He holds an MS in telecommunications from the University of Colorado Boulder and has a passion for computer networking. Outside of work, Dhanil is an avid traveler and enjoys cheering Liverpool, FC.

Phil Griston Phil Griston is the AWS Alliance Director at Equinix. Prior to this, Phil has held partnership leadership roles at both AWS and Oracle Cloud.

Gene McColm Gene McColm is the Senior Principal Solutions Architect for the AWS Alliance at Equinix. For more than 10 years, he has worked with AWS, Equinix, and customers to ensure optimal design and deployment of hybrid cloud infrastructures

Transform your MCP architecture: Unite MCP servers through AgentCore Gateway

As AI agents are adopted at scale, developer teams can create dozens to hundreds of specialized Model Context Protocol (MCP) servers, tailored for specific agent use case and domain, organization functions or teams. Organizations also need to integrate their own existing MCP servers or open source MCP servers for their AI workflows. There is a need for a way to efficiently combine these existing MCP servers–whether custom-built, publicly available, or open source–into a unified interface that AI agents can readily consume and teams can seamlessly share across the organization. Earlier this year, we introduced Amazon Bedrock AgentCore Gateway, a fully managed service that serves as a centralized MCP tool server, providing a unified interface where agents can discover, access, and invoke tools. Today, we're extending support for existing MCP servers as a new target type in AgentCore Gateway. With this capability, you can group multiple task-specific MCP servers aligned to agent goals behind a single, manageable MCP gateway interface. This reduces the operational complexity of maintaining separate gateways, while providing the same centralized tool and authentication management that existed for REST APIs and AWS Lambda functions. Without a centralized approach, customers face significant challenges: discovering and sharing tools across organizations becomes fragmented, managing authentication across multiple MCP servers grows increasingly complex, and maintaining separate gateway instances for each server quickly becomes unmanageable. Amazon Bedrock AgentCore Gateway helps solves these challenges by treating existing MCP servers as native targets, giving customers a single point of control for routing, authentication, and tool management—making it as simple to integrate MCP servers as it is to add other targets to the gateway. Breaking down MCP silos: Why enterprise teams need a unified Gateway Let's explore this through a real-world example of an e-commerce ordering system, where different teams maintain specialized MCP servers for their specific domains. Consider an enterprise e-commerce system where different teams have developed specialized MCP servers:

The Shopping Cart team maintains an MCP server with cart management tools The Product Catalog team runs their MCP server for product browsing and search The Promotions team operates an MCP server handling promotional logic

Previously, an ordering agent would need to interact with each of these MCP servers separately, managing multiple connections and authentication contexts. With the new MCP server target support in AgentCore Gateway, these specialized servers can now be unified under a single gateway while maintaining their team-specific ownership and access controls. The power of this approach lies in its organizational flexibility. Teams can group their MCP servers based on multiple logical criteria:

Business unit alignment: Organize the MCP servers by business unit Product feature boundaries: Each product team owns their MCP server with domain-specific tools allowing them to maintain clear ownership while providing a unified interface for their agents Security and access control: Different MCP servers require different authentication mechanisms. The gateway handles the authentication complexity, making it simple for authorized agents to access the tools they need

The following diagram illustrates how an ordering agent interacts with multiple MCP servers through AgentCore Gateway. The agent connects to the gateway and discovers the available tools. Each team maintains control over their domain-specific tools while contributing to a cohesive agent experience. The gateway handles tool naming collisions, authentication, and provides unified semantic search across the tools.

The AgentCore Gateway serves as an integration hub in modern agentic architectures, offering a unified interface for connecting diverse agent implementations with a wide array of tool providers. The architecture, as illustrated in the diagram, demonstrates how the gateway bridges the gap between agent and tool implementation approaches, now enhanced with the ability to directly integrate MCP server targets. AgentCore Gateway integration architecture In AgentCore Gateway, a target defines the APIs, Lambda functions, or other MCP servers that a gateway will provide as tools to an agent. Targets can be Lambda functions, OpenAPI specifications, Smithy models, MCP servers, or other tool definitions. The target integration side of the architecture showcases the gateway's versatility in tool integration. With the new MCP server target support, the gateway can directly incorporate tools from public MCP servers, treating them as first-class citizens alongside other target types. This capability extends to federation scenarios where one AgentCore Gateway instance can serve as a target for another, for hierarchical tool organization across organizational boundaries. The gateway can seamlessly integrate with AgentCore Runtime instances that expose agents as tools, private MCP servers maintained by customers, traditional AWS Lambda functions, and both Smithy and AWS service APIs. Beyond target diversity, the gateway's authentication architecture provides additional operational benefits. The gateway decouples its inbound authentication from target systems, letting agents access tools that use multiple identity providers through a single interface. This centralized approach simplifies development, deployment, and maintenance of AI agents. Now, the same approach can be used for MCP server targets, where the gateway manages the complexity of interfacing with the server using the configured identity provider for the target. With this authentication foundation you get sophisticated tool management capabilities through a unified architecture. When an agent requests tool discovery, the gateway provides a consistent view across the integrated targets, with tools from MCP servers appearing alongside Lambda functions and traditional APIs. The semantic search capability operates uniformly across the tool types, so agents can discover relevant tools regardless of their implementation. During tool invocation, the gateway handles the necessary protocol translations, authentication flows, and data transformations, presenting a clean, consistent interface to agents while managing the complexity of different target systems behind the scenes. The addition of MCP server target support represents a significant evolution in the gateway's capabilities. Organizations can now directly integrate MCP-native tools while maintaining their investments in traditional APIs and Lambda functions. This flexibility allows for gradual migration strategies where teams can adopt MCP-native implementations at their own pace while facilitating continuous operation of existing integrations. The gateway's synchronization mechanisms make sure that tool definitions remain current across the different target types, while its authentication and authorization systems provide consistent security controls regardless of the underlying tool implementation. The gateway combines MCP servers, traditional APIs, and serverless functions into a coherent tool environment. This capability, along with enterprise-grade security and performance, makes it a beneficial infrastructure for agentic computing.

Solution Walkthrough In this post, we'll guide you through the steps to set up an MCP server target in AgentCore Gateway, which is as simple as adding a new MCP server type target to a new or existing MCP Gateway. Adding an MCP server to an AgentCore Gateway will allow you to centralize your tool management, security authentication, and operational best practices with managing MCP servers at scale.

Get started with adding MCP Server into AgentCore Gateway To get started, you will create an AgentCore Gateway and add your MCP Server as a target. Prerequisites Verify you have the following prerequisites:

AWS account with Amazon Bedrock AgentCore access. For more information review Permissions for AgentCore Runtime documentation. Python 3.12 or later Basic understanding of OAuth 2.0

You can create gateways and add targets through multiple interfaces:

AWS SDK for Python (Boto3) AWS Management Console AWS Command Line Interface (AWS CLI) AgentCore starter toolkit for fast and straightforward setup

The following practical examples and code snippets demonstrate how to set up and use Amazon Bedrock AgentCore Gateway. For an interactive walkthrough, you can use these Jupyter Notebook samples on GitHub. Create a gateway To create a gateway, you can use the AgentCore starter toolkit to create a default authorization configuration with Amazon Cognito for JWT-based inbound authorization. You can also use another OAuth 2.0-compliant authentication provider instead of Cognito.

# Create an authorization configuration, that specifies what client is authorized to access this Gateway auth_config = { "customJWTAuthorizer": { "allowedClients": [' '], # Client MUST match with the ClientId configured in Cognito. "discoveryUrl": ' ', } }

# Call the create_gateway API # This operation is asynchronous so may take time for Gateway creation # This Gateway will leverage a CUSTOM_JWT authorizer, the Cognito User Pool we reference in auth_config def deploy_gateway(poll_interval=5): create_response = gateway_client.create_gateway( name="DemoGateway", roleArn=" ", # The IAM Role must have permissions to create/list/get/delete Gateway protocolType="MCP", authorizerType="CUSTOM_JWT", authorizerConfiguration=auth_config, description="AgentCore Gateway with MCP Server Target", ) gatewayID = create_response["gatewayId"] gatewayURL = create_response["gatewayUrl"]

# Wait for deployment while True: status_response = gateway_client.get_gateway(gatewayIdentifier=gatewayID) status = status_response["status"] if status == "READY": print("✅ AgentCore Gateway is READY!") break elif status in ["FAILED"]: print(f"❌ Deployment failed: {status}") return None print(f"Status: {status} - waiting...") time.sleep(poll_interval)

# Values with needs to be replaced with real valuesPython

Create a sample MCP Server As an example, let's create a sample MCP server with three simple tools that return static responses. The server uses FastMCP with stateless_http=True which is required for AgentCore Runtime compatibility.

@mcp.tool() def getOrder() -> int: """Get an order""" return 123

@mcp.tool() def updateOrder(orderId: int) -> int: """Update existing order""" return 456

@mcp.tool() def cancelOrder(orderId: int) -> int: """cancel existing order""" return 789

Configure AgentCore Runtime deployment Next, we will use the starter toolkit to configure the AgentCore Runtime deployment. The toolkit can create the Amazon ECR repository on launch and generate a Dockerfile for deployment on AgentCore Runtime. You can use your own existing MCP server, we're using the following only as an example. In a real-world environment, the inbound authorization for your MCP server will likely differ from the gateway configuration. Refer to this GitHub code example to create an Amazon Cognito user pool for Runtime authorization.

boto_session = Session() region = boto_session.region_name print(f"Using AWS region: {region}")

required_files = ['mcp_server.py', 'requirements.txt'] for file in required_files: if not os.path.exists(file): raise FileNotFoundError(f"Required file {file} not found") print("All required files found ✓")

auth_config = { "customJWTAuthorizer": { "allowedClients": [ ' ' # Client MUST match with the ClientId configured in Cognito, and can be separate from the Gateway Cognito provider. ], "discoveryUrl": ' ', } }

print("Configuring AgentCore Runtime...") response = agentcore_runtime.configure( entrypoint="mcp_server.py", auto_create_execution_role=True, auto_create_ecr=True, requirements_file="requirements.txt", region=region, authorizer_configuration=auth_config, protocol="MCP", agent_name="mcp_server_agentcore" ) print("Configuration completed ✓")

Launch MCP server to AgentCore Runtime Now that we have the Dockerfile, let's launch the MCP server to AgentCore Runtime:

print("Launching MCP server to AgentCore Runtime...") print("This may take several minutes...") launch_result = agentcore_runtime.launch() agent_arn = launch_result.agent_arn agent_id = launch_result.agent_id print("Launch completed ✓")

Create MCP server as target for AgentCore Gateway Create an AgentCore Identity Resource Credential Provider for the AgentCore Gateway to use as outbound auth to the MCP server agent in AgentCore Runtime:

cognito_provider = identity_client.create_oauth2_credential_provider( name="gateway-mcp-server-identity", credentialProviderVendor="CustomOauth2", oauth2ProviderConfigInput={ 'customOauth2ProviderConfig': { 'oauthDiscovery': { 'discoveryUrl': ' ', }, 'clientId': ' ', # Client MUST match with the ClientId configured in Cognito for the Runtime authorizer 'clientSecret': ' ' } } ) cognito_provider_arn = cognito_provider['credentialProviderArn'] print(cognito_provider_arn)

gateway_client = boto3.client("bedrock-agentcore-control", region_name=region) create_gateway_target_response = gateway_client.create_gateway_target( name="mcp-server-target", gatewayIdentifier=gatewayID, targetConfiguration={"mcp": {"mcpServer": {"endpoint": mcp_url}}}, credentialProviderConfigurations=[ { "credentialProviderType": "OAUTH", "credentialProvider": { "oauthCredentialProvider": { "providerArn": cognito_provider_arn, "scopes": [" "], } }, }, ], ) # Asynchronously create gateway target gatewayTargetID = create_gateway_target_response["targetId"]

After creating a gateway target, implement a polling mechanism to check for the gateway target status using the get_gateway_target API call:

def poll_for_status(interval=5): # Poll for READY status while True: gateway_target_response = gateway_client.get_gateway_target(gatewayIdentifier=gatewayID, targetId=gatewayTargetID) status = gateway_target_response["status"] if status == 'READY': break elif status in ['FAILED', 'UPDATE_UNSUCCESSFUL', 'SYNCHRONIZE_UNSUCCESSFUL']: raise Exception(f"Gateway target failed with status: {status}") time.sleep(interval)

Test Gateway with Strands Agents framework Let's test the Gateway with the Strands Agents integration to list the tools from MCP server. You can also use other MCP-compatible agents built with different agentic frameworks.

from strands import Agent from mcp.client.streamable_http import streamablehttp_client from strands.tools.mcp.mcp_client import MCPClient

with client: # Call the listTools tools = client.list_tools_sync() # Create an Agent with the model and tools agent = Agent(model=yourmodel,tools=tools) ## you can replace with any model you like # Invoke the agent with the sample prompt. This will only invoke MCP listTools and retrieve the list of tools the LLM has access to. The below does not actually call any tool. agent("Hi , can you list all tools available to you") # Invoke the agent with sample prompt, invoke the tool and display the response agent("Get the Order id")Python

Refreshing tool definitions of your MCP servers in AgentCore Gateway The SynchronizeGatewayTargets API is a new asynchronous operation that enables on-demand synchronization of tools from MCP server targets. MCP servers host tools which agents can discover and invoke. With time, these tools might need to be updated, or new tools may be introduced in an existing MCP server target. You can connect with external MCP servers through the SynchronizeGatewayTargets API that performs protocol handshakes and indexes available tools. This API provides customers with explicit control over when to refresh their tool definitions, particularly useful after making changes to their MCP server's tool configurations. When a target is configured with OAuth authentication, the API first interacts with the AgentCore Identity service to retrieve the necessary credentials from the specified credential provider. These credentials are validated for freshness and availability before communication with the MCP server begins. If the credential retrieval fails or returns expired tokens, the synchronization operation fails immediately with appropriate error details, transitioning the target to a FAILED state. For targets configured without authentication, the API proceeds directly to tool synchronization. The tool processing workflow begins with an initialize call to the MCP server to establish a session. Following successful initialization, the API makes paginated calls to the MCP server's tools/list capability, processing tools in batches of 100 to optimize performance and resource utilization. Each batch of tools undergoes normalization where the API adds target-specific prefixes to help prevent naming collisions with tools from other targets. During processing, tool definitions are normalized to facilitate consistency across different target types, while preserving the essential metadata from the original MCP server definitions.

An Ops Admin initiates the SynchronizeGatewayTargets API, triggering AgentCore Gateway to refresh the configured MCP target. The gateway obtains an OAuth token from AgentCore Identity for secure access to the MCP target. The gateway then initializes a secure session with the MCP server to retrieve version capabilities. Finally, the gateway makes paginated calls to the MCP server tools/list endpoint to retrieve the tool definitions, making sure the gateway maintains a current and accurate list of tools.

The SynchronizeGatewayTargets API addresses a critical challenge in managing MCP targets within AgentCore Gateway: maintaining an accurate representation of available tools while optimizing system performance and resource utilization. Here's why this explicit synchronization approach is valuable: Schema consistency management: Without explicit synchronization, AgentCore Gateway would need to either make real-time calls to MCP servers during ListTools operations (impacting latency and reliability) or risk serving stale tool definitions. The SynchronizeGatewayTargets API provides a controlled mechanism where customers can refresh their tool schemas at strategic times, such as after deploying new tools or updating existing ones in their MCP server. This approach makes sure that tool definitions in the gateway accurately reflect the target MCP server's capabilities without compromising performance.

Performance impact trade-offs: The API implements optimistic locking during synchronization to help prevent concurrent modifications that could lead to inconsistent states. While this means multiple synchronization requests might need to retry if there's contention, this trade-off is acceptable because:

Tool schema changes are typically infrequent operational events rather than regular runtime occurrences The performance cost of synchronization is incurred only when explicitly requested, not during regular tool invocations The cached tool definitions facilitate consistent high performance for ListTools operations between synchronizations

Invoke the synchronize gateway API Use the following example to invoke the synchronize gateway operation:

def search_tools(gateway_url, access_token, query): headers = { "Content-Type": "application/json", "Authorization": f"Bearer {access_token}" }

payload = { "jsonrpc": "2.0", "id": "search-tools-request", "method": "tools/call", "params": { "name": "x_amz_bedrock_agentcore_search", "arguments": { "query": query } } }

# Example usage token_response = utils.get_token(user_pool_id, client_id, client_secret, scopeString, REGION) access_token = token_response['access_token'] results = search_tools(gatewayURL, access_token, "order operations") print(json.dumps(results, indent=2))Python

Implicit synchronization of tools schema During CreateGatewayTarget and UpdateGatewayTarget operations, AgentCore Gateway performs an implicit synchronization that differs from the explicit SynchronizeGatewayTargets API. This implicit synchronization makes sure that MCP targets are created or updated with valid, current tool definitions, aligning with the assurance from AgentCore Gateway that targets in READY state are immediately usable. While this might make create/update operations take longer than with other target types, it helps prevent the complexity and potential issues of having targets without validated tool definitions.

An Ops Admin creates or updates the MCP target using CreateGatewayTarget or UpdateGatewayTarget operations. AgentCore Gateway configures the new or updated MCP target. The gateway asynchronously triggers the synchronization process to update the tool definitions. The gateway obtains an OAuth token from AgentCore Identity for secure access. The gateway then initializes a secure session with the MCP server to retrieve version capabilities. Finally, the gateway makes paginated calls to the MCP server's tools/list endpoint to retrieve the tool definitions, making sure the gateway maintains a current and accurate list of tools.

ListTools behavior for MCP targets The ListTools operation in AgentCore Gateway provides access to tool definitions previously synchronized from MCP targets, following a cache-first approach that prioritizes performance and reliability. Unlike traditional OpenAPI or Lambda targets where tool definitions are statically defined, MCP target tools are discovered and cached through synchronization operations. When a client calls ListTools, the gateway retrieves tool definitions from its persistent storage rather than making real-time calls to the MCP server. These definitions were previously populated either through implicit synchronization during target creation/update or through explicit SynchronizeGatewayTargets API calls. The operation returns a paginated list of normalized tool definitions.

InvokeTool (tools/call) Behavior for MCP Targets The InvokeTool operation for MCP targets handles the actual execution of tools discovered through ListTools, managing real-time communication with the target MCP server. Unlike the cache-based ListTools operation, tools/call requires active communication with the MCP server, introducing specific authentication, session management, and error handling requirements. When a tools/call request arrives, AgentCore Gateway first validates the tool exists in its synchronized definitions. For MCP targets, AgentCore Gateway performs an initial initialize call to establish a session with the MCP server. If the target is configured with OAuth credentials, AgentCore Gateway retrieves fresh credentials from AgentCore Identity before making the initialize call. This makes sure that even if ListTools returned cached tools with expired credentials, the actual invocation uses valid authentication.

The MCP client initializes a request with MCP protocol version to AgentCore Gateway. The client then sends the tools/call request to the gateway. The gateway obtains an OAuth token from AgentCore Identity for secure access. The gateway initializes a secure session with the MCP server to invoke and handle the actual execution of the tool.

Search tool behavior for MCP targets The search capability in AgentCore Gateway enables semantic discovery of tools across the different target types, including MCP targets. For MCP targets, the search functionality operates on normalized tool definitions that were captured and indexed during synchronization operations, providing efficient semantic search without real-time MCP server communication. When tool definitions are synchronized from an MCP target, AgentCore Gateway automatically generates embeddings for each tool's name, description, and parameter descriptions. These embeddings are stored alongside the normalized tool definitions, enabling semantic search that understands the intent and context of search queries. Unlike traditional keyword matching, this allows agents to discover relevant tools even when exact terminology doesn't match.

Search for MCP server tools through the gateway Use the following example to search for tools through the gateway.

# Example usage token_response = utils.get_token(user_pool_id, client_id, client_secret, scopeString, REGION) access_token = token_response['access_token'] results = search_tools(gatewayURL, access_token, "math operations") print(json.dumps(results, indent=2))Python

Conclusion Today's announcement of MCP server support as a target type in Amazon Bedrock AgentCore Gateway is an advancement in enterprise AI agent development. This new capability addresses critical challenges in scaling MCP server implementations while maintaining security and operational efficiency. By integrating existing MCP servers alongside REST APIs and Lambda functions, AgentCore Gateway provides a more unified, secure, and manageable solution for tool integration at scale. Organizations can now manage their tools through a single, centralized interface while benefiting from unified authentication, simplified tool discovery and reduced maintenance overhead. For more detailed information and advanced configurations, refer to the code samples on GitHub, the Amazon Bedrock AgentCore Gateway Developer Guide and Amazon AgentCore Gateway pricing.

Frank Dallezotte is a Senior Solutions Architect at AWS and is passionate about working with independent software vendors to design and build scalable applications on AWS. He has experience creating software, implementing build pipelines, and deploying these solutions in the cloud. Ganesh Thiyagarajan is a Senior Solutions Architect at Amazon Web Services (AWS) with over 20 years of experience in software architecture, IT consulting, and solution delivery. He helps ISVs transform and modernize their applications on AWS. He is also part of the AI/ML Technical field community, helping customers build and scale Gen AI solutions. Dhawal Patel is a Principal Generative AI Tech lead at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to Agentic AI, Deep learning, distributed computing.

Optimizing TV and radio media buying using SmartSpot360 on AWS

This blog post was co-authored by Jhon Valencia (SVP of Technology & Data), Preston Porter (Vice President of Operations), and Claire McCue (Director of Marketing) at BMG360. When you depend on high-volume lead generation, media buying efficiency directly impacts revenue and scalability. However, as campaigns expand across channels, optimizing traditional media (like TV and radio) becomes increasingly complex. You need a reliable ability to adjust placements in real time and predict campaign performance using live data. To address this gap, BMG360 developed SmartSpot360™, an AI solution built on Amazon Web Services (AWS). SmartSpot360 transforms linear media buying by applying real-time analytics and predictive modeling to deliver optimized placements at scale. We will share how you can modernize legacy media operations, improve efficiency, and enhance campaign performance using an AWS-powered machine learning (ML) platform. Challenges and business context Linear TV and radio advertising remain valuable channels for reaching broad audiences, but they've historically relied on manual processes and fixed decision rules. These methods worked but didn't scale easily—particularly when trying to optimize spend in real time or predict performance across shifting audience behaviors and market conditions. You couldn't readily adjust campaigns in real time or effectively apply predictive insights to future placements at speed and at scale. BMG360 identified three core challenges in the traditional model:

Limited scalability of manual buying decisions Lack of real-time adaptability to changing performance data No predictive intelligence to guide future media investments

Manual optimization, based on historical data and planner expertise, often required significant time and introduced inefficiencies. Third-party media buying platforms, which offered a degree of automation, lacked the flexibility to integrate with proprietary datasets or adapt to unique campaign logic deployed by BMG360. Traditional business intelligence tools supported retrospective analysis but provided no mechanisms for real-time optimization or predictive modeling.

To address these constraints, BMG360 saw a clear opportunity: build an AI-driven system capable of learning from past performance, optimizing active campaigns in real time, and dynamically allocating spend across variables (such as daypart, rate, and creative mix). The result was SmartSpot360: a machine learning-powered platform designed to bring scale, speed, and precision to traditional media buying. How BMG360 engaged AWS The SmartSpot360 architecture is designed to be use case–dependent, meaning its configuration and service stack may vary depending on campaign scale, data sources, and optimization objectives. The following overview represents one reference implementation designed for high-volume TV and radio use cases. To support the development and scaling of SmartSpot360, BMG360 worked closely with AWS to evaluate architecture options for data processing, machine learning (ML) model training, and real-time API delivery. The AWS team provided guidance on selecting the right mix of serverless and ML services to balance performance, cost, and scalability. BMG360 was able to quickly scale and find efficiency by engaging with a dedicated AWS team, as well as through a referred AWS Partner, Usage.AI, to verify a stable and long-term solution was implemented from the ground up. As the application's potential was identified, several meetings to address resilience, efficiency, and Amazon SageMaker best practices culminated in a successful launch and expanded use case for the product. The business continues to engage with the AWS team, including through on-site discovery sessions with AWS thought leadership. These sessions confirm both the AWS-powered solutions and overall cloud application strategies of BMG360 remain in line with or ahead of industry best practices. Solution overview To bring scale and intelligence to traditional media, SmartSpot360 optimizes TV and radio placements in real time. Built to process and analyze over 3,500 ad spots each week, SmartSpot360 continuously learns from performance data to refine buying decisions across a range of variables. The system evaluates:

Spot cost, length, and frequency Time-of-day and day-of-week trends Creative variation and effectiveness Seasonal and holiday-driven audience behavior

Using advanced predictive modeling, SmartSpot360 dynamically adjusts placements based on historical outcomes and live campaign performance. Instead of relying on fixed schedules or manual rule sets, the system allocates spend where it's most likely to generate value. It automates what previously required extensive manual input and post-campaign analysis. This approach improves operational efficiency, while increasing the likelihood of higher-quality leads and stronger conversion outcomes. By shifting optimization from reactive to proactive, SmartSpot360 enables data-informed decision-making at scale across thousands of placements and changing market conditions.

The SmartSpot360 architecture is powered by AWS services that support high availability, scalability, and real-time performance:

Amazon SageMaker is used to train, validate, and deploy machine learning models that predict spot performance and optimize allocation logic. AWS Glue handles extract, transform, and load (ETL) operations, ingesting performance data from multiple sources and preparing it for modeling. AWS Lambda executes real-time logic for budget recommendations, creative rotation, and time-slot selection without managing infrastructure. Amazon API Gateway facilitates secure, API-driven interactions with SmartSpot360, enabling seamless integration with planning and reporting tools. Snowflake on AWS serves as the centralized data warehouse, supporting large-scale queries across historical and in-flight campaign data. Amazon Simple Notification Service (Amazon SNS) coordinates event-driven communication between services by publishing messages to topics that trigger downstream processes (such as model retraining, data processing, or API updates).

With this architecture, BMG360 can continuously improve the performance, reduce operational complexity, and rapidly scale SmartSpot360 as new use cases emerge. Business outcomes and metrics Adopting AI-driven media optimization produces measurable improvements across efficiency, spend, and revenue. Results achieved by BMG360, using SmartSpot360, during a health insurance TV campaign:

Media spend decreased by 32% Revenue generated increased by 52% Return on investment (ROI) for every TV dollar doubled

Operationally, automating spot selection, rate optimization, and daytime management reduced the manual workload, so teams could focus on strategic planning and creative execution. The broader takeaway When you integrate machine learning into traditional media channels, you can achieve real-time adaptability and predictive performance typically reserved for digital platforms. By building SmartSpot360 on AWS, BMG360 was able to scale media buying operations efficiently, leveraging cloud-based data processing, machine learning models, and real-time analytics. Key shifts enabled by AI and AWS infrastructure:

Dynamic budget optimization based on real-time campaign data Predictive modeling for spot performance and audience trends Reduced time spent on manual media buying processes Improved scalability and faster adaptation to market changes Stronger integration between traditional and digital media strategies

Machine learning improves media buying efficiency and reshapes the way you plan, execute, and scale traditional advertising in a digital-first world. Evolving SmartSpot360 with AWS As SmartSpot360 continues to evolve, BMG360 is expanding the platform's capabilities beyond media buying optimization to support broader planning and creative decision-making across channels. Future development is focused on:

Enhancing predictive models using reinforcement learning for deeper campaign intelligence Extending optimization to creative variants and message testing, allowing for more efficient creative to be identified and scaled faster than traditional methodologies Building a self-service planning interface for in-house teams and agency partners

Conclusion When you run high-volume campaigns across TV and radio, the ability to optimize in real time, predict outcomes, and reduce manual overhead becomes a competitive advantage for both clients and agencies alike. SmartSpot360, built on AWS, gives you that edge, helping you scale performance, not just media spend. As you continue to refine your media mix, the infrastructure you choose matters. Cloud-focused services, tools and machine learning capabilities give you the flexibility to evolve your planning, creative testing, and cross-channel coordination without sacrificing control or insight. Traditional media doesn't have to operate in a silo or fall behind digital channels. With a machine learning-powered solution you can bring precision, adaptability, and intelligence to every aspect of your media buying strategy. BMG360 continues to evolve SmartSpot360 by leveraging the power of AWS, so they don't just keep up with market shifts, they adapt in real time, align resources more effectively, and unlock greater value from every campaign. To learn more about SmartSpot360 contact BMG360. Or contact an AWS Representative to know how we can help accelerate your business. Further reading

AWS for Advertising and Marketing Get started with API Gateway Building Generative AI into Marketing Strategies: A Primer Best practices and design patterns for building machine learning workflows with Amazon SageMaker Pipelines Automate model retraining with Amazon SageMaker Pipelines when drift is detected

About BMG360 BMG360 is a full-service performance marketing and lead generation company that helps businesses grow through proprietary technology, deep media expertise, and high-converting creative.

Jake Bernstein Jake Bernstein is a Solutions Architect at Amazon Web Services with a passion for modernization and serverless first architecture. He focuses on helping customers optimize their architecture and accelerate their cloud journey.

Alexander Medina Alexander Medina is an AWS Solutions Architect with a background in Networking, Infrastructure, Security, and IT Operations. He is passionate about helping customers build Well-Architected systems on AWS.

Sonny Sharif Khan Sonny Sharif Khan is an Amazon Web Services Generative AI Platforms Adoption Lead AMER Strategic. Sonny has 25+ years of experience as a leader in data analytics, and artificial intelligence. He has spoken at the World Economic Forum (WEF) concerning AI impacting lost languages.

Simplify SQL Server Developer edition deployments with AWS Launch Wizard

Microsoft SQL Server Developer edition offers organizations a cost-effective solution for their non-production database workloads on AWS. While many enterprises rely on this edition for development and pre-production testing environments, the traditional deployment process can be cumbersome – requiring manual steps like logging into Amazon Elastic Compute Cloud (Amazon EC2) instances, downloading installation media, and performing individual instance configurations. AWS Launch Wizard now streamlines this entire process, offering an efficient way to deploy SQL Server Developer edition. In this blog post, we will demonstrate how you can leverage AWS Launch Wizard to simplify your SQL Server Developer edition deployment, reducing both the time and complexity traditionally associated with manual setup procedures. Whether you're a database administrator, developer, or solutions architect, you'll discover how to accelerate your deployment workflow while maintaining best practices for your non-production environments. Why use Launch Wizard for SQL Server Developer edition? AWS Launch Wizard deploys Microsoft SQL Server solutions on AWS through a guided deployment process. The service reduces deployment time from hours to minutes and automates infrastructure provisioning. By providing a few high-level details about the application, such as required performance characteristics, the wizard will then take care of identifying, provisioning, and configuring matching AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, and an Amazon Virtual Private Cloud (Amazon VPC). The wizard displays cost estimates for your deployment. Each resource selection update shows the new total cost, helping you align with your budget requirements. After your inputs are complete, Launch Wizard provisions these resources and configures them to create a fully functioning SQL Server deployment. The created resources are tagged, making it easy to identity and work with them, and the wizard also creates AWS CloudFormation templates, providing you with a baseline for repeatable and consistent application deployments. How does Bring Your Own Media (BYOM) work? For installation of SQL Server Developer edition, you must use your own media; setup files. In Launch Wizard, there are two installation mechanisms: 1. Bring Your Own Media (BYOM) – In this option, you can put SQL Server Developer setup files and place it in your S3 bucket. During the console or API experience, you provide AWS Launch Wizard with the location of the S3 object (setup file) for the deployment (for API, use the parameter SQLDeveloperEditionInstallMediaBucket). Launch Wizard places the S3 location of the S3 object (not the object itself) into the CloudFormation template in your account which executes the download and install of the software from the S3 bucket to the Windows instance already launched in your account. The S3 Bucket name must start with launchwizard prefix. 2. Bring Your Own URL (BYOURL) – In this option, you provide the public Microsoft URL of the downloadable setup file in the console or API that would be passed to the CloudFormation template like the BYOM approach (URL instead of S3 object). You can use SQLDeveloperEditionInstallMediaURL parameter to input URL for the API request. The URL below must start with " " prefix. This option eliminates the need to download setup files and provide access to an S3 bucket. The difference between the two methods is BYOM is an S3 bucket with the dev edition installation executable in it, whereas the BYOURL method downloads the executable to the instance. You can select the option that best fits your need. Here, Launch Wizard builds the orchestrations from your provided inputs. Since SQL Server Developer edition requires explicit acceptance of the End User License Agreement (EULA) for non-production workloads, Launch Wizard has integrated this step seamlessly into the deployment process. Instead of manually clicking through the EULA during installation, you can now accept these terms directly through the AWS console. Launch Wizard then automatically scripts this acceptance during the installation process, streamlining your deployment while ensuring compliance with Microsoft's licensing requirements. Supported configurations At the launch time, AWS Launch Wizard supports SQL Server 2022 Developer edition. SQL Server 2025 Developer edition will be supported once Microsoft releases general availability. Getting started with AWS Launch Wizard

Navigate to AWS Launch Wizard console and choose View workloads

Search for "sql" and choose MS SQL Server Choose Launch new deployment.

Under Choose deployment pattern dropdown, select SQL Server Developer Edition – Single Node – Windows. Choose Configure deployment.

In the SQL Server Launch Wizard, Step 1: Review Permissions displays information regarding the AWS Identity and Access Management (IAM) role used for the deployment. Launch Wizard creates this role on your behalf in your account so that it can provision the resources across AWS services for SQL Server deployment. This role includes permissions to get the object from S3 bucket starting with launchwizard prefix for BYOM deployments. If part of an organization, your account must have permissions to create or access AWS resources such as EC2 instances, an Amazon VPC, etc.

Under Step 2: Configure application settings, you can configure the application settings. In the General settings section, enter a Deployment name and specify a Simple Notification Service (SNS) topic ARN (Amazon Resource Name) to receive notifications and alerts about your application deployment status. You also have the option to select Enable rollback on failed deployment, which would immediately delete the resources created by the Launch Wizard deployment. This is useful if the deployment fails due to issues such as lack of permissions or incorrect Active Directory credentials. This option is disabled by default so that you have an opportunity to debug resources if a deployment fails. After diagnosing the issue, when you delete the failed Launch Wizard deployment, the related resources will be deleted. (Optional) You can enable CloudWatch Application Insights monitoring to monitor metrics and logs to detect and correlate anomalies and errors with your SQL application. (Optional) You can enable AWS Service Catalog product creation to export AWS CloudFormation template to create AWS Service Catalog for the deployment.

In the Connectivity section, you provide inputs for the networking infrastructure of your application resources. You can choose an existing Key pair name or create a new one. If you create a new key pair, hit the refresh button on the right to display the newly create key pair in the drop-down menu. The key pair will be used if you want to connect to the instances using RDP or obtain the administrator password. You have the option to select an existing Virtual Private Cloud (VPC) or create a new VPC.

For existing VPC, you need to select the Availability Zone (AZ) and private subnet, and select the Outbound Connectivity check box to confirm that public connectivity is established for the private subnets. If creating a new VPC, Launch Wizard will create and configure the subnets for you. Optionally, you can enable setup Remote Desktop Gateway preferences, which allows you to establish a secure, encrypted connection to your SQL instance via RDP over HTTPS. In Active Directory section, you can choose to enable or disable Active Directory. This gives flexibility to use the SQL instance with or without an Active Directory. The directory allows authorized users to authenticate with this SQL server instance using Windows authentication. Disable Active Directory option is only available for SQL Server Developer edition. If you choose to create a new VPC on the connectivity section, you must select to create a new Active Directory if it is enabled. For existing VPC, you can connect to existing Active Directory.

You can enter the domain administrator password, DNS name and optionally specify the Organization Unit (OU) within which you want to join the SQL instance to Active Directory. Next, under SQL Server Configuration, provide your SQL Server account details. For existing Active Directory, choose to use an existing service account or create a new account. For new Active Directory, you must create new SQL service account. If Active Directory is disabled, you can proceed and select SQL Server install type. For the SQL Server install type, choose License-included AWS provided AMI or use your own custom AMI. The License included AMI will include the Windows Licensing.

In Additional SQL Server settings – optional you can provide SQL node names. If you leave this fields blank, then Launch Wizard will assign unique name for you. Select Next to go to next step, Step 3: Configure infrastructure settings. Under SQL Server Developer Edition Version dropdown, you can choose SQL Server 2022 Developer edition Select the checkbox to accept the Microsoft End User License Agreement (EULA). This gives permissions to download and install SQL Developer edition on EC2 instance. Under installation method, choose one of the options: Bring Your Own Media (BYOM) or Bring your Own URL (BYOURL).

For BYOM, select Browse S3 and select the S3 bucket starting with launchwizard prefix containing the SQL Server installation media. For Bring your Own URL, enter the public Microsoft URL where SQL Server Developer edition setup could be found. The URL must start with Under Choose how you would like to define your infrastructure, you have two options. By default, you will see Launch Wizard recommends AWS infrastructure based on your needs. Here, under Compute needs, you can input your network, memory, and processor requirements for your instance, and Launch Wizard will recommend an instance type that can be seen under Recommended infrastructure. Similarly, under Storage and performance needs, you can input the type of storage drive and SQL Server throughput, and Launch Wizard will recommend the volume type.

Alternately, you can choose Specify your own instance and volume configuration to manually select the instance type and volume type. For volume type, you can consider to choose gp3 volume. Under Advanced settings, you can choose vCPU optimization options. By default, it will use all available vCPU for maximum performance.

Under Drive letters and volume size, you can optionally update the volume letters and volume size. At the end of the page, you can see the estimated monthly costs to deploy the resources, which gets updated as you modify your inputs, and can help you select what's best for your needs.

The final step is Step 4: Review and deploy, which shows the summary with all the configuration options selected on the previous steps. Once you hit Deploy, the Launch Wizard begins to spin up the resources and configure your application.

The deployment completes in 45 to 60 minutes, and you can go into the Amazon EC2 console to access your EC2 instances, AWS Systems Manager Application Manager to manage your application. In AWS CloudFormation, you can view the stack and template generated by Launch Wizard based on your selections, and this code can be re-used for future deployments or for integration with your CloudFormation-supported CI/CD tools. Cleaning up While there is no cost to use Launch Wizard, you will be charged for resources created as part of the application deployment. If you no longer need the application running, you can delete the deployment in Launch Wizard, which will delete all the AWS resources created as part of the deployment. Following are the steps to delete the deployment:

Navigate to AWS Launch Wizard console From the left menu, choose Deployments Select your SQL Server Deployment and choose Delete

Conclusion In this blog post, we have demonstrated how AWS Launch Wizard simplifies the deployment of SQL Server Developer edition, transforming a once manual process into a streamlined, wizard-driven experience. The service offers guided steps, flexible installation options, and automated resource provisioning while maintaining compliance with Microsoft's licensing requirements. AWS Launch Wizard's cost estimation features and reusable CloudFormation templates help teams plan and execute deployments more efficiently. Whether setting up development environments or test instances, this tool significantly reduces complexity and time, eliminating licensing costs for non-production workloads and offering an efficient path for organizations to leverage SQL Server in their development lifecycle. To further explore SQL Server deployments on AWS, we recommend checking out our related resources on Simplify SQL Server Always On deployments with AWS Launch Wizard and Amazon FSx, and SQL Server Always On Failover Cluster Instances with Amazon FSx for NetApp ONTAP.

Suhail Fouzan Suhail Fouzan is a Specialist Solutions Architect at Amazon Web Services (AWS) with over 15 years of experience in the IT industry. Specializing in Microsoft workloads, migration services, and operations management with AWS Systems Manager, Suhail helps customers successfully migrate their infrastructure to AWS. Outside of work, Suhail enjoys playing cricket and spending time with his family.

Yogi Barot Yogi Barot is WorldWide Microsoft Tech leader leading Microsoft Technical field community at AWS. Part of her role, she is leading community for technical enablement and supporting customers migrate and modernize Microsoft workload to AWS. Yogi has 26 years of experience working with different Microsoft technologies, her specialty is in SQL Server and different database technologies. Yogi has in depth AWS knowledge and expertise in running Microsoft workload on AWS.

Organizations are accelerating their global cloud footprints to deliver exceptional customer experiences and enhanced application performance worldwide. As teams scale across AWS Regions, there is opportunity to use hundreds of AWS services with consistent functionality across Regions. The AWS Knowledge Model Context Protocol (MCP) server helps provide programmatic access for deep understanding of AWS capabilities and their availability across AWS Regions. By combining the deep research of AI agents with MCP Servers, you can automate Regional expansion analysis and generate actionable migration strategies in hours instead of weeks. In this post, we show you how to build a multi-agent AI system using the new Regional availability tools in the AWS Knowledge MCP server to automate Regional expansion analysis and planning. These specialized agents run locally to analyze AWS CloudFormation and AWS CloudTrail logs within your environment. The agents then query AWS service, feature, API, and CloudFormation resource availability data through AWS Knowledge MCP tools to identify Regional capability gaps and generate actionable migration strategies. All customer data remains within your local environment and is not transmitted to external systems. By automating this analysis, teams can focus on implementation rather than spending time on manual research and compatibility validation. In this post you will learn how to implement AI agents that can:

Analyze your AWS infrastructure across Regions Identify service dependencies and feature requirements Perform deep research against target Regions when deploying workloads and functionalities Generate detailed migration strategies with implementation guidance Maintain continuous visibility into Regional capability changes

Solution overview Building production-ready AI agents for Regional expansion planning requires a systematic approach that combines infrastructure intelligence using deep research on Regional availability with automated guidance to help decision-making. We demonstrate how to combine the AWS Knowledge MCP server along with the AWS CloudFormation MCP and CloudTrail MCP servers to perform deep research with AI agents to automate Regional expansion planning. The solution implements a multi-agent orchestration pattern built on the Strands Agents SDK, where specialized agents collaborate to deliver comprehensive Regional expansion insights. Each agent serves a specific purpose in the analysis to discover your AWS infrastructure, analyze usage, check for Regional service availability and generate detailed reports with specific migration strategies and implementation timelines. MCP tools integration The multi-agent AI system uses several AWS MCP servers to access comprehensive service and Regional availability data. The following table shows the MCP tools used and their specific functions in the analysis:

CloudTrail Explorer CloudTrail MCP Server event_lookup Advanced CloudTrail event analysis

CFN Explorer AWS CloudFormation MCP Server resource_reading Deep inspection of AWS resource properties and attributes

CFN Explorer AWS CloudFormation MCP Server resource_listing Deep research to provide comprehensive resource enumeration

Capabilities Explorer AWS Knowledge MCP Server list_region Latest region metadata and alternative names

Capabilities Explorer AWS Knowledge MCP Server get_regional_availability API-level compatibility validation and CloudFormation resource support verification across Regions.

Multi Region Expansion Planner andPlanning Report Generator AWS Knowledge MCP Server search_documentation Deep search across AWS documentation

Multi Region Expansion Planner andPlanning Report Generator AWS Knowledge MCP Server read_documentation Convert AWS documentation to structured markdown

Multi Region Expansion Planner andPlanning Report Generator AWS Knowledge MCP Server recommend Get content recommendations for AWS documentation pages

Architecture The following architecture diagram shows how the multi-agent system, orchestrated through the Strands Agents SDK, interacts to analyze your infrastructure and generate expansion recommendations to reduce deployment risks while accelerating time-to-market for global applications.

The Orchestrator agent coordinates the analysis workflow using the Strands Agents SDK. Infrastructure Discovery analyzes CloudFormation stacks and CloudTrail logs using AWS CloudFormation MCP and CloudTrail MCP servers. AWS Knowledge MCP provides deep research on service availability and features. AI agents process the data to identify gaps and generate recommendations. The system produces three reports: Executive summary, Technical analysis, and Implementation plan

An AWS account with programmatic access configured through the AWS CLI (version 2.0 or later) AWS CloudTrail enabled in your source Region for usage analytics AWS CloudFormation Stacks deployed that represent your infrastructure Access to the Claude 4.0 model in Amazon Bedrock Strands Agents SDK (version 1.0 or later). For more information see the Strands Agent SDK User Guide. Python 3.11 or later environment AWS IAM permissions

{ "Version": "2012-10-17", "Statement": [{ "Sid": "CloudFormationAnalysis", "Effect": "Allow", "Action": ["cloudformation:ListStacks", "cloudformation:DescribeStacks", "cloudformation:ListStackResources", "cloudformation:DescribeStackResources"], "Resource": "*" }, { "Sid": "CloudTrailAnalysis", "Effect": "Allow", "Action": ["cloudtrail:LookupEvents"], "Resource": "*" }, { "Sid": "BedrockAccess", "Effect": "Allow", "Action": ["bedrock:InvokeModel", "bedrock:InvokeModelWithResponseStream"], "Resource": ["arn:aws:bedrock:*:*:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0"] }] }JSON

Solution implementation: Building your multi-Region expansion AI agent system We've architected this solution with an Orchestrator agent that coordinates five specialized agents through distinct analysis phases. This pattern ensures modularity, reliability, and extensibility as your expansion requirements evolve:

CFN Explorer – Analyzes all CloudFormation stacks and resources in a Region CloudTrail Explorer -Processes CloudTrail logs for API usage analysis with a default 7-days lookback Capabilities Explorer – Uses AWS Knowledge MCP tools for Regional availability intelligence Multi-Region Expansion Planner – Orchestrates comparative analysis across target Regions Planning Report Generator – Creates executive-ready expansion planning reports

Phase 1: Infrastructure discovery The CFN Explorer agent performs comprehensive infrastructure analysis:

Discovers and analyzes all CloudFormation stacks and their resources Identifies service dependencies and resource relationships Catalogs encryption keys, networking, and security configurations Orchestrator agent can pass any source Region and a list of target Regions as parameters

Implementation insight: The orchestrator accepts any source Region and multiple target Regions as parameters, to support flexible analysis scenarios. The following is an example of a successful connection to AWS Account with discovered CloudFormation stacks:

I'll analyze all CloudFormation stacks in the us-east-1 region using the xxxx AWS profile. Let me start by retrieving the list of stacks and then gather detailed information for each running stack.

Tool #1: use_aws ╭─ 🚀 List all CloudFormation stacks ─╮ │ │ │ Service: cloudformation │ │ Operation: list_stacks │ │ Region: us-east-1 │ │ Parameters: None │ │ │ ╰─────────────────────────────────────╯Code

Phase 2: Usage pattern analysis The CloudTrail Explorer agent analyzes actual service usage patterns:

CloudTrail Explorer processes 7 days of API usage logs by default. This can be configured to a different value as needed. Identifies actual service usage patterns beyond deployed resources. Maps API dependencies and integration patterns. Orchestrator agent can pass any number of days as lookback period for API analysis.

Production tip: Extend the lookback period for comprehensive analysis during peak usage periods. Phase 3: Regional availability assessment The Capabilities Explorer agent provides real-time Regional intelligence through deep research

Capabilities Explorer checks service availability across target Regions Validates API compatibility and CloudFormation resource support Identifies Regional limitations and feature gaps in target Regions

Phase 4: Gap analysis and planning The Multi-Region Planner agent delivers action insights:

Performs comprehensive gap analysis Generates specific workarounds and recommendations based on missing functionality Creates implementation roadmaps with effort estimates Suggests CloudFormation template changes to implement workarounds

Phase 5: Report generation The Planning Report Generator agent produces comprehensive documentation:

Executive summaries with business impact analysis Technical implementation guides with code examples Compliance requirements and risks matrices

Deploy the solution on your local machine To deploy the solution on your local machine, complete the following steps: Step 1: Clone the sample-multi-agent-region-expansion-planner repository:

Step 3: Configure the AWS profiles and target Regions in src/utils/config.py:

# AWS CLI profile to access account where workloads are deployed. PROFILE_NAME = 'your-profile-name'

# Source region where workloads are deployed. SOURCE_REGION = 'us-east-1'

# Target regions where workloads expansion need to be analyzed and planned. TARGET_REGIONS = ['eu-central-1', 'ca-central-1', 'ap-southeast-7'] Bash

Step 4: Configure the AWS profile and Amazon Bedrock model provider account in src/utils/config.py:

# Account ID to use Bedrock Models and create AI Agents INFRA_ACCOUNT = "123456789012"

# AWS CLI profile for auth BEDROCK_AWS_CLI_PROFILE = "bedrock-profile" Bash

Generated insights and deliverables Upon completion, the system generates three production-ready reports:

Executive summary: Business-focused insights with risk assessments Technical analysis: Deep dive into service dependencies and gaps Implementation plan: Step-by-step migration guide with code samples

Executive summary The executive summary is a business-focused report that provides key insights about Regional expansion readiness. It presents a clear view of service availability across target Regions, identifies critical dependencies, and outlines required architectural modifications. The report consolidates complex technical findings into actionable business intelligence that helps inform expansion decisions. The following is an example excerpt of the output for the executive summary:

This comprehensive expansion planning report provides detailed guidance for migrating AWS workloads from **us-east-1** to three target regions: **eu-central-1**, **ca-central-1**, and **ap-southeast-7**. The analysis reveals significant service availability challenges requiring extensive architectural modifications and a carefully orchestrated phased deployment approach.

Our analysis reveals a compelling business opportunity with 78% technical readiness across target regions, representing a $485,000 strategic investment that delivers $2.1M in additional annual revenue by Year 3. The expansion addresses critical market demands for data sovereignty, GDPR compliance, and reduced latency while establishing competitive differentiation through first-mover advantage in multi-region knowledge graph deployment. The technical foundation demonstrates strong compatibility with 94% API parity across regions, though strategic workarounds are required for specific service limitations in eu-south-1.Bash

Technical analysis The technical analysis provides engineers with detailed implementation guidance for Regional expansion. This report includes comprehensive service compatibility analysis, specific API modifications needed, and architectural recommendations for services with Regional variations. The following is an example excerpt of the output for the technical analysis:

Based on comprehensive analysis of current AWS workloads in **us-east-1** and assessment of four potential target regions, this document provides detailed migration feasibility analysis for:

- **ap-southeast-7** (Asia Pacific - Thailand) - **65% compatibility** - **eu-central-1** (Europe - Frankfurt) - **95% compatibility** - **ca-central-1** (Canada - Central) - **80% compatibility**

The source environment (us-east-1) contains **35 CloudFormation stacks** utilizing **21 unique AWS services** with the following critical components:

- **Neptune Global Database** with cross-region replication - **SageMaker notebook instances** for ML workloads - **Lambda functions** (15 total) with VPC configurations - **S3 buckets** (8 total) with cross-region dependencies - **IAM roles and policies** (25 roles, 12 policies) - **ElastiCache serverless** configurations - **CDK bootstrap resources** with region-specific naming

| Region | Migration Status | Recommended Action | |--------------------|------------------------|----------------------------------------------------------| | **eu-central-1** | ✅ **RECOMMENDED** | Proceed with migration | | **ca-central-1** | ⚠️ **CONDITIONAL** | Good foundation - missing CI/CD and development services | | **ap-southeast-7** | ❌ **NOT RECOMMENDED** | Significant service gaps - wait for service expansion |Bash

Implementation plan The implementation plan delivers a detailed migration roadmap. It provides step-by-step guidance for implementing the regional expansion, with clear phases and validation checkpoints. The following is an example excerpt of the output for the implementation plan:

# Multi-Region Expansion Planning Report ### Executive Report for KnowledgeForge Application Expansion

Source Region: us-east-1 Target Regions: eu-central-1, ca-central-1, ap-southeast-7 Analysis Date: Sep 3, 2025

Service Entity | Entity Type | Source Region | eu-central-1 | ca-central-1 | ap-southeast-7 | Status | Notes ---------------|-------------|---------------|--------------|--------------|----------------|-------------|------ dynamodb | AWS Service | Available | Available | Available | ? Unknown | Full Parity | All core APIs supported neptune | AWS Service | Available | Available | Available | ? Unknown | Partial | Global DB limitations in some regions sagemaker | AWS Service | Available | Limited | Available | ? Unknown | API Gaps | DescribeNotebookInstanceLifecycleConfig missing cloudwatch | AWS Service | Available | Available | Available | ? Unknown | Full Parity | All monitoring features available lambda | AWS Service | Available | Available | Available | ? Unknown | Full Parity | All runtimes supported s3 | AWS Service | Available | Available | Available | ? Unknown | Full Parity | Regional buckets required iam | AWS | Available | Available | Available | ? Unknown | Full Parity | GlobalBash

Environment clean up Maintain operational hygiene by running the cleanup command so that agents do not consume data from previous analyses.

Conclusion: Transforming Regional expansion with agent workflows and deep research In this post, we demonstrated how the combination of the AWS Knowledge MCP server and AI agents can transform Regional expansion planning from a manual process into an automated workflow. With this multi-agent system teams can:

Accelerate planning cycles with automated infrastructure analysis and deep research Make data-driven decisions using comprehensive Regional capability data Generate production-ready implementation strategies with specific code examples

For more information about multi-Region architectures, see AWS Capabilities by Region. For more information about building global applications on AWS, see AWS Global Infrastructure. We're excited to see how you use the AWS Knowledge MCP server to accelerate your global expansion. To get started, visit the AWS Knowledge MCP Getting Started page. Alternatively, you can also add the AWS Knowledge MCP server to your AI agentic IDEs and AI assistants such as Kiro, Cursor, Cline, or Amazon Q for command line to perform similar analysis without using the Strands Agents SDK.

About the authors Veera "Bhargav" Nunna is a Senior Data Engineer and Tech Lead at Amazon Web Services (AWS) pioneering Knowledge Graphs for Large Language Models and enterprise-scale data solutions. With over a decade of experience, he specializes in transforming enterprise AI from concept to production by delivering MVPs that demonstrate clear ROI while solving practical challenges like performance optimization and cost control. Veera is among the first to launch production-level MCP Servers, tools, and agentic solutions that solve critical problems at AWS and for customers globally. When not working on AI-driven solutions, he enjoys aviation and flies his family in small aircrafts, holding a private pilot's license. James Liu is a Senior Product Manager – Technical at Amazon Web Services (AWS), where he leads product development for cloud infrastructure and developer tools. With 10+ years of experience in product management, he specializes in driving product strategy, building cross-functional partnerships, and delivering scalable solutions from conception to launch. James focuses on AI-driven infrastructure optimization, dependency management systems that serve enterprise developers at scale and holds multiple patents in AI/ML. Outside of work, James is passionate about mentoring aspiring product managers and exploring the intersection of emerging AI technologies with practical business applications. Nikhil Jivan is a software engineer at AWS specializing in knowledge graphs and scalable data platforms. With 3+ years of experience in distributed systems and data engineering, he builds solutions that help customers unlock insights from their data. When not coding, you'll find him in the gym, at the piano, or exploring a new city. Sucharitha Boinapally is a Data leader at Amazon Web Services (AWS) with 13+ years of experience leading enterprise-scale data solutions. She pioneers AI-driven infrastructure optimization using knowledge graphs and MCP servers to transform complex AWS interdependencies into strategic intelligence, empowering teams to make data-driven decisions about AWS products. Her expertise in agentic AI approaches and advanced data orchestration enables insights that accelerate AWS's global infrastructure deployment and service innovation. Outside of work, Sucharitha enjoys growing her own food and spending quality time with her family, and friends.

Introducing AWS Capabilities by Region for easier Regional planning and faster global deployments

At AWS, a common question we hear is: "Which AWS capabilities are available in different Regions?" It's a critical question whether you're planning Regional expansion, ensuring compliance with data residency requirements, or architecting for disaster recovery. Today, I'm excited to introduce AWS Capabilities by Region, a new planning tool that helps you discover and compare AWS services, features, APIs, and AWS CloudFormation resources across Regions. You can explore service availability through an interactive interface, compare multiple Regions side-by-side, and view forward-looking roadmap information. This detailed visibility helps you make informed decisions about global deployments and avoid project delays and costly rework.

Getting started with Regional comparison To get started, go to AWS Builder Center and choose AWS Capabilities and Start Exploring. When you select Services and features, you can choose the AWS Regions you're most interested in from the dropdown list. You can use the search box to quickly find specific services or features. For example, I chose US (N. Virginia), Asia Pacific (Seoul), and Asia Pacific (Taipei) Regions to compare Amazon Simple Storage Service (Amazon S3) features. Now I can view the availability of services and features in my chosen Regions and also see when they're expected to be released. Select Show only common features to identify capabilities consistently available across all selected Regions, ensuring you design with services you can use everywhere.

The result will indicate availability using the following states: Available (live in the region); Planning (evaluating launch strategy); Not Expanding (will not launch in region); and 2026 Q1 (directional launch planning for the specified quarter). In addition to exploring services and features, AWS Capabilities by Region also helps you explore available APIs and CloudFormation resources. As an example, to explore API operations, I added Europe (Stockholm) and Middle East (UAE) Regions to compare Amazon DynamoDB features across different geographies. The tool lets you view and search the availability of API operations in each Region.

The CloudFormation resources tab helps you verify Regional support for specific resource types before writing your templates. You can search by Service, Type, Property, and Config.For instance, when planning an Amazon API Gateway deployment, you can check the availability of resource types like AWS::ApiGateway::Account.

You can also search detailed resources such as Amazon Elastic Compute Cloud (Amazon EC2) instance type availability, including specialized instances such as Graviton-based, GPU-enabled, and memory-optimized variants. For example, I searched 7th generation compute-optimized metal instances and could find c7i.metal-24xl and c7i.metal-48xl instances are available across all targeted Regions.

Beyond the interactive interface, the AWS Capabilities by Region data is also accessible through the AWS Knowledge MCP Server. This allows you to automate Region expansion planning, generate AI-powered recommendations for Region and service selection, and integrate Regional capability checks directly into your development workflows and CI/CD pipelines.

Now available You can begin exploring AWS Capabilities by Region in AWS Builder Center immediately. The Knowledge MCP server is also publicly accessible at no cost and does not require an AWS account. Usage is subject to rate limits. Follow the getting started guide for setup instructions. We would love to hear your feedback, so please send us any suggestions through the Builder Support page. — Channy

Organizations in regulated industries often mandate control over encryption keys when storing data in the cloud to meet compliance requirements. Although AWS Backup logically air-gapped vault provides secure, isolated backup storage, these customers have needed the ability to use their own AWS Key Management Service (AWS KMS) customer-managed keys (CMKs) to provide greater control of their backup data at rest. Today, we're announcing support for CMKs in AWS Backup logically air-gapped vault. You can use this new capability to maintain full control over your encryption keys while benefiting from the security and compliance advantages of air-gapped backup storage. In this post, we walk you through configuring CMKs with logically air-gapped vault and demonstrate how this feature helps you meet compliance requirements. Furthermore, we discuss additional controls needed to ensure that your setup is secure and protected. What's new? AWS Backup logically air-gapped vault now supports both AWS-owned keys (AOK) and AWS KMS CMKs. This gives you multiple ways to manage your backup encryption. Choosing the right key management option:

AOKs remain our recommended approach for most use cases due to their integrated functionality and robust security. CMKs are designed for organizations with specific governance requirements or regulatory compliance standards.

Key capabilities of CMK support: CMK support addresses three critical customer requirements:

Complete key lifecycle control: Manage key access, rotation, and retirement schedules according to your policies. Enhanced audit visibility: Track all key operations through comprehensive AWS CloudTrail. Streamlined compliance integration: Seamlessly incorporate backup encryption into existing monitoring and compliance workflows.

Architecture and components Core Architecture A central key vault account is a security strategy where encryption keys are centrally managed and shared across multiple accounts, enhancing security and compliance. This approach can be implemented either within the same AWS Organizations for streamlined governance or across different organizations for enhanced isolation. Unlike traditional KMS strategies that provision and manage keys locally within individual accounts, the central key vault model consolidates key management, monitoring, and permission boundaries in a dedicated account.

Figure 1: Using CMKs with AWS Backup logically air-gapped vault in a multi-account architecture This architecture maintains the fundamental security principles that make logically air-gapped vaults resilient against data loss and ransomware events. Your backups remain stored in service-owned accounts, physically isolated from your workloads, and protected by the Write-Once-Read-Many (WORM) model. This multi-layered approach ensures backup security and recoverability, even during account compromise scenarios. AWS Backup logically air-gapped vault integrates with Multi-party approval for AWS Organizations, enhancing security without compromising operational agility. This allows organizations to recover using the backup copies stored in the AWS Backup logically air-gapped vault. The key vault account managed CMKs can then be shared with either the forensics account or the recovery account for integrity validation and restore. Key Management Options When creating an AWS Backup logically air-gapped vault, you can now specify an AWS KMS CMK for encryption. This CMK can be sourced from the following options:

Your current account: For streamlined management within a single AWS account A designated key vault account: For centralized key management, which can be provisioned in the same AWS Organization or a different Organization (recommended for enhanced security isolation)

This integration enables you to maintain complete control over your backup encryption keys while preserving all the robust security benefits of logical air-gapping. Services in other accounts can reference and use the centrally stored keys to encrypt and decrypt their data, enabling secure sharing of customer-managed keys (CMKs) with workload accounts where backups are created and managed. The feature supports symmetric customer-managed keys and enables key reuse across multiple vaults. Although the encryption key cannot be changed after vault creation, AWS KMS automatic key rotation is fully supported, providing continuous updates to the key's backing material without service interruption. Combined with comprehensive audit capabilities and granular AWS Identity and Access Management (IAM) policies, organizations gain the flexibility to tailor their backup encryption strategy to meet specific security and compliance requirements. When using CMKs, customers should follow the best practices discussed in the following section to provision architecture constructs to cater to various aspects of a resilient recovery strategy. Best Practices & Operations Implementing secure backup encryption strategies with AWS Backup logically air-gapped vaults requires careful consideration of operational processes, security governance frameworks, and monitoring practices. The following sections cover security controls, monitoring and alerting, and operational procedures to help organizations establish robust, scalable, and compliant backup encryption strategies that align with enterprise security frameworks and regulatory requirements. Security Controls Comprehensive security controls for vault encryption go beyond basic key policies. Many enterprises adopt centralized encryption management approaches, creating dedicated key vault accounts to separate encryption operations from workload accounts and establish clear boundaries for sensitive key operations. Effective protection necessitates layering multiple security controls to create a robust defense-in-depth strategy. The following four security controls provide comprehensive protection through multi-layered access controls:

Service Control Policies (SCPs) enforce organization-wide restrictions, such as requiring MFA for sensitive operations like PutKeyPolicy, DisableKey, or ScheduleKeyDeletion across all accounts.

{ "Version":"2012-10-17", "Statement":[ { "Sid":"DenyKMSAdminOpsWithoutMFA", "Effect":"Deny", "Action":[ "kms:PutKeyPolicy", "kms:DisableKey", "kms:ScheduleKeyDeletion" ], "Resource":"*", "Condition":{ "BoolIfExists":{ "aws:MultiFactorAuthPresent":"false" } } } ] }JSON

Resource Control Policies (RCPs) apply guardrails directly at the resource level, preventing unauthorized cross-account access and mitigating the confused deputy problem where AWS services might be manipulated into using permissions inappropriately across accounts. Combined with AWS KMS grants, RCPs ensure that only authorized services gain temporary access to your keys. The same restrictions applied through SCPs can be implemented through RCPs, with the advantage that they apply to specific principals and are enforced directly on the CMKs.

{ "Version":"2012-10-17", "Statement":[ { "Sid":"DenyDestructiveKMSOperations", "Effect":"Deny", "Principal":"*", "Action":[ "kms:PutKeyPolicy", "kms:DisableKey", "kms:ScheduleKeyDeletion", "kms:DeleteAlias", "kms:UpdateKeyDescription", "kms:CancelKeyDeletion" ], "Resource":"*", "Condition":{ "BoolIfExists":{ "aws:MultiFactorAuthPresent":"false" } } } ] }JSON

Permission boundaries define maximum privileges for IAM roles, preventing administrators or automation from escalating access beyond approved limits. IAM and key policies provide granular access control at the identity and resource levels respectively.

This layered approach ensures that even if one control is misconfigured, others remain in place to protect your AWS KMS keys and logically air-gapped backups. Each layer addresses different risks: SCPs enforce global organizational rules, RCPs constrain resource usage, permission boundaries contain role privileges, and key policies define specific key access. This creates comprehensive protection for your backup encryption strategy. For detailed implementation guidance, refer to the AWS KMS access control glossary documentation for deeper insights into individual security control mechanisms. Monitoring and alerting Effective monitoring of AWS KMS key usage requires implementing comprehensive observability across multiple AWS services. AWS KMS integrates with Amazon CloudWatch, AWS CloudTrail, and Amazon EventBridge to provide real-time visibility into key operations, usage patterns, and potential security events. This multi-layered monitoring approach enables organizations to detect anomalies, track compliance metrics, and maintain detailed audit trails for regulatory requirements. Key monitoring strategies include:

CloudWatch Alerts for unusual key activity, such as failed operations or attempts to schedule key deletion CloudTrail Log Analysis for sensitive API calls including PutKeyPolicy, DisableKey or ScheduleKeyDeletion Automated Event Processing through EventBridge to trigger immediate responses to critical key management events SIEM Integration for centralized security monitoring and correlation with other enterprise security events

For example, you can create a CloudWatch metric filter to automatically detect and alert on key deletion attempts:

aws logs put-metric-filter \ --log-group-name "CloudTrail/Logs" \ --filter-name "KMSKeyDeletion" \ --metric-transformations \ metricName="KMSKeyPendingDeletion",metricNamespace="Custom/KMS",metricValue=1 \ --filter-pattern '{ ($.eventSource = "kms.amazonaws.com") && ($.eventName = "ScheduleKeyDeletion") }'TypeScript

Additionally, monitor sensitive operations like DisableKey, PutKeyPolicy, and RevokeGrant, which can significantly impact key availability and permissions. Route these events to EventBridge or your SIEM platform to enable automated alerting and remediation workflows. For comprehensive implementation guidance, refer to the AWS KMS monitoring documentation. Operational Procedures Effective key management extends beyond initial setup to encompass ongoing operational considerations that ensure long-term security and compliance. AWS KMS provides automatic annual key rotation, which updates the cryptographic material while retaining the same key ID. For most organizations, this standard rotation frequency is sufficient given that AWS KMS keys are protected by FIPS 140-3 Level 3 validated hardware security modules, and more frequent rotation provides minimal additional security benefits. Key operational considerations include:

Key Rotation Management: Organizations with compliance requirements mandating shorter rotation intervals can implement manual rotation processes in addition to automatic rotation Governance and Auditing: Tag keys with rotation requirements and compliance metadata to streamline audit processes and policy enforcement Lifecycle Management: Establish clear procedures for key creation, usage monitoring, and retirement aligned with data retention policies Access Review Processes: Implement regular reviews of key permissions and cross-account sharing arrangements to maintain least-privilege access Disaster Recovery Testing: Validate key availability and backup restoration procedures as part of regular DR exercises

These operational procedures ensure that your backup encryption strategy remains effective and compliant as your organization scales and regulatory requirements evolve. Implementation Guide Policy configurations Your AWS KMS key policy serves as the foundational security control for your backup encryption strategy. Following the principle of least privilege, we recommend structuring your key policies with distinct roles that separate administrative operations, usage permissions, and deletion capabilities. This role-based approach ensures that no single entity has excessive permissions while maintaining operational flexibility. The following policy statements must be applied to your CMK based on your specific use case and required operations with your logically air-gapped vault: Creating an AWS Backup logically air-gapped vault using CMK When creating a logically air-gapped vault, you must apply the AWS-managed policy AWSBackupFullAccess to your account role. This policy includes Allow actions that enable AWS Backup to interact with AWS KMS for grant creation on AWS KMS keys during backup, copy, and storage operations. Furthermore, you must ensure that the CMK key policy includes specific necessary permissions to share with the account where the logically air-gapped vault resides.

{ "Sid": "Allow use of the key to create a logically air-gapped vault", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::[account-id]:role/TheRoleToAccessAccount" }, "Action": [ "kms:CreateGrant", "kms:DescribeKey" ], "Resource": "*", "Condition": { "StringLike": { "kms:ViaService": "backup.*.amazonaws.com" } } }JSON

Copying or restoring within the same-account using a CMK During same-account copy or restore operations, AWS Backup assumes a customer-defined copy role such as AWSBackupDefaultServiceRole or a custom role that you specify. This role requires specific permissions to interact with your CMK for both encryption operations and grant management.

{ "Sid":"Allow use of the key for copy", "Effect":"Allow", "Principal":{ "AWS":"arn:aws:iam::[source-account-id]:role/service-role/AWSBackupDefaultServiceRole"[ "Source copy role" ] }, "Action":[ "kms:Encrypt", "kms:Decrypt", "kms:ReEncrypt*", "kms:GenerateDataKey*", "kms:DescribeKey" ], "Resource":"*", "Condition":{ "StringLike":{ "kms:ViaService":"backup.*.amazonaws.com" } } }, { "Sid":"Allow AWS Backup to create grant on the key for copy", "Effect":"Allow", "Principal":{ "AWS":"arn:aws:iam::[source-account-id]:role/service-role/AWSBackupDefaultServiceRole"[ "Source copy role" ] }, "Action":[ "kms:CreateGrant" ], "Resource":"*", "Condition":{ "Bool":{ "kms:GrantIsForAWSResource":"true" }, "StringLike":{ "kms:ViaService":"backup.*.amazonaws.com" } } }JSON

Copying or restoring cross-account using a CMK When copying or restoring between accounts, or from a recovery account through AWS Resource Access Manager (AWS RAM) or Multi-party approval, the CMK must trust both the recovery account's service role and the destination account's service-linked role (SLR). This cross-account trust enables secure key operations across organizational boundaries.

{ "Sid":"Allow use of the key for copy/restore from a recovery account", "Effect":"Allow", "Principal":{ "AWS":[ "arn:aws:iam::[recovery-account-id]:role/service-role/AWSBackupDefaultServiceRole", [ "Recovery account copy/restore role" ]"arn:aws:iam::[destination-account-id]:role/aws-service-role/backup.amazonaws.com/AWSServiceRoleForBackup" ][ "Destination SLR" ] }, "Action":[ "kms:Encrypt", "kms:Decrypt", "kms:ReEncrypt*", "kms:GenerateDataKey*", "kms:DescribeKey" ], "Resource":"*" }, { "Sid":"Allow AWS Backup to create grant on the key for copy", "Effect":"Allow", "Principal":{ "AWS":[ "arn:aws:iam::[recovery-account-id]:role/service-role/AWSBackupDefaultServiceRole"[ "Recovery account copy/restore role" ]"arn:aws:iam::[destination-account-id]:role/aws-service-role/backup.amazonaws.com/AWSServiceRoleForBackup" ], [ "Destination SLR" ] }, "Action":[ "kms:CreateGrant" ], "Resource":"*", "Condition":{ "Bool":{ "kms:GrantIsForAWSResource":"true" } } }JSON

These policy statements implement least-privilege service access by restricting principals per operation and ensuring that all grants are explicitly for AWS resources through kms:GrantIsForAWSResource. The recovery-account-id refers to the source account in normal cross-account and cross-Region restore scenarios, while it represents the restore account when using AWS RAM or Multi-party approval shares. Depending on your specific use case, you can apply a combined policy that accommodates multiple scenarios. API & Console Implementation Having established the necessary policy configurations for secure key management, we now shift our focus to practical implementation. These enhancements provide organizations with greater control over their backup encryption strategy through:

Enhanced Vault Creation: The ability to specify a CMK using EncryptionKeyArn when creating AWS Backup logically air-gapped vaults. Encryption Visibility: The addition ofEncryptionKeyType in API responses to indicate whether a vault uses an AWS Backup service-owned key by default or a customer-managed key.

This section focuses on the specific APIs that have been modified to support these CMK capabilities. For comprehensive coverage of all AWS Backup APIs, refer to the AWS Backup API Reference documentation. Create an AWS Backup logically air-gapped vault with CMK To create a logically air-gapped vault with a CMK, you can use the 'CreateLogicallyAirGappedBackupVault' API. This API necessitates specifying both 'MinRetentionDays' and 'MaxRetentionDays' parameters, because AWS Backup logically air-gapped vault is a vault locked in compliance mode. To support CMK, we've added an optional EncryptionKeyArn field to the API input. If you don't provide an EncryptionKeyArn, then AWS Backup creates the AWS Backup logically air-gapped vault using an AWS Backup service owned key by default. Vault creation is asynchronous, so the vault will likely be in a CREATING state initially. The following is an example using the AWS Command Line Interface (AWS CLI):

aws backup create-logically-air-gapped-backup-vault --region us-east-1 --backup-vault-name ExampleLAGVault --encryption-key-arn arn:aws:kms:us-east-1:123456789012:key/1234abcd-12ab-34cd-56ef-1234567890ab --min-retention-days 7 --max-retention-days 35 --creator-request-id 12312873218-13213-1321TypeScript

Similarly, you can create an AWS Backup logically air-gapped vault with CMK in console using the example in the following figure.

Figure 2: Creating an AWS Backup logically air-gapped vault with CMK encryption Describe an AWS Backup logically air-gapped vault You can use the 'DescribeBackupVault' API to view the attributes and state of your AWS Backup logically air-gapped vault. The response includes an EncryptionKeyType field that indicates whether the vault is encrypted using an AOK or a CMK. The following shows how to describe an AWS Backup logically air-gapped vault using the AWS CLI:

The command returns detailed information about the vault, including its encryption configuration:

{ BackupVaultName:"ExampleLAGVault", BackupVaultArn:"arn:aws:backup:us-east-1:123456789012:backup-vault:ExampleLAGVault", VaultType:"LOGICALLY_AIR_GAPPED_BACKUP_VAULT", VaultState:"AVAILABLE", EncryptionKeyArn:"arn:aws:kms:us-east-1:123456789012:key/1234abcd-12ab-34cd-56ef-1234567890ab", CreationDate:"2025-09-25T14:03:49.412000-07:00", CreatorRequestId:"86eacd7f-6f0e-4aed-8be6-20e1f8a28c1f", NumberOfRecoveryPoints:0, Locked:true, MinRetentionDays:7, MaxRetentionDays:35, LockDate:"2025-09-25T14:03:49.412000-07:00", EncryptionKeyType:"CUSTOMER_MANAGED_KMS_KEY" }TypeScript

Similarly, in console you can go to Vaults and search your AWS Backup logically air-gapped vault by name and observe the CMK encryption key.

A logically air-gapped vault with an CMK encryption key List AWS Backup vaults You can use the 'ListBackupVaults' API to view all vaults in your account. The response includes the EncryptionKeyType field for each vault, indicating whether it uses an AOK or a CMK. The following shows how to list all backup vaults using the AWS CLI:

The command returns a list of vaults with their configurations. The EncryptionKeyType field in the response shows either CUSTOMER_MANAGED_KMS_KEY or AWS_OWNED_KMS_KEY for each vault.

{ BackupVaultList:[ { BackupVaultName:"Default", BackupVaultArn:"arn:aws:backup:us-east-1:123456789012:backup-vault:Default", VaultType:"BACKUP_VAULT", CreationDate:"2020-11-05T14:15:22.329000-08:00", EncryptionKeyArn:"arn:aws:kms:us-east-1:123456789012:key/9493c201-1d02-4d2a-943a-650fee1c949e", CreatorRequestId:"Default", NumberOfRecoveryPoints:41358, Locked:false }, { BackupVaultName:"ExampleLAGVault", BackupVaultArn:"arn:aws:backup:us-east-1:974288443796:backup-vault:ExampleLAGVault", VaultType:"LOGICALLY_AIR_GAPPED_BACKUP_VAULT", VaultState:"AVAILABLE", CreationDate:"2025-09-25T14:21:01.166000-07:00", EncryptionKeyArn:"arn:aws:kms:us-east-1:974288443796:key/96b4d755-8228-44c5-8e23-7bf29017653a", CreatorRequestId:"8f5528ff-4ed8-408c-855c-5f89388330f7", NumberOfRecoveryPoints:2, Locked:true, MinRetentionDays:7, MaxRetentionDays:35, LockDate:"2025-09-25T14:21:01.166000-07:00", EncryptionKeyType:"CUSTOMER_MANAGED_KMS_KEY" } ] }TypeScript

Describe a recovery point You can use the 'DescribeRecoveryPoint' API to view details of a specific recovery point. To describe a recovery point using the AWS CLI:

Replace with the Amazon Resource Name (ARN) of the recovery point you want to describe, and with the name of the vault containing the recovery point. List recovery points by backup vault You can use the 'ListRecoveryPointsByBackupVault' API to view all recovery points within a specific backup vault. To list recovery points in a vault using the AWS CLI:

Replace with the name of the vault containing the recovery point. List recovery points by resource You can also use the 'ListRecoveryPointsByResource' API to view all recovery points for a specific resource across backup vaults. To list recovery points for a resource using the AWS CLI:

Replace with the ARN of the resource want to describe. The EncryptionKeyType field in the response of all of the preceding commands return either CUSTOMER_MANAGED_KMS_KEY or AWS_OWNED_KMS_KEY, indicating the type of encryption key used for the vault containing this recovery point. Cleaning up After you've created your logically air-gapped vault, you can clean up any resources to avoid unnecessary charges by following the steps in the Cleaning up backups section of the AWS Prescriptive Guidance. Summary In this post, we introduced CMK support for AWS Backup logically air-gapped vault. This enhancement builds upon the AWS Backup logically air-gapped vault's core security features: immutable backup storage, automatic vault locking, and cross-account sharing capabilities. This is done while enabling organizations to maintain complete control over their encryption keys. This add-on is particularly valuable for regulated industries that must meet specific key management requirements while maintaining robust data protection. Getting started with CMKs for AWS Backup logically air-gapped vault is direct through the AWS Backup console, API, or AWS CLI. Organizations can create new vaults with CMKs while preserving all security benefits of logical air-gapping. To learn more about using CMKs with AWS Backup logically air-gapped vault, visit the AWS Backup product page and documentation.

TAGS: AWS Backup, AWS Cloud Storage, AWS Key Management Service (AWS KMS), data protection, data resiliency

Desiree Brunner Desiree is a Security Solutions Architect at AWS specializing in enterprise security transformations. Her background is in data protection and security architecture, focused on a variety of security domains, including applied cryptography and regulatory compliance. She partners with enterprise customers to design and implement comprehensive security strategies that protect critical workloads while enabling business agility and innovation.

Mani Manasa Mylavarapu Mani Manasa Mylavarapu is an Engineering Manager at Amazon Web Services specializing in cloud security and data protection. She has played a key role in building enterprise-grade backup and encryption systems that safeguard customer data globally. Passionate about leadership and innovation, she combines deep technical expertise with a people-first approach to scaling secure, reliable cloud platforms.

Sabith Venkitachalapathy Sabith Venkitachalapathy is an expert in designing AWS recovery resilience solutions, ensuring disaster recovery and high availability for critical workloads. Focused on Financial Services (FSI) and Healthcare and Life Sciences (HCLS), Sabith leverages AWS to tackle industry challenges and drive innovation. He shares practical insights to help organizations build resilient, secure cloud architectures.

MSP Incentive for Customer Management: recognizes MSP Partners who help customers adopt and use cloud solutions. MSP Incentive for Strategic Services: rewards MSP Partners who help customers innovate and enhance their cloud capabilities through specific strategic AWS services. MSP Government Practice Benefit: supports MSP Partners that serve government customers through their digital transformation journey.

Get started We're excited to roll out these benefits to help you build and scale your MSP practice.

Review the FAQs in AWS Partner Central (login required). Review the updated MSP Program Guide with detailed information on benefits, incentive structures, and eligibility requirements. Available in AWS Partner Central by the end of December 2025. Attend enablement sessions in November and December 2025 (invites to follow), continuing throughout 2026.

For questions or to learn how these new benefits can support your MSP business strategy, contact your AWS PDM. Not yet a validated AWS MSP Partner? Join the AWS MSP Program to unlock revenue opportunities, technical enablement, incentives, and workshops with AWS experts. ¹ IDC, Worldwide Managed Cloud Services Forecast, 2025–2029: A Competitive Services View of Technology Outsourcing Services Markets, doc #US52883625, August 2025

TAGS: APN Launches, APN Programs, AWS MSP Partner Program, AWS Partner Network, AWS Public Sector Partners, Government, Managed Service Provider

Empowering personalized learning at scale: Loyola Marymount University's AI course companion

The traditional model of academic support—which includes office hours, tutoring centers, and teaching assistants—can't always keep pace with student needs in today's universities. Studies have shown that 85% of students have already adapted to the changing landscape of academic support by using generative AI tools, but these generic solutions often fall short of providing content tailored to their classes and personalized guidance—and their use can raise concerns about data privacy and academic integrity. Loyola Marymount University (LMU) envisioned something different: an AI companion that could speak in each professor's voice, reference the exact materials from their courses, and be available around the clock. Working with Amazon Web Services (AWS), the university developed a secure, course-specific tool called the AI Study Companion, which has earned strong faculty approval while providing students with the support they need at substantial cost savings compared to commercial alternatives. Addressing student needs with 24/7 course-specific support LMU's mission emphasizes personal connections in learning through a high-touch, individualized approach. With most students turning to generic, off-the-shelf AI tools, the university saw an opportunity. "One of the things that sparked this is, 'How do we make a better version of what's currently available?'" said Matt Frank, director of teaching, learning, and research technology at LMU. Brian Drawert, manager of research computing at LMU and the AI Study Companion's developer, explained the core issue: "AI was already trying to help students with their coursework, but doing it poorly. The challenge was giving them a chat interface that actually answered questions for their class." Modern learners also juggle complex schedules, including jobs, family commitments, and study abroad programs, making traditional faculty office hours inaccessible to many students. Building a 24/7 solution was particularly important. Building on AWS for data control and student privacy LMU's existing relationship with AWS made it a natural foundation for the project. Through ongoing data center migrations to AWS, the university had already established trust in the platform's security capabilities—which was a crucial factor given the sensitive nature of educational data. "We didn't want to use a tool where their data would be going to some outside vendor, and we had no idea what was happening with that data," Frank explained. "So, we really wanted to build an environment where LMU would be in control of the data and the security." This control was essential for meeting the Family Educational Rights and Privacy Act (FERPA) compliance requirements and protecting faculty intellectual property (IP). FERPA protects students' educational records and personal information, while faculty course content—including lectures, syllabi, and teaching materials—represents significant scholarly work and IP that required robust protection. Since the system would ingest classroom recordings containing both professor and student voices, maintaining strict access controls was non-negotiable. Building within LMU's controlled AWS environment helped keep both student data and faculty IP secure and inaccessible to external vendors or AI training models. The technical foundation of the AI Study Companion leverages multiple AWS services: Amazon Bedrock for core generative AI capabilities, Amazon Transcribe for processing classroom recordings, Amazon Simple Storage Service (Amazon S3) for data storage, Amazon Elastic Container Service (Amazon ECS) for scalability, Amazon OpenSearch for search functionality, AWS Lambda for automated processes, and AWS Web Application Firewall (WAF) for security. "We wanted to start with a robust toolkit," Frank explained. "Amazon Bedrock presented itself as that correct toolkit. We were able to build our specific needs and use cases on top of an already robust and flexible platform." Rapid development through AWS specialist collaboration The project timeline proved remarkably fast. LMU hired Drawert in late 2024 and held its first AWS meeting on January 27, 2025. Drawert made his first code commit on March 28, 2025 and by early May, LMU demonstrated a working solution at the Association of Jesuit Colleges & Universities IT Management Conference (AJCU-CITM). The timeline was particularly compressed because LMU wanted to launch the AI Study Companion in classrooms by August 2025 to start helping students at the beginning of the fall semester. AWS solutions architect Lorin Miller worked closely with Drawert throughout the development process. The collaboration began with discovery sessions to understand LMU's vision and requirements, then moved quickly into proof-of-concept development. AWS brought in specialists for AI, containers, and networking to make sure each component was properly optimized. LMU's existing cloud team also provided essential support in getting Drawert trained on the platform. Echo360, LMU's classroom capture vendor, also supported the project with an innovation grant that helped offset hosting costs. Weekly content ingestion mirrors faculty teaching approach LMU's AI Study Companion works differently from generic AI tools. Instead of providing all course information at once, the system mirrors how faculty structure learning throughout a semester. "We don't give the tool all of the information right at the start of the semester," Frank said. "We will give it the actual information that the students are going to learn in week one in week one, and then week two in week two." This approach respects faculty instructional choices while providing students with appropriately leveled guidance. In practice, this means that Amazon Transcribe processes classroom recordings and transcripts are reviewed for accuracy before being uploaded to the knowledge base—maintaining human oversight. From the students' perspective, the experience is straightforward. Students log in with LMU credentials, select their course, and can interact conversationally with the AI Study Companion on any device. They can request study guides, clarify missed material, or ask questions about course content. Beyond providing accurate answers about actual course content, the AI Study Companion has one unexpected benefit that emerged during testing. "We discovered that it not only provided class-specific information, but also captured the professor's unique style, such as their affectations, inflections, and even their jokes," Drawert explained. Early results show faculty satisfaction and cost savings The project pilot launched in August 2025 with approximately 125 students. Faculty feedback has been consistently positive, with professors noting the tool's accuracy and alignment with course material. While formal research data remains under Institutional Review Board (IRB) review, early impressions suggest strong potential. "The AI Study Companion has been an exciting and innovative project," said Kat Weaver, interim executive vice president and provost at LMU. "Faculty using the tool have reported their satisfaction with the ease of integration as well as student engagement and comprehension." Based on these encouraging results, the university plans aggressive expansion. The solution's cost-effectiveness makes this kind of expansion feasible. Compared to enterprise AI tool licenses costing $30 per student monthly, LMU's custom solution provides substantial savings while delivering more tailored functionality. This combination of lower costs and superior personalization has attracted attention from other institutions, leading to discussions about potential partnerships to adopt similar solutions. Because of this, LMU is exploring how to package the solution for broader adoption. "This could help a lot of students, potentially thousands of students within the next 12 months," Drawert noted. Supporting responsible AI innovation in higher education LMU's AI Study Companion demonstrates how institutions can leverage cloud technology and generative AI to enhance learning without sacrificing core educational values. By building on AWS infrastructure, the university developed a solution with the desired functionality of commercial alternatives while maintaining complete ownership of the educational experience. For higher education leaders considering AI adoption, LMU's approach offers a blueprint for responsible innovation that puts student success and faculty expertise at the center while leveraging cloud-native AI services. Learn how AWS helps education institutions use generative AI to deliver customized, secure student experiences. Read related stories on the AWS Public Sector Blog:

How Northwestern University built a multilingual generative AI search tool with AWS 6 EdTech AI trends: How artificial intelligence is reshaping education Benchmark Education accelerates grading and boosts student feedback with generative AI on AWS

TAGS: Artificial Intelligence, AWS Public Sector, customer story, higher education

Lorin Miller Lorin is a solutions architect at AWS who works with higher education institutions in California. She helps customers build secure, scalable solutions and develop forward-looking cloud and IT strategies. Lorin's background in artificial intelligence, IT process optimization, and cloud migration patterns enables organizations to accelerate digital innovation.

Matt Frank Matt is the director of teaching, learning, and research technology at Loyola Marymount University, where he oversees instructional technology, research computing, learning space design, and the creation of multimedia-based academic resources that enhance student learning and promote faculty outreach.

Brian Drawert Brian is the manager of research computing at Loyola Marymount University, supporting high-performance computing and data-driven research. With a background in software development and expertise in designing high-scale distributed systems, he has contributed to projects spanning computational science, virtual reality, and biochemical modeling. An avid sailor, he also applies his technical skills to marine navigation and communications.

By: Dhanya Kurup, Partner Solutions Architect – AWS By: Nirmal Mukhi, VP & Head of Engineering, GenerativeAgent – ASAPP By: Rachel Corbat, Partner Solutions Architect – AWS

The airline industry stands at a crossroads in customer service evolution. Airlines are seeing increased customer service demands as global air travel returns to pre-pandemic levels. Customers want immediate, personalized service across all channels, including mobile applications and social media platforms, airport self-service kiosks, and traditional channels like call centers and service counters. In turn, airlines need to manage operational costs and variable service demands. This transformation isn't just about incremental improvements – it's about reimagining customer experience through the lens of artificial intelligence and cloud technology. ASAPP, an AWS Advanced Partner, provides AI-powered solutions for customer service. Founded in 2014, ASAPP helps transform contact centers by automating tasks, assisting representatives, and improving customer experience. Their GenerativeAgent platform handles complex conversations while maintaining enterprise-level performance and safety standards. This blog explains how ASAPP's GenerativeAgent platform, built on AWS infrastructure, combines cloud capabilities with specialized AI to transform customer service operations. The solution uses Amazon Bedrock with Claude Sonnet and Claude Haiku foundation models, integrating with services like Amazon Connect for comprehensive capabilities. These include intelligent conversation management, human-AI collaboration, and enterprise integration. The Evolution of Airline Customer Service Research shows that existing phone-based customer service centers that rely on manual call routing and scripted responses are struggling to meet modern demands. These systems typically include basic Interactive Voice Response (IVR) menus, queue-based call distribution, and isolated customer relationship management (CRM) databases that don't integrate with reservation systems. Customers expect faster and more convenient service while interactions grow increasingly complex. Airlines must navigate sudden contact surges from weather events, time-sensitive customer needs, and international regulatory requirements. The complexity increases with the integration of multiple legacy aviation systems and processes. These include Passenger Service System (PSS) for reservations, Global Distribution Systems (GDS) for inventory, Departure Control Systems (DCS) for boarding, and Revenue Management Systems (RMS) for calculations. This fragmentation means customer service representatives often need to manually access and navigate multiple platforms to resolve even a single customer request. Current self-service solutions have limitations. First-generation IVR systems and rule-based chatbots operate on pre-defined decision trees and fixed response templates. For example, when a customer calls about a delayed flight, these systems can only provide basic status updates but cannot handle complex requests such as multi-carrier rebooking or refund processing. Industry reports confirm these limitations lead to low containment rates, resulting in increased operational costs, decreased customer satisfaction, and added complexity to interactions, ultimately frustrating both customers and service representatives. Transforming Self-Service Through AI As per this Forbes report, customer experience improvements deliver measurable business value, driving revenue 4-8% higher than industry competitors. Companies with superior customer experience reduce service costs by 15-20% and generate 5 times more revenue than competitors. ASAPP's GenerativeAgent platform helps enterprises capture these opportunities through seamless integration with AWS services, maintaining existing technology investments while adding powerful generative AI capabilities.

Figure 1 – Workflow diagram showing ASAPP's Multi Agent Architecture Key Platform Capabilities The GenerativeAgent platform transforms customer service with intelligent conversation management, human-AI collaboration, and enterprise integration. This diagram illustrates the step-by-step process of how customer interactions flow through the platform using AWS services: Customer Interaction Initiation

Customer interaction begins when contacts arrive via voice using Amazon Connect or through digital channels The system enables conversational AI-powered end-to-end interactions with seamless topic identification and switching through advanced natural language processing and contextual memory management The platform maintains interaction history, manages backend API access, and leverages document context to enable complex problem-solving capabilities

The platform uses different foundational models for different tasks: Claude Haiku for input safety and scope validation, Claude Sonnet for reasoning, tool calling, and output verification The precise choice of which models are used in the overall orchestration is influenced by the use case and customer preferences Amazon Bedrock manages model deployment, model output, and scaling

Deep integration with existing airline systems through an adaptation layer that transforms legacy API formats and data structures into standardized formats that can be efficiently processed by AI models Real-time access via APIs to operational data such as booking, flight status, and baggage enables GenerativeAgent to reason and solve customer problems Secure retrieval and handling of customer information: API transforms redact sensitive data, and any exposed sensitive data is redacted prior to storage and encrypted in transit and at rest

The system intelligently routes tasks between AI and customer service representatives Most requests including complex workflows can be handled with full automation Human-In-the-Loop Agent (HILA™) functionality for complex scenarios

Interaction outcomes feed back into the system Continuous monitoring of system performance such as which conversations may identify opportunities for improvement, summaries, structured data (including sentiment analysis, resolution rates, and use case-specific metrics) Automated scenario testing for chat and voice to simulate and evaluate system behaviors prior to production launch or to test changes

Leveraging Amazon Bedrock for Flexibility and Performance The GenerativeAgent platform uses Amazon Bedrock foundation models with a built-in orchestrator that coordinates task execution. This flexibility optimizes responses by enabling specialized model selection: one model may handle empathetic dialogue, while another model focuses on transactional tasks like flight changes. The orchestrator coordinates tasks including input safety validation, reasoning, RAG processing, and output verification, while models used for each can be selected based on specific use case requirements. This approach ensures contact centers receive tailored AI solutions without performance trade-offs or additional security complexity. Enterprise-Grade Security and Compliance ASAPP's platform maintains strong security practices and compliance frameworks including SOC 2, GDPR, and HIPAA requirements. The platform implements industry-standard encryption, data protection, and comprehensive access controls with audit trails. ASAPP provides privacy controls such as data redaction, access governance, monitoring, and incident response. This approach allows airlines to maintain robust data protection and demonstrate compliance while focusing on delivering exceptional customer experiences. Real-World Impact: A Leading Airline's Transformation A major U.S. airline's implementation of the GenerativeAgent platform demonstrates the solution's transformative potential. Completed in 39 days, the implementation delivered impressive results:

49% increase in customer self-service adoption within this timeframe 91% first-contact resolution rate, representing a 21% improvement from baseline 79% reduction in average handling time: customer issues resolved significantly faster than with representatives only 92% customer satisfaction scores, reflecting improved service quality and experience

These results showcase how AI-driven customer service can deliver tangible benefits while improving experiences for both customers and representatives. Looking Ahead ASAPP's GenerativeAgent platform will continue to evolve with innovative AI/ML technologies and foundation models. Future enhancements will enable more predictive capabilities, cross-channel integration, and personalized customer interactions. Through this evolution, airlines can expect AI to anticipate and address customer needs while maintaining the balance between automation and human expertise. Conclusion ASAPP's GenerativeAgent platform, built on Amazon Bedrock and AWS infrastructure, sets a new standard in customer service automation. For airlines and other industries, it offers enhanced customer experiences through AI that works with customer service representatives while maintaining enterprise security and scalability. This work between ASAPP and AWS shows how technology can transform challenges into opportunities for improved customer engagement and operational efficiency. To learn more about ASAPP's GenerativeAgent platform or to get started, contact ASAPP through the AWS Partner Network or check out their listing on AWS Marketplace. . .

ASAPP – AWS Partner Spotlight ASAPP is an AWS Advanced Technology Partner and AWS Generative AI Competency Partner that provides AI-powered contact center software designed to transform customer service operations through advanced artificial intelligence and automation technologies that help organizations expand contact center capacity while reducing costs and improving customer experiences. Contact ASAPP | Partner Overview | AWS Marketplace

TAGS: Amazon Bedrock, Amazon Connect, Artificial Intelligence, AWS Marketplace, Contact Center, Intermediate (200), Partner solutions, Thought Leadership, Travel and Hospitality

Introducing structured output for Custom Model Import in Amazon Bedrock

With Amazon Bedrock Custom Model Import, you can deploy and scale fine-tuned or proprietary foundation models in a fully managed, serverless environment. You can bring your own models into Amazon Bedrock, scale them securely without managing infrastructure, and integrate them with other Amazon Bedrock capabilities. Today, we are excited to announce the addition of structured output to Custom Model Import. Structured output constrains a model's generation process in real time so that every token it produces conforms to a schema you define. Rather than relying on prompt-engineering tricks or brittle post-processing scripts, you can now generate structured outputs directly at inference time. For certain production applications, the predictability of model outputs is more important than their creative flexibility. A customer service chatbot might benefit from varied, natural-sounding responses, but an order processing system needs exact, structured data that conforms to predefined schemas. Structured output bridges this gap by maintaining the intelligence of foundation models while verifying their outputs meet strict formatting requirements. This represents a shift from free-form text generation to outputs that are consistent, machine-readable, and designed for seamless integration with enterprise systems. While free-form text excels for human consumption, production applications require more precision. Businesses can't afford the ambiguity of natural language variations when their systems depend on structured outputs to reliably interface with APIs, databases, and automated workflows. In this post, you will learn how to implement structured output for Custom Model Import in Amazon Bedrock. We will cover what structured output is, how to enable it in your API calls, and how to apply it to real-world scenarios that require structured, predictable outputs. Understanding structured output Structured output, also known as constrained decoding, is a method that directs LLM outputs to conform to a predefined schema, such as valid JSON. Rather than allowing the model to freely select tokens based on probability distributions, it introduces constraints during generation that limit choices to only those that maintain structural validity. If a particular token would violate the schema by producing invalid JSON, inserting stray characters, or using an unexpected field name the structured output rejects it and requires the model to select another allowed option. This real-time validation helps keep the final output consistent, machine readable, and immediately usable by downstream applications without the need for additional post-processing. Without structured output, developers often attempt to enforce structure through prompt instructions like "Respond only in JSON." While this approach sometimes works, it remains unreliable due to the inherently probabilistic nature of LLMs. These models generate text by sampling from probability distributions, introducing natural variability that makes responses feel human but creates significant challenges for automated systems. Consider a customer support application that classifies tickets: if responses vary between "This seems like a billing issue," "I'd classify this as: Billing," and "Category = BILLING," downstream code cannot reliably interpret the results. What production systems require instead is predictable, structured output. For example:

With a response like this, your application can automatically route tickets, trigger workflows, or update databases without human intervention. By providing predictable, schema-aligned responses, structured output transforms LLMs from conversational tools into reliable system components that can be integrated with databases, APIs, and business logic. This capability opens new possibilities for automation while maintaining the intelligent reasoning that underpin the value of these models. Beyond improving reliability and simplifying post-processing, structured output offers additional benefits that strengthens performance, security and safety in production environments.

Lower token usage and faster responses: By constraining generation to a defined schema, structured output removes unnecessary verbose, free-form text, resulting in reduced token count. Because token generation is sequential, shorter outputs directly translate to faster responses and lower latency, improving overall performance and cost efficiency. Enhanced security against prompt injection: Structured output narrows the model's expression space and helps prevent it from producing arbitrary or unsafe content. Bad actors cannot inject instructions, code or unexpected text outside the defined structure. Each field must match its expected type and format, making sure outputs remain within safe boundaries. Safety and policy controls: Structured output enables you to design schemas that inherently help prevent harmful, toxic, or policy-violating content. By limiting fields to approved values, enforcing patterns, and restricting free-form text, schemas make sure outputs align with regulatory requirements.

In the next section, we will explore how structured output works with Custom Model Import in Amazon Bedrock and walks through an example of enabling it in your API calls. Using structured output with Custom Model Import in Amazon Bedrock Let's start by assuming you have already imported a Hugging Face model into Amazon Bedrock using the Custom Model Import feature. Prerequisites Before proceeding, make sure you have:

An active AWS account with access to Amazon Bedrock A custom model created in Amazon Bedrock using the Custom Model Import feature Appropriate AWS Identity and Access Management (IAM) permissions to invoke models through the Amazon Bedrock Runtime

With these prerequisites in place, let's explore how to implement structured output with your imported model. To start using structured output with a Custom Model Import in Amazon Bedrock, begin by configuring your environment. In Python, this involves creating a Bedrock Runtime client and initializing a tokenizer from your imported Hugging Face model. The Bedrock Runtime client provides access to your imported model using the Bedrock InvokeModel API. The tokenizer applies the correct chat template that aligns with the imported model, which defines how user, system, and assistant messages are combined into a single prompt, how the role markers (for example, , ) are inserted, and where the model's response should begin. By calling tokenizer.apply_chat_template(messages, tokenize=False) you can generate a prompt that matches the exact input format your model expects, which is essential for consistent and reliable inference, especially when structured encoding is enabled.

import boto3 from transformers import AutoTokenizer from botocore.config import Config

# HF model identifier imported into Bedrock hf_model_id = " >" # Example: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B model_arn = "arn:aws:bedrock: >: >:imported-model/your-model-id" region = " >"

# Initialize tokenizer aligned with your imported model tokenizer = AutoTokenizer.from_pretrained(hf_model_id)

Implementing structured output When you invoke a custom model on Amazon Bedrock, you have the option to enable structured output by adding a response_format block to the request payload. This block accepts a JSON schema that defines the structured of the model's response. During inference, the model enforces this schema in real-time, making sure that each generated token conforms to the defined structure. Below is a walkthrough demonstrating how to implement structured output using a simple address extraction task. Step 1: Define the data structure You can define your expected output using a Pydantic model, which serves as a typed contract for the data you want to extract.

class Address(BaseModel): street_number: str = Field(description="Street number") street_name: str = Field(description="Street name including type (Ave, St, Rd, etc.)") city: str = Field(description="City name") state: str = Field(description="Two-letter state abbreviation") zip_code: str = Field(description="5-digit ZIP code")Python

Step 2: Generate the JSON schema Pydantic can automatically convert your data model into a JSON schema:

schema = Address.model_json_schema() address_schema = { "name": "Address", "schema": schema }Python

This schema defines each field's type, description, and requirement, creating a blueprint that the model will follow during generation. Step 3: Prepare your input messages Format your input using the chat format expected by your model:

messages = [{ "role": "user", "content": "Extract the address: 456 Tech Boulevard, San Francisco, CA 94105" }]Python

Step 4: Apply the chat template Use your model's tokenizer to generate the formatted prompt:

Step 5: Build the request payload Create your request body, including the response_format that references your schema:

request_body = { 'prompt': prompt, 'temperature': 0.1, 'max_gen_len': 1000, 'top_p': 0.9, 'response_format': { "type": "json_schema", "json_schema": address_schema } }Python

Step 6: Invoke the model Send the request using the InvokeModel API:

Step 7: Parse the response Extract the generated text from the response:

Because the schema defines required fields, the model's response will contain them:

{ "street_number": "456", "street_name": "Tech Boulevard", "city": "San Francisco", "state": "CA", "zip_code": "94105" }Python

The output is clean, valid JSON that can be consumed directly by your application with no extra parsing, filtering, or cleanup required. Conclusion Structured output with Custom Model Import in Amazon Bedrock provides an effective way to generate structures, schema-aligned outputs from your models. By shifting validation into the model inference itself, structured output reduce the need for complex post-processing workflows and error handling code. Structured output generates outputs that are predictable and straightforward to integrate into your systems and supports a variety of use cases, for example, building financial applications that require precise data extraction, healthcare systems that need structured clinical documentation, or customer service systems that demand consistent ticket classification. Start experimenting with structured output with your Custom Model Import today and transform how your AI applications deliver consistent, production-ready results.

About the authors Manoj Selvakumar is a Generative AI Specialist Solutions Architect at AWS, where he helps organizations design, prototype, and scale AI-powered solutions in the cloud. With expertise in deep learning, scalable cloud-native systems, and multi-agent orchestration, he focuses on turning emerging innovations into production-ready architectures that drive measurable business value. He is passionate about making complex AI concepts practical and enabling customers to innovate responsibly at scale—from early experimentation to enterprise deployment. Before joining AWS, Manoj worked in consulting, delivering data science and AI solutions for enterprise clients, building end-to-end machine learning systems supported by strong MLOps practices for training, deployment, and monitoring in production. Yanyan Zhang is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. Lokeshwaran Ravi is a Senior Deep Learning Compiler Engineer at AWS, specializing in ML optimization, model acceleration, and AI security. He focuses on enhancing efficiency, reducing costs, and building secure ecosystems to democratize AI technologies, making cutting-edge ML accessible and impactful across industries. Revendra Kumar is a Senior Software Development Engineer at Amazon Web Services. In his current role, he focuses on model hosting and inference MLOps on Amazon Bedrock. Prior to this, he worked as an engineer on hosting Quantum computers on the cloud and developing infrastructure solutions for on-premises cloud environments. Outside of his professional pursuits, Revendra enjoys staying active by playing tennis and hiking. Muzart Tuman is a software engineer utilizing his experience in fields like deep learning, machine learning optimization, and AI-driven applications to help solve real-world problems in a scalable, efficient, and accessible manner. His goal is to create impactful tools that not only advance technical capabilities but also inspire meaningful change across industries and communities.

Amazon VPC Lattice is a fully managed application networking service that lets you connect, secure, and monitor communication between your services and resources within and across VPCs and accounts. Today, we announce custom domain names for Lattice resources. In this post, we will examine this feature in detail. Resource owners can now specify an Fully Qualified Domain Name (FQDN) as the custom domain name for a Lattice resource before sharing it with consumers in other VPCs and accounts. Consumers can then access the resource using this FQDN. VPC Lattice manages a private hosted zone in the consumer's VPC that enables clients to connect to the resource using the custom domain name. Resource owners have the flexibility to either use domains they own or domains they don't own such as Amazon-provided or third-party domains. Consumers can choose which types of domains they want to allow VPC Lattice to manage private hosted zones for in their VPCs. In the rest of this post, we will first go over the use cases that this feature and describe how the feature works with examples. Prerequisites This guide assumes you understand VPC Lattice fundamentals. If you need background information, review the VPC Lattice User Guide before proceeding. Key Use Cases Custom domain names for resources extend VPC Lattice's capabilities by addressing several important new use cases:

Cluster-based resources: Customers can now use resource configurations to share and access cluster-based resources across VPC and account boundaries, such as cache clusters, Kafka clusters, etc. To access cluster-based resources, a client needs to know the cluster topology and be able to query specific nodes in the cluster. With this feature, cluster owners can specify a custom domain name for each node in the cluster, thereby enabling clients to identify and query individual nodes directly. TLS-based applications: Customers can now use resource configurations to share and access TLS-based applications. In Transport Layer Security (TLS), clients must connect using a domain for which the server can present a valid certificate. With this feature, resource owners can specify the required domain name as the custom domain name in the resource configuration, ensuring seamless TLS connectivity for clients. Amazon-provided or third-party owned domains: Customers can now share resources whose domains are provided by Amazon or third parties. AWS-native resources often have endpoints of the form example.amazonaws.com or example.aws. With this feature, a resource owner can share such resources by creating a resource configuration and specifying the Amazon-provided endpoint as custom domain name. For example, an Amazon MSK cluster might have multiple broker endpoints such as b-1.example-cluster.abc123.c2.kafka.us-east-1.amazonaws.com and b-2.example-cluster.abc123.c2.kafka.us-east-1.amazonaws.com. Resource owners can create resource configurations for each broker endpoint, specifying the MSK-provided domains as custom domain names, enabling consumers to connect to specific Kafka brokers using the AWS-provided endpoints. Enhanced Security posture: Resource owners can use both verified and unverified custom domains. Verified domains are those whose ownership is verified by AWS, ensuring that a given domain is used only by its legitimate owner. They provide enhanced security posture to consumers. Unverified domains offer flexibility for development scenarios or when using Amazon/third-party domains. This dual approach maintains strong security controls for production workloads while providing flexibility for development and testing environments.

Now, let's examine the important VPC Lattice components that enable this functionality. Introducing Domain Verification Domain verification is the process of proving ownership of a domain (e.g., example.com) that you want to use as a custom domain name for resource configurations. For instance, if you are part of Example Organization, your resource and application endpoints follow patterns such as abc.example.com or xyz.example.com. You may therefore want to use FQDNs (Fully Qualified Domain Names)—complete domain names that specify the exact location in the DNS hierarchy—ending in 'example.com' as custom domain names for your resource configurations. Domain Verification streamlines this process. A Domain Verification object contains two key properties: the domain and its verification status. Verification Process: When you initiate verification for a domain, VPC Lattice automatically creates a Domain Verification object in 'pending' status and returns a TXT record configuration comprising of a (Name, Value) pair. To prove your ownership of the domain, you must create a TXT record using this pair in your domain's DNS zone. VPC Lattice checks whether the TXT record matches what was returned. Upon successful validation, the status of the Domain Verification changes to 'verified'. Integration with Resource Configuration: A resource configuration reflects its custom domain name's verification status. When you describe a resource configuration using a verified domain such as example.com, you will see the verification status as a property of the resource configuration. Subdomain Inheritance: Once you verify a domain, all its subdomains are automatically considered verified. For instance, after verifying example.com, you can immediately use FQDNs such as abc.example.com and def.xyz.example.com as custom domain names for resource configurations in your account with no additional verification required. Configuration Steps The example below captures the verification process. Step 1: Start domain verification To start domain verification from the AWS Management Console, navigate to the VPC menu, select Domain Verifications and click Start domain verification.

Figure 1 shows how to start the domain verification process using the AWS Management Console

To check the verification status, navigate to Domain Verification under the Lattice and PrivateLink menu, where you will see the TXT record name and value that need to be added to your DNS zone

Step 2: Create TXT record Add this name-value pair as a TXT record in your domain's DNS zone to prove ownership. VPC Lattice will then update the Domain Verification to 'verified'. The status change may take a few hours to complete.

Figure 3: Shows how to create TXT records for Route53 hosted zones

How It Works Now that we understand Domain Verification, let us walk through an end-to-end workflow covering three personas: resource owner, consumer (VPC owner), and service network owner. Resource Owner Experience Step 1 (Optional): Verify your domain As a resource owner, you have the flexibility to use a verified or unverified domain. The verification status is visible to consumers with whom you share the resource configuration. For verified domains, you will need a Domain Verification object with 'verified' status. Keep the Domain Verification ID ready for the next step. Step 2: Create Resource Configuration When creating a resource configuration of type 'Single' with a custom domain name:

For verified domains: Provide the Domain Verification ID in the Resource Configuration. For unverified domains: Leave the ID field blank if you are using an Amazon-provided domain, a third-party domain that you do not own, or a domain you have not verified. In this case, the resource configuration will reflect a status of 'unverified'.

Figure 4: Shows how to specify a custom domain name and verification ID for a resource configuration

Resource Configuration of type 'Group' and 'Child' A resource configuration of type 'group' does not have a custom domain name, but it dictates the custom domain names of its children. You first define a 'Group Domain' on the group. Subsequently, for each child you can specify a custom domain name that is a subdomain of the Group Domain. When using verified domains, you only need to provide the Domain Verification ID corresponding to the Group Domain. This Domain Verification automatically applies to each child. Step 1: Create the Resource Configuration Group Begin by creating the group that will contain your child configurations:

Open the Amazon VPC console Navigate to Resource configurations under PrivateLink and Lattice Click Create Resource configuration Enter a unique name for your group Select Resource group as the configuration type Enter Group Domain Name for the group

Figure 5: Shows how to create a group resource configuration group and specify a Group Domain

Step 2: Specify a custom domain name for the child resource configuration

Figure 6: Creating a child resource configuration and specifying a custom domain name

VPC Owners As a consumer, you can access resource configurations shared with you from your VPC and/or on-premises network. To do so, you must first connect the resource configuration to your VPC either directly using a Resource Endpoint (RE), or by placing the resource configuration in a service network and connecting the service network to your VPC using a Service Network Endpoint (SN-E) or Service Network Association (SN-A). Refer to the Understanding VPC Lattice Components section to learn more about RE, SN-E, and SN-A. Regardless of which connection method you choose, you have three controls to configure the types of domains for which you want VPC Lattice to manage private hosted zones in your VPC. The first control in the console is 'Enable DNS Names', which tells VPC Lattice to manage private hosted zones in your VPC. If you do not enable DNS names, no zones will be created. In the CLI and APIs, this control is called 'private-dns-enabled'. Once you have enabled DNS names, you can use the second control called 'Private DNS preferences' to configure the types of domains for which you want to allow zone creation. You can choose among several options:

'All domains' (VPC Lattice manages private hosted zones for all custom domain names), 'Verified domains only' (VPC Lattice manages private hosted zones only for custom domain names that are verified), 'Specified domains only' (VPC Lattice manages private hosted zones only if the custom domain name is in the list of domains you have allowed), and 'Verified domains and specified domains' (VPC Lattice manages private hosted zones if the custom domain name is either verified or is in the list of domains you have allowed). The default is 'Verified domains only'.

If you select either "Specified domains only" or "Verified domain and allowed domains", you can use the third control called 'Specified domains' to provide the list of domains you want to allow. You can use wildcards in your specified domains list. Service Network Owners As a Service Network Owner, you have an 'Enable DNS Name' control that overrides the Enable DNS Names control configured by a VPC owner for their SN-E or SN-A. When creating a service-network-resource-association to add a resource configuration to a service network, you must enable DNS name for the resource configuration. If you do not enable DNS name, VPC Lattice will not manage private hosted zones for that resource configuration in any VPC to which the service network is connected, even if the SN-E or SN-A has enabled DNS names. This creates a hierarchical control structure where the service network setting acts as the top-level control that overrides VPC-level settings. Step 1: From the VPC console:

Navigate to Lattice Service networks Select your service network Go to the Resource associations tab Click Create resource association Select your resource configuration Enable host names Create the association.

Navigate to Endpoints Click Create endpoint Select VPC Lattice service networks as the service category Choose your service network and VPC Configure the networking settings Create the endpoint.

Figure 8: Shows the three controls when creating a Resource endpoint

Considerations Domain Verification: Multiple AWS accounts can verify ownership of the same domain. VPC Lattice periodically checks your DNS zone for the TXT record. If the TXT record is deleted, the Domain Verification corresponding to that domain will switch to a status of 'unverified'. The 'unverified' status will be inherited by all its subdomains and resource configurations where the Domain Verification is being used. Resource Configuration Requirements: Custom domain names and Domain Verification ID must be specified at the time of creation of the resource configuration. A resource configuration cannot be modified after creation to add a custom domain name or Domain Verification ID. Group Domain: A group domain is required on a group resource configuration if you want VPC Lattice to create private hosted zones in consumer VPCs. If you do not specify a Group Domain, you can still specify custom domain names for each child. The custom domain names will be discoverable by the consumers who whom you share the group resource configuration, but VPC Lattice will not manage private hosted zones for them in the consumer VPC. Domain Conflicts: Multiple resource configurations can have the same custom domain name. However, two resource configurations with the same custom domain name cannot be added to the same service network or as child resource configurations of the same group. If a VPC already has a hosted zone for given domain, then VPC Lattice skips private hosted zone creation in that VPC for other resources with the same custom domain name. Conclusion Custom domain names for VPC Lattice resource configurations allow customers to share and access resources in VPC Lattice, such as clusters and TLS applications. By eliminating manual DNS configuration steps, this feature enables you to focus on building and deploying applications rather than managing DNS infrastructure. Domain Verification provides a scalable approach to prove ownership for domains. Automatic private hosted zone creation simplifies accessing resources. This enhancement reinforces VPC Lattice's core value proposition of simplifying application networking so that developers can focus on building and deploying applications. Whether you are running databases, clusters, or other resources, custom domain names reduce infrastructure management overhead. The feature is available in all AWS Regions where VPC Lattice is offered. For more information, visit the VPC Lattice documentation or explore the VPC Lattice Getting Started guide.

Ashish Kumar Ashish Kumar is a Senior Product Manager Tech at AWS, based out of the San Francisco Bay Area. He manages advanced cloud services in the virtual private cloud and application networking areas, such as AWS PrivateLink, VPC Lattice, and Elastic Load Balancing. Ashish has a Bachelor's from the Indian Institute of Technology Guwahati and an MBA + Master's in Business Analytics from the University of Notre Dame, USA.

Natti Swaminathan Natti is a Principal solutions architect on the US federal civilian team at AWS. He works closely with customers to build and architect mission critical solutions. Natti has extensive experience leading, architecting, and implementing high-impact technology solutions that address diverse business needs. He has a master's degree in electrical and computer engineering from Wichita State University and an MBA from North Carolina State.

Enhance retail operations using Concierto Insights & Agentic AI Platform with AWS

Retail enterprises are under increasing pressure to drive intelligent decisions, personalize experiences, and operate efficiently while managing fragmented data across ecommerce, Point of Sale (POS), warehouse, and supplier systems. Unifying and governing this data facilitates advanced analytics, AI adoption, and agile execution. In this post, we describe how the Concierto Insights & Agentic AI Platform, a third-party solution available in AWS Marketplace, helps retail and consumer goods organizations build high-quality, governed data products on AWS through domain-driven modeling and built-in quality controls. Organizations can use Concierto Insights to standardize data management processes and support their forecasting, planning, and innovation initiatives. This post demonstrates how to subscribe to the Concierto Insights & Agentic AI in AWS Marketplace and provides step-by-step guidance on how to use AWS Marketplace streamlined procurement processes to deploy a solution that helps teams design, publish, and consume trusted data products, standardizing data pipelines and powering enterprise-wide AI and decision-making. Retail persistent data challenges Retailers often face the following data challenges:

Retail search – Inadequate tagging reduces product discovery, driving search abandonment and revenue loss Inventory visibility – System gaps create stockouts, diverting customers to competitors Product planning – Data fragmentation impairs mix decisions, impacting revenue Fulfilment and returns – Data inconsistencies delay shipping and complicates return processing Supplier performance – Limited integration causes inventory and quality challenges Demand forecasting – Disconnected data creates inventory imbalances and margin loss

Solution overview Concierto Insights by Trianz is a modern analytics platform on AWS that transforms fragmented retail data into business-ready data products without coding.oncierto's federation engine queries data across multiple data sources in real time and leverages its Data Build Tool (dbt) based transformation engine to enrich data at the source instead of relying on ETL. Concierto Insights addresses critical retail challenges by enabling real-time inventory visibility, unifying planning data for better product mix decisions, streamlining fulfilment, integrating supplier performance, and supporting accurate demand forecasting. Concierto's approach consists of the following key tenets:

Federation over migration – Connect to diverse data sources without relocating data Semantic modeling – Define a common business language across data silos Data quality and governance – Enforce accuracy and compliance upfront Self-service data marketplace – Allow stakeholders to search, access, and act on data directly AI and BI assistant – Deliver insights conversationally with Concierto's Conversa and Live Intel

Figure 1: Concierto Insights high-level architecture In this post, we set up Concierto Insights using Amazon Elastic Container Service (ECS), Amazon Relational Database Service (Amazon RDS), AWS Network Firewall, Amazon Simple Storage Service (Amazon S3), and supporting network components. The following diagram depicts the overall solution architecture.

Figure 2: Concierto Insights deployment architecture The Concierto Insights solution consists of the following key components:

DNS and load balancing – A private hosted zone resolves DNS for internal and external load balancers and Amazon RDS, with traffic flowing from the Application Load Balancer (ALB) to the Network Load Balancer (NLB), then to the internal ALB for security. Private subnets –The application core runs in a virtual private cloud's (VPC's) private subnets, with traffic controlled through security groups and routed through an external ALB to a Multi-AZ Amazon ECS Fargate cluster. Internal service communications – Internal ECS services communicate securely through an internal ALB, providing service-to-service interaction within the private network. Data persistence and encryption – Amazon RDS for PostgreSQL provides highly available, Multi-AZ data persistence, with AWS Key Management Service (AWS KMS) encryption securing data at rest. Monitoring and security – Application logs and metrics are stored in Amazon CloudWatch, integrated with Amazon Simple Notification Service (Amazon SNS) for alerts. ALBs are protected by AWS WAF, and TLS provides secure internal and external communication.

Prerequisites To implement this solution, you must have the following prerequisites:

Active AWS account AWS Identity and Access Management (IAM) role with administrative permissions

Launch Concierto Insights & Agentic AI Platform in AWS Marketplace Complete the following steps to launch the Concierto Insights & Agentic AI Platform:

Go to the Concierto Insights & Agentic AI Platform in AWS Marketplace. Choose View purchase options. The Subscribe to Concierto Insights & Agentic AI screen is displayed. Choose Subscribe. Once complete the Launch Concierto Insights & Agentic AI screen is displayed. From the Launch Concierto-Enterprise Agentic AI screen, refer to the downloadable deployment guide PDF in the Follow the vendor's instructions In the Deployment template section, choose the CFT_Path link to download the AWS CloudFormation Navigate to AWS CloudFormation console. Create a new stack by uploading the template file (yaml). Specify stack details. Configure stack options. Review and create the stack. CloudFormation automatically executes the stack.

After the CloudFormation stack is complete, you will receive a follow-up email. This email will inform the user that their CloudFormation stack has been provisioned and will provide essential details such as the username, password, and the application URL to access their environment. Log in to Concierto Complete the following steps to log into Concierto Insights:

Visit the URL received in the email after you deployed the CloudFormation stack. Log in using the username and password provided in the email.

Create unstructured data source Complete the following steps to create an unstructured data source:

Choose Configure in the navigation pane and choose Data Sources.

Choose Amazon S3 or SharePoint as your data source (for this post, we use Amazon S3). Enter a data source name, description, and domain. Enter a database name and S3 location. Choose On Demand or Scheduled trigger.

Figure 4: Unstructured data source creation Create structured data source Complete the following steps to create a structured data source:

Choose Configure in the navigation pane and choose Data Sources. Choose your database type, such as PostgreSQL, MySQL, or Snowflake Enter a data source name, description, and domain. Provide connection details, including host, port, database name, schema, user, password, and optional Amazon Resource Name (ARN). Choose On Demand or Scheduled trigger.

Figure 5: Structured data source creation Create data product Complete the following steps to create a data product:

On the Discovery tab, run a SQL query and choose Publish. Review and optionally edit the auto-filled name and description. For your columns, assign business names, toggle cache, and Conversa options. Review the summary of your settings and choose Submit.

Figure 6: Data product creation The data product appears on the Published tab with all attributes. Using the Concierto Insights & Agentic AI Platform In this section, we explore the features of Concierto Insights and provide some examples of how to use it. Published tab The Published tab shows only data products created by the logged-in user.

Figure 7: Published data products Marketplace tab The Marketplace tab displays all data products across Concierto; they are marked as Accessible or Restricted based on IAM role permissions.

Figure 8: Published Marketplace data products Concierto Conversa With Conversa, users can query a data product:

Choose Load DP and choose the data product to load. The "DP Loaded Successfully" message appears, and the product is now visible in the left pane. Pick from query suggestions or enter a custom query. Press Enter or choose Go and turn on Ad-hoc mode as needed.

The results appear with a visualization. You can save or publish the results as insights.

Figure 9: Concierto Insights Conversa SQL notebook The SQL notebook feature supports SQL operations like SELECT, INSERT, UPDATE, and DELETE on data products.

Figure 10: SQL Notebook Python notebook Users can use the Python notebook feature to write and execute Python code for data exploration and visualization.

Figure 11: Python Notebook AI assistant Users can also generate queries using the AI assistant:

Choose the AI assistant icon to open the AI Assistant Choose your domain, subdomain, and data product. Enter a natural language prompt to generate a query. Choose Run to run the query.

You can publish the query results as a data product.

Figure 12: AI Assistant Live Intel: Charts With the Live Intel feature, users can choose a data product and configure chart type, axes, filters, and settings, and then save the chart with a name and description.

Figure 13: Live Intel – Charts Live Intel: Applications The Live Intel feature also provides pre-built applications with multiple charts to monitor and explore key metrics in real time.

Figure 14: Live Intel – Applications Conclusion This post demonstrates how Concierto Insights, available in AWS Marketplace, enhances AWS's comprehensive data services by creating an integrated, business-user-friendly platform. Concierto Insights amplifies AWS capabilities through enterprise governance, conversational AI, and Agentic orchestration that accelerates business outcomes. Data teams can focus on deriving insights rather than tool orchestration, while business users can naturally interact with federated data without deep technical expertise. For retail organizations, Concierto Insights streamlines federated data implementation by connecting diverse data sources like POS, inventory, and eCommerce systems through its federation-first approach and semantic modeling layer. The platform helps retailers drive customer-focused decisions across search, inventory, fulfillment, and planning while benefiting from AWS Marketplace's security, scalability, and cost optimizations. To find out more, visit Concierto Insights & Agentic AI Platform or request a demo. About the Authors

Hamdy Eed Hamdy Eed is a Sr. Partner Solution Architect with AWS and is a member of the Serverless Technical Field Community. With over 20 years of industry experience, he is well-recognized as a Subject Matter Expert in serverless and enterprise integration. Prior to AWS, he worked at NIC Inc. and held several positions at IBM. In his spare time, he coaches soccer to elementary school students and enjoys international travel. You can reach him on LinkedIn.

Karthik Rajendran Karthik Rajendran is a Senior Solutions Architect at AWS with over 25 years of extensive experience in enterprise architecture across diverse sectors including Finance, Retail, Insurance, and Energy. He specializes in architecting and building distributed, hybrid, and cloud-native applications, passionately supporting enterprise customers through their cloud migration journeys. Beyond his professional expertise, Karthik enjoys Milky Way photography and riding motorcycles.

Gaurav Mittal Gaurav Mittal is the Product Head for Concierto at Trianz, overseeing strategy and innovation for the company's cloud modernization platform. With more than 20 years of experience in data, AI, and enterprise platforms, he focuses on architecting scalable, automation-driven solutions for clients worldwide. Prior to Trianz, he led product initiatives across enterprise SaaS organizations. In his spare time, Gaurav enjoys technology blogging and mentoring startup teams. You can reach him on LinkedIn

Ramandeep Singh Ramandeep Singh is a Data & AI Solutions Lead at Trianz, helping enterprises accelerate transformation through cloud, analytics, and AI-driven innovation. With 14 years of experience in IT services and consulting, he leads Presales initiatives, GTM Offerings strategy, and Client workshops. He has co-authored whitepapers on AI and modernization. In his spare time, he follows technology trends and travels to explore new perspectives. You can reach him on LinkedIn

Vendor Insight Report: AWS recognized for AI investment, operational expertise, and customer commitment

Gartner has recently published its "Vendor Insight Report: Amazon Web Services." This comprehensive analysis authored by industry experts Jim Hare, Lydia Leong, Ed Anderson, and Dolores Ianni helps Chief Information Officers (CIOs) evaluate strategic investments in complex IT vendors. The Vendor Insight Report complements other Gartner market and product research with a high-level overview of select vendors and their top strengths and challenges. We are pleased that AWS is recognized in this year's report for our artificial intelligence investments, operational expertise, and customer commitment. Let's explore the key areas that shaped this recognition. Strategic investment in AI infrastructure and services At AWS, we believe continued strategic investment in AI is critical for enabling our customers to pioneer new products and services, transform customer experiences, and scale their businesses. This commitment is exemplified by Amazon's decision to increase capital expenditures to $100 billion in FY2025, the vast majority of which will be spent on building new state-of-the-art AI data centers. In addition, we've continued to innovate with our custom server processors and chips, including our specialized in-house AI accelerators AWS Trainium and AWS Inferentia, which provide customers with more cost effective, higher performing, and more energy-efficient AI infrastructure. Alongside this infrastructure and hardware innovation, we're continuing to expand our core AI service portfolio to offer customers the most comprehensive set of AI services, tools, and resources available in the cloud. Over the past year, we have made more than 100 AI-related product announcements. Among the most notable are Amazon Bedrock AgentCore, a platform that enables customers to build, deploy, and operate highly capable AI agents securely at scale, and Kiro, an agentic IDE that streamlines the developer journey from prototype to production through spec-driven development. We've also launched a dedicated AI agent and tool category in the AWS Marketplace, giving customers access to software with embedded agents, pre-built agents, development solutions, and professional services from AWS Partners. The Amazon Nova family of foundation models deliver frontier intelligence with industry-leading price performance, and Amazon Bedrock Guardrails provide organizations with integrated safety and privacy safeguards that work across multiple foundation models while maintaining compliance and responsible AI policies. As Amazon CEO Andy Jassy said in his 2024 Letter to Shareholders, "we continue to believe AI is a once-in-a-lifetime reinvention of everything we know, the demand is unlike anything we've seen before, and our customers, shareholders, and business will be well-served by our investing aggressively now." Additional funding for customer AI initiatives We believe in investing in our customers' success as well. When we launched the AWS Generative AI Innovation Center in 2023, we had one clear goal: help customers turn AI potential into real business value. Through this initiative, we provide dedicated teams of AWS developers, solution architects, and vertical industry experts to work side-by-side with AWS customers to build new AI applications customized for their businesses. The results are noteworthy: thousands of customers have already deployed solutions that have driven millions of dollars in productivity gains and transformed customer experiences. Now, as AI evolves toward more autonomous, agentic systems, we're strengthening our commitment with an additional $100 million investment in the program to help customers pioneer this next wave of AI innovation. Sovereign-by-design When we speak to public sector and regulated industry customers in Europe, they consistently highlight a common challenge: balancing cloud adoption with stringent regulatory requirements for data residency and operational independence. Many feel forced to choose between AWS's full capabilities and feature-limited sovereign cloud solutions. To better serve these customers, we've built the AWS European Sovereign Cloud, a new, independent cloud infrastructure that will operate with an independent governance structure, a dedicated Security Operations Center, and under a newly established parent company, and will be locally controlled in the EU, led by EU citizens, and subject to local laws. Customers will benefit from the full power of AWS—including our familiar architecture, expansive service portfolio, and APIs—while maintaining strict operational independence and complete control over data residency. Launching in 2025, the AWS European Sovereign Cloud will be the only fully-featured, independently operated sovereign cloud in the industry, backed by strong technical controls, sovereign assurances, and legal protections. Choice and flexibility Customers need the freedom to innovate and don't want to be restricted by limited technology options. AWS provides this freedom through the industry's broadest and deepest set of cloud capabilities, enabling customers to build optimized solutions that balance performance and cost-effectiveness. Our comprehensive portfolio of 240+ services spans core infrastructure services (compute, storage, and databases) to emerging technologies (artificial intelligence, machine learning, quantum computing, and IoT), complemented by 40+ purpose-built industry services and over 450 industry-specific solutions in the AWS Solutions Library. We understand that not every customer requires the same level of flexibility in their solutions. For those seeking simplicity, Amazon Lightsail provides a streamlined way to host and manage cloud-based applications, offering pre-configured Linux and Windows application and development stacks (including WordPress, Windows Server, Plesk, LAMP, and Nginx). These come with essential features like automated database management, simplified load balancers, and easy-to-use firewalls, with the option to integrate over 90 AWS services as needs evolve. Additionally, our robust suite of low-code and no-code solutions, including tools like Amazon SageMaker Canvas for AI/ML tasks, AWS Amplify for web development, and AWS Infrastructure Composer for visual application development, enable users of all technical levels to leverage sophisticated cloud services effectively. Performance, scalability, and reliability A common pain point we hear from customers revolves around infrastructure provisioning and management, particularly when it comes to scaling applications internationally. Luckily, our vast global network of data centers and our diverse infrastructure solutions provide customers with best-in-class infrastructure, designed to deliver the highest performance, scalability, and reliability for applications whether they are running in the cloud, on-premises, or at the edge. A key differentiator is our Region architecture: Every AWS Region consists of at least three independent, physically distinct Availability Zones (AZs) that are separated by meaningful distance, each with isolated and redundant power infrastructure and connectivity, ensuring that mission critical applications and workloads continue operating even during equipment failures, natural disasters, or human-caused disruptions. This resilient infrastructure is further strengthened by the AWS Global Network, which spans over 9 million kilometers of terrestrial and subsea fiber-optic cabling, delivering ultra-low latency and superior data throughput while maintaining industry-leading reliability. End-to-end security Lastly, customers are looking for ways to move security from something that is tacked on at the end of a process, slowing innovation, to something that is automated and integral to everything they do. At AWS, security is our top priority, embedded in our organizational structure, and built into every product roadmap, engineering plan, and weekly stand-up meeting. This commitment has resulted in what we believe is the most secure global cloud infrastructure on which to build, migrate, and manage applications and workloads. Our comprehensive security approach is backed by over 300+ cloud security tools and validated by millions of customers, including those in the most security-sensitive industries like government, healthcare, and financial services. Looking forward A new era of IT is unfolding before us, characterized by the intelligent automation of routine, day-to-day tasks enabling teams to focus on innovation, revenue generation, and customer satisfaction, and new agentic systems that can act autonomously across digital boundaries while maintaining the security, reliability, and governance standards required for enterprise deployment. AWS remains at the forefront of this revolution, investing heavily in technologies and services that enable customers to capitalize on this paradigm shift. Our commitment remains unwavering: to deliver the most comprehensive and innovative cloud platform in the industry while helping customers transform their businesses. To learn more about how AWS can help your organization innovate with confidence, contact your AWS account team or visit our website. For detailed insights, access the complete 2025 Gartner AWS Vendor Insights report. Gartner does not endorse any vendor, product or service depicted in its research publications and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose. GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved.

Christopher Swenson Christopher Swenson is a Principal Product Marketing and Messaging Manager at Amazon Web Services based in Seattle, WA, where he helps drive messaging strategy and execution.

By: David MacDonald, Solutions Architect – AWS By: Justin Soong, Founder & CEO – Authsignal By: Ashutosh Bhadauriya, Developer Experience Engineer – Authsignal

Authentication needs to be both secure and user-friendly. Users expect smooth, seamless experiences while businesses require strong security controls to protect accounts and transactions from fraud. In this post, we'll explore how Amazon Cognito and Authsignal work together to create great authentication experiences that adapt to user context and business needs. Learn how non-technical business users can configure and optimize authentication flows throughout the user journey to manage both fraud and user experience. Choosing the right authentication approach Authentication can rely on three fundamental factors:

Knowledge: Something you know (such as a password) Possession: Something you have (such as a security token) Inherence: Something you are (such as a fingerprint)

Combining these factors enables multi-factor authentication (MFA), which helps mitigate threats such as credential stuffing. Common implementations pair knowledge factors (passwords) with possession factors (one-time passwords (OTPs)). When selecting authentication methods, consider:

Assurance: how difficult it is for someone other than the legitimate user to use or copy the authentication method Friction: how much effort users need to spend to complete the authentication process Cost: initial setup costs, ongoing operational expenses (such as SMS message fees), and maintenance requirements Availability: whether users have reliable access to the authentication method across different devices and locations

Your business needs to balance authentication methods against both risk tolerance and business needs. For example, high-value financial transactions or sensitive data access might warrant high-assurance authentication methods such as hardware security keys despite their increased friction and cost. However, lower-risk activities like viewing public account information might only require basic authentication methods to maintain user convenience. Businesses also need to adapt as authentication method effectiveness evolves with technology and threats. For example, passkeys use public-key cryptography to combine possession (device bound) with inherence (biometrics), enabling low-friction passwordless authentication. Meanwhile, traditional methods like Short Message Service (SMS) OTP have become less secure as interception techniques advance. Businesses can maintain strong security while minimizing friction through adaptive authentication. This approach adjusts authentication requirements based on risk signals such as:

Device characteristics: new or unfamiliar devices, browser fingerprints, and operating system integrity Location patterns: unexpected countries, unusual locations for the user, and IP reputation Behavioral patterns: sign-in velocity, unusual times of day, and abnormal navigation patterns Transaction patterns: amount thresholds, new recipients, and unusual merchant categories Session attributes: connection type, virtual private network (VPN) or proxy usage, and session duration and activity Account history: previous suspicious activity, account age, and historical transaction patterns

For example, sign-ins from new devices in unexpected locations might trigger additional steps such as a push notification for authentication through your mobile application, while routine access patterns follow a streamlined flow. However, authentication can extend beyond the initial sign-in step. Applications can benefit from a more dynamic approach to security. The right authentication method at the right time Single sign-in authentication requires both high-assurance authentication methods and robust session management to protect against threat actors who might attempt to intercept authenticated sessions. However, forcing users to complete high-assurance authentication steps every time they sign in creates unnecessary friction—imagine requiring a hardware security key just to check your account balance. A more effective approach implements contextual authentication throughout the user journey, balancing security and user experience based on the sensitivity of each action. This lets users start with streamlined authentication for basic access, then complete additional authentication only when attempting higher-risk actions such as large payments or changing security settings. You can enhance this approach by incorporating risk signals and transaction information to determine the most appropriate authentication method for each user action. High-value actions combined with suspicious signals might require stronger authentication methods. For example, when a user attempts a bank transfer over $1,000 from a new country, the application requires hardware token authentication before proceeding. By combining adaptive and continuous authentication, businesses can implement low-friction sign-in processes when risk signals permit, while enforcing additional authentication requirements for subsequent high-value actions based on both signals and business risk tolerance. But how can you implement adaptive and continuous authentication in your application to create great authentication experiences? Let's explore how to architect a solution to this challenge. Architecting for adaptive and continuous authentication A robust adaptive authentication system combines user actions, risk signals, and business rules to determine appropriate authentication requirements at the right time in the user journey. Figure 1 illustrates this architecture, where an authentication rules engine processes user actions (like sign-in attempts and transactions), risk signals, and transaction data to determine the necessary authentication methods. Key authentication decision points might include:

Initial sign-in attempts High-value transactions Security setting changes Access to sensitive data Account recovery processes

Figure 1: User actions, including sign-in, are combined with signals to trigger a rules engine to decide on the right authentication method before allowing the user to proceed For this architecture to effectively serve your business needs, it should include the following characteristics:

Business-friendly configuration: The rules engine must be configurable by business users who understand risk tolerance and user impact, not only developers. While developers implement the initial integration, business users should be able to adjust authentication rules without code changes. For example, a fraud analyst should be able to lower transaction thresholds or strengthen authentication requirements for specific regions in response to emerging threats. Extensibility: The system must evolve alongside both emerging threats and authentication technologies. This means building a flexible architecture that can incorporate new signal sources ranging from basic IP address validation to sophisticated device fingerprinting and behavioral analysis. As authentication technology advances, your system should readily adopt new methods like WhatsApp OTP without major architectural changes. Finally, it must support custom business logic and risk models that reflect your organization's unique security requirements and risk tolerance. Observability: Effective authentication management requires clear visibility into system behavior and outcomes. Business users need detailed insights into authentication patterns across different methods, helping them understand how users interact with security measures. They should be able to analyze which rules are triggering and how effectively they mitigate risks. Understanding user friction points and drop-off rates helps optimize the balance between security and usability. Additionally, comprehensive monitoring helps identify emerging fraud patterns and anomalies, enabling proactive security adjustments before they impact your business significantly.

Now that you know the required characteristics, how can you build an adaptive and continuous authentication system on AWS? Better together: Amazon Cognito and Authsignal Authsignal builds on the Amazon Cognito customer identity and access management (CIAM) user pool service by providing its own suite of authentication methods. Amazon Cognito sends authentication requests to Authsignal's rules engine during sign-in, where the engine selects which Authsignal authentication method to present. As shown in Figure 2, your application also sends ongoing user actions, transaction data, and user signals to Authsignal's rules engine. The engine makes authentication decisions based on where the user is in their journey, enabling adaptive and continuous authentication. This integration creates a system that balances security with user experience.

Figure 2: Authsignal integrates with Amazon Cognito to drive authentication at all stages of the user journey The suite of authentication methods offered by Authsignal gives additional options for adjusting the balance between assurance, friction, cost, and availability. Methods like WhatsApp OTP offer a cost-effective alternative to SMS authentication while also reducing interception threats such as SIM swapping. High assurance biometric verification incorporating liveness detection gives businesses options to mitigate evolving deepfake threats. You can integrate Authsignal with Amazon Cognito at any point — whether you're starting with a new Amazon Cognito user pool or extending your existing one. For adaptive authentication during sign-in, you can continue to use Amazon Cognito Adaptive Authentication, so that you can configure your user pool to block suspicious sign-ins or add second-factor authentication based on risk levels. Alternatively, you can use Authsignal's Amazon Cognito integration to select from Authsignal's suite of authentication methods based on user signals and your own business rules. You can then integrate with Authsignal's pre-built UI to progressively enhance security by adding continuous authentication for specific high-risk actions after sign-in. This flexible approach preserves your existing Amazon Cognito implementation while strengthening authentication controls where they matter most. After being integrated, Authsignal's no-code rules and policy engine can be used to define conditional logic based on signals and transaction data that drives the right level of authentication for a given action. This gives your non-technical business users the ability to update thresholds and mitigate risks without requiring developers to update code. And with a unified user timeline that displays all authentication activities and outcomes, your users can analyze authentication patterns and optimize policies based on real-world usage data. Conclusion Creating great authentication experiences requires balancing security and user experience throughout the user journey. By combining the robust CIAM capabilities provided by Amazon Cognito with Authsignal's authentication service, businesses can implement continuous and adaptive authentication that evolves with their needs. Ready to enhance your authentication experience? Get started with Authsignal in AWS Marketplace today. .

Authsignal – AWS Partner Spotlight Authsignal provides drop-in passwordless authentication, passkeys, biometrics, and adaptive MFA without the complexity of migrations, development overhead, and operational costs of building from scratch. Contact Authsignal | Partner Overview | AWS Marketplace

TAGS: Amazon Cognito, AWS Partner Network, Best Practices, Foundational (100), Partner solutions, Thought Leadership

Democratizing AI: How Thomson Reuters Open Arena supports no-code AI for every professional with Amazon Bedrock

This post is cowritten by Laura Skylaki, Vaibhav Goswami, Ramdev Wudali and Sahar El Khoury from Thomson Reuters. Thomson Reuters (TR) is a leading AI and technology company dedicated to delivering trusted content and workflow automation solutions. With over 150 years of expertise, TR provides essential solutions across legal, tax, accounting, risk, trade, and media sectors in a fast-evolving world. TR recognized early that AI adoption would fundamentally transform professional work. According to TR's 2025 Future of Professionals Report, 80% of professionals anticipate AI significantly impacting their work within five years, with projected productivity gains of up to 12 hours per week by 2029. To unlock this immense potential, TR needed a solution to democratize AI creation across its organization. In this blog post, we explore how TR addressed key business use cases with Open Arena, a highly scalable and flexible no-code AI solution powered by Amazon Bedrock and other AWS services such as Amazon OpenSearch Service, Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB, and AWS Lambda. We'll explain how TR used AWS services to build this solution, including how the architecture was designed, the use cases it solves, and the business profiles that use it. The system demonstrates TR's successful approach of using existing TR services for rapid launches while supporting thousands of users, showcasing how organizations can democratize AI access and support business profiles (for example, AI explorers and SMEs) to create applications without coding expertise. Introducing Open Arena: No-code AI for all TR introduced Open Arena to non-technical professionals to create their own customized AI solutions. With Open Arena users can use cutting-edge AI powered by Amazon Bedrock in a no-code environment, exemplifying TR's commitment to democratizing AI access. Today, Open Arena supports:

High adoption: ~70% employee adoption, with 19,000 monthly active users. Custom solutions: Thousands of customized AI solutions created without coding, used for internal workflows or integrated into TR products for customers. Self-served functionality: 100% self-served functionality, so that users, irrespective of technical background, can develop, evaluate, and deploy generative AI solutions.

The Open Arena journey: From prototype to enterprise solution Conceived as a rapid prototype, Open Arena was developed in under six weeks at the onset of the generative AI boom in early 2023 by TR Labs – TR's dedicated applied research division focused on the research, development, and application of AI and emerging trends in technologies. The goal was to support internal team exploration of large language models (LLMs) and discover unique use cases by merging LLM capabilities with TR company data. Open Arena's introduction significantly increased AI awareness, fostered developer-SME collaboration for groundbreaking concepts, and accelerated AI capability development for TR products. The rapid success and demand for new features quickly highlighted Open Arena's potential for AI democratization, so TR developed an enterprise version of Open Arena. Built on the TR AI Platform, Open Arena enterprise version offers secure, scalable, and standardized services covering the entire AI development lifecycle, significantly accelerating time to production. The Open Arena enterprise version uses existing system capabilities for enhanced data access controls, standardized service access, and compliance with TR's governance and ethical standards. This version introduced self-served capabilities so that every user, irrespective of their technical ability, can create, evaluate, and deploy customized AI solutions in a no-code environment.

"The foundation of the AI Platform has always been about empowerment; in the early days it was about empowering Data Scientists but with the rise of Gen AI, the platform adapted and evolved on empowering users of any background to leverage and create AI Solutions." – Maria Apazoglou, Head of AI Engineering, CoCounsel

As of July 2025, the TR Enterprise AI Platform consists of 15 services spanning the entire AI development lifecycle and user personas. Open Arena remains one of its most popular, serving 19,000 users each month, with increasing monthly usage. Addressing key enterprise AI challenges across user types Using the TR Enterprise AI Platform, Open Arena helped thousands of professionals transition into using generative AI. AI-powered innovation is now readily in the hands of everyone, not just AI scientists. Open Arena successfully addresses four critical enterprise AI challenges:

Enablement: Delivers AI solution building with consistent LLM and service provider experience and support for various user personas, including non-technical. Security and quality: Streamlines AI solution quality tracking using evaluation and monitoring services, whilst complying with data governance and ethics policies. Speed and reusability: Automates workflows and uses existing AI solutions and prompts. Resources and cost management: Tracks and displays generative AI solution resource consumption, supporting transparency and efficiency.

The solution currently supports several AI experiences, including tech support, content creation, coding assistance, data extraction and analysis, proof reading, project management, content summarization, personal development, translation, and problem solving, catering to different user needs across the organization.

Figure 1. Examples of Open Arena use cases. AI explorers use Open Arena to speed up day-to-day tasks, such as summarizing documents, engaging in LLM chat, building custom workflows, and comparing AI models. AI creators and Subject Matter Experts (SMEs) use Open Arena to build custom AI workflows and experiences and to evaluate solutions without requiring coding knowledge. Meanwhile, developers can develop and deploy new AI solutions at speed, training models, creating new AI skills, and deploying AI capabilities. Why Thomson Reuters selected AWS for Open Arena TR strategically chose AWS as a primary cloud provider for Open Arena based on several critical factors:

Comprehensive AI/ML capabilities: Amazon Bedrock offers easy access to a choice of high-performing foundation models from leading AI companies like AI21 Labs, Anthropic, Cohere, DeepSeek, Luma AI, Meta, Mistral AI, OpenAI, Qwen, Stability AI, TwelveLabs, Writer, and Amazon. It supports simple chat and complex RAG workflows, and integrates seamlessly with TR's existing Enterprise AI Platform. Enterprise-grade security and governance: Advanced security controls, model access using RBAC, data handling with enhanced security features, single sign-on (SSO) enabled, and clear operational and user data separation across AWS accounts. Scalable infrastructure: Serverless architecture for automatic scaling, pay-per-use pricing for cost optimization, and global availability with low latency. Existing relationship and expertise: Strong, established relationship between TR and AWS, existing Enterprise AI Platform on AWS, and deep AWS expertise within TR's technical teams.

"Our long-standing partnership with AWS and their robust, flexible and innovative services made them the natural choice to power Open Arena and accelerate our AI initiatives." – Maria Apazoglou, Head of AI Engineering, CoCounsel

Open Arena architecture: Scalability, extensibility, and security Designed for a broad enterprise audience, Open Arena prioritizes scalability, extensibility and security while maintaining simplicity for non-technical users to create and deploy AI solutions. The following diagram illustrates the architecture of Open Arena.

Figure 2. Architecture design of Open Arena. The architecture design facilitates enterprise-grade performance with clear separation between capability and usage, aligning with TR's enterprise cost and usage tracking requirements. The following are key components of the solution architecture:

No-code interface: Intuitive UI, visual workflow builder, pre-built templates, drag-and-drop functionality. Enterprise integration: Seamless integration with TR's Enterprise AI Platform, SSO enabled, data handling with enhanced security, clear data separation. Solution management: Searchable repository, public/private sharing, version control, usage analytics.

TR developed Open Arena using AWS services such as Amazon Bedrock, Amazon OpenSearch, Amazon DynamoDB, Amazon API Gateway, AWS Lambda, and AWS Step Functions. It uses Amazon Bedrock for foundational model interactions, supporting simple chat and complex Retrieval-Augmented Generation (RAG) tasks. Open Arena uses Amazon Bedrock Flows as the custom workflow builder where users can drag-and-drop components like prompts, agents, knowledge bases and Lambda functions to create sophisticated AI workflows without coding. The system also integrates with AWS OpenSearch for knowledge bases and external APIs for advanced agent capabilities. For data separation, orchestration is managed using the Enterprise AI Platform AWS account, capturing operational data. Flow instances and user-specific data reside in the user's dedicated AWS account, stored in a database. Each user's data and workflow executions are isolated within their respective AWS accounts, which is required for complying with Thomson Reuters data sovereignty and enterprise security policies with strict regional controls. The system integrates with Thomson Reuters SSO solution to automatically identify users and grant secure, private access to foundational models. The orchestration layer, centrally hosted within the Enterprise AI Platform AWS account, manages AI workflow activities, including scheduling, deployment, resource provisioning, and governance across user environments. The system features fully automated provisioning of Amazon Bedrock Flows directly within each user's AWS account, avoiding manual setup and accelerating time to value. Using AWS Lambda for serverless compute and DynamoDB for scalable, low-latency storage, the system dynamically allocates resources based on real-time demand. This architecture makes sure prompt flows and supporting infrastructure are deployed and scaled to match workload fluctuations, optimizing performance, cost, and user experience.

"Our decision to adopt a cross-account architecture was driven by a commitment to enterprise security and operational excellence. By isolating orchestration from execution, we make sure that each user's data remains private and secure within their own AWS account, while still delivering a seamless, centrally-managed experience. This design empowers organizations to innovate rapidly without compromising compliance or control." – Thomson Reuters' architecture team

Evolution of Open Arena: From classic to Amazon Bedrock Flows-powered chain builder Open Arena has evolved to cater to varying levels of user sophistication:

Open Arena v1 (Classic): Features a form-based interface for simple prompt customization and basic AI workflow deployment within a single AWS account. Its simplicity appeals to novice users for straightforward use cases, though with limited advanced capabilities. Open Arena v2 (Chain Builder): Introduces a robust, visual workflow builder interface, enabling users to design complex, multi-step AI workflows using drag-and-drop components. With support for advanced node types, parallel execution, and seamless cross-account deployment, Chain Builder dramatically expands the system's capabilities and accessibility for non-technical users.

Thomson Reuters uses Amazon Bedrock Flows as a core feature of Chain Builder. Users can define, customize, and deploy AI-driven workflows using Amazon Bedrock models. Bedrock Flows supports advanced workflows combining multiple prompt nodes, incorporating AWS Lambda functions, and supporting sophisticated RAG pipelines. Operating seamlessly across user AWS accounts, Bedrock Flows facilitates secure, scalable execution of personalized AI solutions, serving as the fundamental engine for the Chain Builder workflows and driving TR's ability to deliver robust, enterprise-grade automation and innovation. What's next? TR continues to expand Open Arena's capabilities through the strategic partnership with AWS, focusing on:

Driving further adoption of Open Arena's DIY capabilities. Enhancing flexibility for workflow creation in Chain Builder with custom components, such as inline scripts. Developing new templates to represent common tasks and workflows. Enhancing collaboration features within Open Arena. Extending multimodal capabilities and model integration. Expanding into new use cases across the enterprise.

"From innovating new product ideas to reimagining daily tasks for Thomson Reuters employees, we continue to push the boundaries of what's possible with Open Arena." – Maria Apazoglou, Head of AI Engineering, CoCounsel

Conclusion In this blog post, we explored how Thomson Reuters' Open Arena demonstrates the successful democratization of AI across an enterprise by using AWS services, particularly Amazon Bedrock and Bedrock Flows. With 19,000 monthly active users and 70% employee adoption, the system proves that no-code AI solutions can deliver enterprise-scale impact while maintaining security and governance standards. By combining the robust infrastructure of AWS with innovative architecture design, TR has created a blueprint for AI democratization that empowers professionals across technical skill levels to harness generative AI for their daily work. As Open Arena continues to evolve, it exemplifies how strategic cloud partnerships can accelerate AI adoption and transform how organizations approach innovation with generative AI.

About the authors Laura Skylaki, PhD, leads the Enterprise AI Platform at Thomson Reuters, driving the development of GenAI services that accelerate the creation, testing and deployment of AI solutions, enhancing product value. A recognized expert with a doctorate in stem cell bioinformatics, her extensive experience in AI research and practical application spans legal, tax, and biotech domains. Her machine learning work is published in leading academic journals, and she is a frequent speaker on AI and machine learning Vaibhav Goswami is a Lead Software Engineer on the AI Platform team at Thomson Reuters, where he leads the development of the Generative AI Platform that empowers users to build and deploy generative AI solutions at scale. With expertise in building production-grade AI systems, he focuses on creating tools and infrastructure that democratize access to cutting-edge AI capabilities across the enterprise. Ramdev Wudali is a Distinguished Engineer, helping architect and build the AI/ML Platform to enable the Enterprise user, data scientists and researchers to develop Generative AI and machine learning solutions by democratizing access to tools and LLMs. In his spare time, he loves to fold paper to create origami tessellations, and wearing irreverent T-shirts As the director of AI Platform Adoption and Training, Sahar El Khoury guides users to seamlessly onboard and successfully use the platform services, drawing on her experience in AI and data analysis across robotics (PhD), financial markets, and media. Vu San Ha Huynh is a Solutions Architect at AWS with a PhD in Computer Science. He helps large Enterprise customers drive innovation across different domains with a focus on AI/ML and Generative AI solutions. Paul Wright is a Senior Technical Account Manager, with over 20 years experience in the IT industry and over 7 years of dedicated cloud focus. Paul has helped some of the largest enterprise customers grow their business and improve their operational excellence. In his spare time Paul is a huge football and NFL fan. Mike Bezak is a Senior Technical Account Manager in AWS Enterprise Support. He has over 20 years of experience in information technology, primarily disaster recovery and systems administration. Mike's current focus is helping customers streamline and optimize their AWS Cloud journey. Outside of AWS, Mike enjoys spending time with family & friends.

IPv4 address exhaustion is a challenge in modern networking, as most IPv4 addresses have been depleted with the growth of the internet. Previously, AWS Lambda only supported inbound and outbound connectivity over IPv4, but it has since introduced support for dual-stack endpoints, so that you can transition from IPv4 to IPv6. AWS continues to add support for IPv6, recently announcing support for inbound IPv6 connectivity over AWS PrivateLink, and dual-stack endpoint support for Amazon API Gateway. With these IPv6 capabilities now available in Lambda, you should understand how to use them effectively. This post examines the benefits of transitioning Lambda functions to IPv6, provides practical guidance for implementing dual-stack support in your Lambda environment, and considerations for maintaining compatibility with existing systems during migration. Benefits of transitioning You can transition to IPv6 to future-proof your overall architecture by preparing ahead of the broader transition to IPv6, and establish compatibility with IPv6 clients or services. IPv6 also eliminates the need for a NAT gateway when the Lambda functions need internet connectivity from a private subnet in your Amazon Virtual Private Cloud (Amazon VPC). Lambda functions can direct traffic to the egress-only internet gateway, potentially eliminating the NAT gateway and its associated charges and streamlining network design. This transition provides cost savings, as egress-only internet gateways are free to use, as opposed to NAT gateways that incurs an hourly charge. Furthermore, IPv6 offers improved network efficiency by eliminating NAT translation overhead, so that Lambda functions can establish direct connections with clients. IPv6 also has more advantages such as native Quality of Service (QoS), which streamlines header structure and reduces packet fragmentations. Architectural implications Lambda functions are often deployed inside of a VPC to access VPC resources. For VPC Lambda functions to access the internet, routing traffic through an NAT gateway is a common approach. For Lambda functions with IPv6 support, Lambda functions can now route traffic directly through the egress-only internet gateway, which eliminates the need for a NAT gateway and the extra hop, as shown in the following figures.

Figure 1. Lambda internet connectivity through a NAT Gateway (IPv4) and Lambda internet connectivity through an egress-only internet gateway (IPv6).

Once the egress-only internet gateway is in place, you need to update the route table to reflect this. If you have used 0.0.0.0/0 as the default route for IPv4 traffic, you should add ::/0 as the default route for IPv6 traffic. The following image shows the updated route table.

Figure 2. Lambda private subnet routing tables for an NAT Gateway (IPv4) as opposed to a dual-stack including an egress-only internet gateway (IPv6)

If you are using Lambda function URLs, no transition is needed. Lambda function URLs are inherently IPv6-capable and can be accessed by IPv6 clients without needing architectural changes or modifications. This IPv6 compatibility for function URLs operates independently of your Lambda function's VPC configuration, and clients can reach your Lambda function URLs over IPv6 even when dual-stack is not enabled in your VPC. For Lambda functions that interact exclusively with AWS services through internal traffic, IPv6 offers limited benefits. For example, in an architecture where a Lambda function processes requests from Amazon API Gateway and queries a database hosted on Amazon Relational Database Service (Amazon RDS), no architectural change is expected. Internal traffic routes using the RDS cluster endpoint and Lambda Amazon Resource Name (ARN), not IP addresses, as shown in the following figure.

Figure 3. A common architecture pattern where Lambda processes events from API Gateway and reads/writes to Amazon RDS. You reference the Lambda function ARN and RDS cluster endpoint instead of IPv4/IPv6 addresses.

Transitioning from IPv4 to IPv6 By default, Lambda functions communicate over IPv4 to their destinations. For Lambda functions to communicate with IPv6 destinations, dual-stack VPC configuration is needed. This allows Lambda functions to communicate over both IPv4 and IPv6. If your VPC does not have IPv6 support, then you need to first add IPv6 support for your VPC. You need to follow these steps to enable IPv6 traffic for a Lambda function:

Assign IPv6 block to VPC: You need to edit the existing VPC CIDRs to add an IPv6 CIDR block. If you select the option of Amazon-provided IPv6 CIDR block, then you are assigned a /56 IPv6 CIDR block from the Amazon pool of IPv6 addresses. You also have the option to assign an Amazon VPC IP Address Manager allocated or your own IPv6 CIDR block. Assign IPv6 block to Subnets: After assigning an IPv6 CIDR block to the VPC, you must manually configure IPv6 CIDR blocks for each existing subnet, with each subnet receiving a portion of the VPC's IPv6 address space. Update route tables: For your Lambda function's IPv6 traffic to reach the internet, you need to add a route (::/0) to the egress-only internet gateway. Update security groups: By default, security groups allow all outbound traffic. To restrict outbound IPv6 traffic from your Lambda function, you must remove the default egress rule and add specific restrictive outbound rules. For inbound traffic, security group rules are needed when your Lambda function receives direct network connections, such as traffic through AWS PrivateLink connections. Enable IPv6 dual-stack on the Lambda function: When you assign IPv6 addresses for your Lambda function's subnet, you can enable IPv6 dual-stack for the Lambda function. Then, Lambda creates new Elastic network interfaces (ENI) with IPv4 and IPv6 protocols with both IPv4 and IPv6 addresses. Although most updates to the Lambda function have zero downtime, enabling dual-stack may cause disruption in connectivity. To prevent downtime during the transition, we recommend using Lambda versions and aliases to implement a blue/green deployment strategy. You can publish your IPv6-enabled Lambda function as a new version while keeping the current version active and serve traffic through the alias. After testing the new IPv6 version, you can update the alias to switch the traffic. This approach provides a rollback capability, and you can revert the alias to point back to the previous version if needed.

When you have completed these steps, your Lambda function can support dual-stack networking and communicate over both IPv4 and IPv6. Conclusion In this post, we covered the benefits of transitioning your AWS Lambda functions from IPv4 to IPv6, the architectural implications, and steps for how you could make the transition.We recommend transitioning your Lambda functions to support both IPv4 and IPv6 traffic to gain its benefits. The Lambda IPv6 support helps address IPv4 exhaustion while providing cost savings and network clarification. Once organizations transition to supporting only IPv6 traffic, they can eliminate NAT gateways for Lambda functions needing internet access, thus reducing both costs and architectural complexity. As AWS expands IPv6 support across services, transitioning Lambda functions to dual-stack networking positions organizations for long-term compatibility while delivering immediate operational benefits. For more information on how to enable IPv6 access for Lambda functions in dual-stack VPC, see the Lambda documentation. For more serverless learning resources, visit Serverless Land.

AWS Partners Demonstrate Enterprise AI Agent Solutions with Amazon Bedrock AgentCore

By: Veena Vasudevan, Principal Partner Solutions Architect, Data & AI – AWS by: Salman Taherian, Senior Partner Development Specialist, Data & AI – AWS Following the general availability of Amazon Bedrock AgentCore on October 13, 2025, AWS Partners are seizing first-mover advantage in the enterprise AI agent market, rapidly turning proof-of-concept innovations into production solutions. AgentCore provides the foundation to build, deploy, and operate agents using any framework or model without infrastructure management by offering services for Runtime, Memory, Gateway, Identity, and Observability.

Amazon Bedrock AgentCore is driving unprecedented enterprise transformation. These select success stories showcase how early-adopter partners are delivering immense value for their customers today. Caylent Optimizes CloudZero Advisor CloudZero provides a FinOps solution that enables enterprises to understand and optimize cloud costs in real time by connecting engineering decisions directly to business outcomes. As more companies onboarded to their platform, CloudZero faced growing challenges scaling their AI-driven cloud cost optimization and infrastructure analysis capabilities. Operations were increasingly slowed by complexity and high developer cognitive load required to manage and tune custom agentic AI workflows processing petabytes of cost and usage data.

Caylent implemented CloudZero Advisor, an agentic platform built on AgentCore Runtime that orchestrates five specialized agents—Cost/Billing, Cloud Pricing, Benchmarking, CostFormation, and Knowledge Base—to deliver comprehensive cloud cost analysis through natural language interactions. AgentCore Memory manages context across cost analysis sessions, while AgentCore Gateway services enable seamless integration with cloud resources. By migrating from Amazon Bedrock Agents to AgentCore, CloudZero removed the complexity of maintaining custom memory systems and achieved 5x faster response times, delivering 2–4s Time to First Token (TTFT) per step versus 30+s previously.

"AgentCore enabled us to build CloudZero Advisor into a production-ready, agentic platform that delivers real speed and efficiency. The performance gains we're seeing turn agentic AI from a proof of concept into a core part of CloudZero's data platform, unlocking use cases that were previously too slow or costly and scaling seamlessly to meet 50x growth," said Randall Hunt, CTO, Caylent.

The implementation reduced developer cognitive load by 75% through automated memory management and standardized CI/CD pipelines that simplified code maintenance. DeepEval validation and built-in failover mechanisms strengthened system resilience, enabling seamless multi-agent collaboration with secure multi-tenant isolation for complex cloud optimization workloads. Cisco Brings Zero Trust Security to AI Agents for Enterprise Customers Traditional identity and security systems weren't designed for AI agents, creating governance challenges as enterprises adopt agentic AI. Organizations need visibility over agent-tool communications and data exfiltration prevention.

Cisco partners with AWS, integrating Cisco Duo and Cisco Secure Access with AgentCore to build comprehensive zero trust architectures for agentic AI. The solution combines Cisco Duo with AgentCore Identity to create an identity fabric with granular access policies and just in time access for human and non-human identities, including machine and agentic identities. Cisco Secure Access's MCP semantic inspection proxy, integrated with AgentCore Gateway, provides a unique capability—the ability to intercept and inspect all MCP messages between AI agents and tools, detecting threats such as tool poisoning and prompt injections while preventing data exfiltration through DLP controls.

The integration extends zero trust principles to AI agents with the same rigorous policies as humans. AgentCore Gateway facilitates secure discovery and access to MCP servers and tools with zero trust policies and user consent. Cisco Duo integration provides strong authentication and adaptive access controls for both human users and AI agents. Organizations monitor MCP communications, view threats detected and blocked and enforce policies through Cisco Secure Access's dashboard. AgentCore Observability tracks agent behavior and detects anomalies.

"Cisco is teaming up with AWS AgentCore and creating secure identities for AI agents", said Pradeep Musugu, Sr. Director, IT Security, Cisco. "We need clear rules to establish and manage access from human and non-human identities, and constant inspection of the meaning/intent of each MCP message. The combined approach of extending Duo's identity provider capabilities and Secure Access' MCP message security into AWS AgentCore will allow us to build and use AI confidently, keeping both our AI agents, team members, and data secure as we grow with AI."

Customers can leverage Cisco Duo IAM to integrate with AWS AgentCore for inbound and outbound authentication today. Cisco's MCP semantic inspection proxy is in production with selected customers, with broader integration planned for early 2026. This enables organizations to build and scale AI agent deployments with strict security and compliance. Genpact Enhances Financial Crime Detection at Scale for Apex Fintech Solutions Apex Fintech Solutions needed to transform financial crime investigation workflows across their digital clearing and custody platform. Their existing process struggled with siloed reporting structures, making it difficult to analyze patterns across accounts, entities, and transactions—limiting their ability to detect and prevent financial crimes while maintaining regulatory compliance.

Genpact implemented their riskCanvas® Data Explorer solution with a sophisticated agent-to-agent communication system built on AgentCore Runtime and leveraging AgentCore Observability. The implementation features a Supervisor Agent pattern coordinating specialized agents for accounts, entities, items, and transactions analysis. This agent intelligently translates domain-specific questions into data query and analysis tasks routed to the appropriate specialist agent. AgentCore Memory maintains context across complex investigations, transforming previously disconnected reporting structures into an integrated intelligence platform that delivers insights through natural language queries.

"Moving from efficiency gains to enterprise value requires a holistic approach to agent orchestration and lifecycle management – and that's exactly what we did for Apex leveraging Genpact's Agentic Development Lifecycle with Amazon Bedrock AgentCore to deliver a riskCanvas® Data Explorer roll-out that helped them improve financial crime detection and compliance", said Matt Arellano, AWS Practice Lead at Genpact.

The solution enables financial crime analysts to process daily trades across millions of brokerage accounts through natural language interactions. Following successful deployment, Apex plans to expand AgentCore across all financial crime detection workflows including AML monitoring, fraud detection, and compliance reporting. PwC Australia Accelerates Digital Modernization for Major Australian Bank PwC Australia is leading an Agentic transformation for a major Australian bank's modernization, streamlining architecture functions to accelerate projects, improve budgets, and prevent rework. By leveraging AgentCore Runtime and Gateway services, PwC is integrating strategic architecture AI agents within the bank's existing business processes to automate complex architectural reviews and ensure alignment with the bank's governance framework and pre-defined implementation patterns. The agents replace manual review processes with AI-driven pattern compliance checks and human-in-the-loop oversight for critical decision points.

"PwC is excited to be the transformation partner for pivotal Agentic transformation initiatives in the banking industry. These strategic initiatives will streamline technology development and modernisation programs – unlocking consistent improvements, returns on investment, along with business outcomes such as speed to market, competitive positioning, and the ability to launch innovative products at scale. PwC is leveraging AgentCore runtime and gateway services, to build an initial architecture agent, within a bank's existing processes, to target an estimated productivity improvement of over 30%, streamlining the bank's architecture functions, to accelerate projects, reduce budgets, and prevent the late emergence of critical issues," said Noel Williams, Banking and Capital Markets Lead Partner, PwC Australia.

AgentCore's serverless runtime enables PwC to deploy agents without managing infrastructure complexity, while Gateway services provide seamless integration with the bank's enterprise systems. Reply Transforms Totemia's Vacation Discovery with Conversational AI Totemia, France's leading childhood vacation camp platform, faced a critical challenge. Parents were overwhelmed with over 60 generic search results, couldn't handle multi-child searches, and saw no real-time availability, leading to customer abandonment and conversion losses.

Reply partnered with Totemia to develop an Offer Agent built on Strands SDK and hosted on AgentCore Runtime. The solution leverages AgentCore Gateway to invoke AWS Lambda functions generating dynamic SQL queries via Amazon Bedrock foundation models (Anthropic Claude) based on client criteria. These functions connect to Amazon RDS to fetch real-time offers and return the top 5 matches. AgentCore Memory enables multi-turn conversations, refining recommendations based on previous interactions. When no exact match exists, the agent proactively suggests adjusting criteria through iterative refinement. AgentCore Identity via Amazon Cognito provides authentication, while AgentCore Observability tracks performance and reliability.

"With AgentCore, we can easily develop and test our solutions using different Python frameworks, giving us flexibility and faster experimentation," said Xihui Wang, Reply Data Scientist. "The built-in gateway enables us to securely integrate Lambda functions as tools while also making them reusable across multiple agents. In addition, AgentCore provides built-in memory management, observability, and identity services, which reduce development overhead and simplify scaling to production."

AgentCore's ready-to-use building blocks eliminated custom infrastructure development, enabling Reply to move from prototyping to production rapidly. The agent narrows results from 60 to 5-10 tailored recommendations, achieving 65% reduced search time, 40% increased bookings, 24/7 availability, 25% conversion growth, and supporting 30,000 users monthly—projecting 200-300% ROI within year one. Conclusion These early-adopter success stories demonstrate how AWS Partners are capitalizing on AgentCore to deliver transformative value across industries – from financial services to travel, security to banking. As enterprise demand for AI agents accelerates, AgentCore enables partners to rapidly scale production-ready solutions while maintaining enterprise-grade security and performance.

The opportunity is immediate. Visit Amazon Bedrock AgentCore to start building today. Partners can access exclusive resources and latest capabilities through AWS Partner Central's Agentic AI Hub, and showcase their expertise through the AWS Generative AI Competency program.

TAGS: AI Agents, Amazon Bedrock, Amazon Bedrock Agents, Announcements, AWS Partner Network, Customer Solutions, Partner solutions

Introduction In this blog we outline options for online and offline data transfer scenarios to AWS. Customers' migration goals, available network bandwidth speed and reliability, and size of data to be imported, can then guide the choice on which service to use. AWS DataSync is a secure, online service that automates and accelerates moving data between on premises and AWS storage services and is the recommended service for data migration when network bandwidth is not a limiting factor. To mitigate network bandwidth limitations while using AWS DataSync, the general recommendation is to leverage an on-demand AWS Direct Connect hosted connection, for the duration of the data transfer, through AWS Direct Connect Delivery Partners. When your data is in a location where network bandwidth is limited or network connectivity is intermittent, we recommend AWS Data Transfer Terminal along with bring-your-own storage device. This blog post focuses on data transfer scenarios and provides guidance in choosing the right hardware, software, and upload suite reservation durations for your migration needs. We walk through possible factors preventing transfers from achieving optimal results and conclude with an overview of fully managed offerings taking care of the entire transfer experience. What is AWS Data Transfer Terminal AWS Data Transfer Terminal provides access to a network-ready, physical location ("Terminal") for customers to bring their storage devices for accelerated high-volume data transfer to or from the cloud. The service allows high-bandwidth connections of up to 100 Gbps to AWS public endpoints with a single fiber connection, available in secure, reservable AWS locations globally. Each AWS Data Transfer Terminal provides at least two fibers, allowing up to 200 Gbps by means of Equal-Cost Multi-Path (ECMP) routing or by connecting two devices in parallel, while select locations present four fibers (400 Gbps aggregate). While Amazon S3 buckets are a common destination for Data Transfer Terminal uploads, Amazon EC2 and other publicly accessible AWS services can be targets. Upload suite reservations can last up to 24 hours, and you must bring all necessary equipment, as AWS staff can't handle your data or assist with transfers. You'll be charged for on-demand use per hour for each location. There is a port hour charge, with lower costs if you upload data into AWS Regions in the same continent as your location. To learn more, visit the AWS Data Transfer Terminal pricing page. An overview of the service and the underlying infrastructure can be found in the following blog post. For an updated list of Data Transfer Terminal locations, see the FAQs. As prerequisites, customers should be familiar with data transfer methods, including loading on-premises data to the hardware storage device of choice, before bringing it to a Data Transfer Terminal location. To get started with data transfers using AWS Data Transfer Terminal, follow these steps:

Determine the appropriate location first. You can find an updated list of locations in the FAQs, under the section "Where are Data Transfer Terminals located?". Schedule the AWS Data Transfer Terminal session at your preferred site through the AWS Management Console at least 24 hours in advance. Prepare your hardware, ensuring compatibility with 100G QSFP-LR4 fiber connections and adequate system configurations for maximum throughput and optimal data upload experience. Refer to the documentation for more information. Make sure to jot down serial numbers of each server and storage device that will be brought in. They will have to be communicated to the security staff at check-in, and verified again upon leaving the location, as per standard datacenter security procedures. On the day of your visit, present government-issued ID for escorted access to a private transfer suite. Once in the upload suite, connect your pre-configured hardware to the provided high-speed ports and initiate your data transfer. When done, clear the room without leaving anything behind, and allow staff to cross-check serial numbers before leaving.

The Challenge of Uploading at 100 Gbps and Beyond For high-rate data transfers, the hardware and software you choose determines overall performance. Transferring at 10 Gbps is mainstream at the time of writing and poses little to no challenge for most existing devices with default configurations and no specific tuning. But as you get closer to target 100 Gbps and beyond, optimal results can only be obtained by using dedicated transfer software running on an OS with proper drivers and kernel tuning, installed on a balanced system with appropriate storage throughput, interface bandwidth, and network capacity to avoid bottlenecks. Fall short of any of these aspects, and you won't achieve the optimal speeds being targeted. Choosing a solution that fits your business and technical requirements can be daunting, especially considering the wealth of options on the market. To help navigate the landscape, we present the results of tests performed on select solutions in conjunction with AWS Data Transfer Terminal. Each solution offers different transfer speed, storage, weight, form and cost factors to suit different data transfer use cases. While a 100 Gbps bandwidth connection might seem straightforward to achieve, the effective throughput (that is, the rate of actual data transferred end-to-end) is lower because of various overheads. For instance, when transferring data through AWS Data Transfer Terminal, the maximum bandwidth of 100 Gbps (or 12.5 GB/s) can be witnessed on the fiber connection at the networking infrastructure level. However, the throughput (actual data transferred) over the same is impacted by factors such as TCP/IP overhead, storage performance, and the characteristics of the dataset being transferred. An Ethernet frame requires extra bits for preambles, headers, and spacing, which consumes bandwidth that is not used for data. Depending on packet size, the effective throughput can end up being 85% of the total bandwidth. Large files typically transfer faster than multiple small files because of reduced protocol overhead, making it essential to consider these nuances when planning data transfer operations. For these reasons, to estimate the effective throughput, we recommend accounting for a 15% overhead with respect to the connection bandwidth speeds provided in this article. Properly estimating transfer rates is crucial to reduce the risk of running out of time, or unnecessary costs because of over-booking. Choosing the Right Solution Customers have asked us what systems to bring along to the AWS Data Transfer Terminal location, as they all have different data upload needs. Some customers need an affordable way to upload small amounts of data on a regular basis, while others need to upload large amounts of data one time. Customers might have TV productions that need to upload several terabytes of footage daily to their geo-distributed post-production contractors. On the higher end, organizations may need to perform complete infrastructure migrations to AWS, moving petabyte-scale amounts of data within tight timeframes. This blog post navigates the reader through the aspects involved in choosing the most appropriate transfer solutions for your needs. Once the right solution is determined, customers can take advantage of AWS Data Transfer Terminal on-demand high bandwidth connectivity to get data into AWS quickly. For example, automotive companies such as Rivian have reduced their data upload times by 3x, as narrated in this blog post. Understanding Hardware and Software Components for Optimal Data Transfer Performance The following sections walk through the three key aspects of uploading data to an S3 bucket with AWS Data Transfer Terminal: the software stack, storage options, and the hardware solutions. Several options are discussed, considering budget, form factor, and time constraints. Transfer Software Agents To complete your data transfer onsite, we recommend the aws s3 cp and aws s3 sync commands that are part of our AWS Command Line Interface. For best transfer rates, review the documentation to enable optimizations such as leveraging the CRT (Common Runtime) base library. The CRT is a modular family of independent packages, written in C, more performant than the default Python interpreter. As an alternative, customers with demanding transfer goals are encouraged to explore their own implementation of S3 transfer agents by using AWS SDKs, available for different languages. The highest performing tested configurations leverage proprietary custom code implementing AWS SDK for C++. Parallelization and batching can help maximize hardware utilization, especially when dealing with multiple smaller files. Encryption, while essential for security and confidentiality, can introduce overhead and reduce transfer speeds, particularly on systems without hardware encryption acceleration. It's important to note that smaller files reduce overall transfer efficiency and increase the resulting transfer times, as each file requires its own overhead in terms of protocol headers, connection setup, and system processing, Storage Considerations The storage subsystem is generally the most common bottleneck hindering the maximum achievable throughput by a solution. Transferring at 100 Gbps requires reading from storage at a speed of 12.5 GB/s. Modern storage solutions offer various performance tiers. NVMe (Non-Volatile Memory Express) SSDs offer the highest performance. Gen 5 NVMe drives deliver around 14 GB/s, while SATA SSDs are limited to about 550 MB/s. Traditional spinning hard drives (HDDs) lag, with speeds ranging from 80-160 MB/s, making them suitable for archival storage but less ideal for high-performance data transfer operations. Creating RAID (Redundant Array of Independent Disks) with lower performing drives by combining multiple HDDs helps improve performance by aggregating capacity, performance, and redundancy. Only high-performing NVMe Gen5 can theoretically support 100 Gbps speeds without the need to be part of a RAID volume for improved aggregate read speeds. An overview of SSD vs HDD can be found here.

Figure 1: Storage devices, from Hard Disk Drives (HDD) with spinning disks, to modern compact Solid State Drives (SSD)

For most transfer solutions on the market, storage capacity is configurable. Hardware vendors allow configuring specific amounts of onboard storage at the time of order. Alternatively, mass storage can be directly attached and mounted as external storage. Extra storage can be externally mounted using Thunderbolt or SAS (Serial Attached SCSI) interfaces. For example, Thunderbolt 4, offers a theoretical bandwidth up to 40 Gbps (5 GB/s). SAS interfaces, generally used in enterprise environments, can achieve up to 22.5 Gbps per lane with SAS-4, making them excellent options for high-performance storage systems. Network Interfaces When considering network interfaces, modern Network Interface Cards (NIC) support speeds from 1 Gbps to 100 Gbps, though real-world performance is lower because of TCP/IP overhead and other factors. Ethernet ports with the standard RJ-45 copper connection and speed of 1 Gbps are nowadays mainstream, whereas 10 Gbps are gaining adoption. The 2×100 Gbps fiber connections provided by AWS Data Transfer Terminal require 100GBASE-LR4 optical QSFP28 transceivers, which most of the solutions reviewed in this blog post support. Jumbo frames are currently not supported.

Multiple Devices via a 100G Switch Another option is to leverage the onboard 10GbE Ethernet port commonly found in mainstream servers and devices nowadays, connected via a CAT7+ cable to a 10GbE port on a switch supporting 100 Gbps uplinks. The QSFP28 on the switch is then connected to the AWS Data Transfer Terminal fiber port. The approach removes the need for multiple expensive PCIe cards and/or external enclosures, using just a single transceiver per switch. At the time of writing, each AWS Data Transfer Terminal fiber connection provides up to a maximum of five IPs via Dynamic Host Configuration Protocol (DHCP) leases, therefore capping the maximum speed to 50 Gbps in aggregate when using Small Form-factor Pluggable (SFP) to combine multiple 10 Gbps copper connections to the storage devices. As every AWS Data Transfer Terminal has at least two fibers available, another setup in parallel can be used to provide another 50Gbps, bringing this up to 100 Gbps in aggregate.

Figure 3: Expected aggregated bandwidth up to 50 Gbps when connecting 5x 10GbE devices

Leveraging Multiple Fiber Connections Concurrently Transfer solutions supporting multiple fiber connections concurrently can go beyond 100 Gbps. A common approach leverages Equal-Cost Multi-Path (ECMP) for increased bandwidth, and it is generally available out of the box on most operating systems by default. Currently ECMP is the only option available to AWS Data Transfer Terminal customers to aggregate multiple connections from a single device. Other approaches requiring Border Gateway Protocol (BGP) or Link Aggregation Control Protocol (LACP) are not available at the time of writing. White-Glove End-to-End Transfer Services For customers looking for a fully managed experience, contracting a comprehensive AWS Partner service that includes support, physical transit and end-to-end storage transfer via AWS Data Transfer Terminal is a compelling solution. These partners can provide a fully managed experience that includes:

Initial delivery of storage device(s) to customer sites System set-up support loading data onto the devices on site Scheduling of the upload suite reservation Transporting the device to the AWS Data Transfer Terminal facility Data upload to AWS and associated transfer monitoring Clearing of customer data upon upload validation Returning storage devices to conclude the engagement

Conclusions Successful high-speed data transfer depends on the right hardware, storage, and software choices. This blog post covered various aspects involved to perform optimal data transfers via AWS Data Transfer Terminal, spanning from basic systems to advanced solutions hitting triple-digit Gbps. The solutions vary significantly in terms of throughput, storage capacity, form factor, and cost, allowing customers to choose based on their specific requirements. Correctly estimating the time to transfer data is paramount to support accurate planning of reservation hours, accounting for transfer protocols overheads and other real-world factors. For customers looking for a fully managed service, white-glove services are available through AWS Partners who can manage the entire transfer process from pickup to upload. As the AWS Data Transfer Terminal service continues to evolve, we encourage customers to share their experiences and testing results to help expand our knowledge base of solutions available for our customers. Submissions of new test results to be evaluated by our team can be done via Command Center, creating a case as Service: Data Transfer Terminal, Category: Other, Severity: General guidance.

Marcello Golfieri Marcello Golfieri is a Principal Solutions Architect at AWS in Strategic Accounts. With extensive experience in enterprise infrastructure engineering, resilient architectures, and DevOps, Marcello advises organizations operating at the most massive scale. Beyond work, loves playing Contract Bridge, learning foreign languages, and retrocomputing.

Sofia Guevara Sofia Guevara is part of the Customer Solutions team in Amazon Web Services, where she brings her enthusiasm for sustainable technology and solving enterprise challenges. Working backwards from customer needs, she helped shape AWS Data Transfer Terminal from beta to its launch at re:Invent 2024. As a strategic advisor to Amazon Devices, she champions sustainable architecture and cloud implementations. She believes in challenging customers to push boundaries and actively practice innovation. Prior to AWS, she drove technical consulting projects for business process management as a product owner of cloud modernizations.

Bhavya Paliwal Bhavya Paliwal is a Senior Product Manager at AWS focused on Data Transfer Terminal. She is also focused on product management for AWS Networking services where she drives the development of remote connectivity products. Bhavya has led product management for 6+ years and her goal is to deliver secure, user-friendly solutions that enable customers to seamlessly access their applications and workloads.

Camden Forgia Camden Forgia is a Principal Product Manager for Amazon Project Kuiper focused on delivering the best connectivity experience using Amazon Project Kuiper's constellation of low-earth orbit satellites. Previously, he was a product manager in Amazon Web Services (AWS) focused on building and launching products that deliver the AWS global network infrastructure in novel ways for AWS customers. He was the product manager for external services such as AWS Data Transfer Services and AWS Data Transfer Terminal. Camden has led operations and product management over 10 years for technology companies ranging from semiconductor to cloud industries.

Subbu Mahadevan Subbu Mahadevan is a Principal Product Manager at AWS focused on AWS Outposts and Snowball Edge services. Previously, he led product management for AWS network monitoring and data transfer services. Subbu has led product management for over two decades across branch office networking products, tools for infrastructure automation, and services for hybrid cloud environments.

Organizations need seamless access to their structured data repositories to power intelligent AI agents. However, when these resources span multiple AWS accounts integration challenges can arise. This post explores a practical solution for connecting Amazon Bedrock agents to knowledge bases in Amazon Redshift clusters residing in different AWS accounts. The challenge Organizations that build AI agents using Amazon Bedrock can maintain their structured data in Amazon Redshift clusters. When these data repositories exist in separate AWS accounts from their AI agents, they face a significant limitation: Amazon Bedrock Knowledge Bases doesn't natively support cross-account Redshift integration. This creates a challenge for enterprises with multi-account architectures who want to:

Leverage existing structured data in Redshift for their AI agents. Maintain separation of concerns across different AWS accounts. Avoid duplicating data across accounts. Ensure proper security and access controls.

Solution overview Our solution enables cross-account knowledge base integration through a secure, serverless architecture that maintains secure access controls while allowing AI agents to query structured data. The approach uses AWS Lambda as an intermediary to facilitate secure cross-account data access.

Users enter their natural language question in Amazon Bedrock Agents which is configured in the agent account. Amazon Bedrock Agents invokes a Lambda function through action groups which provides access to the Amazon Bedrock knowledge base configured in the agent-kb account above. Action group Lambda function running in agent account assumes an IAM role created in agent-kb account above to connect to the knowledge base in the agent-kb account. Amazon Bedrock Knowledge Base in the agent-kb account uses an IAM role created in the same account to access Amazon Redshift data warehouse and query data in the data warehouse.

Amazon Bedrock agent in the agent account that handles user interactions. Amazon Redshift serverless workgroup in VPC and private subnet in the agent-kb account containing structured data. Amazon Bedrock Knowledge base using the Amazon Redshift serverless workgroup as structured data source. Lambda function in the agent account. Action group configuration to connect the agent in the agent account to the Lambda function. IAM roles and policies that enable secure cross-account access.

Two AWS accounts. Create an AWS account if you do not have one. Specific permissions required for both account which will be set up in subsequent steps. Install the AWS CLI (2.24.22 – current version) Set up authentication using IAM user credentials for the AWS CLI for each account Make sure you have jq installed, jq is lightweight command-line JSON processor. For example, in Mac you can use the command brew install jq (jq-1.7.1-apple – current version) to install it. Navigate to the Amazon Bedrock console and make sure you enable access to the meta.llama3-1-70b-instruct-v1:0 model for the agent-kb account and access for us.amazon.nova-pro-v1:0 model in the agent account in the us-west-2, US West (Oregon) AWS Region.

Assumption Let's call the AWS account profile, agent profile that has the Amazon Bedrock agent. Similarly, the AWS account profile be called agent-kb that has the Amazon Bedrock knowledge base with Amazon Redshift Serverless and the structured data source. We will use the us-west-2 US West (Oregon) AWS Region but feel free to choose another AWS Region as necessary (the prerequisites will be applicable to the AWS Region you choose to deploy this solution in). We will use the meta.llama3-1-70b-instruct-v1:0 model for the agent-kb. This is an available on-demand model in us-west-2. You are free to choose other models with cross-Region inference but that would mean changing the roles and polices accordingly and enable model access in all Regions they are available in. Based on our model choice for this solution the AWS Region must be us-west-2. For the agent we will be using an Amazon Bedrock agent optimized model like us.amazon.nova-pro-v1:0. Implementation walkthrough The following is a step-by-step implementation guide. Make sure to perform all steps in the same AWS Region in both accounts. These steps are to deploy and test an end-to-end solution from scratch and if you are already running some of these components, you may skip over those steps.

Make a note of the AWS account numbers in the agent and agent-kb account. In the implementation steps we will refer them as follows:

Note: These steps use example profile names and account numbers, please replace with actuals before running. Create the Amazon Redshift Serverless workgroup in the agent-kb account:

Log on to the agent-kb account Follow the workshop link to create the Amazon Redshift Serverless workgroup in private subnet Make a note of the namespace, workgroup, and other details and follow the rest of the hands-on workshop instructions.

Set up your data warehouse in the agent-kb account. Create your AI knowledge base in the agent-kb account. Make a note of the knowledge base ID. Train your AI Assistant in the agent-kb account. Test natural language queries in the agent-kb account. You can find the code in aws-samples git repository: sample-for-amazon-bedrock-agent-connect-cross-account-kb. Create necessary roles and policies in both the accounts. Run the script create_bedrock_agent_kb_roles_policies.sh with the following input parameters.

–agent-kb-profile agent-kb The agent knowledgebase profile that you set up with the AWS CLI with aws_access_key_id, aws_secret_access_key as mentioned in the prerequisites.

–lambda-role lambda_bedrock_kb_query_role This is the IAM role the agent account Bedrock agent action group lambda will assume to connect to the Redshift cross account

–kb-access-role bedrock_kb_access_role This is the IAM role the agent-kb account which the lambda_bedrock_kb_query_role in agent account assumes to connect to the Redshift cross account

–kb-access-policy bedrock_kb_access_policy IAM policy attached to the IAM role bedrock_kb_access_role

–lambda-policy lambda_bedrock_kb_query_policy IAM policy attached to the IAM role lambda_bedrock_kb_query_role

–knowledge-base-id XXXXXXXXXX Replace with the actual knowledge base ID created in Step 4

–agent-account 111122223333 Replace with the 12-digit AWS account number where the Bedrock agent is running. (agent account)

–agent-kb-account 999999999999 Replace with the 12-digit AWS account number where the Bedrock knowledge base is running. (agent-kb acccount)

Download the script (create_bedrock_agent_kb_roles_policies.sh) from the aws-samples GitHub repository. Open Terminal in Mac or similar bash shell for other platforms. Locate and change the directory to the downloaded location, provide executable permissions:

If you are still not clear on the script usage or inputs, then you can run the script with the –help option and the script will display the usage: ./create_bedrock_agent_kb_roles_policies.sh –help Run the script with the right input parameters as described in the previous table.

./create_bedrock_agent_kb_roles_policies.sh --agent-profile agent \ --agent-kb-profile agent-kb \ --lambda-role lambda_bedrock_kb_query_role \ --kb-access-role bedrock_kb_access_role \ --kb-access-policy bedrock_kb_access_policy \ --lambda-policy lambda_bedrock_kb_query_policy \ --knowledge-base-id XXXXXXXXXX \ --agent-account 111122223333 \ --agent-kb-account 999999999999PowerShell

The script on successful execution shows the summary of the IAM, roles and policies created in both accounts.

Log on to both the agent and agent-kb account to verify the IAM roles and policies are created.

For the agent account: Make a note of the ARN of the lambda_bedrock_kb_query_role as that will be the value of CloudFormation stack parameter AgentLambdaExecutionRoleArn in the next step.

For the agent-kb account: Make a note of the ARN of the bedrock_kb_access_role as that will be the value of CloudFormation stack parameter TargetRoleArn in the next step.

Run the AWS CloudFormation script to create a Bedrock agent:

Download the CloudFormation script: cloudformation_bedrock_agent_kb_query_cross_account.yaml from the aws-samples GitHub repository. Log on to the agent account and navigate to the CloudFormation console, and verify you are in the us-west-2 (Oregon) Region, choose Create stack and choose With new resources (standard).

In the Specify template section choose Upload a template file and then Choose file and select the file from (1). Then, choose Next. Enter the following stack details and choose Next.

BedrockAgentDescription Agent to query inventory data from Redshift Serverless database Keep this as default

BedrockAgentInstructions You are an assistant that helps users query inventory data from our Redshift Serverless database using the action group. Do not change

Complete the acknowledgement and choose Next. Scroll down through the page and choose Submit. You will see the CloudFormation stack is getting created as shown by the status CREATE_IN_PROGRESS. It will take a few minutes, and you will see the status change to CREATE_COMPLETE indicating creation of all resources. Choose the Outputs tab to make a note of the resources that were created. In summary, the CloudFormation script does the following in the agent account.

Creates a Bedrock agent Creates an action group Also creates a Lambda function which is invoked by the Bedrock action group Defines the OpenAPI schema Creates necessary roles and permissions for the Bedrock agent Finally, it prepares the Bedrock agent so that it is ready to test.

Verify Nova Pro (us.amazon.nova-pro-v1:0) model access in the agent account. Navigate to the Amazon Bedrock console and choose Model access under Configure and learn. Search for Model name : Nova Pro to verify access. If not, then enable model access.

Verify access to the meta.llama3-1-70b-instruct-v1:0 model in the agent-kb account. This should already be enabled as we set up the knowledge base earlier.

Run the agent. Log on to agent account. Navigate to Amazon Bedrock console and choose Agents under Build. Choose the name of the agent and choose Test. You can test the following questions as mentioned the workshop's Stage 4: Test Natural Language Queries page. For example:

Who are the top 5 customers in Saudi Arabia? Who are the top parts supplier in the United States by volume? What is the total revenue by region for the year 1998? Which products have the highest profit margins? Show me orders with the highest priority from the last quarter of 1997.

Phrase your question to be more specific Use terminology that matches your table descriptions Try questions similar to your curated examples Verify your question relates to data that exists in the TPCH dataset Use Amazon Bedrock Guardrails to add configurable safeguards to questions and responses.

Clean up resources It is recommended that you clean up any resources you do not need anymore to avoid any unnecessary charges:

Navigate to the CloudFormation console for the agent and agent-kb account, search for the stack and and choose Delete. S3 buckets need to be deleted separately.

For deleting the roles and policies created in both accounts, download the script delete-bedrock-agent-kb-roles-policies.sh from the aws-samples GitHub repository.

Open Terminal in Mac or similar bash shell on other platforms. Locate and change the directory to the downloaded location, provide executable permissions:

If you are still not clear on the script usage or inputs, then you can run the script with the –help option then the script will display the usage: ./ delete-bedrock-agent-kb-roles-policies.sh –help Run the script: delete-bedrock-agent-kb-roles-policies.sh with the same values for the same input parameters as in Step7 when running the create_bedrock_agent_kb_roles_policies.sh script. Note: Enter the correct account numbers for agent-account and agent-kb-account before running.

./delete-bedrock-agent-kb-roles-policies.sh --agent-profile agent \ --agent-kb-profile agent-kb \ --lambda-role lambda_bedrock_kb_query_role \ --kb-access-role bedrock_kb_access_role \ --kb-access-policy bedrock_kb_access_policy \ --lambda-policy lambda_bedrock_kb_query_policy \ --agent-account 111122223333 \ --agent-kb-account 999999999999PowerShell The script will ask for a confirmation, say yes and press enter.

Summary This solution demonstrates how the Amazon Bedrock agent in the agent account can query the Amazon Bedrock knowledge base in the agent-kb account. Conclusion This solution uses Amazon Bedrock Knowledge Bases for structured data to create a more integrated approach to cross-account data access. The knowledge base in agent-kb account connects directly to Amazon Redshift Serverless in a private VPC. The Amazon Bedrock agent in the agent account invokes an AWS Lambda function as part of its action group to make a cross-account connection to retrieve response from the structured knowledge base. This architecture offers several advantages:

Uses Amazon Bedrock Knowledge Bases capabilities for structured data Provides a more seamless integration between the agent and the data source Maintains proper security boundaries between accounts Reduces the complexity of direct database access codes

As Amazon Bedrock continues to evolve, you can take advantage of future enhancements to knowledge base functionality while maintaining your multi-account architecture.

About the Authors Kunal Ghosh is an expert in AWS technologies. He passionate about building efficient and effective solutions on AWS, especially involving generative AI, analytics, data science, and machine learning. Besides family time, he likes reading, swimming, biking, and watching movies, and he is a foodie. Arghya Banerjee is a Sr. Solutions Architect at AWS in the San Francisco Bay Area, focused on helping customers adopt and use the AWS Cloud. He is focused on big data, data lakes, streaming and batch analytics services, and generative AI technologies. Indranil Banerjee is a Sr. Solutions Architect at AWS in the San Francisco Bay Area, focused on helping customers in the hi-tech and semi-conductor sectors solve complex business problems using the AWS Cloud. His special interests are in the areas of legacy modernization and migration, building analytics platforms and helping customers adopt cutting edge technologies such as generative AI. Vinayak Datar is Sr. Solutions Manager based in Bay Area, helping enterprise customers accelerate their AWS Cloud journey. He's focusing on helping customers to convert ideas from concepts to working prototypes to production using AWS generative AI services.

This post was contributed by Jiaqi Leng, Joseph Li, Xiaodi Wu Scientists and engineers face numerous computational challenges in fields like fluid dynamics [1], modeling heat and sound propagation [2], and aircraft design [3]. Simulating partial differential equations (PDEs) in high dimensions offers a powerful approach to addressing these challenges. However, solving these high-dimensional differential equations is challenging for classical computers, as the computational complexity increases exponentially with the problem dimension. Quantum computers, capable of efficiently manipulating high-dimensional data in a non-classical way, offer potential for addressing these complex problems. Over the past decades, there has been progress in developing quantum algorithms for PDEs, including both linear and nonlinear equations [4,5,6]. However, most existing quantum algorithms rely on sophisticated input models of the problem data, such as block-encoded matrices and QRAM, which require large, fault-tolerant quantum computers and are thus unlikely to be implemented with near-term quantum hardware. We, a group of researchers in the University of Maryland and the University of California, Berkeley, introduce a novel technique named Hamiltonian embedding [7], which aims to be a leap toward harnessing near-term quantum technology for simulating qudit Hamiltonians (e.g., high-dimensional PDEs with spatial discretization, bosonic systems, etc.) This technique allows us to map the desired simulation to a quantum evolutionary process that can be efficiently implemented with near-term quantum hardware. Upon measurement and simple post-selection, the dynamical properties of the desired high-dimensional PDE can be recovered. Notably, realizations of Hamiltonian embedding are not restricted to gate-based quantum computers. In this post, we demonstrate the use cases of Hamiltonian embedding in both the IonQ and QuEra devices, both of which are accessible through Amazon Braket.

Hamiltonian embedding: mapping differential operators to local spin operators To simulate differential equations on qubit-based quantum computers, it is necessary to discretize the differential equation in some way. Here, we consider the finite difference method applied to first and second order differential operators. Upon discretization, we can map the differential equation to a quantum dynamics problem via a finite-dimensional Hamiltonian called the problem Hamiltonian. For example, the evolution of a free particle is governed by the Schrödinger equation.

To discretize the (1D) Laplace operator, we can use the stencil method:

The resulting problem Hamiltonian (up to a minus sign) is a tridiagonal matrix with main diagonal elements Hj,j = -2h-2 and sub/super diagonal elements Hj,j+1=Hj+1,j=h-2. In practice, the problem Hamiltonians arising from many differential equations have the form of sparse, banded matrices (such as tridiagonal matrices) [8]. Ideally, one would like to have a quantum computer that directly simulates the time evolution of the problem Hamiltonian, without requiring any additional overhead. However, existing quantum computers are using local spin operators (i.e., Pauli matrices), and their native operators allow for only local interactions between qubits. Meanwhile, the qubit operator representation of sparse matrices often requires highly non-local interaction terms. How do we address this mismatch between the problem Hamiltonian and the device Hamiltonian? To enable the simulation of differential equations on quantum hardware, we develop a technique known as Hamiltonian embedding. The main idea is to map the problem Hamiltonian to an embedding Hamiltonian (i.e., local spin operators) which are more easily simulated by the physical hardware. More concretely, given a target problem Hamiltonian A, we aim to design a larger operator H (called an embedding Hamiltonian) composed of local spin operators that admits a block-diagonal decomposition: H = diag(A,*), where A is embedded in the upper left corner of H. Therefore, by simulating the time evolution of H on a quantum computer —presumably easier than simulating A itself — we effectively implement the evolution of A in the upper left corner e-iHt=diag(e-iAt,*). In what follows, we present an example to illustrate the idea. Let us consider an 8-by-8 binary-valued matrix A with all zeros except for A1,8=A8,1=1 and Aj,j+1=Aj+1,j=1 for j = 1,…,7.

This matrix is circulant and is a simple version of a Laplace operator with periodic boundary conditions. In the language of quantum computing, A corresponds to a three-qubit Hamiltonian. Although A has a simple structure, representing it as a quantum circuit (for example, using block-encoding) is challenging and may require multiple extra qubits to assist in the process [9]. If we break down A into combinations of basic Pauli strings, we end up with many component operators; some of them, like XXX, involve interactions between 3 qubits at once. This makes simulating such a seemingly simple Hamiltonian a challenge for today's quantum computers. Alternatively, Hamiltonian embedding allows us to represent A using only very simple quantum operations. The idea is to consider a 4-qubit Hamiltonian Sx=X1+X2+X3+X4, where each Xj is a Pauli-X operator acting on the site j. The Hilbert space of this Hamiltonian has basis. Then, we consider an 8-dimensional subspace spanned by the basis shown in Table 1 (called circulant unary code, see Sec. B.2.1 in [7]):

Table 1: Example of the circulant unary code for representing banded circulant matrices. We can readily verify that projecting Sx onto this subspace yields exactly our target matrix A, as the Hamming distance between two adjacent codewords (including between 1 and 8) is always 1. By preparing an initial state within this subspace and penalizing any leakage outside of it, researchers can achieve the quantum simulation of A using a small number of elementary gates (or analog evolution time) without ancilla qubits. We post-select measurement outcomes confined to the relevant subspace, disregarding those outside of it. For this problem, alternative embedding schemes — such as those based on antiferromagnetic or one-hot codes — can achieve the same goal. For more details of the Hamiltonian embedding technique, we encourage interested readers to read the original paper [7].

Simulating 2D Schrödinger equation using QuEra The time evolution of a quantum system in real space is described by the Schrödinger equation:

where u(t,x) is the wave function, f(x) is a potential function, and the spatial variable x is in -dimensional real space. In practice, the dimension d may be very high, corresponding to the number of electrons in quantum chemistry [10] or the dimension of complex nonlinear optimization problems [11]. In this blog, we showcase how to leverage Hamiltonian embedding to simulate the dynamics generated by a two-dimensional Schrödinger equation using real quantum computing devices. By performing spatial discretization of the system Hamiltonian

Where D is the discretized 1D Laplace operator (as discussed above), and U is a diagonal matrix corresponding to the potential field f(x,y). The size of the problem Hamiltonian is N2, where N is the discretization number per dimension. On the QuEra device, the machine Hamiltonian is given by

Where A(t) and B(t) denote the Rabi frequency and detuning, respectively; Xj's are the Pauli-X matrices, nj counts if the j-th qubit is in the excited state (i.e., number operator); Vij=C6/|ri – rj|6 denotes the Rydberg interaction between atoms i and j. Since the Rydberg interaction coefficient Vij must be positive, the Rydberg Hamiltonian HRyd requires a ingenious design of the embedding scheme, as the unary and one-hot embeddings cannot be straightforwardly applied. In order to match the device Hamiltonian, we devise an embedding scheme which we call the antiferromagnetic embedding. Intuitively, this encoding allows us to label the states using domain walls in an antiferromagnet (i.e., two neighboring qubits measured in the opposite states), as given in Table 2.

Table 2: Example of antiferromagnetic code for representing the real-space Schrödinger equation. As it turns out, this embedding scheme enables us to map the problem Hamiltonian Hprob to the Rydberg Hamiltonian HRyd via a deliberate choice of the parameters A(t), B(t), and the atom locations. In our experiment, we choose a discretization number N=7, which amounts to a total of 12 qubits, where the qubits are grouped into two chains of 6 qubits each. Each chain of qubits is used to represent a single spatial variable (x and y). With the Hamiltonian embedding, the (discretized) kinetic operator is mapped to the Pauli-X operators:

and the (discretized) potential operator is mapped to the diagonal part in the QuEra machine Hamiltonian:

Although encoding arbitrary continuous potential U requires more complex programmability than what is currently available, the native Rydberg interactions give rise to an effective potential field with checkerboard-like pattern. In Figure 1, we demonstrate the quantum simulation results on QuEra at various times. The 3D bar plot shows the quantum wave packets at T = 0.5, 1, 1.5, where the QuEra results are obtained by performing computational basis measurement. Despite some hardware-induced noise, our experimental results show a qualitative match between the QuEra device and the numerical simulation.

Figure 1 – Setup and results for simulating 2D Schrödinger equation using QuEra. Rydberg atom positions in the experiment¬ (left), measurement resu¬lts from numerical simulation (top right) and experiment results obtained from QuEra Aquila (bottom right). Taken from [7].

Simulating 1D Schrödinger equation using IonQ In the previous experiment, the engineering of the potential field U is heavily restricted by the neutral atom configuration. We now demonstrate the simulation of Schrödinger equations with more structured potentials. For demonstration purposes, we illustrate how to simulate a one-dimensional Schrödinger equation on IonQ's 25-qubit trapped ion quantum c­omputer. It is worth noting that our method can be readily generalized to arbitrary spatial dimensions with only polynomial overheads in the dimension of the Schrödinger equations. We consider a quantum Hamiltonian for a single bosonic mode:

The corresponding Schrödinger equation is defined over the real line. Due to the unbounded physical space, we map the physical Hamiltonian Hbs to a finite-dimensional tridiagonal matrix using a method known as Fock space truncation (a different method than the previously used spatial discretization). To map this matrix (i.e., the problem Hamiltonian) to one that is more easily implemented on IonQ, we use the one-hot embedding. Intuitively, the one-hot embedding makes use of the location of a single excitation to determine which state the system is in. Importantly, the use of Hamiltonian embedding gives us an embedding Hamiltonian which involves at most two-qubit operators (such as XX interactions). While one-hot embedding is known to be less compact and does not lead to speedups in one spatial dimension, our Hamiltonian embedding framework allows us to leverage the natural tensor structure in the Schrödinger equation. As a result, to simulate a Schrödinger equation in d dimensions, we only require O(d) qubits and polynomial-in-d interactions terms, yielding an exponential quantum advantage over naïve mesh-based approaches to PDE simulation. The resulting embedding Hamiltonian contains up to 2-body interaction terms, which is suitable for near-term devices. More importantly, the embedding Hamiltonian can be split into a sum of an ''off-diagonal'' part and a ''diagonal'' part: the ''off-diagonal'' part can be directly implemented using the Mølmer–Sørensen gate (native to IonQ), and the ''diagonal part" can be realized by parameterized single-qubit Z rotations. Since both Hamiltonian components preserve the number of excitations, by Trotterizing the dynamics, we effectively implement flip-flop dynamics that approximately simulate the 1D bosonic Hamiltonian Hbs. For details, we refer the readers to Sec. 3.4 in [7]. In our experiment, we truncate the Fock space to 5 levels and simulate a one-dimensional example. The initial state is a Gaussian distribution centered at 0 and the potential field is a quadratic function centered at 0.25. The wave function oscillates left and right over time, and we can simulate the dynamics for various time points to observe its behavior. The position and momentum operators are embedded using both XX and Z operators, so we perform observable measurements in both bases to compute their expectation values (Figure 2). Although there is substantial error due to noisy gates, we still observe the same oscillating behavior in our experiments as predicted by analytical closed-form solutions.

Figure 2 – Problem setup and results for real-space quantum simulation on IonQ Aria-1. As shown in the leftmost figure, the initial state is a Gaussian distribution centered at 0, and the potential field is a parabola centered at 0.25. Experiment results for the expected position (middle) and kinetic energy (right) are shown in blue, with the closed form solutions shown in pink. Taken from [4].

Discussion and outlook We have shown how our Hamiltonian embedding technique expands the realm of quantum applications on near-term hardware. Our experiments demonstrate the first steps towards simulating high-dimensional differential equations, one of the most anticipated practical applications of quantum computers. Since the technique is broadly applicable to both analog and digital quantum computers, we expect that Hamiltonian embedding can enable quantum applications both on near-term devices and in the future with the rapid advancement of quantum technologies. Beyond quantum dynamics problems, we are excited to see how Hamiltonian embedding can accelerate practical implementations of quantum computing applications in other domains. References [1] Kundu P. K., Cohen I.M., Dowling D.R., & Capecelatro J. (2024). Fluid mechanics. Elsevier. [2] Evans, L. C. (2022). Partial differential equations. American Mathematical Society. [3] Anderson, J. (2011). Fundamentals of Aerodynamics (SI units). McGraw hill. [4] Childs, A. M., Liu, J. P., & Ostrander, A. (2021). High-precision quantum algorithms for partial differential equations. Quantum, 5, 574. [5] Liu, J. P., Kolden, H. Ø., Krovi, H. K., Loureiro, N. F., Trivisa, K., & Childs, A. M. (2021). Efficient quantum algorithm for dissipative nonlinear differential equations. Proceedings of the National Academy of Sciences, 118(35). [6] Jin, S., & Liu, N. (2022). Quantum algorithms for computing observables of nonlinear partial differential equations. arXiv preprint arXiv:2202.07834. [7] Leng, J., Li, J., Peng, Y., & Wu, X. (2024). Expanding Hardware-Efficiently Manipulable Hilbert Space via Hamiltonian Embedding. Quantum, 9, 1857. [8] Morton, K. W., & Mayers, D. F. (2005). Numerical solution of partial differential equations: an introduction. Cambridge university press. [9] Camps, D., Lin, L., Van Beeumen, R., & Yang, C. (2024). Explicit quantum circuits for block encodings of certain sparse matrices. SIAM Journal on Matrix Analysis and Applications, 45(1), 801-827. [10] Babbush, R., Berry, D. W., McClean, J. R., & Neven, H. (2019). Quantum simulation of chemistry with sublinear scaling in basis size. npj Quantum Information, 5(1), 92. [11] Leng, J., Hickman, E., Li, J., & Wu, X. (2023). Quantum Hamiltonian Descent. arXiv preprint arXiv:2303.01471.

TAGS: Amazon Braket, Hamiltonian, ionQ, quantum algorithms, quantum computing, quantum research, Quantum Technologies, QuEra

Jiaqi Leng Jiaqi Leng is a Postdoctoral Researcher at UC Berkeley. His research focuses on quantum algorithms for mathematical optimization and scientific computing.

Joseph Li Joseph Li is a PhD student the University of Maryland, advised by Prof. Xiaodi Wu. He is interested in near-term implementable quantum algorithms for scientific computing and practical approaches for Hamiltonian simulation.

Xiaodi Wu Xiaodi Wu is an Associate Professor in the Department of Computer Science at the University of Maryland and a Fellow at the Joint Center for Quantum Information and Computer Science (QuICS). He is also an Amazon Visiting Academic working for Amazon Braket. He aims to bridge the gap between the theoretical foundation of quantum computing and the limitation of realistic quantum machines.

Mastering Multi-Region Resilience and Scalability: Active-Active Design with Amazon ElastiCache Redis

This post is co-written by Jayadev Sirimamilla from Citibank, along with Sayan Deb Ghosh and Dibyarup Basu from TCS. Introduction: The Active-Active Redis Challenge in Cloud Migration In today's digital-first world, milliseconds (ms) matter, especially for global financial institutions processing millions of transactions. Customers need their financial transactions to be instantly recorded, reflected, and accessible across geographical regions in real-time. Translating the business promise of delivering sensitive data like financial transactions, reflecting latest state reliably, in real-time, over a geographically distributed landscape, for millions of transactions per hour, necessitates solutions like Distributed Cache e.g. Redis Cache. While Amazon Web Services (AWS) drives digital transformation, migrating Active-Active Redis deployments presents challenges. Enterprises use Active-Active Redis across global data centers through Redis Enterprise, providing cross-regional reliability and seamless failover. However, Amazon ElastiCache for Redis, while offering superior elasticity, operational simplicity, cost efficiency and cross-region replication, doesn't natively support Active-Active configuration across regions—a feature many organizations rely upon in their on-premise Enterprise Redis deployments. This perceived gap often raises concerns among technical decision-makers looking to escape the constraints of traditional infrastructure:

Capital-intensive capacity planning requiring upfront investments Burdensome annual licensing compliance and audits Inflexible multi-year contractual commitments Over-provisioning to accommodate peak loads

This blog demonstrates how Tata Consultancy Services (TCS) developed an innovative approach to achieve Active-Active functionality for Amazon ElastiCache across multiple AWS regions, bridging the gap between on-premise capabilities and cloud-native services—all while eliminating the need for Enterprise Redis licensing costs. TCS is an AWS Premier Tier Services Partner and Managed Cloud Services Provider (MSP) with Migration Competency. Citigroup's Existing Architecture and Migration Challenge: Citigroup, one of the world's leading financial institutions digital banking systems relied on a dual-region infrastructure in Singapore and Tokyo, using Active-Active Enterprise Redis Cluster for caching user sessions and business rules. The system processed 3 million cache hits hourly (60% read, 40% write), with 70% of reads accessing local data and 30% requiring cross-region access. During their AWS migration, a critical challenge emerged: while their existing Redis Enterprise supported Active-Active configuration across regions, Amazon ElastiCache (mandated by their Enterprise Architecture board) only supported Active-Passive setups. This limitation threatened to increase latency for write operations from the secondary region, potentially impacting their global digital banking services' performance and customer experience. The situation required an innovative solution to maintain global write capabilities within AWS's architecture. Pilot Scope and requirements: The customer's Technology Pilot requirements focus on validating the core Active-Active Redis architecture across regions while incorporating essential production features such as at-rest and in-transit encryption, multi-AZ deployment, cluster mode sharding, and VPC peering configurations as a minimum viable product. While these production-grade features add complexity to baseline performance measurements and introduce some performance overhead, their inclusion provides a realistic assessment of the production-ready solution. The Pilot should be able to prove that it can replicate the active-active geo distribution capability in Amazon ElastiCache Redis without a need to performing any application code refactor. It should achieve the same by deploying Multiple Clusters of Amazon Global ElastiCache Redis ensuring latest copy of data always being stored locally using its native Cross-Region replication feature. Pilot Success Criteria:

Regional Writes: Achieve single digit ms write latency in each geographic region without adding the complexity of multi-AZ, encryption etc. Regional Reads without conflict resolution: All read operations accessing data from the local region's read/write node must complete in less than 10ms, which addresses the 70% of workload that occurs within the same region. Regional Reads with conflict resolution: When data is replicated across regions, multiple versions of the same record may exist simultaneously, accounting for 30% of the total workload. For these cross-region operations, read requests requiring conflict resolution must complete in less than 20ms to maintain application performance. Seamless Failover: In case of ElastiCache failure in any region, the replicated node in the surviving region should be able accept applications Write traffic from another region along with Reads.

Figure1: Cross-Region Active-Active Amazon ElastiCache Architecture Active-Active Amazon ElastiCache Architecture Walk through:

Setup two Amazon ElastiCache for Redis clusters in Active/ Passive mode:

Cluster One (C1): In the Singapore region, C1:Node1 serves as the primary Read/Write node, handling local write operations for Singapore traffic while also providing read access to all data directly written to this region. To ensure data availability across regions, all data written to C1:Node1 in Singapore is asynchronously replicated to C1:Node2 in Tokyo. This replication performed within a range of 5-10 ms enables local read access in the Tokyo region, allowing applications in Tokyo to retrieve Singapore-originated data without the need for cross-region queries, thus optimizing read performance and reducing latency. Cluster Two (C2): In the Tokyo region, C2:Node1 functions as the primary Read/Write node, processing local write operations for Tokyo traffic while providing read access to all data written directly to this region. To maintain data consistency, all data written to C2:Node1 in Tokyo is asynchronously replicated to C2:Node2 in Singapore, enabling local read access in the Singapore region. This replication strategy allows Singapore-based applications to access Tokyo-originated data through local reads, optimizing performance and minimizing cross-region latency.

Write Operations: Write operations are processed by the local Node1 in each region, with data being asynchronously replicated to Node2 of the same cluster in the other region. For example, when a write occurs in Singapore, C1:Node1 (Singapore) processes it and then replicates the data to C1:Node2 (Tokyo). Each write operation includes timestamp metadata, which is crucial for the system's conflict resolution mechanism during read operations. Read Operations and Conflict Resolution: The architecture employs a concurrent dual-node query strategy for read operations, enhancing data consistency across regions. Applications simultaneously query both local nodes in their region (e.g., in Tokyo, C2.Node1 and C1.Node2). To manage conflicts, each data record includes a timestamp field ("ts"). When responses are received, the system compares timestamps, considering the record with the most recent timestamp as authoritative. By automatically selecting the most up-to-date version, the system ensures users receive consistent information regardless of which regional node they query. This approach effectively handles asynchronous replication between regions, providing a coherent global data view. Seamless Failover: In case, of any Amazon ElastiCache failure in any region, the Node2 in other region can be manually promoted to ACTIVE status. The promoted Node2 will then handle both READ and WRITE operations. This failover mechanism ensures business continuity with minimal disruption.

Testing and Validation: Write Propagation Scenarios: When a new record is written in Region-1:

Cluster1:Node1 (Read/ Write) {"id":1, "name":"John", "ts":"20240730″} Cluster1:Node2 (Read) {"id":1, "name":"John", "ts":"20240730″}

Reading from either region returns: {"name": "John", "ts": "20240730"} Update Conflict Resolution: When an update occurs in Region-2:

The system fetches all records with ID:1, compares timestamps, and returns the most recent: {"id":1, "name": "John Doe", "ts": "20240731"} Delete Operations and Data Consistency: When records expire in one or more regions:

The system returns the only surviving record: {"id":1, "name": "John Doe", "ts": "20240731"} Pilot Results: Citigroup's Expectation: Amazon ElastiCache must match or exceed the performance metrics offered by current on-premise Active-Active Enterprise Redis Cluster for successful migration approval. Performance Requirements Comparison:

Regional Read Latency without conflict resolution < 10 ms Met

Regional Read Latency with conflict resolution < 20 ms Met

Business Benefits: 1. Time to Market: The solution built using Amazon ElastiCache Redis enables fast re-platform migration without major code refactoring, improving operational efficiency and market responsiveness. 2. Resilience: Architecture ensures business continuity during regional disruptions through seamless failover capabilities, maintaining enterprise-grade reliability for global banking operations. 3. Architecture Alignment: The architecture complies with Citigroup's Enterprise Architecture standards, enabling smooth migration of critical workloads to AWS without exceptions. 4. Performance: It matches previous Redis Enterprise system's performance, maintaining single-digit ms latency and cross-region data consistency. 5. Cost Efficiency: Eliminates Redis Enterprise licensing fees and related compliance costs. Optimizes resources through auto-scaling, following pay-as-you-go model instead of over-provisioning Conclusion: This innovative approach to implementing Active-Active functionality with Amazon ElastiCache demonstrates how organizations can maintain enterprise-grade capabilities while embracing cloud-native services in lieu of procuring expensive licensed enterprise grade products. For financial institutions and other enterprises with similar requirements, this architecture provides a blueprint for successful migration of mission-critical Redis workloads to AWS. TCS has a proven record of migrating mission-critical applications to AWS with associates who are trained and certified in AWS services implementation. For more information about migrating Enterprise Redis workloads and implementing Active-Active functionality with Amazon ElastiCache on AWS, please contact the TCS team.

Jayadev Sirimamilla Jayadev Sirimamilla is a Digital Lead Principal Engineer & Emerging Tech at Citibank. He brings extensive experience in software engineering, platform architecture, and guiding strategic technology initiatives

Sayan Deb Ghosh Sayan Deb Ghosh is a Director of Solutions Architecture at TCS, specializing in Application and Data modernization on Cloud. He has 20+ years of experience in designing and delivering solutions for complex application transformations. He is focused on enabling AI-led platforms to accelerate cloud modernization. Outside of work, Sayan is interested in hiking, playing Table Tennis and demystifying data

Dibyarup Basu Dibyarup Basu is a Solution Architect at TCS, specializing in Cloud Modernization. With over 20 years of experience, Dibyarup has led solution architecture and design for cloud-native backend microservices and integration stacks across diverse geographies and domains. Outside of consulting, he enjoys Indian classical music, playing cricket, and spending quality time with family and friends.

Sanjay Gupta Sanjay Gupta is a Principal Solution Architect managing TCS partnerships worldwide. With over 25 years of experience in technology and industry, he brings a unique blend of domain knowledge and technical leadership. Sanjay specializes in migrations, modernization, and large-scale transformation initiatives across multiple industries, including financial services and the travel sector. Outside of work, Sanjay enjoys hiking, watching cricket, and exploring new places.

Today, Microsoft and the US General Services Administration (GSA) announced a comprehensive agreement to bring a suite of productivity, cloud and AI services, including Microsoft 365 Copilot at no cost for up to 12 months for millions of existing Microsoft G5 users, to help agencies rapidly adopt secure and compliant advanced AI tools that will enhance operations, strengthen security and accelerate innovation for the American people. As an unparalleled milestone in advancing GSA's OneGov strategy, Microsoft's offerings will be available through a governmentwide unified pricing strategy that is expected to drive $3 billion in cost savings in the first year alone.

This expansive offering will help agencies achieve key pillars of the America's AI Action Plan by enabling federal agencies to serve at the forefront on driving AI innovation and adoption in service to the American people. Through this agreement federal agencies will access the latest AI capabilities at scale, now integrated in many of the products they already use, to achieve key administration priorities:

Transforming productivity with AI: A unique Microsoft 365 and Copilot suite, offered exclusively to the federal government, enables agencies to automate workflows, analyze data and collaborate more efficiently, freeing public servants to focus on their core mission. Driving automation with AI agents: With AI agents, and no per-agent fees, agencies can build solutions for citizen inquiries, case management and contact centers, extending the reach and responsiveness of government services. Accelerating cloud modernization: With significant Azure discounts and the waiving of data egress fees, agencies can modernize infrastructure, reduce barriers to interagency collaboration and unlock the full power of advanced analytics and AI. Streamlining government operations: Dynamics 365 applications help agencies enhance citizen service, optimize supply chains and increase field responsiveness, directly impacting everyday public outcomes. Strengthening security across all levels: Integrated platforms such as Microsoft Entra ID and Sentinel provide advanced identity and threat protection, supporting the Zero Trust journey across federal environments.

Federal agencies can opt-in to any or all of these offers through September 2026, with discounted pricing available for up to 36 months.

Agencies can quickly adopt these solutions knowing these services have already achieved key FedRAMP security and compliance authorizations, meeting more than 400 critical security controls established in NIST 800-53 standards. Microsoft 365, Azure and our key AI services are authorized at FedRAMP High. Microsoft 365 Copilot received provisional authorization from the US Department of Defense, with FedRAMP High expected soon.

Our commitment goes beyond technology and savings. Microsoft is also committing $20 million in additional support services to help agencies implement the offers and maximize the value of these services, along with complimentary cost-optimization workshops that will enable agencies to identify opportunities to reduce software duplication, automate services and improve cross-team interoperability. These investments reflect our belief that technology's greatest value lies in its ability to empower people.

Taken together, we anticipate these services have the potential to deliver more than $6 billion in total estimated value over three years.

For more than four decades, Microsoft has been privileged to support the US government's most vital missions. Today, as we stand at the forefront of the AI era, we reaffirm our dedication to serving as a trusted partner — one that listens, innovates responsibly and shares in the mission to advance the nation's public good. We look forward to the next chapter helping agencies harness secure AI and cloud solutions to build a stronger, more resilient and more innovative future for all.

To learn how to take advantage of these offers, contact your Microsoft representative or authorized reseller*. For any additional questions, you can email our Microsoft OneGov team.

*Microsoft OneGov offers are applicable to Microsoft federal customers with Enterprise Agreements and exclude AOS-G and CSP programs; Azure Consumption Discounts and waived egress fees applicable to select Governmentwide Acquisition Contracts.

Tags: Azure, Copilot, Dynamics 365, Entra ID, Microsoft 365, Microsoft 365 Copilot, Sentinel

Over the past fiscal year, our customers and partners have driven pragmatic outcomes by implementing AI-first strategies across their organizations. With AI Transformation as their framework, we helped them enrich employee experiences, reinvent customer engagement, reshape business processes and bend the curve on innovation for their people, businesses and industries. Now, we are partnering to go beyond what they thought possible to unlock even greater potential by restructuring and centralizing their business strategies with an AI-first mindset. Our cloud and AI capabilities are leading the industry, and we are committed to working closely with our customers and partners to meet their increasingly complex needs and help them become frontier AI firms.

Below are several stories from the past quarter reflecting the success we have seen broadly this past year. Each showcases what we can achieve together with an approach grounded in AI business solutions, cloud and AI platforms, and security.

AI is blurring the lines between personal and organizational productivity, and we are helping our customers leverage Copilots and agents combined with human ambition to create differentiation.

With over one million customers in Argentina, Banco Ciudad launched a digital transformation initiative focused on AI, productivity and security. What began as a pilot program quickly grew into broad adoption with the bank implementing Microsoft 365 Copilot to improve productivity, Microsoft Copilot Studio to develop agents and Microsoft Azure to scale their AI solutions. As a result, the bank strengthened its operational resilience, empowered teams, drove sustainable growth and improved customer engagement — even in a challenging economic environment. So far, the bank has freed up 2,400 employee work hours annually with savings projected to generate $75,000 USD monthly.

As one of the country's largest financial institutions, Commonwealth Bank of Australia is harnessing AI to meet rising customer expectations by developing smarter, more secure and highly customizable banking experiences at scale. To ensure employees have the confidence and expertise to leverage AI effectively, the bank launched a structured skilling initiative to empower them with the knowledge needed to adopt AI effectively. Eighty-four percent of 10,000 Microsoft 365 Copilot users reported they would not go back to working without it; and nearly 30% of GitHub Copilot code suggestions were adopted, driving efficiency and smarter decision-making.

Nonprofit Make-A-Wish is dedicated to granting hope by fulfilling wishes for children with critical illnesses across the United States. Fragmented systems, limited data access and the need to protect sensitive information were creating challenges for the organization to operate effectively, so it turned to partner Redapt for support. Make-A-Wish deployed comprehensive Microsoft cloud and AI solutions — including Azure Cloud Services, Microsoft Fabric, Microsoft 365 Copilot and Copilot Studio — to unify its data, rebuild core applications and boost staff productivity. This transformation enabled the organization to increase operational efficiency, improve collaboration across national and regional chapters and strengthen its data security to protect sensitive family data.

Sheló NABEL, a wellness and beauty company based in Mexico, faced operational challenges as it expanded its network of independent entrepreneurs. With support from partner Best Practices Consulting, the company integrated Microsoft Dynamics 365 to gain real-time market insights and optimize its demand planning across more than 400 products. They also integrated Microsoft Copilot to enhance customer service and increase operational efficiency with AI. As a result, the company has achieved a 17% increase in sales, 5X faster reporting processes and real-time inventory control.

Based in Saudi Arabia, technology and communications company Unifonic serves millions of people across 160 countries. As their business began to scale rapidly, they faced challenges managing a growing hybrid workforce while maintaining strong security and compliance standards. To solve this, the company deployed Microsoft 365 E5 and Microsoft 365 Copilot to automate workflows and secure data in one platform. This unified ecosystem enabled teams to reduce time spent on audits by 85%, save two hours per day on cybersecurity governance, save $250,000 USD in costs and reduce time to set up client demos by 15%.

Microsoft has the leading cloud platform for AI innovation with Azure as the infrastructure, Azure AI Foundry as the applications server and Fabric as the data platform.

As legal professionals face challenges with manual data entry, document generation and compliance-heavy processes, Assembly Software aimed to transform how they handle complex, time-consuming workflows. Using Azure AI Foundry, the company built NeosAI — a fully embedded generative AI solution that automates nearly every aspect of the legal workflow — from document intake to drafting and reporting. As a result, law firms using NeosAI report saving up to 25 hours per case, with document drafting time reduced from 40 hours to just minutes. This AI solution is not only boosting productivity and reducing stress for legal professionals but also enabling firms to serve more clients with greater speed and accuracy.

One of the world's oldest continuously operating companies, Husqvarna Group, faced increasing pressure to modernize its network of factories, supply chains and distribution channels to stay competitive in a rapidly evolving landscape. The company implemented a comprehensive Microsoft Azure solution — including Azure Arc, Azure IoT Operations and Azure OpenAI — to unify cloud and on-premises systems, enable real-time data insights and drive innovation across global manufacturing operations. As a result, the company achieved a 98% reduction in data deployment time, cut infrastructure imaging costs by 50% and significantly improved productivity and uptime across its connected factories.

Serving over 600,000 members in the United States, Members 1st Federal Credit Union sought to modernize its data infrastructure to deliver more personalized member experiences and support data-driven decision-making. The credit union faced challenges with siloed data across more than 15 sources and legacy systems with limited analytics capabilities. With support from partner 3Cloud, the credit union combined Azure SQL, Azure Data Factory and Azure Databricks to extract, log and centralize enterprise-wide data into a cutting-edge data lakehouse. Machine learning models that took 36 hours to run can now be done in three to four hours — a reduction of about 89%. Additionally, updates within its customer relationship management software now take 30 to 40 minutes compared to three to four hours previously.

NTT DATA, a global IT and business services leader headquartered in Japan, sought to accelerate decision-making and unlock deeper insights by overcoming limitations of legacy dashboards and siloed data systems. Serving clients in over 50 countries, the company needed a more intuitive, scalable and AI-driven approach to data access and analysis. NTT DATA deployed Microsoft Fabric, Azure AI Foundry Agent Service and Azure AI Foundry to enable the creation of conversational AI tools that allow employees to retrieve and act on real-time data. This agentic AI platform significantly improved productivity, reduced time to market for new solutions by 50% and laid the foundation for broader adoption of multi-agent frameworks across the organization.

University of Venda, a public higher education institution in South Africa, modernized its IT infrastructure to support its strategic goal of producing globally competitive graduates while meeting the evolving needs of its regional and international students. Facing challenges with aging on-premises servers, frequent service interruptions and a six-month hardware procurement cycle, the university sought a more agile and reliable solution. By deploying Microsoft 365, migrating 18 systems to Microsoft Azure and leveraging Microsoft Unified Support, the university achieved 99% service uptime and reduced resource provisioning time from six months to under 12 hours. This transformation significantly improved system reliability, scalability and security, enabling students and staff to access essential services seamlessly from anywhere.

Security is the foundation for AI Transformation. We are helping our customers and partners defend against threat actors and secure their environments with Microsoft's cloud and AI solutions.

Following its spinoff from Eli Lilly, Elanco sought to modernize its IT and security infrastructure by building a secure, scalable digital environment from the ground up. The company deployed a comprehensive Microsoft solution stack comprised of Microsoft 365 E5, Microsoft Defender suite, Microsoft Sentinel, Microsoft Intune and Microsoft Security Copilot. This integrated approach streamlined global IT operations across 90 countries, enabling Elanco to accelerate security response times by 50% with a future-ready, secure and efficient digital ecosystem that empowers their workforce.

Kern County faced significant challenges securing and governing their data across 40 departments — each operating with fragmented IT systems. With an approach grounded in data protection, the county deployed Microsoft Purview as part of its Microsoft 365 Government G5 suite. The implementation resulted in the classification of over 13 million files, near-total adoption of sensitivity labels and over 3,000 data loss prevention alerts in a single month. A validation assessment also showed the county saved about $1 million in mitigation risk and potential noncompliance. This transformation strengthened audit readiness, reduced data exposure risks and laid a secure foundation for future innovation.

Headquartered in the Czech Republic, Mews provides cloud-based property management solutions to help modernize hotel operations worldwide. As it scaled its platform to serve thousands of properties worldwide, the company faced increasing cybersecurity threats and sought to strengthen its security posture and streamline threat detection. By deploying Microsoft Sentinel, Microsoft Defender for Cloud, Microsoft Entra ID and Microsoft Security Copilot, they enabled real-time monitoring, automated threat response and centralized visibility across its cloud infrastructure. As a result, the company reduced incident response times from hours to minutes and enhanced its ability to meet stringent compliance requirements to ensure secure growth in a highly regulated industry.

Puritan Life Insurance Company of America transformed its business model by launching Canvas Annuity — a direct-to-consumer digital distribution platform built entirely on Microsoft Azure. With the security features built into the technology out of the box, the team can detect and block malware attacks and threats, manage security efficiently and more easily pass regulatory, financial and information security audits. By creating a secure, scalable and user-friendly platform, customers can now purchase annuities online in just 10 minutes — a speed previously unheard of. Since launching, the company has seen a 700% increase in annual premium revenue, with the platform now accounting for 75% of premiums every year.

Facing growing cyber threats to its research, Singapore Management University needed to maintain seamless access for students and faculty while ensuring compliance with the country's stringent data privacy regulations. The university implemented Microsoft's Zero Trust framework by integrating Microsoft Security Copilot with Microsoft's comprehensive security suite to strengthen protection while maintaining academic accessibility. This helped reduce response times and ease workloads for security teams while also supporting security analysts to streamline incident investigations, summarize complex multistage attacks and receive guided response recommendations in real time. The integrated security solution reduced operational costs and enhanced compliance reporting while maintaining seamless access to research resources.

For additional AI Transformation stories, check out AI-powered success—with more than 1,000 stories of customer transformation and innovation.

I could not be more excited by our mission than I am today — and I believe we have never been closer to bringing it to life than we are right now. With Microsoft's leading technology and expertise, we are helping our customers and partners implement AI to bring out the best in individuals, organizations and companies. We are partnering to reinvent their business strategies and move beyond AI adoption alone to innovate with purpose and shape their futures as frontier AI firms. The opportunity to differentiate your business through AI Transformation and pave the way for industry leadership is now.

Tags: AI, Azure AI Foundry, Azure Cloud Services, Azure Data Factory, Azure Databricks, Azure OpenAI, Microsoft 365, Microsoft 365 Copilot, Microsoft Azure, Microsoft Copilot Studio, Microsoft Dynamics 365, Microsoft Entra ID, Microsoft Fabric, Security Copilot

Satya Nadella, Chairman and CEO, shared the below communication with Microsoft employees this morning.

As we begin a new fiscal year, I've been reflecting on the road we've traveled together and the path ahead.

Before anything else, I want to speak to what's been weighing heavily on me, and what I know many of you are thinking about: the recent job eliminations. These decisions are among the most difficult we have to make. They affect people we've worked alongside, learned from, and shared countless moments with—our colleagues, teammates, and friends.

I want to express my sincere gratitude to those who have left. Their contributions have shaped who we are as a company, helping build the foundation we stand on today. And for that, I am deeply grateful.

I also want to acknowledge the uncertainty and seeming incongruence of the times we're in. By every objective measure, Microsoft is thriving—our market performance, strategic positioning, and growth all point up and to the right. We're investing more in CapEx than ever before. Our overall headcount is relatively unchanged, and some of the talent and expertise in our industry and at Microsoft is being recognized and rewarded at levels never seen before. And yet, at the same time, we've undergone layoffs.

This is the enigma of success in an industry that has no franchise value. Progress isn't linear. It's dynamic, sometimes dissonant, and always demanding. But it's also a new opportunity for us to shape, lead through, and have greater impact than ever before.

The success we want to achieve will be defined by our ability to go through this difficult process of "unlearning" and "learning." It requires us to meet changing customer needs, by continuing to maintain and scale our current business, while also creating new categories with new business models and a new production function. This is inherently hard, and few companies can do both.

But I have full confidence that we can, and we will once again find the resolve, courage, and clarity to deliver on our mission in this new paradigm.

With that context, I want to re-ground ourselves in our why, what, and how: our mission, our priorities, and our culture.

What does achieving our mission look like and feel like for us as a company? When Microsoft is succeeding, the world around us must succeed too. This is why each of us chose to be here, and as a company it's how we earn our social permission to operate. When Bill founded Microsoft, he envisioned not just a software company, but a software factory, unconstrained by any single product or category. That idea has guided us for decades. But today, it's no longer enough.

We must reimagine our mission for a new era. What does empowerment look like in the era of AI? It's not just about building tools for specific roles or tasks. It's about building tools that empower everyone to create their own tools. That's the shift we are driving—from a software factory to an intelligence engine empowering every person and organization to build whatever they need to achieve.

Just imagine if all 8 billion people could summon a researcher, an analyst, or a coding agent at their fingertips, not just to get information but use their expertise to get things done that benefit them. And consider how organizations, empowered with AI, could unlock entirely new levels of agility and innovation by transforming decision-making, streamlining operations, and enabling every team to achieve more together than ever before.

That's the empowerment our mission enables, creating local surplus in every company, community, and country. And that's our opportunity ahead.

To deliver on our mission, we need to stay focused on our three business priorities: security, quality, and AI transformation.

We are doubling down on the fundamentals while continuing to define new frontiers in AI.

Security and quality are non-negotiable. Our infrastructure and services are mission critical for the world, and without them we don't have permission to move forward.

We've made substantial progress across SFI, QEI, and Engineering Thrive this year, and they remain top priorities to ensure that we continuously improve our innovation velocity and our operational metrics.

We will reimagine every layer of the tech stack for AI—infrastructure, to the app platform, to apps and agents. The key is to get the platform primitives right for these new workloads and for the next order of magnitude of scale. Our differentiation will come from how we bring these layers together to deliver end-to-end experiences and products, with the core ethos of a platform company that fosters ecosystem opportunity broadly. Getting both the product and platform right for the AI wave is our North Star!

Our performance this past year has positioned us well. And we must move forward with the intentionality and intensity that these industry shifts demand.

Growth mindset has served us well over the last decade—the everyday practice of being a learn-it-all, not a know-it-all. It has reshaped our culture and helped us lead with greater humility and empathy. We need to keep that.

It starts with each of us as individuals and our personal drive to learn, improve, and get better every day. Professional rewards, growth, and pride in our craft will always be the prime drivers. Beyond that, we each have the opportunity to connect our personal passion and philosophy of how we derive meaning from the work we do with Microsoft's mission to empower the world. This is what makes it all worthwhile.

This platform shift is reshaping not only the products we build and the business models we operate under, but also how we are structured and how we work together every day. It might feel messy at times, but transformation always is. Teams are reorganizing. Scopes are expanding. New opportunities are everywhere. It reminds me of the early '90s, when PCs and productivity software became standard in every home and every desk! That's exactly where we are now with AI.

Years from now, when you look back at your time here, I hope you'll say: "That's when I learned the most. That's when I made my biggest impact. That's when I was part of something transformational."

What we've learned over the past five decades is that success is not about longevity. It's about relevance. Our future won't be defined by what we've built before, but by what we empower others to build now.

And I know that with your dedication, drive, and hard work we can go win together, and change the world in the process.

I look forward to sharing more at Earnings next week and addressing your questions at our next Town Hall.

TL;DR? Hear the news as an AI-generated audio overview made using Microsoft 365 Copilot. You can read the transcript here.

Audio Player Up/Down Arrow keys to increase or decrease volume. We've entered the era of AI agents. Thanks to groundbreaking advancements in reasoning and memory, AI models are now more capable and efficient, and we're seeing how AI systems can help us all solve problems in new ways. For example, 15 million developers are already using GitHub Copilot, and features like agent mode and code review are streamlining the way they code, check, deploy and troubleshoot. Hundreds of thousands of customers are using Microsoft 365 Copilot to help research, brainstorm and develop solutions, and more than 230,000 organizations — including 90% of the Fortune 500 — have already used Copilot Studio to build AI agents and automations. Companies like Fujitsu and NTT DATA are using Azure AI Foundry to build and manage AI apps and agents that help prioritize sales leads, speed proposal creation and surface client insights. Stanford Health Care is using Microsoft's healthcare agent orchestrator to build and test AI agents that can help alleviate the administrative burden and speed up the workflow for tumor board preparation. Developers are at the center of it all. For 50 years Microsoft has been empowering developers with tools and platforms to turn their ideas into reality, accelerating innovation at every stage. From AI-driven automation to seamless cloud integration and more, it's exciting to see how developers are fueling the next generation of digital transformation. So, what's next? We envision a world in which agents operate across individual, organizational, team and end-to-end business contexts. This emerging vision of the internet is an open agentic web, where AI agents make decisions and perform tasks on behalf of users or organizations. At Microsoft Build we're showing the steps we're taking to make this vision a reality through our platforms, products and infrastructure. We're putting new models and coding agents in the hands of developers, introducing enterprise-grade agents, making our platforms like Azure AI Foundry, GitHub and Windows the best places to build, embracing open protocols and accelerating scientific discovery with AI, all so that developers and organizations can go invent the next big thing. Here's a glimpse at just a few of the announcements today: Reimagining the software development lifecycle with AI AI is fundamentally shifting how code is written, deployed and maintained. Developers are using AI to stay in the flow of their environment longer and to shift their focus to more strategic tasks. And as the software development lifecycle is being transformed, we're providing new features across platforms including GitHub, Azure AI Foundry and Windows that enable developers to work faster, think bigger and build at scale.

GitHub Copilot coding agent and new updates to GitHub Models: GitHub Copilot is evolving from an in-editor assistant to an agentic AI partner with a first-of-its-kind asynchronous coding agent integrated into the GitHub platform. We're adding prompt management, lightweight evaluations and enterprise controls to GitHub Models so teams can experiment with best-in-class models, without leaving GitHub. Microsoft is also open-sourcing GitHub Copilot Chat in VS Code. The AI-powered capabilities from GitHub Copilot extensions will now be part of the same open-source repository that drives the world's most popular development tool. As the home of over 150 million developers, this reinforces our commitment to open, collaborative, AI-powered software development. Learn more about GitHub Copilot updates.

Introducing Windows AI Foundry: For developers, Windows remains one of the most open and widely used platforms available, with scale, flexibility and growing opportunity. Windows AI Foundry offers a unified and reliable platform supporting the AI developer lifecycle across training and inference. With simple model APIs for vision and language tasks, developers can manage and run open source LLMs via Foundry Local or bring a proprietary model to convert, fine-tune and deploy across client and cloud. Windows AI Foundry is available to get started today. To learn more visit our Windows Developer Blog.

Azure AI Foundry Models and new tools for model evaluation: Azure AI Foundry is a unified platform for developers to design, customize and manage AI applications and agents. With Azure AI Foundry Models, we're bringing Grok 3 and Grok 3 mini models from xAI to our ecosystem, hosted and billed directly by Microsoft. Developers can now choose from more than 1,900 partner-hosted and Microsoft-hosted AI models, while managing secure data integration, model customization and enterprise-grade governance. We're also introducing new tools like the Model Leaderboard, which ranks the top-performing AI models across different categories and tasks, and the Model Router, designed to select an optimal model for a specific query or task in real-time. Read more about Azure AI Foundry Models.

Making AI agents more capable and secure AI agents are not only changing how developers build, but how individuals, teams and companies get work done. At Build, we're unveiling new pre-built agents, custom agent building blocks, multi-agent capabilities and new models to help developers and organizations build and deploy agents securely to help increase productivity in meaningful ways.

With the general availability of Azure AI Foundry Agent Service, Microsoft is bringing new capabilities to empower professional developers to orchestrate multiple specialized agents to handle complex tasks, including bringing Semantic Kernel and AutoGen into a single, developer-focused SDK and Agent-to-Agent (A2A) and Model Context Protocol (MCP) support. To help developers build trust and confidence in their AI agents, we're announcing new features in Azure AI Foundry Observability for built-in observability into metrics for performance, quality, cost and safety, all incorporated alongside detailed tracing in a streamlined dashboard. Learn more about how to deploy enterprise-grade AI agents in Azure AI Foundry Service.

Discover, protect and govern in Azure AI Foundry: With Microsoft Entra Agent ID, now in preview, agents that developers create in Microsoft Copilot Studio or Azure AI Foundry are automatically assigned unique identities in an Entra directory, helping enterprises securely manage agents right from the start and avoid "agent sprawl" that could lead to blind spots. Apps and agents built with Foundry further benefit from Purview data security and compliance controls. Foundry also offers enhanced governance tools to set risk parameters, run automated evaluations and receive detailed reports. Learn more about Microsoft Entra Agent ID and Azure AI Foundry integrations with Microsoft Purview Compliance Manager.

Introducing Microsoft 365 Copilot Tuning and multi-agent orchestration: With Copilot Tuning, customers can use their own company data, workflows and processes to train models and create agents in a simple, low-code way. These agents perform highly accurate, domain-specific tasks securely from within the Microsoft 365 service boundary. For example, a law firm can create an agent that generates documents aligned with its organization's expertise and style. Additionally, new multi-agent orchestration in Copilot Studio connects multiple agents, allowing them to combine skills and tackle broader, more complex tasks. Check out the Microsoft 365 blog to learn how to access these new tools as well as the Microsoft 365 Copilot Wave 2 spring release, which has moved to general availability and begins rolling out today.

Supporting the open agentic web To realize the future of AI agents, we're advancing open standards and shared infrastructure to provide unique capabilities for customers.

Supporting Model Context Protocol (MCP): Microsoft is delivering broad first-party support for Model Context Protocol (MCP) across its agent platform and frameworks, spanning GitHub, Copilot Studio, Dynamics 365, Azure AI Foundry, Semantic Kernel and Windows 11. In addition, Microsoft and GitHub have joined the MCP Steering Committee to help advance secure, at-scale adoption of the open protocol and announced two new contributions to the MCP ecosystem, an updated authorization specification, which enables people to use their existing trusted sign-in methods to give agents and LLM-powered apps access to data and services such as personal storage drives or subscription services, and the design of an MCP server registry service, which allows anyone to implement public or private, up-to-date, centralized repositories for MCP server entries. Check out the GitHub repository. As we expand our MCP capabilities, our top priority is to ensure we're building upon a secure foundation. To learn more about this approach see: Securing the Model Context Protocol: Building a Safe Agentic Future on Windows.

A new open project called NLWeb: Microsoft is introducing NLWeb, which we believe can play a similar role to HTML for the agentic web. NLWeb makes it easy for websites to provide a conversational interface for their users with the model of their choice and their own data, allowing users to interact directly with web content in a rich, semantic manner. Every NLWeb endpoint is also an MCP server, so websites can make their content easily discoverable and accessible to AI agents if they choose. Learn more here.

Accelerating scientific discovery with AI Science may be one of the most important applications of AI, helping to tackle humanity's most pressing challenges, from drug discovery to sustainability. At Build we're introducing Microsoft Discovery, an extensible platform built to empower researchers to transform the entire discovery process with agentic AI, helping research and development departments across various industries accelerate the time to market for new products and accelerate and expand the end-to-end discovery process for all scientists. Learn more here. This is only a small selection of the many exciting features and updates we will be announcing at Build. We're looking forward to connecting with those who have registered to join us virtually and in-person, for keynote sessions, live code deep dives, hack sessions and more — much of which will be available on demand. Plus, you can get more on all these announcements by exploring the Book of News, the official compendium of all today's news. Tags: AI, Azure AI, Azure AI Foundry, Book of News, GitHub, GitHub Copilot, Microsoft 365 Copilot, Microsoft Copilot, Microsoft Purview

The role of agentic AI has grown rapidly over the past several months as organizational leaders seek ways to accelerate AI Transformation. We firmly believe that Agents + Copilot + Human Ambition can deliver real AI differentiation for our customers. By putting the autonomous capabilities of an agent to work for their businesses, our customers are unlocking AI opportunity to realize greater value. The recent introduction of Microsoft 365 Copilot Chat is delivering on our promise of "Copilot for all" by providing frontline workers with a free, secure and enterprise-ready AI chat interface. Our customers are building their own custom agents with the no-code, low-code features of Microsoft Copilot Studio, allowing citizen and professional developers to extend the capabilities of Copilot and deliver on the unique needs of their industry. We also offer the best prebuilt agent framework right out-of-the-box, such as Sales Agent that works autonomously to help sellers build pipeline and close more deals with greater speed. Similarly, we recently announced general purpose reasoning agents — such as Researcher and Analyst — and invite all of our Microsoft 365 Copilot users to try these in their environments.

It is exciting to see how agents are driving pragmatic AI innovation for our customers by increasing productivity, creating capacity across every role and function and improving business processes. Below are a few highlights from the past quarter that underscore the impact of an agentic AI approach — from improving employee experiences to streamlined workflows and significant cost savings.

Agentic service management software provider Atomicwork leveraged Azure AI Foundry to create Atom — an AI agent that transforms the digital workplace experience for employees and automates service delivery. Adopters of this agentic management platform recognize significant benefits, such as reduced operational costs and increased employee satisfaction, with one customer achieving a 65% deflection rate within six months of implementation and projections of 80% by the end of the year. Integration within Microsoft Teams and other enterprise tools have further streamlined service delivery, allowing employees easier access to information and support. The company's AI-driven approach has resulted in a 20% increase in accuracy and 75% reduction in response latency when compared to competing solutions.

To support employees as they manage the high demand of internal requests and to create a more satisfying work environment, BDO Colombia used Copilot Studio and Power Platform to develop BeTic 2.0 — an agent that centralizes and automates key payroll and finance processes. The agent reduced operational workload by 50%, optimized 78% of internal processes and showed 99.9% accuracy in managed requests. It also helped reduce duplicative work, optimized workflows, improved the employee-client experience and continues to serve as a competitive differentiator for the company in the market.

Dow is using agents to automate the shipping invoice analysis process and streamline its global supply chain to unlock new efficiencies and value. Receiving more than 100,000 shipping invoices via PDF each year, Dow built an autonomous agent in Copilot Studio to scan for billing inaccuracies and surface them in a dashboard for employee review. Using Freight Agent — a second agent built in Copilot Studio — employees can investigate further by "dialoguing with the data" in natural language. The agents are helping employees solve the challenge of hidden losses autonomously within minutes rather than weeks or months. Dow expects to save millions of dollars on shipping costs through increased accuracy in logistic rates and billing within the first year.

As a leading provider of sustainable energy in Belgium, Eneco serves over 1.5 million customers. Facing performance issues with their existing chatbot, Eneco developed a new AI-driven agent using the no-code, graphical interface in Copilot Studio. This multilingual agent was deployed on the company website in just three months, integrating seamlessly with its live chat platform. The new agent manages 24,000 chats per month — an increase of 140% over the previous solution — and resolves 70% more customer conversations without a handoff to a live representative. For requests that do require escalation, the agent provides an AI-generated summary of the conversation for a more optimized call center experience.

To reimagine trend forecasting and consumer marketing, The Estée Lauder Companies Inc. leveraged Copilot Studio to develop ConsumerIQ — an agent that centralizes and streamlines consumer data to enable instant access to actionable insights. Using natural language prompts, the agent reduced the time required for marketers to gather data from hours to seconds, while accelerating decision-making and helping prevent duplicated research. Together with Azure OpenAI Service and Azure AI Search, teams can gather data, identify trends, build marketing assets, inform research and move products to market faster.

To create proposals and streamline knowledge retrieval and organization, Fujitsu leveraged Azure AI Agent Service within Azure AI Foundry to develop an intelligent, scalable AI agent for sales automation. The agent boosted productivity of sales teams by 67% while addressing knowledge gaps and allowing them to build stronger customer relationships. This transformation allowed teams to shift from time-intensive tasks to strategic planning and customer relationship building, while also supporting new hires with product information and strategic guidance.

To reduce manual tasks and help employees deliver exceptional experiences, global baker Grupo Bimbo established its first ever technology Center of Excellence. Using Power Platform solutions and Copilot Studio, teams created 7,000 power apps, 18,000 processes and 650 agents to reduce busy work and enhance consumer service. By automating low-value tasks, the company saved tens of millions of dollars annually in development efforts and operational efficiencies. Grupo Bimbo also migrated to Azure for its AI capabilities, scalability, security and rapid time to market for apps.

KPMG developed Comply AI — an agent that helps identify environment, social and governance compliance. Using Microsoft AI technologies, the agent helps identify relevant obligations, generate statements in natural language, assess control effectiveness and redraft control descriptions. This has already helped one of its customers achieve 70% improvement in Controls and Risks descriptions, an 18-month reduction in compliance program timelines and a 50% cut in ongoing compliance efforts. KPMG is also using an agent to support new hires by providing templates and historical references to speed up the onboarding process and reduce follow-up calls by 20%.

To significantly enhance its customer service operations, T-Mobile used Power Apps to develop PromoGenius — an app that combines promotional data from multiple systems and documents to keep frontline retail employees equipped with the latest promotional information for customers. Using Copilot Studio, the company embedded an agent in the app so customer service representatives can instantly search for technical details from device manufacturers and create a customer-facing view of product information in a fraction of the time a manual search would require. PromoGenius is the second most-used app in the company, with 83,000 unique users and 500,000 launches a month.

Using Copilot Studio, Virgin Money developed Redi — an agent serving as a digital host within a mobile app for credit card customers. The agent, trained to understand colloquialisms and even known to tell jokes, serves as a secure way for customers to get answers quickly while understanding appropriate context for when a live representative is required. The company views this agent as a tool for its employees to better serve customers, handling over one million interactions, boosting customer satisfaction and becoming one of the bank's top-rated service channels. Redi now supports customers across Virgin Money's digital platforms and has been recognized with an industry award for AI in financial services.

To help employees navigate countless procedures, evolving regulations and complex banking systems, Wells Fargo built an agent through Teams to ensure fast and accurate customer support. Using large language models, the agent provides instant access to guidance on 1,700 internal procedures across 4,000 bank branches. Employees can now locate needed information faster without support from a colleague, with 75% of searches happening through the agent and response times reduced from 10 minutes to 30 seconds.

There is immense potential for agents to drive AI-first differentiation for organizations everywhere, especially when combined with Copilot and human ambition. At Microsoft, we believe AI is about empowering human achievement, unlocking potential and democratizing intelligence for as many people as possible with our cloud and AI solutions — as evidenced by these AI Transformation stories of more than 700 customers and partners. I look forward to partnering with you to unlock continued AI opportunity, drive pragmatic innovation and realize meaningful business impact for your organization.

Tags: AI, Azure AI Agent Service, Azure AI Foundry, Azure AI Search, Copilot, Copilot Studio, Microsoft 365 Copilot Chat, Microsoft Azure OpenAI Service, Microsoft Teams, Power Platform, Researcher and Analyst, Sales Agent

We are entering a new reality — one in which AI can reason and solve problems in remarkable ways. This intelligence on tap will rewrite the rules of business and transform knowledge work as we know it. Like the Industrial Revolution and the internet era, this transformation will take decades to show its full promise — and will bring broad technological, societal and economic change.

The 2025 Work Trend Index Annual Report is designed to prepare leaders and employees for this shift. As in previous years, it draws on a large global survey, Microsoft 365 telemetry and LinkedIn hiring and labor trends. New this year are insights from AI-native startups, economists, scientists and academics — all pointing to a seismic shift already underway. In fact, 82% of leaders say this is a pivotal year to rethink core aspects of strategy and operations.

The data reveals the emergence of a new kind of organization: the Frontier Firm — built around intelligence on tap, human-agent teams and a new role for everyone: agent boss. While the shift ahead is profound, human ambition, creativity and ingenuity will continue to create new economic value and opportunity. In fact, 71% of workers at these firms say their company is thriving, compared to just 37% globally.

Imagine if you knew, before the internet took off, just how much it would reshape business. That's where we are with AI. Intelligence is no longer bound by headcount or expertise. It's an essential durable good: abundant, affordable and scalable on-demand. As economic and shareholder pressures grow, this on-demand intelligence offers a new lever for growth — one that can close the growing gap between business demands and human capacity. While 53% of leaders say productivity must increase, 80% of the global workforce reports lacking the time or energy to do their job. And on average, employees are interrupted by a meeting, email or ping every 2 minutes. To bridge this Capacity Gap, 82% of leaders expect to use digital labor to expand their workforce in the next 12 to 18 months.

Digital labor will spur the reinvention of even the most established firms — and the birth of new companies we haven't even thought of yet. On LinkedIn, top AI startups are hiring at 2X the rate of Big Tech. Much of that talent is flowing out of Big Tech and staying in the startup world, pointing to a deeper shift where innovation — and opportunity — are rising. As incumbents adapt and challengers scale, like we saw in the .com boom, the rules of talent and competition are being rewritten in real time.

As AI continues to democratize expertise, we're seeing a move from rigid org charts to more fluid, outcome-driven Work Charts. These structures flex with the needs of the business, drawing on the right mix of humans and agents to get the job done. While every function will evolve at a different pace and scale, 46% of leaders say their organization is using agents to fully automate workstreams or business processes — with customer service, marketing and product development as the top AI investment priorities.

To maximize the impact of these human-agent teams, organizations need a new metric: the human-agent ratio. Leaders must ask two critical questions: How many agents are needed for which roles and tasks? And how many humans are needed to guide them? Getting that ratio right will be critical — and task-specific. Organizations will need to consider if there are times when human and digital labor outperform AI alone, when customers prefer a human touch or when society expects people to be responsible for the consequences — like a high-stakes product or finance decision. Whether it's a customer conversation, a strategic decision or a product launch, knowing how to staff the right mix of humans and agents will define how work gets done — and how success is measured.

As agents increasingly join the workforce, we'll see the rise of the agent boss: someone who builds, delegates to and manages agents to amplify their impact and take control of their career in the age of AI. From the boardroom to the frontline, every worker will need to think like the CEO of an agent-powered startup. In fact, leaders expect their teams will be training (41%) and managing (36%) agents within five years.

For those ready to lean in, AI will be a career accelerator — but leaders are ahead. We measured the agent boss mindset across seven indicators — from regular use and trust to career impact. Leaders outpace employees on every measure: 67% are familiar with agents (vs. 40% of employees) and 79% believe AI will accelerate their careers (vs. 67%). But this shift won't stop at the top. As agents become embedded into daily work, roles across every level and function will evolve — along with the broader workforce. While 33% of leaders are considering headcount reductions, 78% are considering hiring for new AI roles. And 83% say AI will enable employees to take on more complex and strategic work earlier in their careers.

This shift is multifaceted — every industry and role will evolve differently as the technology diffuses across business and society. Just as the internet era created billions of new knowledge jobs — from social media managers to UX designers — the AI era is already giving rise to new roles, with many more to come. Preparing for what's next is no longer optional. Employees must build AI skills and companies must support them with the right tools and training. This moment calls for honest conversations, intentional communication and real investment in reskilling. The companies that invest now won't just keep up — they'll shape what comes next.

Today we're announcing the Microsoft 365 Copilot Wave 2 Spring release — designed to power the next era of human–agent collaboration. The Microsoft 365 Copilot app is now your window into the world of agents, with new capabilities for this next phase — driven by more advanced models, adaptive memory and reasoning agents that work alongside you. Updates include:

Researcher and Analyst agents powered by OpenAI's deep reasoning models rolling out to customers through the Frontier program — and with our new Agent Store, you can easily find, pin and use agents — from partners like Jira, Monday.com and Miro — or your own custom agents. Create brings OpenAI's GPT-4o AI image generator to work, unlocking design and content creation skills for everyone. Easily modify or customize brand images or generate AI images aligned to your company's approved brand guidelines and create everything from marketing copy and social assets to newsletter banners, videos and more. Copilot Notebooks transforms your notes, documents and data into immediate insights and actions. By grounding Copilot in a notebook containing specific chats, files, meeting recordings and more, it can focus on the most relevant information — all while constantly scanning your source material to update in real time as your data evolves. Notebooks can even create an audio overview of your content with two hosts that walk you through the key points — a fun, flexible way to stay informed. Copilot Search is a new AI-powered enterprise search that helps you find what you need instantly with rich, context-aware answers from across your organization's apps and data. It connects to first- and third-party apps — from ServiceNow to Google Drive, Slack, Confluence, Jira and more — so you get fast, relevant results at work no matter where your data lives. New capabilities in the Copilot Control System empower IT pros to enable, disable or block agents for specific users or groups — to help ensure the right agents are being used by the right people.

2025 will be remembered as the year the Frontier Firm was born — the moment companies moved beyond experimenting with AI and began rebuilding around it. Like the digital native companies of a generation ago, they understand the power of pairing irreplaceable human insight with AI and agents to unlock outsized value. We're giving customers the insight to anticipate what's next — and the technology to help shape it.

Read the Annual Work Trend Index on WorkLab and visit the Microsoft 365 Blog to learn more about our product announcements. Learn what this year's Work Trend Index findings mean for small and medium-sized businesses.

For all the blogs, videos and assets related to today's announcement, please visit our microsite.

Tags: AI, Copilot Notebooks, Copilot Search, LinkedIn, Microsoft Copilot, Work Trend Index

Satya Nadella, Chairman and CEO, shared the following remarks at Microsoft's 50th anniversary today. It's so wonderful to be here with all of you celebrating 50 years of Microsoft. And it's especially exciting to be doing it at a time like this. For me, though, it starts with Bill [Gates] and Steve [Ballmer], who are both here with us today. I want to say a very big thank you to the two of you, and to Paul [Allen], and what you've meant to me personally and your vision that you had building this extraordinary company of ours that I've had the privilege to be part of. Thank you for your vision, your leadership, your passion, and for building the Microsoft that we know today. A company that has truly changed the world. Fifty years ago, Bill and Paul started Microsoft with a simple but powerful idea: to build technology so people everywhere could build more technology. The very first product Microsoft built was the Basic interpreter for the Altair, giving people the power to create software, jumpstarting the PC revolution, and creating an entirely new sector for our economy.

YouTube Video Click here to load media But today, it's not just about the past 50 years, it's about the next 50. If there's one thing that I have learned during my time at Microsoft, it's not about longevity, but relevance. Our future will not be defined by what we have built, but what we empower others to build. This is why we are leading this new wave of AI innovation and more importantly, democratizing it, just like we did with the PC. From there we've gone to chat, to multi file edits, and now to agents. More than 150 million developers in nearly every country around the world are using GitHub. So, I thought to myself, what if I could take that power and rebuild Microsoft's very first product? And so, I tried it.

YouTube Video Click here to load media You really know you're on to something. Intelligence has been commoditized when CEOs can start vibe coding. But in all seriousness, this is not just a cool party trick, It's transformational. It's empowering. It's unleashing human ambition. And it's happening now. In fact, I'm excited to share that the capability I just showed with Agent Mode is rolling out to all Visual Studio Code users starting today. We now have autonomous AI agents or peer programmers who can collaborate with us to anticipate our needs and help us think more creatively, and it does not stop there. We are bringing full MCP support to Agent Mode; we're also launching Code Review Agent today to fix and find bugs automatically; we're also making it easier than ever before for developers to build their own agents in Azure AI Foundry. Think of it like an agent factory. It's a production line for intelligence. Tens and thousands of organizations are using Foundry to build their own agents. And today, we're going further. We now have a new agent framework for building multi-agent systems. But building agents is just the very beginning. We're also building tools for all the evals, fine tuning, observability, and feedback. And you'll also have a red teaming agent and tools to measure code vulnerabilities. All of this in support of building trustworthy AI systems. There's much, much more to come. What started out as a developer tools company 50 years ago is now a platform company where everyone can be a developer. Our mission has not changed, it's only expanded. Just as we have done always, we are putting the power in people's hands so that they can build software that moves their communities, their countries forward. The same ethos is driving Copilot, not just for developers, but for every aspect of working life. Helping people do things that allow them to do more things, whether it's building apps, doing homework, shopping, planning, and so much more. Ultimately, it all comes down to our mission to empower every person and every organization on the planet to achieve more. I've always thought of Microsoft as a platform and partner-first company, and this has only been possible because of our customers, our partners and developers, and our 1.6 million employees, past and present, who have connected their passion with our purpose to get us where we are today. And it's you who will continue to build this company to have impact around the world well into the future. So, from the bottom of my heart, a big thank you to everyone who has contributed to Microsoft in getting us to this moment. I can't wait to see what is next with Copilot and everything that we have for you. Remarks have been edited for clarity. Photo: Bill Gates, Microsoft Co-founder and Gates Foundation Chair; Steve Ballmer, former Microsoft CEO and Ballmer Group Co-founder; and Satya Nadella, Microsoft Chairman and CEO, on stage at the anniversary event. (Photo by Dan DeLong) Tags: 50th anniversary, Azure AI Foundry, Copilot, GitHub, Visual Studio Code

As I look back on the incredible impact that Microsoft has had over its now 50 years of relentless innovation, I'm inspired by the simplicity and power of Bill Gates' bold ambition all those years ago: to put a PC on every desk and in every home. At Microsoft AI we're driven by that same spirit. Today, we're creating Copilot, an AI companion for everyone. What does this mean? What does it look like? What does it do? Truth is, there's no single answer that captures it. Because it's going to look and feel a little different for each of us. An AI companion is completely personal, built around individual needs, values and expectations. That's why ultimately, there will be as many different Copilots as there are people using them. Today, we are embarking on the journey to take Copilot from an AI companion to your AI companion. With your permission, Copilot will now remember what you talk about, so it learns your likes and dislikes and details about your life: the name of your dog, that tricky project at work, what keeps you motivated to stick to your new workout routine. Copilot will understand you in the context of your life, and show up, on your terms, in the right way at the right time. This is far richer, more dynamic, supportive and emergent than any software we've seen before. It's a new kind of relationship with technology, a new era. Of course, we're also focusing on the fundamentals: engaging personality, incredible search capability, custom-made answer cards, bespoke podcasts. With Deep Research you have a world-class researcher working alongside you. With Shopping, a trusted, impartial partner, looking out for the best products and deals. With Actions, let Copilot shoulder some of the burden, from scoring the gig tickets to sorting the ride home. Our new Windows app means your computer finally works on your behalf. And now with Copilot Vision on Mobile, Copilot can see what you see, responding in real time to create a shared understanding of your world. But the important point is that throughout Copilot is more than an AI, it's yours. It remembers not just what you said, but who you are. Copilot helps you stay organized, think clearly, learn more intuitively. It's there when you need a quick factual answer, a long exploratory debate or when you fancy just downloading after a hard day. Every day we hear the most incredible stories about the many and varied ways people enjoy Copilot. So, as we continue on our journey to create a Copilot for everyone, we're focusing on one critical goal: to make it truly yours. Each will have its own unique style and blend of attributes that distinctly suit each and every one of us. All the while, we'll stick to our core promise. You remain in control, you are the pilot and you make the calls and set the boundaries. We can't wait for you to discover your personal Copilot.

Mustafa Suleyman, Executive Vice President and CEO of Microsoft AI __________ Today, we announced a set of new updates that represent the next chapter in our vision to deliver a true AI companion — helping make AI more useful by making it more personal. Table of contents

Memory Actions Copilot Vision Pages Podcasts Shopping Deep Research Copilot Search

YouTube Video Click here to load media Memory and Personalization Memory is essential for creating a true AI companion. With Memory, Copilot can remember important details: your favorite food, the types of films you enjoy and your nephew's birthday and his interests. As you interact with Copilot, it notes your preferences, building a richer user profile and offering tailored solutions, proactive suggestions and timely reminders. Copilot prioritizes security and privacy, giving you control through the user dashboard and the option to choose which types of information it remembers about you or to opt out entirely. You remain in control.

YouTube Video Click here to load media As we explore the full range of what true personalization means, we are experimenting with new ways for you to interact with your Copilot. For example, what if your Copilot had an appearance and you could shape and form it to be whatever you want? We're early in this thinking but soon you'll have the ability to personalize Copilot and interact with your AI companion in a fun way while it offers advice and support when you want it. You can strike up a conversation with Copilot and now have an entertaining appearance that's unique to you.

YouTube Video Click here to load media Actions Another key advancement that makes Copilot a helpful AI companion is its ability to now take action on your behalf. With Actions, Copilot can now partner with you to complete tasks behind the scenes. Use simple chat prompts to ask Copilot to book event tickets, grab dinner reservations or send a thoughtful gift to a friend and it will check that task off your list. Copilot Actions will work with most websites across the web, and we're particularly excited to highlight our launch partners: 1-800-Flowers.com, Booking.com, Expedia, Kayak, OpenTable, Priceline, Tripadvisor, Skyscanner, Viator and Vrbo.

YouTube Video Click here to load media Copilot Vision We introduced Vision in Copilot for the web late last year and now we're bringing it to mobile and Windows.

Copilot and your phone's camera now enable an interactive experience with the real world, in real time. From the Copilot app on your phone, you can look around at your surroundings and request information, guidance or ideas. It can analyze both real-time video from the camera and photos stored on your camera roll. For example, use Vision to improve plant health by asking it to examine your plants and suggest actions, or to scan your office and provide tips on decoration. Vision on Mobile is available today in the Copilot app for iOS and Android. The new native Windows app will allow you to call upon Copilot while working across multiple applications, browser tabs or files. It will read the screen and interact with the content. You'll be able to use Copilot to search, change settings, organize files and collaborate on projects without switching between files or apps. The Copilot app can be accessed by hitting Alt+Space or holding Alt+Space for two seconds to use voice commands. The native Copilot app for Windows is available today. Vision will be available first for Windows Insiders starting next week and will be rolled out more broadly afterwards.

Pages Copilot now provides a way to organize your thoughts and content from chaos to calm. It takes all your scattered notes, content, research — whatever it is — and puts it in a canvas that Copilot organizes, simplifies and helps you with from your first rough draft to your very final edits. Pages lets you refine, iterate and keep the conversation going whenever you need it.

YouTube Video Click here to load media Podcasts Copilot can now generate AI-powered podcasts that curate and deliver personalized audio content based on your interests. Podcasts give you an easy, engaging and different way to consume information with minimal effort. Copilot can create a personalized podcast to analyze and compare options like with a vacation plan or home purchase or you can provide content to Copilot like a study or specific web sites and Copilot will generate a podcast that helps explain the topic. While listening, you can continue to talk and interact with Copilot to learn more and keep the conversations going. It's a daily audio solution that helps you solve tough problems, explore a new hobby or just enjoy an easy to digest solution to all that endless scrolling.

Shopping Copilot is now your personal shopper: on your side, doing the research, building comparisons, offering advice. It lets you know about price drops and sales and will let you purchase directly from the app. Shopping with Copilot is the easy solution to find the products and services you love.

YouTube Video Click here to load media Deep Research Deep Research is a new feature for Copilot that lets you conduct complex, multi-step research tasks more efficiently — shaving hours off the time it would normally take to complete. Copilot can find, analyze and combine information from online sources or large amounts of documents and images.

Copilot Search Copilot Search in Bing transforms web search by bringing traditional and generative search together. It cross-checks information across multiple sites to deliver detailed, comprehensive responses directly in Bing, complete with helpful cited sources and suggestions for further exploration. You can learn more about Copilot Search here.

YouTube Video Click here to load media We're beginning to roll out the initial versions of these new experiences starting today. We will continue to expand availability in the coming weeks and months, which will vary by platform, market and language as we work through the rollout process. We will continue to listen to feedback from customers, adjusting and refining these experiences over time. If you haven't tried Copilot yet, download the app for free today at the Microsoft Store or at app stores for iOS, Mac or Android, experience the best AI companion for Windows on a Copilot+ PC or use Copilot on the web at copilot.microsoft.com. Thank you for being on this journey with us and here's to the next 50 years of Microsoft. Tags: Actions, AI, Copilot, Copilot Search, Copilot Vision, Copilot Voice, Copilot+ PCs, Deep Research, Memory, Microsoft AI, Pages

As we celebrate Microsoft's 50th anniversary, our annual State of the Partner Ecosystem moment is a great opportunity to reflect on the incredible journey we've shared with our partners, employees and customers. Together, we've harnessed technology as a force for good, transforming industries and communities. From our early days of revolutionizing personal computing to leading the way in cloud innovation and now AI, our shared milestones highlight the power of collaboration and reinvention.

Fifty years ago, Microsoft started with a bold idea: the belief that technology could change the world. Thanks to the largest partner ecosystem in the industry, numbering 500,000 and growing, that vision became a reality, and I know we are just getting started. From the early days of distributing Windows PCs and Office to now delivering AI transformation strategies that solve the most complex customer challenges, our ability to stay at the forefront of innovation as technology evolves is a testament to our culture of continuous reinvention.

According to IDC, for every $1 of Microsoft revenue, services partners earn $8.45, and software partners earn $10.93. This underscores the immense opportunity available to partners of all types. As we look ahead to the future, we know that generative AI (GenAI) is forecast to grow exponentially faster than the overall IT market. Partners generating at least 25% of their Microsoft-related revenue from AI can expect higher margins and revenue growth, unlocking even more potential for transformation and success.*

Microsoft has always been a partner-led company. Our partners are core to our heritage and our future. Their innovation and collaboration have driven real transformation and customer success and will continue to shape the future of industries around the world. As we commemorate this historic moment, I want to take the opportunity to say Thank You to our partners for being on this incredible journey with us.

Here are just a few ways you can join us to celebrate this milestone:

Watch this video from Judson Althoff, Executive Vice President and Chief Commercial Officer, Microsoft. Join the Microsoft AI Skills Fest for 50 days of learning and discovery starting April 8! Gain skills that will empower you and your team to build innovative AI solutions with Microsoft's apps and services.

"For decades, Intel's partnership with Microsoft has sparked innovation and delivered value to our customers. Together, we've revolutionized industries and established new benchmarks for excellence. We look forward to collaborating for the next 50 years — and beyond."

— Jim Johnson, Senior Vice President, Client Computing Group, Intel

Preparing for the future with the Microsoft AI Cloud Partner Program (MAICPP)

Microsoft succeeds when our partners succeed. MAICPP has evolved to enable partners worldwide to deliver customer outcomes across every industry, from small businesses to the largest enterprises. Our program is designed to provide our partners with the most relevant tools and resources they need to thrive in a rapidly changing market, and it serves as the home for all partner types.

"As a proud Microsoft alum, I've seen firsthand how our collaboration has evolved to drive meaningful change for businesses across industries. From strategy through engineering and implementation, PwC and Microsoft drive innovation and deliver real business outcomes for clients worldwide."

— Stephanie Mosticchio, Principal, US and Global Microsoft Alliance Leader, PwC

Through MAICPP, all partners can access updated benefits packages designed to accelerate growth and meet specific business needs. Software development companies are encouraged to explore ISV Success, a pathway offering additional benefits to expand development capabilities and shorten time to market. Whether building, publishing or growing sales, partners can leverage targeted offers to get the support they need.

"As someone who has led global partnerships at several of the world's leading technology companies, I am impressed by how Microsoft has leaned in with their partner ecosystem and taken a leadership position in cloud computing and AI. We, at Snowflake, are excited to continue to strengthen our partnership in the years to come, and we look forward to jointly driving customer success in the age of enterprise AI. Congratulations!

— Tyler Prince, Senior Vice President of Worldwide Alliances & Channels, Snowflake

Depending on business goals, partners may pursue a Solutions Partner designation or specialization, both of which provide tailored benefits to help differentiate their business in a competitive market. Achieving a designation unlocks valuable go-to-market resources, sales support, new incentives and product benefits to help expand customer reach, sharpen skills and drive growth. For software development companies, becoming a Solutions Partner** with certified software*** further enhances market presence by validating software capabilities in high-demand areas.

"Having worked alongside every CEO of Microsoft in my career, I would like to personally congratulate Microsoft for its 50 extraordinary years of driving relentless innovation."

"Lenovo is proud to be a major part of this amazing journey with Microsoft and we are committed to this partnership for many more decades to come."

For partners holding an Azure designation or Azure specialization, additional incentives are available through Azure Migrate and Modernize and Azure Innovate — both underpinned by Azure Essentials. With comprehensive resources, extensive coverage across scenarios and tailored incentives in one easy-to-navigate hub, Azure partners can better support customers from migration to innovation. Learn more in What's new for Azure partner-led offerings: ISV Success and specialization updates.

Our program offers benefits for partners aligned to their growth stage and across all customer segments. We have recently made the process of obtaining an Azure Solutions Partner designation more aligned to our partners who specialize in working with small and midsize customers. We are also expanding access to Azure Migrate and Modernize and Azure Innovate incentives for SMB pathways. Read more about the SMB path to Azure Solutions Partner designations.

Cloud Solution Provider is our partner hero motion for small and medium enterprises

In November at Microsoft Ignite, we highlighted the $661 billion total addressable market (TAM) opportunity for SME&C in FY25 and beyond. Cloud Solution Provider (CSP) partners are the trusted advisors who serve these customers and accelerate their AI transformation with the value-added services and solutions that create real business impact. CSP is our hero motion that enables those partners to drive this business transformation.

"Our Microsoft partnership has evolved to meet the needs of our business and our partners. Together we've been able to support our partners to deliver true solution and value selling, leveraging the robust resources available through Microsoft AI Cloud Partner Program and benefitting from the rich incentives. It has enabled us to drive innovation and deliver exceptional experiences for our partners through our ArrowSphere platform and broader enablement programs to ensure they're empowered to deliver real customer outcomes. Together, we're enabling the channel to deliver solutions that deliver real impact for customers around the world."

We strive to provide CSP partners with the skilling, capabilities and investments to make this opportunity a reality. So far in FY25, we have:

focused our incentives to clearly align to our five strategic priorities — Copilot on every device across every role, AI design wins with every customer, securing the cyber foundation of every customer, a focus on migrations and Microsoft 365 execution dedicated 70% of our total incentive spend to partners that serve the Small, Medium Enterprises and Channel (SME&C) segment introduced a series of new promos, including a new-to-Microsoft 365 E5 offer to enable CSP partners to win new customers

Expanding our portfolio of CSP offers and capabilities is an ongoing priority. We share updates as they become available.

As customers increasingly centralize their solution procurement, marketplaces have become the preferred buying platform. For software companies, adopting cloud marketplaces accelerates deal closure and increases deal sizes. Serving as a global B2B commerce engine, our marketplace empowers Microsoft partners to provide solutions to customers worldwide. It offers various sales models: digital direct, through partners or with Microsoft — providing flexibility to align with how customers want to buy and how partners want to sell. Learn more in this recent blog.

The speed of technology innovation requires continuous learning. To support this, we offer our partners a variety of skilling opportunities, such as our popular in-person Microsoft AI Partner Training Days, designed to help partners develop both technical and sales capabilities.

We are also streamlining and simplifying our skilling portals through initiatives like Microsoft Sales Titan (currently in private preview for CSP Accelerate partners and available for all partners in summer 2025), a program tailored to equip sales professionals with in-depth knowledge of Microsoft Threat Protection SKUs, empowering them to position themselves as industry leaders. Discover these and other skilling opportunities.

Looking ahead, we invite our partners to join us at Microsoft Build, taking place May 19–22, 2025. This flagship event offers an exclusive opportunity to explore the latest advancements in AI, learn how to work smarter and elevate your projects. Connect with peers, industry experts and Microsoft leadership while diving into the code and innovations that will shape the future.

"Schneider Electric and Microsoft have been driven by a shared vision of a world that is more electric and digital. We're thrilled to celebrate Microsoft's 50th anniversary and excited to continue pioneering innovative solutions together, harnessing the transformative power of AI, pushing the boundaries of what's possible for our customers and shaping a sustainable future for generations to come."

— Frédéric Godemel, Executive Vice President, Energy Management, Schneider Electric

As we celebrate this remarkable milestone, we remain focused on and optimistic for the future. We continue to innovate, collaborate and empower our partners to thrive in the era of AI and beyond. The past 50 years have been defined by shared success, and this will continue for our future. Together, we will unlock new opportunities, drive transformation and shape the future of technology.

Throughout this journey, stories of innovation have inspired us. A few examples of how partners are celebrating our 50th anniversary are included in this blog. See the full list of partner quotes on the Microsoft 50th Anniversary celebration site.

Thank you for being an integral part of our story. We can't wait to see what we'll accomplish together next!

**"Solutions Partner" refers to a company that is a member of the Microsoft AI Cloud Partner Program and may offer software, services, and/or solutions to customers. Reference to "Solutions Partner" in any content, materials, resources, web properties, etc. and any associated designation should be not interpreted as an offer, endorsement, guarantee, proof of effectiveness or functionality, a commitment or any other type of representation or warranty on the part of Microsoft. All decisions pertaining to and related to your business needs including but not limited to strategies, solutions, partner selection, implementation, etc. rest solely with your business.

***A certification is (A) specific to the solution's interoperability with Microsoft products and (B) based on self-attestation by the solution owner. Solutions are only certified as of the date the solution is reviewed. Solution functionality and capability are controlled by the solution owner and may be subject to change. The inclusion of a solution in marketplace and any such designations should not be interpreted as an offer, endorsement, guarantee, proof of effectiveness or functionality, a commitment or any other type of representation or warranty on the part of Microsoft. All decisions pertaining and related to your business needs including but not limited to strategies, solutions, partner selection, implementation, etc. rest solely with your business.

Tags: AI, Azure, Build, Ignite, Microsoft AI Cloud Partner Program, Microsoft AI Partner Training Days, Microsoft AI Skills Fest, Microsoft Partners

As we've seen time and again throughout our 50-year history, times of great change for the world and for our industry require us to have a mindset that enables us to continually adapt and transform ourselves. There's no question that we are at the forefront of another such moment, with the rapid changes across every industry and business function in this AI era.

This means we must have the right product portfolio, the right business models, attract and retain top talent, and optimize our processes to meet changing customer expectations and succeed in the marketplace.

With this context, I've asked Kathleen Hogan to transition to a new role focused on defining our overarching corporate strategy and structure and leading our continuous transformation as a company. Kathleen will assume a new role as EVP, Office of Strategy and Transformation, reporting to me.

It is hard to overstate the impact Kathleen has had on Microsoft as Chief People Officer. Over the past 10+ years, she has led our cultural transformation, as we embraced a growth mindset, positioning us to seize new opportunities with agility and attract and retain world-class talent. She is recognized externally as a consequential HR leader transforming culture and the world of work. Her more than 20-year tenure at Microsoft, including leading our global services business, paired with her prior experience as a McKinsey partner in Silicon Valley, and a development manager at Oracle, makes her uniquely suited to lead this work as we accelerate our pace of change across our people, processes, and portfolio. Kathleen will work across the SLT as we chart this next phase of our transformation, which requires both interpreting the outside and redefining the inside.

Kathleen and I have been discussing this transition and succession planning for some time, and we both agree this is the critical juncture to apply new focus and intention to this work.

With this transition, I'm very pleased to share that Amy Coleman will assume the role of EVP, Chief People Officer, leading our HR organization. She joins the senior leadership team reporting to me.

Amy has led HR for our corporate functions across the company for the past six years, following various HR roles partnering across engineering, sales, marketing, and business development spanning 25 years. In that time, she has been a trusted advisor to both Kathleen and to me as she orchestrated many cross-company workstreams as we evolved our culture, improved our employee engagement model, established our employee relations team, and drove enterprise crisis response for our people. Amy's commitment to operational excellence and high performance will be key in driving our continued success, and I'm confident in the perspective, expertise, and thoughtful approach she'll bring as we navigate the next phase of our journey.

Please join me in congratulating Kathleen and Amy on their new roles.

Today, we hosted the 15th annual Microsoft Ability Summit, with over 20,000 attendees from 164 countries coming together virtually to discuss the future of AI and accessibility. Microsoft has a long-standing commitment to investing in accessibility, grounded in our business model and going back over three decades from the earliest accessibility features in Windows 95 and continuing today with new hardware and software functionalities powered by AI. We are innovating faster than ever before and people with disabilities continue to lead the way. Accessibility is a fundamental right for people with disabilities and makes technology easier for everyone. We see this reflected in how customers are using Microsoft technologies around the world. Copilot for Microsoft 365 is unlocking workplace productivity like never before, more than 10 million people use Edge each month to have the web Read Aloud, over 1 million people use Immersive Reader to make webpages easier to read and partners like Tobii Dynavox and Special Olympics are bringing AI to people with disabilities globally. And these are just some of the highlights of what we shared today! Here's a quick summary of the new accessibility products, features and programs announced at the 2025 Ability Summit. What's new in 2025? We announced that the Xbox Adaptive Joystick is now available for purchase exclusively at Microsoft Store. With more than 429 million players with disabilities worldwide, we know each player has unique needs and preferences for how they choose to play. The Xbox Adaptive Joystick is a singular, wired controller primarily designed to meet the needs of players with limited mobility. Its versatility helps players seamlessly incorporate it into their existing gaming setups. Built with the Gaming and Disability community who inform the development of Xbox products from the beginning. The joystick joins our family of adaptive accessories including the mouse, pen, adaptive kit and Xbox Adaptive Controller. For more details visit Xbox Support. And all our Microsoft hardware comes in packaging designed to be accessible and sustainable. No more plastic clamshells! To help others with accessible packaging, today at the Summit, the Packaging and Content Team at Microsoft published its Accessible Packaging Design Guide, which offers practical guidelines, best practices and strategies to create accessible packaging and foster a trusted customer experience. At the Summit, Microsoft teams and partners also shared ways they are working to further advance accessibility through technology:

Tobii Dynavox is integrating Microsoft Neural Voice, a capability of Azure AI Speech, into their assistive communication solutions. This AI-powered feature gives more personal options for individuals who use assistive communication devices using eye gaze. Neural Voices are available in over 50 languages within their apps TD Talk and TD Phone. Microsoft Teams will improve for those using Sign Language View. Later this year, Teams will be able to identify when someone is using sign language and feature them prominently as a speaker in the meeting. These video customizations are part of our ongoing product development to help deliver clear and accessible communication for everyone. Copilot is powering neurodiverse talent. Recently, an EY study found that Copilot helped 76% of neurodiverse employees perform better at work by enhancing communication, memory recall and focus. At the Summit, we shared how new simple features like Team Reflow and PowerPoint Designer are helping people do their work. See four early adopters of Copilot in New York share their stories.

AI comes to Narrator. Rich image descriptions powered by AI will be coming to Narrator in Windows Insider Preview this spring and Azure AI Foundry announced new UI improvements to reduce cognitive load. Over 5 million learners around the world have participated in our Accessibility Skilling program and we're grateful to our partners including Teach Access, Computacenter UK and the City of New York. The free, virtual training includes the latest on AI and is available for organizations to use in their learning management systems. Speech recognition improved up to 60%. The Speech Accessibility Project, led by the University of Illinois Urbana-Champaign, unlocked a breakthrough improving accuracy gains for non-standard speech, and the Azure platform team demonstrated how developers can leverage GitHub Copilot to write accessible code. Special Olympics shared how Copilot has been a game-changing training companion for their coaches and athletes with intellectual and development disabilities for the Special Olympics World Winter Games in Turin, Italy.

All this progress is possible because of the people who design technology with accessibility in mind. In this way, technology benefits everyone, creating a more productive and efficient workplace. It is beautiful to see that reflected in this profile of Dave Dame, Senior Director of Accessibility and Human Factors at Microsoft, where he shared how accessible technology helps him thrive as a leader. Onward For over 30 years, Microsoft has focused on accessibility in our products. Accessibility makes our tools and technologies easier for everyone and accelerates innovation for the world. From the introduction of Sticky Keys and speech recognition in Windows 95 to Seeing AI in 2016 and beyond, accessibility innovations have benefited people in ways we designed for and ways we could have never expected. Just think about how closed captions are now invaluable for everyone watching videos and calls. AI has the potential to create significant advancements across every sector of our economy and society. We will continue to be grounded and responsible in our approach as we work to get the latest technology to the people who can benefit from it the most. Whether this is your first or fifteenth Ability Summit, thank you for joining and we hope you picked up a new feature, skill or nugget that helps you, your community or your organization get the most out of technology. All content will be available to watch after the event. Let's move forward together. Tags: accessibility, AI, AI for Accessibility, Azure, Azure AI Foundry, Microsoft Ability Summit, Microsoft Teams, Xb

Satya Nadella, Chairman and CEO, shared the below communication on social media this morning. Click here to load media

Today Microsoft released Muse, a first-of-its-kind generative AI model that we are applying to gaming. But it's so much more than that. What we're sharing today is a huge step forward for gameplay ideation. And what's even more exciting is what this breakthrough represents in our journey of building and using generative AI, and what industries, developers and creators of all interests will be enabled to do next.

The impressive abilities we first witnessed with ChatGPT and GPT-4 to learn human language are now being matched by AI's abilities to learn the mechanics of how things work, in effect developing a practical understanding of interactions in the world. As a computer scientist, this ability to understand and model a 3D world is something I and many other great researchers have pursued for over 10 years and, personally, I was not sure that it could be made possible with such speed and quality.

In the case of Muse, just from observing human gameplay, this model develops a deep understanding of the environment, including its dynamics and how it evolves over time in response to actions. This unlocks the ability to rapidly iterate, remix and create in video games so developers can eventually create immersive environments and unleash their full creativity.

Beyond gaming, I'm excited by the potential of this capability to enable AI assistants that understand and help visualize things, from reconfiguring the kitchen in your home to redesigning a retail space to building a digital twin of a factory floor to test and explore different scenarios. All these things are just now becoming possible with AI. From the perspective of computer science research, it's pretty amazing, and the future applications of this are likely to be transformative for creators.

At Microsoft, we have a long history of collaboration between research and engineering. Today, as we release Muse, we are also announcing Azure AI Foundry Labs, where the AI community can explore the latest from Microsoft Research. Azure AI Foundry Labs will help accelerate the transition from research to solutions, bringing new ideas to the broader community to help shape the future of AI. Learn more.

Organizational leaders in every industry around the world are evaluating ways AI can unlock opportunities, drive pragmatic innovation and yield value across their business. At Microsoft, we are dedicated to helping our customers accelerate AI Transformation by empowering human ambition with Copilots and agents, developing differentiated AI solutions and building scalable cybersecurity foundations. At Microsoft Ignite we made over 100 announcements that bring the latest innovation directly to our customers and partners, and shared how Microsoft is the only technology leader to offer three distinct AI platforms for them to build AI solutions:

Copilot is your UI for AI, with Copilot Studio enabling low-code creation of agents and extensibility to your data. Azure AI Foundry is the only AI app server for building real-world, world-class, AI-native applications. Microsoft Fabric is the AI data platform that provides one common way to reason over your data —no matter where it lives.

All three of these platforms are open and work synchronously to enable the development of modern AI solutions; and each is surrounded by our world-class security offerings so leaders can move their AI-first strategies forward with confidence.

As we look ahead to what we can achieve together, I remain inspired by the work we are doing today. Below are a handful of the many stories from the past quarter highlighting the differentiated AI solutions our customers and partners are driving to move business forward across industries and realize pragmatic value. Their success clearly illustrates that real results can be harnessed from AI today, and it is changing the way organizations do business.

To power its industrial IoT and AI platform, ABB Group leveraged Microsoft Azure OpenAI Service to create Genix Copilot: a generative AI-powered analytics suite aimed at solving some of the most complex industrial problems. The solution helps customers analyze key functions in their operations —such as asset and process performance, energy optimization and emission monitoring — with real-time operational insights. As a result, customers are seeing up to 35% savings in operations and maintenance, and up to 20% improvement in energy and emission optimization. ABB also saw an 80% decrease in service calls with the self-service capabilities of Genix Copilot.

Serving government healthcare agencies across the US, Acentra Health turned to Microsoft to help introduce the latest AI capabilities that maximize talent and cut costs in a secure, HIPAA-compliant manner. Using Azure OpenAI Service, the company developed MedScribe — an AI-powered tool reducing the time specially trained nursing staff spend on appeal determination letters. This innovation saved 11,000 nursing hours and nearly $800,000, reducing time spent on each appeal determination letter by about 50%. MedScribe also significantly enhanced operational efficiency, enabling nurses to process 20 to 30 letters daily with a 99% approval rate.

To ease challenges for small farmers, Romanian agribusiness group Agricover revolutionized access to credit by developing MyAgricover. Built with help from partner Avaelgo, the scalable digital platform utilizes Microsoft Azure, Azure API Management and Microsoft Fabric to automate the loan process and enable faster approvals and disbursements. This has empowered small farmers to grow their businesses and receive faster access to financing by reducing loan approval time by 90 percent — from 10 working days to a maximum of 24 hours.

Building on its status as a world-class airline with a strong Indian identity, Air India sought ways to enhance customer support while managing costs. By developing AI.g, one of the industry's first generative AI virtual assistants built on Azure OpenAI Service, the airline upgraded the customer experience. Today, 97% of customer queries are handled with full automation, resulting in millions of dollars of support costs saved and improved customer satisfaction — further positioning the airline for continued growth.

BMW Group aimed to enhance data delivery efficiency and improve vehicle development and prototyping cycles by implementing a Mobile Data Recorder (MDR) solution with Azure App Service, Azure AI and Azure Kubernetes Service (AKS). The solution achieved 10 times more efficient data delivery, significantly improved data accessibility and elevated overall development quality. The MDR monitors and records more than 10,000 signals twice per second in every vehicle of BMW's fleet of 3,500 development cars and transmits data within seconds to a centralized cloud back end. Using Azure AI Foundry and Azure OpenAI Service, BMW Group created an MDR copilot fueled by GPT-4o. Engineers can now chat with the interface using natural language, and the MDR copilot converts the conversations into KQL queries, simplifying access to technical insights. Moving from on-premises tools to a cloud-based system with faster data management also helps engineers troubleshoot in real time. The vehicle data covered by the system has doubled, and data delivery and analysis happen 10 times faster.

Coles Group modernized its logistics and administrative applications using Microsoft Azure Stack HCI to scale its edge AI capabilities and improve efficiency and customer experience across its 1,800 stores. By expanding its Azure Stack HCI footprint from two stores to over 500, Coles achieved a six-fold increase in the pace of application deployment, significantly enhancing operational efficiency and enabling rapid innovation without disrupting workloads. The retailer is also using Azure Machine Learning to train and develop edge AI models, speeding up data annotation time for training models by 50%.

Multinational advertising and media company Dentsu wanted to speed time to insights for its team of data scientists and media analysts to support its media planning and budget optimization. Using Microsoft Azure AI Foundry and Azure OpenAI Service, Dentsu developers built a predictive analytics copilot that uses conversational chat and draws on deep expertise in media forecasting, budgeting and optimization. This AI-driven tool has reduced time to media insights for employees and clients by 90% and cut analysis costs.

To overcome the limitations of its current systems, scale operations and automate processes across millions of workflows, Docusign created the Intelligent Agreement Management (IAM) platform on Azure. Using Azure AI, Azure Cosmos DB, Azure Logic Apps and AKS, the platform transforms agreement data into actionable insights to enhance productivity and accelerate contract review cycles. IAM also ensures better collaboration and unification across business systems to provide secure solutions tailored to diverse customer needs. For example, its customer KPC Private funds reported a 70% reduction in time and resources dedicated to agreement processes.

Emirates Global Aluminium (EGA) transformed its manufacturing operations by leveraging a hybrid environment with Azure Arc, Azure Stack HCI and Azure Kubernetes Service. This digital manufacturing platform resulted in 86% cost savings for AI image and video analytics and a 13-fold improvement in AI response times. The seamless hybrid cloud architecture has enhanced EGA's operational efficiency and agility, supporting its Industry 4.0 transformation strategy.

EY collaborated with Microsoft to enhance the inclusivity of AI development using Azure AI Foundry. By involving neurodivergent technologists from EY's Neuro-Diverse Centers of Excellence, they improved the accessibility and productivity of AI tools, resulting in more inclusive AI solutions, fostering innovation and ensuring that AI tools unlock the potential of all users. With an estimated 20% of the global workforce identifying as neurodivergent, inclusive AI solutions are crucial for maximizing creativity and productivity. Neurodivergent EY technologists also collaborated with Microsoft developers to make Azure AI Foundry more inclusive and help all users work productively to create innovative AI solutions.

Colombian household appliance manufacturer Haceb integrated AI to optimize processes, reduce costs and improve service quality. Using Microsoft Copilot Studio and Azure OpenAI Service, the company created a virtual technical support assistant, saving its 245 technicians 5 minutes per visit — a total of 5,000 minutes saved daily. This AI solution has enhanced efficiency and boosted customer satisfaction by allowing for faster issue resolution. Haceb's AI adoption has also empowered employees, boosted productivity and positioned the company as a leader in AI innovation in Colombia.

To better serve its global patients, Operation Smile — in collaboration with partner Squadra — leveraged Azure AI, Machine Learning and Microsoft Fabric to develop an AI-powered solution to predict surgical outcomes and optimize resource allocation. This innovation resulted in a 30% increase in surgical efficiency, a 90% reduction in translation errors and improved patient outcomes. Additionally, report generation is now up to 95% quicker, and repeated medical events have decreased by 15%, enabling Operation Smile to provide better care to more children worldwide.

Ontada — a McKesson business dedicated to oncology data and evidence, clinical education and point-of-care technologies — needed a way to generate key insights across 150 million unstructured oncology documents. Using Microsoft Azure AI and Azure OpenAI Service, Ontada developed a data platform solution called ON.Genuity to provide AI-driven insights into the patient journey, enhance patient trial matching and identify care gaps. The company also implemented large language models to target nearly 100 critical oncology data elements across 39 cancer types, enabling the company to analyze an estimated 70% of previously inaccessible data, reduce processing time by 75% and accelerate product time-to-market from months to just one week.

As the UK's largest pet care company, Pets at Home sought a way to combat fraud across its retail operations — particularly as its online business continued to grow. Working closely with its fraud team, it adopted Copilot Studio to develop an AI agent that quickly identifies suspicious transactions. The agent autonomously gathers relevant information, performs analysis and shares it with a fraud agent to enable a manual, data-intensive investigative process while ensuring a human remains in the loop. With this low-code agent extending and seamlessly integrating into existing systems, the company's fraud department can act more quickly; what used to take 20 to 30 minutes is now handled by the AI agent within seconds. The company is identifying fraud 10 times faster and is processing 20 times more cases a day. Now, the company can operate at scale with speed, efficiency and accuracy — with savings expected to be in the seven figures as it continues to build more agents.

Revenue Grid, a technology company specializing in sales engagement and revenue optimization solutions, partnered with Cloud Services to modernize its data infrastructure and develop a unified data warehouse capable of handling unstructured, semi-structured and structured data. By migrating to Microsoft Fabric, Revenue Grid can now deliver data-powered revenue intelligence, driven by a unified platform, elastic scalability, enhanced analytics capabilities and streamlined operations. Revenue Grid has reduced infrastructure costs by 60% while enhancing its analytical capabilities to improve real-time data processing, empowering sales teams with accurate and diverse data.

To better manage and integrate employee data across diverse regions and systems, UST built a comprehensive Employee Data platform on Microsoft Fabric. In under a year, UST migrated 20 years of employee data with all security measures to enhance data accessibility and employee productivity. The Meta Data Driven Integration (MDDI) framework in Fabric also helped the company cut data ingestion time by 50% so employees can focus more on analysis than preparation. As a result of this implementation, the company has seen an increase in collaboration and innovation from employees, helping put its values into action.

The Microsoft Commercial Marketplace offers millions of customers worldwide a convenient place to find, try and buy software and services across 140 countries. As a Marketplace partner, WeTransact is helping independent software vendors (ISVs) list and transact their software solutions — and find opportunities for co-selling and extending their reach to enterprise customers through development of the WeTransact platform. Powered by Azure OpenAI Service, the platform is changing the way partnerships are being built by using AI pairing to facilitate a "plug and play" reseller network. More than 300 ISVs worldwide have joined the Microsoft Commercial Marketplace using the WeTransact platform, cutting their time to publish by 75%.

The opportunity for AI to create value is no longer an ambition for the future — it is happening now, and organizational leaders across industries are investing in AI-first strategies to change the way they do business. We believe AI should empower human achievement and enrich the lives of employees; and we are uniquely differentiated to help you accelerate your AI Transformation responsibly and securely. Choosing the right technology provider comes down to trust, and I look forward to what we will achieve together as we partner with you on your AI journey.

Tags: AI, Azure, Azure AI, Azure AI Foundry, Azure Arc, Azure OpenAI Service, Azure Stack HCI, Copilot, Copilot Studio, Microsoft Fabric, Microsoft Ignite 2024

We are thrilled to continue our strategic partnership with OpenAI and to partner on Stargate. Today's announcement is complementary to what our two companies have been working on together since 2019. The key elements of our partnership remain in place for the duration of our contract through 2030, with our access to OpenAI's IP, our revenue sharing arrangements and our exclusivity on OpenAI's APIs all continuing forward – specifically:

Microsoft has rights to OpenAI IP (inclusive of model and infrastructure) for use within our products like Copilot. This means our customers have access to the best model for their needs. The OpenAI API is exclusive to Azure, runs on Azure and is also available through the Azure OpenAI Service. This agreement means customers benefit from having access to leading models on Microsoft platforms and direct from OpenAI. Microsoft and OpenAI have revenue sharing agreements that flow both ways, ensuring that both companies benefit from increased use of new and existing models. Microsoft remains a major investor in OpenAI, providing funding and capacity to support their advancements and, in turn, benefiting from their growth in valuation.

In addition to this, OpenAI recently made a new, large Azure commitment that will continue to support all OpenAI products as well as training. This new agreement also includes changes to the exclusivity on new capacity, moving to a model where Microsoft has a right of first refusal (ROFR). To further support OpenAI, Microsoft has approved OpenAI's ability to build additional capacity, primarily for research and training of models. We thank OpenAI for their continued partnership and look forward to what's to come. Tags: AI, Azure OpenAI Service

As we begin the new year, it's clear that we're entering the next innings of this AI platform shift. 2025 will be about model-forward applications that reshape all application categories. More so than any previous platform shift, every layer of the application stack will be impacted. It's akin to GUI, internet servers, and cloud-native databases all being introduced into the app stack simultaneously. Thirty years of change is being compressed into three years!

We will build agentic applications with memory, entitlements, and action space that will inherit powerful model capabilities. And we will adapt these capabilities for enhanced performance and safety across roles, business processes, and industry domains. Further, how we build, deploy, and maintain code for these AI applications is also fundamentally changing and becoming agentic.

This is leading to a new AI-first app stack — one with new UI/UX patterns, runtimes to build with agents, orchestrate multiple agents, and a reimagined management and observability layer. In this world, Azure must become the infrastructure for AI, while we build our AI platform and developer tools — spanning Azure AI Foundry, GitHub, and VS Code — on top of it. In other words, our AI platform and tools will come together to create agents, and these agents will come together to change every SaaS application category, and building custom applications will be driven by software (i.e. "service as software").

The good news is that we have been working at this for more than two years and have learned a lot in terms of the systems, app platform, and tools required for the AI era. To more rapidly and boldly advance our roadmap across each of these layers, we are creating a new engineering organization: CoreAI – Platform and Tools.

This new division will bring together Dev Div, AI Platform, and some key teams from the Office of the CTO (AI Supercomputer, AI Agentic Runtimes, and Engineering Thrive), with the mission to build the end-to-end Copilot & AI stack for both our first-party and third-party customers to build and run AI apps and agents. This group will also build out GitHub Copilot, thus having a tight feedback loop between the leading AI-first product and the AI platform to motivate the stack and its roadmap.

Jay Parikh will lead this group as EVP of CoreAI – Platform and Tools, with Eric Boyd, Jason Taylor, Julia Liuson, Tim Bozarth, and their respective teams reporting to Jay.

Jay will work closely with Scott, Rajesh, Charlie, Mustafa, and Kevin to optimize our entire tech stack for both performance and efficiency. Additionally, Jay and team will lead our progress and work around developer productivity and Engineering Thrive across the company.

As our cloud infrastructure business continues to grow and scale to become Microsoft's largest business, Scott will continue to lead Cloud + AI to ensure we're delivering the quality, security, and innovation that our customers and partners count on for their most mission-critical applications, databases, and AI workloads.

Ultimately, we must remember that our internal organizational boundaries are meaningless to both our customers and to our competitors. When we talk about operating as One Microsoft, we are effectively talking about how we are continually increasing our customer focus, raising the bar on our innovation, and driving accountability, so we can truly live up to our mission.

Our success in this next phase will be determined by having the best AI platform, tools, and infrastructure. We have a lot of work to do and a tremendous opportunity ahead, and together, I'm looking forward to building what comes next.

In the case of AI, it is absolutely true that the industry is moving incredibly fast and evolving quickly. It's also true that hundreds of thousands of customers are using Microsoft AI technology today and, by making early bets on the platform, are seeing big benefits now and future-proofing their ability to benefit from the next big wave of AI improvements.

Microsoft Ignite is our annual event that spotlights the updates and creations that enable customers, partners and developers to unleash the full potential of Microsoft's technology and change the way we approach work.

This year, we are announcing about 80 new products and features, including new capabilities in Microsoft 365 Copilot, additions to the Copilot + AI stack and new Copilot+ devices offerings. Underpinning each of these innovations is our commitment to security. Since launching our Secure Future Initiative (SFI) one year ago, we have made security the No. 1 job of every employee at Microsoft, dedicated 34,000 engineers to this focus and, at Ignite, we will announce innovations that are rooted in our SFI principles: secure by design, secure by default and secure operations.

More than 200,000 people have registered to join us for this year's Ignite, with more than 14,000 attendees at our in-person events in Chicago. Attendees can choose from more than 800 sessions, demos and expert-led labs from Microsoft and our partners. Most of the Ignite content will be available on demand for those who can't attend the live event.

Microsoft 365 Copilot is your AI assistant for work, and we have seen the momentum grow as more organizations are moving to Copilot and deploying it to great success. All up, nearly 70% of the Fortune 500 now use Microsoft 365 Copilot.

That echoes an industry trend: A recent IDC study showed that generative AI is on the rise, with 75% adoption among companies surveyed in 2024. In addition, for every $1 invested, companies are realizing a return of $3.70, and leaders are saying they are realizing as much as a $10 return, according to the study.

The investments that Microsoft has made in Copilot are paying dividends for our customers.

We recently highlighted some of the more than 200 customer stories of accelerated AI Transformation, with Copilot helping many of them spark innovation and transform their organization for the better. Several examples include:

Intelligent power management company Eaton leveraged Microsoft 365 Copilot to help streamline and automate operations, improve data access, centralize knowledge and empower teams to focus on higher-value tasks. One immediate challenge addressed through Copilot focused on the manual, time-consuming documentation process in Eaton's Finance operations. Copilot helped Eaton document over 9,000 standard operating procedures (SOPs), resulting in an 83% time savings for each SOP. Consulting firm McKinsey & Company is creating an agent to speed up the client onboarding process. The pilot showed lead time could be reduced by 90% and administrative work reduced by 30%. The agent automates complex processes, such as identifying the right expert capabilities and staffing teams and acts as a single place where colleagues can ask questions and request follow-ups. By streamlining tasks and reducing manual inputs, this agent could potentially save consultants many hours, allowing them to spend more time with clients. Boosting productivity with Microsoft 365 Copilot

Microsoft is continuing to supercharge productivity with new capabilities in Microsoft 365 Copilot designed to help simplify the workday.

Copilot Actions, now in private preview, enable anyone to automate everyday tasks with simple, fill-in-the-blank prompts, whether it's getting a daily summary of meeting actions in Microsoft Teams, compiling weekly reports or getting an email upon return from vacation that summarizes missed meetings, chats and emails.

Anyone can easily set up Actions right in their Microsoft 365 app, allowing users to focus on more impactful work, save time and boost productivity.

New agents in Microsoft 365 are designed to help scale individual impact and transform business process. At Ignite we will introduce:

Agents in SharePoint: These natural language AI assistants are grounded on relevant SharePoint sites, files and folders to make it easy to find answers from that content, and to make quicker decisions as a result. Now generally available, every SharePoint site will include an agent tailored to its content. Users can also create customized agents scoped to select SharePoint files, folders or sites with as little as one click. Interpreter: This agent in Teams helps users overcome language barriers by enabling real-time, speech-to-speech interpretation in meetings. Available in public preview in early 2025, meeting participants will also have the option to have the agent simulate their personal voice. The Employee Self-Service Agent: An agent available in private preview in Business Chat expedites answers for the most common policy-related questions and simplifies action-taking on key HR and IT-related tasks — like helping employees understand their benefits or request a new laptop. It can be customized in Copilot Studio to meet an organization's unique needs. Other agents in public preview take real-time meeting notes in Teams and automate project management from start to finish in Planner. Copilot + AI Stack

The Copilot stack empowers users to build more ambitious products by leveraging advanced technology at each layer of the stack. To create a unified experience where customers can design, customize and manage AI applications and agents, we are introducing Azure AI Foundry, which gives customers access to all existing Azure AI services and tooling, plus new capabilities like:

Azure AI Foundry SDK, now available in preview, provides a unified toolchain for designing, customizing and managing AI apps and agents with enterprise-grade control and customization. With tools that help organizations responsibly scale their applications, Foundry also provides 25 prebuilt app templates and a simplified coding experience they can access from familiar tools like GitHub, Visual Studio and Copilot Studio. Azure AI Foundry portal (formerly Azure AI Studio), now available in preview, is a comprehensive visual user interface to help developers discover AI models, services and tools. With a new management center experience that brings essential subscription information into a single dashboard, the portal also helps IT admins, operations and compliance teams manage AI applications at scale. Azure AI Agent Service, coming soon to preview, will enable professional developers to orchestrate, deploy and scale enterprise-ready agents to automate business processes.

We also continue to back up our Trustworthy AI commitments with new tools. Today we're announcing AI reports and risk and safety evaluations for images to help organizations ensure AI applications are safe and compliant. AI reports will help organizations improve observability, collaboration and governance for AI apps and fine-tuned models, while evaluations for image content will help customers assess the frequency and severity of harmful content in their app's AI-generated outputs.

As organizations move more workloads to the cloud to enhance security and flexibility, Microsoft is expanding its Cloud PC solution by introducing the first in a new class of devices purpose-built to connect securely to Windows 365 in seconds.

Windows 365 Link is the simple, secure, purpose-built device for Windows 365. It is in preview now and will become generally available for purchase starting in April 2025 in select markets with an MSRP of $349, allowing users to work securely in a familiar Windows desktop in the Microsoft Cloud with responsive, high-fidelity experiences.

Windows 365 Link is secure by design. The device has no local data, no local apps and admin-less users so corporate data stays protected within the Microsoft Cloud.

Other new capabilities for Copilot+ PCs for commercial customers include harnessing the power of inbuilt native processing units (NPUs) to deliver local AI. With improved Windows Search, and the new Recall experience (preview), finding what you need on your PC is easier than ever by just describing what you are looking for. These features are releasing first to our Windows Insider community on Copilot+ PCs before rolling out more broadly to our customers.

Four years ago, BlackRock, one of the world's pre-eminent asset management firms, formed a strategic alliance with Microsoft to move its Aladdin platform to Microsoft Azure. With this foundation on Azure, BlackRock rolled out generative AI tools for global clients with Aladdin Copilot. Through generative AI, Aladdin Copilot serves to strengthen the connective tissue across the platform, leveraging Microsoft technology to help users receive answers instantly to unlock new efficiencies and discover important business insights even faster. Aladdin Copilot makes BlackRock's Aladdin platform even more intelligent and responsive. That results in enhanced productivity, enables scale and keeps users more informed.

BlackRock's move to Azure and launch of Aladdin Copilot are just two of the many ongoing milestones in a long-term partnership that also includes an enterprise-wide deal for 24,000 seats of Microsoft 365 Copilot. Today, about 60% of BlackRock's Copilot user population is leveraging Copilot on a weekly basis. Additionally, BlackRock also recently made the choice to move its on-prem CRM solution to the cloud with Dynamics 365, citing its native integration with Teams and Outlook as one of its primary decision-making factors.

We know that the threat landscape is rapidly evolving, and it's imperative that we stay ahead of bad actors. At Microsoft we believe that security is a team sport, and we are stronger when we partner as a security community to share information, collaborate and stop bad actors.

In that spirit, and as part of our Secure Future Initiative (SFI), at Ignite we are announcing the largest public security research event in history: the Zero Day Quest. This event, which focuses on AI and cloud security, will offer the largest award pool in the industry at $4 million, in addition to our existing $16 million annual bounty program. This competition aims to attract the world's best security minds to tackle high-impact scenarios critical to our customers' security, with award multipliers, starting today.

As the threat landscape has changed, we have seen rapid evolution in the way attackers exploit weaknesses within systems — particularly by navigating graph relationships between identities, files and devices to uncover attack paths. Attackers thinking in graphs cause wider damage from the first point of intrusion. Traditional security products, with limited visibility into these graph relationships, are often better suited to protect specific devices or mediums — like laptops or inboxes — rather than the full scope of potential attack surface.

Today's Microsoft Security Exposure Management launch is a pivotal step in transforming cybersecurity with savvy data and AI-based strategies. The power of incorporating Microsoft graph data, in context with data from customers' other third-party security tools, creates a powerful single pane of glass to visualize attack paths before threat actors do. With computing power and cloud-scale performance to distill powerful real-time mapping of assets and evolving risks, Exposure Management assists security teams in preventing intrusions and provides IT, operations and risk leaders with real-time data to support cyber risk decision-making.

This is only a small section of the many exciting features and updates we will be announcing at Ignite. As a reminder, you can view keynote sessions from Microsoft executives including Satya Nadella, Rajesh Jha, Scott Guthrie, Charlie Bell and Vasu Jakkal, live or on-demand.

Plus, you can get more on all these announcements by exploring the Book of News, the official compendium of all today's news.

Tags: AI, Azure AI Foundry, Book of News, Copilot, Dynamics 365, Employee Self-Service Agent, Interpreter, Microsoft 365, Microsoft Azure, Microsoft Cloud, Microsoft Ignite 2024, Recall, Secure Future Initiative, Windows 365 Link

As fans of books, especially in their physical format, it is our great pleasure to launch 8080 Books, an imprint of Microsoft. Our first title, No Prize for Pessimism, is authored by Sam Schillace, deputy chief technology officer at Microsoft, and is available today. Our second title, Platform Mindset, by Marcus Fontoura, will be available later this year.

Computing has become an essential ingredient to almost every endeavor on our planet, and, as students of both Microsoft and technology, our goal with 8080 Books is to publish original research, ideas and insights at the intersection of science, technology and business, and, in doing so, to help advance discourse on this important landscape.

The name of our imprint takes its inspiration from the 8080 microprocessor — a foundation for the company's earliest software breakthroughs. Not coincidentally, 8080 is also the last four digits of Microsoft's corporate headquarters phone number.

With a combined tenure of, well, let's just say a long time, we're both acutely aware of the rich well of talent at Microsoft from which we can draw upon and publish under the 8080 Books imprint over time. However, our intention is that we will seek to use this not just as a platform for Microsoft authors but also to showcase minds and ideas from outside of the company.

While we are not currently accepting unsolicited manuscripts, our website does provide more details about our plans, such as evaluating out of print titles that we feel remain relevant to today's leaders, and why we feel the time is right to launch this imprint.

We hope you enjoy our launch title, which is available here, and we look forward to hearing your feedback, questions and ideas as we embark on this new adventure.

For anyone in the Puget Sound area, we invite you to Schillace's first reading and signing at Brick & Mortar Books, on Wednesday, Dec. 11 in Redmond, Washington. Check here for details. Space is limited.

Every day, NASA's satellites orbit Earth, capturing a wealth of information that helps us understand our planet. From monitoring wildfires to tracking climate change, this vast trove of Earth Science data has the potential to drive scientific discoveries, inform policy decisions and support industries like agriculture, urban planning and disaster response. But navigating the over 100 petabytes of collected data can be challenging, which is why NASA has collaborated with Microsoft to explore the use of a custom copilot using Azure OpenAI Service to develop NASA's Earth Copilot, which could transform how people interact with Earth's data. Geospatial data is complex, and often requires some level of technical expertise to navigate it. As a result, this data tends to be accessible only to a limited number of researchers and scientists. As NASA collects more data from new satellites, these complexities only grow and may further limit the potential pool of people able to draw insights and develop applications that could benefit society. Recognizing this challenge, NASA embarked on a mission to make its data more accessible and user-friendly. Through NASA's Office of the Chief Science Data Officer, the agency seeks to democratize data access, breaking down technical barriers to empower a diverse range of audiences, from scientists and educators to policymakers and the general public.

YouTube Video Click here to load media The challenge: Navigating the complexity of data NASA's Earth Science Data Systems Program is responsible for collecting an incredible variety of data from spaceborne sensors and instruments. This data spans everything from atmospheric conditions to land cover changes, ocean temperatures and more. However, the sheer scale and complexity of this information can be overwhelming. For many, finding and extracting insights requires navigating technical interfaces, understanding data formats and mastering the intricacies of geospatial analysis — specialized skills that very few non-technical users possess. AI could streamline this process, reducing time to gain insights from Earth's data to a matter of seconds. This issue isn't just a matter of convenience; it has real-world implications. For example, scientists who need to analyze historical data on hurricanes to improve predictive models, or policymakers who want to study deforestation patterns to implement environmental regulations, may find themselves unable to easily access the data they need. This inaccessibility affects a broad array of sectors, including agriculture, urban planning and disaster response, where timely insights from spaceborne data could make a significant difference. Moreover, as new satellites with new instruments continue to launch and collect more data, NASA is constantly faced with the challenge of building new tools to manage and make sense of this growing repository. The agency explored emerging technologies that could not only streamline data discovery but also broaden accessibility, enabling more people to engage with the data and uncover new insights. The solution: AI-powered data access through Microsoft Azure To address these challenges, NASA IMPACT worked with Microsoft to develop an AI-driven customer copilot, called Earth Copilot, which could simplify data access and encourage a wider range of users to interact with its Earth Science data. Together, they built the proof of concept AI model that leverages Microsoft's Azure cloud platform and advanced AI capabilities to transform how users can search, discover and analyze NASA's geospatial data. The key to NASA's Earth Copilot lies in the integration of cloud-based technologies like Azure OpenAI Service, which provides access to powerful AI models and natural language processing capabilities that enable developers to integrate intelligent, conversational AI into their applications. This approach allows NASA to integrate AI into its existing data analysis platform — VEDA. These technologies together make it easier for users to search, discover and analyze Earth Science data By combining these technologies, Earth Copilot enables users to interact with NASA's data repository through plain language queries. Instead, they can simply ask questions such as "What was the impact of Hurricane Ian in Sanibel Island?" or "How did the COVID-19 pandemic affect air quality in the US?" AI will then retrieve relevant datasets, making the process seamless and intuitive. NASA's EARTHDATA VEDA Dashboard. "Azure's robust suite of services, including machine learning, data analytics and scalable cloud infrastructure, powers this AI prototype," said Juan Carlos López, former NASA engineer and current Azure Specialist at Microsoft. "We've designed the system to handle complex queries and large datasets efficiently, ensuring that users can quickly find the information they need without getting bogged down by technical complexities. Our goal was to create a seamless, scalable solution that could evolve as NASA's data, tools and applications grow." Democratizing data for open science The collaboration between NASA IMPACT and Microsoft has resulted in a solution that democratizes access to spaceborne data, enabling a broader range of users to engage with NASA's science data. This has significant benefits for the scientific community, as researchers can now spend less time on data retrieval and more on analysis and discovery. For example, climate scientists can quickly access historical data to study trends, while agricultural experts can gain insights into soil moisture levels to improve crop management. Educators and teachers can use real-world examples to engage students in Earth Science, fostering curiosity and encouraging the next generation of scientists and engineers. Policymakers can leverage the data to make informed decisions on critical issues like climate change, urban development and disaster preparedness, ensuring they have the most accurate information at their fingertips. "The vision behind this collaboration was to leverage AI and cloud technologies to bring Earth's insights to communities that have been underserved, where access to data can lead to tangible improvements," said Minh Nguyen, Cloud Solution Architect at Microsoft. "By enabling users to interact with the data through simple, plain language queries, we're helping to democratize access to spaceborne information." The development of this AI prototype aligns with NASA's Open Science initiative, which aims to make scientific research more transparent, inclusive and collaborative. By removing barriers to data discovery, NASA and Microsoft are setting the stage for a new era of discovery, where insights are not confined to a select few but can be explored and expanded by anyone curious about the world. Looking ahead: Bridging the gap between data and insights At the moment, the NASA Earth Copilot is available to NASA scientists and researchers to explore and test its capabilities. Any responsible deployment of AI technologies requires rigorous assessments to ensure the data and outputs cannot be misused. After a period of internal evaluations and testing, the NASA IMPACT team will explore the integration of this capability into the VEDA platform. This collaboration exemplifies how technology can empower people, drive innovation and create positive change. Solutions like this will be essential in ensuring the benefits of data are shared widely, enabling more people to engage with, analyze and act upon information that shapes our world. Tags: AI, Azure OpenAI Service, Copilot, Microsoft Azure

Across every industry, AI is creating a fundamental shift in what's possible, enabling new use cases and driving business outcomes. While organizations around the world recognize the value and potential of AI, for AI to be truly effective it must be tailored to specific industry needs.

Today, we're announcing adapted AI models, expanding our industry capabilities and enabling organizations to address their unique needs more accurately and effectively. In collaboration with industry partner experts like Bayer, Cerence, Rockwell Automation, Saifr, Siemens Digital Industries Software, Sight Machine and more, we're making these fine-tuned models, pre-trained using industry-specific data, available to address customers' top use cases.

Underpinning these adapted AI models is the Microsoft Cloud, our platform for industry innovation. By integrating the Microsoft Cloud with our industry-specific capabilities and a robust ecosystem of partners, we provide a secure approach to advancing innovation across industries. This collaboration allows us to create extensive scenarios for customers globally, with embedded AI capabilities — from industry data solutions in Microsoft Fabric to AI agents in Microsoft Copilot Studio to AI models in Azure AI Studio — that enable industries to realize their full potential.

We're pleased to introduce these new partner-enabled models from leading organizations that are leveraging the power of Microsoft's Phi family of small language models (SLMs). These models will be available through the Azure AI model catalog, where customers can access a wide range of AI models to build custom AI solutions in Azure AI Studio, or directly from our partners. The models available in the Azure AI model catalog can also be used to configure agents in Microsoft Copilot Studio, a platform that allows customers to create, customize and deploy AI-powered agents, which can be applied to an industry's top use cases to address its most pressing needs.

Bayer, a global enterprise with core competencies in the life science fields of healthcare and agriculture, will make E.L.Y. Crop Protection available in the Azure AI model catalog. A specialized SLM, it is designed to enhance crop protection sustainable use, application, compliance and knowledge within the agriculture sector. Built on Bayer's agricultural intelligence, and trained on thousands of real-world questions on Bayer crop protection labels, the model provides ag entities, their partners and developers a valuable tool to tailor solutions for specific food and agricultural needs. The model stands out due to its commitment to responsible AI standards, scalability to farm operations of all types and sizes and customization capabilities that allow organizations to adapt the model to regional and crop-specific requirements. Cerence, which creates intuitive, seamless and AI-powered user experiences for the world's leading automakers, is enhancing its in-vehicle digital assistant technology with fine-tuned SLMs within the vehicle's hardware. CaLLM™ Edge, an automotive-specific, embedded SLM, will be available in the Azure AI model catalog. It can be used for in-car controls, such as adjusting air conditioning systems, and scenarios that involve limited or no cloud connectivity, enabling drivers to access the rich, responsive experiences they've come to expect from cloud-based large language models (LLMs), no matter where they are. Rockwell Automation, a global leader in industrial automation and digital transformation, will provide industrial AI expertise via the Azure AI model catalog. The FT Optix Food & Beverage model brings the benefits of industry-specific capabilities to frontline workers in manufacturing, supporting asset troubleshooting in the food and beverage domain. The model provides timely recommendations, explanations and knowledge about specific manufacturing processes, machines and inputs to factory floor workers and engineers. Saifr, a RegTech within Fidelity Investments' innovation incubator, Fidelity Labs, will introduce four new models in the Azure AI model catalog, empowering financial institutions to better manage regulatory compliance of broker-dealer communications and investment adviser advertising. The models can highlight potential regulatory compliance risks in text (Retail Marketing Compliance model) and images (Image Detection model); explain why something was flagged (Risk Interpretation model); and suggest alternative language that might be more compliant (Language Suggestion model). Together, these models can enhance regulatory compliance by acting as an extra set of review eyes and boost efficiency by speeding up review turnarounds and time to market. Siemens Digital Industries Software, which helps organizations of all sizes digitally transform using software, hardware and services from the Siemens Xcelerator business platform, is introducing a new copilot for NX X software, which leverages an adapted AI model that enables users to ask natural language questions, access detailed technical insights and streamline complex design tasks for faster and smarter product development. The copilot will provide CAD designers with AI-driven recommendations and best practices to optimize the design process within the NX X experience, helping engineers implement best practices faster to ensure expected quality from design to production. The NX X copilot will be available in the Azure Marketplace and other channels. Sight Machine, a leader in data-driven manufacturing and industrial AI, will release Factory Namespace Manager to the Azure AI model catalog. The model analyzes existing factory data, learns the patterns and rules behind the naming conventions and then automatically translates these data field names into standardized corporate formats. This translation makes the universe of plant data in the manufacturing enterprise AI-ready, enabling manufacturers to optimize production and energy use in plants, balance production with supply chain logistics and demand and integrate factory data with enterprise data systems for end-to-end optimization. The bottling company Swire Coca-Cola USA plans to use Factory Namespace Manager to efficiently map its extensive PLC and plant floor data into its corporate data namespace.

We also encourage innovation in the open-source ecosystem and are offering five open-source Hugging Face models that are fine-tuned for summarization and sentiment analysis of financial data.

Partner-enabled adapted AI models for industry will be available through the Azure AI model catalog or directly from partners.

Additionally, last month we announced new healthcare AI models in Azure AI Studio. These state-of-the-art multimodal medical imaging foundation models, created in partnership with organizations like Providence and Paige.ai, empower healthcare organizations to integrate and analyze a variety of data types, leveraging intelligence in modalities other than text in specialties like ophthalmology, pathology, radiology and cardiology.

Microsoft also offers AI agents that are purpose-built for industry scenarios. Available in Copilot Studio, these agents can be configured to support organizations' industry-specific needs. For example, retailers can use the Store Operations Agent to support retail store associates and the Personalized Shopping Agent to enhance customers' shopping experiences. Manufacturers can use the Factory Operations Agent to enhance production efficiency and reduce downtime by enabling engineers and frontline workers to quickly identify and troubleshoot issues.

All this AI innovation wouldn't be possible without a solid data estate, because AI is only as good as the data it's built upon. By ensuring data is accurate, accessible and well integrated, organizations can unlock deeper insights and drive more effective decision-making with AI. Microsoft Fabric, a data platform built for the era of AI, helps unify disparate data sources and prepares data for advanced analytics and AI modeling. It offers industry data solutions that address each organization's unique needs and allows them to discover, deploy and do more with AI.

At the core of our AI strategy is a commitment to trustworthy AI. This commitment encompasses safety, security and privacy, ensuring that AI solutions are built with the highest standards of integrity and responsibility. Trustworthy AI is foundational to everything we do, from how we work with customers to the capabilities we build into our products.

At Microsoft, we combine industry AI experience, insights and capabilities with a deep understanding of customer challenges and objectives. Along with a trusted ecosystem of experienced partners, we unlock the full potential of AI for each industry and business. Our goal is not just to offer or implement AI tools but to help customers succeed by embedding AI into the very core of what each industry does.

AI transformation is here, and Microsoft is at the forefront of this revolution. As we continue to navigate this new era of innovation, it's clear that AI will play a pivotal role in shaping the future of business across all industries and that Microsoft will continue to lead the way. To learn more about how customers in a variety of industries are transforming with AI, visit How real-world businesses are transforming with AI.

Tags: AI, Azure, Azure AI Studio, Microsoft Cloud, Microsoft Copilot, Microsoft Copilot Studio, Trustworthy AI

Updated Dec. 9, 2024: The number of individuals Microsoft helped train and certify over the past year has been updated to 23 million and includes initiatives across Microsoft.

In 2024, generative AI emerged as a key driver for business outcomes across every industry. Already this new generation of AI is having an incredible impact on our world — yet companies and industries are just scratching the surface of what's possible as they continue to develop new use cases across every role and function.

To help guide organizations on their AI transformation journey, Microsoft recently commissioned a new study through IDC, The Business Opportunity of AI. IDC's findings show that when organizations truly commit to and invest in AI, the return on investment (ROI) potential grows significantly.

According to IDC, the study's findings reflect a tipping point as AI gains momentum across industries. As companies worldwide go deeper with AI, Microsoft customers continue to deploy innovative new solutions and discover how tools like Copilot can transform their day-to-day work. In telecommunications, Lumen Technologies estimates Copilot is saving sellers an average of four hours a week, equating to $50 million annually. In healthcare, Chi Mei Medical Center doctors now spend 15 minutes instead of an hour writing medical reports, and nurses can document patient information in under five minutes. Pharmacists are now able to double the number of patients they see per day. In retail, AI models help Coles predict the flow of 20,000 stock-keeping units to 850 stores with remarkable accuracy, generating 1.6 billion predictions daily.

IDC's findings align with what Microsoft is seeing as we work with companies across industries to deploy AI. We've highlighted more than 200 of our top AI customer stories to show a sampling of how AI is already driving impact today. Below is a look at the top trends we're seeing in IDC's study and the impact of those trends on organizations working with AI today.

#1 Enhanced productivity has become table stakes. Employee productivity is the No. 1 business outcome that companies are trying to achieve with AI. The study shows that 92% of AI users surveyed are using AI for productivity, and 43% say productivity use cases have provided the greatest ROI. While productivity is a top goal, generative AI use cases that are close behind include customer engagement, topline growth, cost management and product or service innovation — and nearly half of the companies surveyed expect AI to have a high degree of impact across all those areas over the next 24 months.

At the global marketing and advertising agency dentsu, employees are already saving 15 to 30 minutes a day using Copilot for tasks such as summarizing chats, generating presentations and building executive summaries.

"Copilot has transformed the way we deliver creative concepts to our clients, enabling real-time collaboration. Agility, security and uniqueness are crucial, but our goal is to lead this transformation company-wide, from top to bottom."

#2 Companies are gravitating to more advanced AI solutions. In the next 24 months, more companies expect to build custom AI solutions tailored directly to industry needs and business processes, including custom copilots and AI agents. This shows a growing maturity in AI fluency as companies realize the value of out-of-the-box use cases and expand to more advanced scenarios.

Siemens has developed the Siemens Industrial Copilot, which has eased the challenges caused by increasing complexity and labor shortages for dozens of customers in different industries.

"In full appreciation of GenAI's transformational potential, it's important to remember that production does not have an 'undo' button. It takes diligence and effort to mature AI to industrial-grade quality. The Siemens Industrial Copilot for Engineering significantly eases our customers' workload and addresses the pressing challenges of skill shortages and increasing complexity in industrial automation. This AI-powered solution is a game-changer for our industry with over 50 customers already using it to boost efficiency and tackle labor shortages."

#3 Generative AI adoption and value is growing across industries. Even though it is relatively new to the market, generative AI adoption is rapidly expanding — 75% of respondents report current usage up from 55% in 2023. The ROI of generative AI is highest in Financial Services, followed by Media & Telco, Mobility, Retail & Consumer Packaged Goods, Energy, Manufacturing, Healthcare and Education. Overall, generative AI is generating higher ROI across industries.

Providence has leveraged AI to extend and enhance patient care, streamline processes and workflows and improve the effectiveness of caregivers.

"Whether we're partnering with organizations on the leading edge of this technology — like Microsoft — and building bespoke solutions through Azure OpenAI Service, advancing clinical research to help cancer patients receive personalized and precise treatments faster, or 'hitting the easy button' and adopting established technologies like Microsoft 365 Copilot or DAX Copilot, we have successfully stayed on the forefront of this tech revolution. For example, physicians who use DAX Copilot save an average of 5.33 minutes per visit, and 80% of physicians have reported lower cognitive burden after using DAX Copilot."

— Sarah Vaezy, EVP, Chief Strategy and Digital Officer at Providence

#4 AI leaders are seeing greater returns and accelerated innovation. While companies using generative AI are averaging $3.7x ROI, the top leaders using generative AI are realizing significantly higher returns, with an average ROI of $10.3. In addition to the enhanced business value, leaders are also on an accelerated path to build and implement new solutions — 29% of leaders implement AI in less than 3 months versus 6% of companies in the laggard category.

Södra is an international forest industry group that processes forest products from 52,000 owners into renewable, climate-smart products for international market. Every day Södra collects and interprets climate impact data to make thousands of decisions for every part of the value chain.

"With innovative AI technology from Microsoft, our business experts and data scientists have been able to help make us more sustainable while also improving revenue significantly."

#5 Looking ahead: Skilling remains a top challenge. Thirty percent of respondents indicated a lack of specialized AI skills in-house, and 26 percent say they lack employees with the skills needed to learn and work with AI. This dovetails with findings from the Microsoft and LinkedIn 2024 Work Trend Index Annual Report, which found that 55 percent of business leaders are concerned about having enough skilled talent to fill roles.

That is why over the past year we have helped train and certify over 23 million people in more than 200 countries in digital skills. And we are committed to working in partnership with governments, educational institutions, industry and civil society to help millions more learn to use AI.

The University of South Florida (USF) is partnering with Microsoft to streamline processes and enhance innovation for all aspects of university operations with AI.

"We're giving students a leg up to do amazing things with AI as part of tomorrow's workforce. Our focus on generative AI not only drives operational efficiency but also empowers our community to unlock new levels of creativity and impact, further positioning USF as a leader in AI adoption, which includes being among the first universities in the nation to form a college dedicated to AI, cybersecurity and computing."

— Sidney Fernandes, CIO & VP of Digital Experiences at University of South Florida

While companies today are largely implementing out-of-the-box generative AI solutions and seeing significant ROI, more than half of those surveyed expect to build custom industry and line-of-business applications in the next 24 months — demonstrating that today's ROI is quickly becoming tomorrow's competitive edge.

"We are at an inflection point of autonomous agent development and are beginning an evolution from using just off-the-shelf assistants and copilots that support knowledge discovery and content generation to custom AI agents to execute complex, multistep workflows across a digital world," says Ritu Jyoti, GVP/GM, AI and Data Research at IDC. "With responsible technology usage and workplace transformation, IDC predicts that business spending to adopt AI will have a cumulative global economic impact of $19.9 trillion through 2030 and drive 3.5% of global GDP in 2030."

Key findings from IDC's The Business Opportunity of AI study include:

Generative AI usage jumped from 55% in 2023 to 75% in 2024. For every $1 a company invests in generative AI, the ROI is $3.7x. The top leaders using generative AI are realizing an ROI of $10.3. On average, AI deployments are taking less than 8 months and organizations are realizing value within 13 months. Within 24 months, most organizations plan to expand beyond pre-built AI solutions to advanced AI workloads that are customized or custom-built. The ROI of generative AI is highest in Financial Services, followed by Media & Telco, Mobility, Retail & Consumer Packaged Goods, Energy, Manufacturing, Healthcare and Education. 43% say productivity use cases have provided the greatest ROI. The primary way that organizations are monetizing AI today is through productivity use cases. In the next 24 months, a greater focus will be placed on functional and industry use cases. The top barrier when implementing AI is the lack of both technical and day-to-day AI skills. Learn how to fuel your AI journey

IDC's study, which included more than 4,000 business leaders and AI decision-makers around the world, also identifies the top barriers organizations face when implementing AI. As businesses integrate new solutions, they navigate important considerations such as data privacy, responsible use and the need for investment in both technology and skills.

No matter where you are in your cloud and AI transformation journey, Microsoft can help. To learn more about how customers across industries are shaping their AI transformation with Microsoft, please visit Microsoft's AI in Action page. For more on how to get started in your AI transformation journey, visit Microsoft AI.

IDC InfoBrief: sponsored by Microsoft, 2024 Business Opportunity of AI, IDC# US52699124, November 2024

Tags: AI, Azure OpenAI Service, Copilot, Dax Copilot, Microsoft 365 Copilot

When I look to the next phase of Microsoft, both in terms of our scale and our massive opportunity ahead, it's clear that we need to continue adding exceptional talent at every level of the organization to increase our depth and capability across our business priorities – spanning security, quality, and AI innovation.

With that context, I'm excited to share that Jay Parikh is joining Microsoft as a member of the senior leadership team (SLT), reporting to me. Jay was the global head of engineering at Facebook (now Meta) and most recently was CEO of Lacework. He has an impressive track record, with a unique combination of experiences building and scaling technical teams that serve both commercial customers and consumers. His deep connections across the start-up and VC ecosystems, coupled with his leadership roles at Akamai and Ning, will bring valuable perspective to Microsoft.

Over the years I've known Jay, I've admired him as a technology leader and respected engineer with a deep commitment to driving innovation and striving for operational excellence. His focus extends beyond technology, with his passion for and dedication to developing people, fostering a strong culture, and building world-class talent, all in service of delivering faster value to customers and driving business growth. In fact, there are very few leaders in our industry with Jay's experience in leading teams through the rapid growth and scale required to support today's largest internet businesses.

As he onboards, Jay will immerse himself in learning about our company priorities and our culture and will spend time connecting with our senior leaders and meeting with customers, partners, and employees around the world. We will share more on his role and focus in the next few months.

The pace of AI innovation today continues to be extraordinary, and at Microsoft we are focused on helping organizations embrace it. By providing our customers with the most advanced AI technology across every product we build — combined with our unparalleled partner ecosystem and co-innovation approach — we are helping them make real progress in ways that matter. I am proud to share over 100 customer stories from this quarter alone showing how we are helping customers accelerate AI Transformation — no matter where they are on their journey.

Recently during the Microsoft AI Tour, I spoke with customers who shared ways they are adopting Copilots to empower human achievement, democratize intelligence and realize significant business value. I also discussed the concept of an AI-first business process and the differentiation you can drive when bringing together the power of Copilots and human ambition with the autonomous capabilities of an agent. I was inspired by the outcomes our customers have achieved through pragmatic innovation and the progress they are making to evolve the future of industry. I am pleased to share ten stories from the past quarter that illustrate how Copilots have yielded results for our customers, while highlighting AI Transformation experiences in their own words.

Accenture and Avanade have a long history of helping customers implement cutting-edge solutions, with internal testing a key factor in their ability to deliver customizable Microsoft solutions with deep expertise. Putting Microsoft 365 Copilot into the hands of employees helped them realize ways to increase productivity, with 52% of employees seeing a positive impact on the quality of their work, 31% reporting less cognitive fatigue and 84% finding Copilot's suggestions fair, respectful and non-biased. Accenture also piloted GitHub Copilot to help build better solutions faster with developers spending less time debugging, resulting in 95% of developers reporting they enjoyed coding more.

"Using our extensive Microsoft technology expertise and practical learnings from our own experience implementing Microsoft 365 Copilot, our solutions empower clients to fully tap into Microsoft AI capabilities."

Nigerian multinational financial services group Access Holdings Plc. serves more than 56 million customers across 18 countries. As the business grew and transitioned from a small bank to a major holding company, it adopted Microsoft 365 Copilot to address challenges in data management, meeting productivity and software development. With the integration of Copilot into daily tools, the company significantly enhanced efficiency and engagement across the business. Writing code now takes two hours instead of eight, chatbots can be launched in 10 days instead of three months and presentations can be prepared in 45 minutes instead of six hours. Copilot has also driven a 25% increase in staff engagement during meetings.

"To inspire everyone in the organization to take advantage of AI, we knew we had to integrate AI into the tools people use every day. Microsoft 365 Copilot made the most sense and was a natural fit for us."

Lanre Bamisebi, Executive Director IT and Digitalization at Access Holdings, Plc.

To improve resident services and reinvent customer engagement, the City of Burlington, Ontario, embraced AI and low-code tools to develop new online services that transform and automate internal processes. In just eight weeks, the city utilized Copilot Studio to develop and launch a custom copilot designed to help residents quickly find answers to frequently asked questions. The city also developed a portal that streamlines building permit reviews and enables customers to track the status of their own applications. As a result, the average time it takes to process a permit approval decreased from 15 weeks to 5-7 weeks, allowing more time for city employees to evaluate complex submissions.

"Our staff and citizens do not have to worry about mundane tasks as much anymore. Now they're able to have rich, collaborative conversations about how to creatively solve problems, making for a much more fulfilling and rewarding work and customer experience."

Chad MacDonald, Executive Director and Chief Information Officer at the City of Burlington

Finastra empowers financial institutions with leading software for lending, payments, treasury, capital markets and universal banking. To transform its marketing processes, the company used Microsoft 365 Copilot to automate tasks, enhance content creation, improve analytics and personalize customer interactions. Since integrating Copilot, the team reduced time-to-market for campaigns from three months to less than one. Copilot also significantly reduced the time marketers spend generating and gathering insights from each campaign, with employees citing a 20%-50% time savings across tasks like full-funnel analysis, supply management analysis and budget management.

"Copilot makes you more effective because you get better insights, and it makes you more efficient because you can produce results faster. It also makes work more meaningful and fun because your team can focus on what matters — strategy, creativity and everything that sets you apart from the competition."

Joerg Klueckmann, Head of Corporate Marketing and Communications at Finastra

GoTo Group provides technology infrastructure and solutions across Indonesia. It is bending the curve on innovation by significantly enhancing productivity and code quality across its engineering teams by adopting GitHub Copilot. With real-time code suggestions, chat assistance and the ability to break down complex coding concepts, the company has saved over seven hours per week and achieved a 30% code acceptance rate within the first month. With 1,000 engineers already using GitHub Copilot, the tool allows them to innovate faster, reduce errors and focus more time on complex tasks to deliver greater value to their users.

"GitHub Copilot has significantly reduced syntax errors and provided helpful autocomplete features, eliminating repetitive tasks and making coding more efficient. This has allowed me to focus on the more complex elements in building great software."

South Africa's Milpark Education faced operational challenges when shifting to online learning due to legacy systems slowing down student interactions and support. Through close collaboration with Enterprisecloud, Milpark migrated its back-office infrastructure to Azure within three months, replacing its legacy student admissions system with an extensible, integrated digital platform powered by technologies such as Microsoft Copilot and Copilot Studio. In just four months, the educational institution improved efficiency and accuracy of student support, decreasing the average resolution time by 50% and escalations by more than 30%.

"Using Copilot, agents are now able to use generative AI to rapidly get up to speed on case details and respond to students using standardized templates that help them provide more personalized and professional responses. The results speak for themselves."

For over two decades, Teladoc Health has been offering a broad spectrum of services to patients using virtual care services — from primary care to chronic condition management. After the rapid growth of telehealth adoption post-pandemic, operational efficiency was instrumental in managing internal processes and external client interactions. By deploying Microsoft 365 Copilot and using Copilot in Power Automate, the company has reshaped business processes to help employees realize greater time savings while enhancing the client experience. The Copilots and agents helped employees save five hours per week and thousands of enterprise hours annually by eliminating mundane daily processes and fostering better cross-department communications, while also helping new employees get set up to run their workflows 20% faster.

"Copilot is changing the way we work. It's not just about saving time; it's about enhancing the quality of our work, allowing us to focus on what truly matters: delivering exceptional care to our members."

Heather Underhill, SVP Client Experience & Operations at Teladoc Health

International energy company Uniper adopted a single-cloud strategy with Azure as its foundation to drive rapid AI innovation. To help its employees focus on using core competencies, the company implemented Microsoft 365 Copilot to reduce time spent on manual and repetitive tasks, and help workers focus on more pressing work, such as developing enhanced solutions to speed up the energy transition. Its in-house auditors have already increased productivity by 80% by using Copilot to create plans and checklists. Uniper is also using Copilot for Security to help identify risks twice as fast and take appropriate action sooner.

"As an operator of critical infrastructure, we have to contend with a growing number of reports of phishing and attacks by hackers. AI can help us implement a sensible way of managing the sheer number of threats."

British telecommunications company Vodafone has transformed its workplace productivity with Microsoft 365 Copilot, already seeing strong ROI from its adoption. In early trials, Copilot saved employees an average of three hours per week by using the tool to draft emails, summarize meetings and search for information. Copilot is also enriching the employee experience, with 90% of users reporting they are eager to continue using Copilot and 60% citing improved work quality. For Vodafone's legal and compliance team, Copilot has significantly accelerated the processes of drafting new contracts, reducing the time required to complete this work by one hour. As a result of these efficiency gains, Vodafone is rolling out Copilot to 68,000 employees.

"Our AI journey is focusing on three areas: operational efficiency inside the organization; rewiring the business to provide an enhanced customer experience; and unlocking growth opportunities through new products and services that we can create around generative AI. Copilot will help drive all three."

Wallenius Wilhelmsen, a global leader in roll-on/roll-off shipping and vehicle logistics, is empowering better decision-making while fostering a culture of innovation and inclusion with AI tools. After participating in an early access program, the company broadly adopted Microsoft Copilot 365 to help streamline processes, enhance data management and improve communication across its 28 countries. To help strengthen Copilot immersion and realize value faster, they introduced a seven-week Microsoft Viva campaign to teach, communicate and measure Copilot adoption. The campaign resulted in 80% of employees using Copilot, with some teams realizing time savings of at least 30 minutes per day. The company also uses Copilot Dashboard to manage usage and gather user feedback, helping demonstrate ROI and measure results outside of time savings alone.

"Copilot changes the way we think and work while keeping us curious and open to embracing opportunities. I think that is the sort of benefit that is not so measurable, but important. So, my time management and structured approach to my everyday work life has been enhanced with Copilot and Viva."

I believe that no other company has a better foundation to facilitate your AI Transformation than Microsoft. As we look ahead to Microsoft Ignite, I am excited by the latest innovation we will announce as a company, and the customer and partner experiences we will share. We remain committed to driving innovation that creates value in ways that matter most to our customers, and believe we are at our best when we serve others. There has never been a better opportunity for us to accomplish our mission of empowering every person and every organization on the planet to achieve more than now, and I look forward to the ways we will partner together to help you achieve more with AI.

Accelleron: Accelleron turbocharges IT support solutions and resolution times with Power Platform

Agnostic Intelligence: Agnostic Intelligence transforms risk management with Azure OpenAI Service, achieving up to 80% time savings

Alaska Airlines: How Alaska Airlines uses technology to ensure its passengers have a seamless journey from ticket purchase to baggage pickup

Allgeier: Allgeier empowers organizations to own and expand data operations

ANZ Group: ANZ launches first-of-its-kind AI Immersion Centre in partnership with Microsoft

Asahi Europe & International: Asahi Europe & International charts new paths in employee productivity with Microsoft Copilot

Auburn University: Auburn University empowers thousands of students, faculty and staff to explore new ways of using AI with Microsoft Copilot

Avanade: Avanade equips 10,000 employees with Microsoft Fabric skills to help customers become AI-driven and future-ready

Azerbaijan Airlines: Azerbaijan Airlines expands data access to increase efficiency by 70% with Microsoft Dynamics 365

Aztec Group: Aztec Group uses Copilot for Microsoft 365 to enhance the client experience whilst powering efficiencies

Bader Sultan: Bader Sultan uses Microsoft Copilot to boost productivity and serve clients faster

BaptistCare: BaptistCare supports aging Australians and tackles workforce shortages through Microsoft 365 Copilot

Barbeque Mania!: Barbecue Mania! centralizes your data with Microsoft Azure and saves $3.5 million over 5 years

Bank of Montreal: Bank of Montreal reduces costs by 30% with Azure

BlackRock: How BlackRock's 'flight crew' helped Copilot for Microsoft 365 take off

Capita: Capita uses GitHub Copilot to free developers and deliver faster for customers

Cassidy: Cassidy and Azure OpenAI Service: Making AI simple for all

Cdiscount: Cdiscount, Azure OpenAI Service and GitHub Copilot join forces for e-commerce

Chalhoub Group: Chalhoub Group's People Analytics team speeds reporting with Microsoft Fabric

ClearBank: ClearBank processes 20 million payments a month — up from 8,000 — with platform built on Azure

Cloud Services: Faster with Fabric: Cloud Services breaks new ground with Microsoft

Coles Supermarkets: Coles Supermarkets embraces AI, cloud applications in 500-plus stores with Azure Stack HCI​

Commercial Bank of Dubai: Commercial Bank of Dubai: innovating a future proof banking platform with Microsoft Azure

CPFL: CPFL expands its data repository by 1500% with Mega Lake project on Microsoft Azure

Cummins: Cummins uses Microsoft Purview to automate information governance more efficiently in the age of AI

Dubai Electricity and Water Authority (DEWA): DEWA pioneers the use of Azure AI Services in delivering utility services

Digi Rogaland: Digi Rogaland prioritizes student safety with Bouvet and Microsoft Fabric

E.ON: A modern workspace in transition: E.ON relies on generative AI to manage data floods with Copilot for Microsoft 365

EPAM Systems: Efficiency inside and out: EPAM streamlines communications for teams and clients with Copilot for Microsoft 365

Fast Shop: Fast Shop consolidated its data platform on Microsoft Azure and is now ready for the era of AI

FIDO Tech: AI tool uses sound to pinpoint leaky pipes, saving precious drinking water

Florida Crystals Corporation: Telecom expenses for Florida Crystals dropped 78% with Teams Phone and Teams Rooms

Four Agency: Four Agency innovates with Microsoft 365 Copilot to deliver better work faster

Fractal: Fractal builds innovative retail and consumer goods solutions with Microsoft's AI offerings including Azure OpenAI Service

GE Aerospace: GE Aerospace launches company-wide generative AI platform for employees

Georgia Tech Institute for Data Engineering and Science: Georgia Tech is accelerating the future of electric vehicles using Azure OpenAI Service

Hitachi Solutions: Hitachi Solutions transforms internal operations with Microsoft Fabric

IBM Consulting: How IBM Consulting drives AI-powered innovation with Fabric expertise

Insight Enterprises: Insight Enterprises achieves 93% Microsoft Copilot use rate, streamlining business operations to pave the way for customer success

Intesa Sanpaolo: Intesa Sanpaolo accrues big cybersecurity dividends with Microsoft Sentinel, Copilot for Security

ITOCHU Corporation: ITOCHU uses Microsoft Fabric and Azure AI Studio to evolve its data analytics dashboard into a service delivering instant recommendations

IU International University of Applied Sciences (IU): IU revolutionizes learning for its students with the AI study buddy Syntea and Azure OpenAI Service

John Cockerill: John Cockerill engages pro developers to build enterprise-wide apps with Power Platform

Kaya Limited: Kaya Limited elevates customer experience and operational efficiency with Microsoft Dynamics 365 and Power BI

LexisNexis: LexisNexis elevates legal work with AI using Copilot for Microsoft 365

Lionbridge: Lionbridge disrupts localization industry using Azure OpenAI Service and reduces turnaround times by up to 30%

Lotte Hotels & Resorts: Hotelier becomes a citizen developer, building a smart work culture based on Power Platform and hyper-automated work environment

Lumen Technologies: Microsoft and Lumen Technologies partner to power the future of AI and enable digital transformation to benefit hundreds of millions of customers

LS ELECTRIC: LS ELECTRIC uses data to optimize power consumption with Sight Machine and Microsoft Cloud for Manufacturing

MAIRE: MAIRE, transforming the energy sector and an entire company culture with Microsoft 365 Copilot

McKnight Foundation: McKnight Foundation accelerates its mission and supports community partners with Microsoft 365 Copilot

MISO: MISO undergoes a digital transformation with Microsoft Industry Solutions Delivery

Mitsubishi Heavy Industries, Ltd. (MHI): Recognizing the essence of AI and building the future with clients: MHI's DI to create proprietary architecture using Azure OpenAI Service

Molslinjen: Molslinjen develops an AI-powered dynamic pricing strategy with Azure Databricks

National Australia Bank: National Australia Bank invests in an efficient, cloud-managed future with Windows 11 Enterprise

Nagel-Group: Works agreements and contracts: Nagel-Group uses Azure OpenAI Service to help employees find information

NC Fusion: Elevating experiences with AI, from productivity to personalization

National Football League Players Association: The National Football League Players Association and Xoriant use Azure AI Services to provide protection to players across 32 teams

Northwestern Medicine: Northwestern Medicine deploys DAX Copilot embedded in Epic within its enterprise to improve patient and physician experiences

Oncoclínicas: Oncoclínicas creates web portal and mobile app to store clinical and medical procedures with Azure Cognitive Services

PA Consulting: PA Consulting saves hours a week with Copilot for Microsoft 365 and Copilot for Sales

Parexel: Parexel speeds operational insights by 70% using Microsoft Azure, accelerating data product delivery and reducing manual work

Petrochemical Industries Company (PIC): From weeks to days, hours to seconds: PIC automates work processes to save time with Microsoft 365 Copilot

PKSHA Technology: PKSHA leans on Copilot for Microsoft 365 as part of their team

Planted: Planted combines economic growth and environmental sustainability — with Microsoft Azure OpenAI

Programa De Atención Domiciliaria: The Home Care Program in Panama helped more than 17,000 people with the power of Microsoft Power Automate

PwC: PwC scales GenAI for enterprise with Microsoft Azure AI

QNET: QNET increases security response efficiency 60 percent with Microsoft Security Solutions

RTI International: Research nonprofit RTI International improves the human condition with Microsoft 365 Copilot

Rijksmuseum: Rijksmuseum transforms how art lovers engage with the museum, with Dynamics 365

Sandvik Coromant: Sandvik Coromant hones sales experience with Microsoft Copilot for Sales

Simpson Associates: Simpson Associates spurs justice for at-risk communities with Azure AI

Softchoice: Softchoice harnesses Microsoft Copilot and reduces content creation time by up to 70%, accelerating customer AI journeys with its experience

Sonata Software: Sonata Software goes from early adopter to market leader with Fabric

Swiss International Air Lines (SWISS): SWISS targets 30% cost savings, increased passenger satisfaction with Azure

SymphonyAI: SymphonyAI is solving real problems across industries with Azure AI

Syndigo: Syndigo accelerates digital commerce for its customers by more than 40% with Azure

TAL: TAL and Microsoft join forces on strategic technology deal

Tecnológico de Monterrey: Tecnológico de Monterrey university pioneers ambitious AI-powered learning ecosystem

Telstra: Telstra and Microsoft expand strategic partnership to power Australia's AI future

The University of Sydney: The University of Sydney utilizes the power of Azure OpenAI to allow professors to create their own AI assistants

Torfaen County Borough: Torfaen County Borough Council streamlines organizational support for Social Care using Copilot for Microsoft 365

Trace3: Trace3 expands the realm of clients' possibilities with Windows 11 Pro and Microsoft Copilot

Unilever: Unilever is reinventing the fundamentals of research and development with Azure Quantum Elements

University of Wisconsin: Microsoft collaborates with Mass General Brigham and University of Wisconsin–Madison to further advance AI foundation models for medical imaging

Via: Marketplace, online support, and remote work: Via embraces the digital world supported by Microsoft 365, Dynamics 365 and Azure

Virgin Atlantic: How Virgin Atlantic is flying higher with Copilot

Virgin Money: Redi, set, go: Virgin Money delivers exceptional customer experiences with Microsoft Copilot Studio

Visier: Visier achieves performance improvements of up to five times using Azure OpenAI Service

World2Meet (W2M): World2Meet, the travel company providing a better customer experience and operations with a new virtual assistant powered by Microsoft Azure

Xavier College: Xavier College begins a process of modernizing its student information systems on Dynamics 365 and AI, unlocking powerful insights

ZEISS: More time for research: ZEISS supports businesses and researchers with ZEISS arivis Cloud based on Microsoft Azure

ZF Friedrichshafen AG (ZF Group): ZF Group builds manufacturing efficiency with over 25,000 apps on Power Platform

Tags: Azure, Azure AI Services, Azure Cognitive Services, Azure Databricks, Azure OpenAI Service, Azure Quantum Elements, Azure Stack HCI, Copilot, Copilot for Sales, Copilot for Security, Copilot Studio, Dax Copilot, GitHub Copilot, Microsoft 365, Microsoft 365 Copilot, Microsoft AI Tour, Microsoft Cloud for Manufacturing, Microsoft Dynamics 365, Microsoft Fabric, Microsoft Ignite, Microsoft Power Platform, Microsoft Sentinel, Microsoft Teams, Microsoft Viva, Power Automate, Power BI

Today, I am sharing Microsoft's 2024 Diversity & Inclusion Report, our most global and transparent report to date. This marks our sixth consecutive annual report and the eleventh year sharing our global workforce data, highlighting our progress and areas of opportunity.

Our ongoing focus on diversity and inclusion is directly tied to our inherently inclusive mission — to empower every person and every organization on the planet to achieve more, enabling us to innovate in the era of AI. As we approach our company's 50th anniversary, we remain deeply committed to D&I because it is what creates transformational solutions to the most complex challenges for customers, partners and the world.

We gather a range of data, which is presented in specific ways throughout the report. In the following section, it is important to understand the distinction between our *Broader Microsoft business and our **Core Microsoft business.

Datacenters: As we lead the AI platform shift, our workforce continues to expand to include employees with varied backgrounds and roles, and we are sharing new data this year on a growing employee population in datacenter roles. The population of datacenter employees grew 23.9% globally and 28.9% in the US in 2024, more than tripling since 2020. In our most global report to date, we expanded new global Self-ID data to include Indigenous and military employees, as well as those with disabilities. For example, 5.7% of global employees in our core Microsoft business self-identified as having a disability, an increase of 0.2 percentage points year over year. We continue to have pay equity. For median unadjusted pay analysis, the data shows we have made progress in narrowing the gaps. This year we expanded pay equity analysis and median unadjusted pay analysis to not only include women inside and outside the US, but also include a combined view of women globally. Increasing representation for women and racial and ethnic minority groups at more senior levels, combined with maintaining pay equity for all, will continue to reduce the median unadjusted pay gap.

Representation of women in our core Microsoft workforce is 31.6%, an increase of 0.4 percentage points year over year. Additionally, the representation of women in technical roles is 27.2%, an increase of 0.5 percentage points year over year. Representation of women in our core Microsoft workforce rose year over year at all leadership levels except Executive. Leadership representation in our core Microsoft workforce of Black and African American employees at the Partner + Executive level grew to 4.3%, an increase of 0.5 percentage points year over year. Leadership representation in our core Microsoft workforce of Hispanic and Latinx employees at the Executive level rose to 4.6%, an increase of 0.8 percentage points year over year. In our broader Microsoft workforce, representation of racial and ethnic minority groups is 53.9%, an increase of 0.6 percentage points year over year. Culture and inclusion in focus

Our semi-annual Employee Signals survey focuses on employee experience and helps us deepen our understanding so we can adjust our efforts where needed. These insights show that employees continue to feel like they are thriving, with a global and US score of 76. Within Employee Signals, we focus on thriving, which we define as "being energized and empowered to do meaningful work." This is designed to measure employees' sense of purpose, which is important to personal and professional fulfillment. We survey employees on three dimensions of thriving: Feeling energized, feeling empowered and doing meaningful work. Our Daily Signals survey results indicate employee perceptions around Microsoft's commitment to creating a more diverse and inclusive workplace increased two points year over year to an average score of 79. Since introducing the concept of allyship to employees in 2018, we have inspired and led a positive impact on our culture. As of June 2024, 95.6% of employees reported some level of awareness of the concept of allyship, up from 65.0% in 2019 when we first started asking employees about their awareness. A commitment that spans decades

Our annual D&I report not only reviews our data, but also illuminates the intentional strategy and actions that have helped us make progress across our company's journey.

Being one of the first Fortune 500 companies to expand antidiscrimination policy and benefits to LGBTQIA+ employees in 1989. Announcing our Racial Equity Initiative in June 2020, outlining actions and progress we expect to make by 2025 to help address racial injustice and inequity in the US for Black and African American communities. Launching immersive D&I learning simulations in 2021, allowing employees to practice crucial D&I skills, such as recognizing and addressing bias, responding to microaggressions and demonstrating effective allyship. Building on more than a decade of helping to reskill military service members through our Microsoft Software and Systems Academy (MSSA), and this year expanding this skilling opportunity to train military spouses for portable, in-demand IT roles. Introducing pronouns and self-expression features in Microsoft 365, an innovation brought directly to fruition because we listened to, and collaborated with, customers, partners and employees who asked for these features. A mission as bold as ours

At Microsoft, we're guided by our mission, worldview and culture. Our mission is the why; it drives our actions. Our worldview is the what, shaping our strategy and products. Culture is the how, influencing everything with a focus on growth and innovation. Culture is also the who: Who makes up the workforce, who services our customers, who innovates the future of tech. The diversity of the workforce, combined with inclusion, unlocks individual and collective potential. This is what is necessary to stay relevant, compete at scale and win.

* Broader Microsoft business: Includes the core Microsoft business, plus minimally integrated companies. Employees of joint ventures and newly acquired companies are not included in the data, including Activision, Blizzard, and King. LinkedIn was acquired in December 2016, GitHub was acquired in June 2018, and Activision, Blizzard, and King were acquired in October 2023. We provide standalone data for these three acquisitions. Nuance Communications was acquired in March 2022 and fully integrated in August 2023.

**Core Microsoft business: Represents 88.4% of the worldwide broader Microsoft workforce.

Already, 60 percent of the Fortune 500 are using Microsoft 365 Copilot to accelerate business results and empower their teams. With Copilot supporting sales associates, Lumen Technologies projects $50 million dollars in savings annually. Honeywell(1) equates productivity gains to adding 187 full-time employees and Finastra is reducing creative production time from seven months to seven weeks. Today, we're announcing new agentic capabilities that will accelerate these gains and bring AI-first business process to every organization.

First, the ability to create autonomous agents with Copilot Studio will be in public preview next month.

Second, we're introducing ten new autonomous agents in Dynamics 365 to build capacity for every sales, service, finance and supply chain team.

Copilot is your AI assistant — it works for you — and Copilot Studio enables you to easily create, manage and connect agents to Copilot. Think of agents as the new apps for an AI-powered world. Every organization will have a constellation of agents — ranging from simple prompt-and-response to fully autonomous. They will work on behalf of an individual, team or function to execute and orchestrate businesses process. Copilot is how you'll interact with these agents, and they'll do everything from accelerating lead generation and processing sales orders to automating your supply chain. Empowering more customers to build autonomous agents in Copilot Studio Earlier this year, we announced a host of powerful new capabilities in Copilot Studio, including the ability to create autonomous agents. Next month, these capabilities are shifting from private to public preview, allowing more customers to reimagine critical business processes with AI. Agents draw on the context of your work data in Microsoft 365 Graph, systems of record, Dataverse and Fabric, and can support everything from your IT help desk to employee onboarding and act as a personal concierge for sales and service. Organizations like Clifford Chance, McKinsey & Company, Pets at Home and Thomson Reuters are already creating autonomous agents to increase revenue, reduce costs and scale impact. Pets at Home, the U.K.'s leading pet care business, created an agent for its profit protection team to more efficiently compile cases for skilled human review, which could have the potential to drive a seven-figure annual savings. McKinsey & Company is creating an agent that will speed up the client onboarding process. The pilot showed lead time could be reduced by 90% and administrative work reduced by 30%. Thomson Reuters built a professional-grade agent to speed up the legal due diligence workflow, with initial testing showing some tasks could be done in half the time. This agent can help Thomson Reuters increase the efficiency of work for clients and boost its new business pipeline. Scaling your teams with 10 new autonomous agents in Dynamics 365 New autonomous agents enable customers to move from legacy lines of business applications to AI-first business process. AI is today's ROI and tomorrow's competitive edge. These new agents are designed to help every sales, service, finance and supply chain team drive business value — and are just the start. We will create many more agents in the coming year that will give customers the competitive advantage they need to future-proof their organization. Today, we're introducing ten of these autonomous agents. Here are a few examples:

Sales Qualification Agent: In a profession where time literally equals money, this agent enables sellers to focus their time on the highest priority sales opportunities while the agent researches leads, helps prioritize opportunities and guides customer outreach with personalized emails and responses.

Supplier Communications Agent: This agent enables customers to optimize their supply chain and minimize costly disruptions by autonomously tracking supplier performance, detecting delays and responding accordingly — freeing procurement teams from time consuming manual monitoring and firefighting.

Customer Intent and Customer Knowledge Management Agents: A business gets one chance to make a first impression, and these two agents are game changers for customer care teams facing high call volumes, talent shortages and heightened customer expectations. These agents work hand in hand with a customer service representative by learning how to resolve customer issues and autonomously adding knowledge-based articles to scale best practices across the care team.

As agents become more prevalent in the enterprise, customers want to be confident that they have robust data governance and security. The agents coming to Dynamics 365 follow our core security, privacy and responsible AI commitments. Agents built in Copilot Studio include guardrails and controls established by maker-defined instructions, knowledge and actions. The data sources linked to the agent adhere to stringent security measures and controls — all managed in Copilot Studio. These include data loss prevention, robust authentication protocols and more. Once these agents are created, IT administrators can apply a comprehensive set of features to govern their use. Microsoft's own transformation At Microsoft, we're using Copilot and agents to reimagine business process across every function while empowering employees to scale their impact. Using Copilot, one sales team has achieved 9.4% higher revenue per seller and closed 20% more deals(2). And thanks to Copilot, one team is resolving customer cases nearly 12% faster(3). Our Marketing team is seeing a 21.5% increase in conversion rate on Azure.com with a custom agent designed to assist buyers(4). And in Human Resources, our employee self-service agent is helping answer questions with 42% greater accuracy(5). With Copilot and agents, the possibilities are endless — we can't wait to see what you create. Start building agents in Copilot Studio today. Read more about autonomous agent capabilities on the Copilot Studio and Dynamics 365 blogs. Head to WorkLab for more insights on Microsoft's own AI transformation.

Statistics are from an internal Honeywell survey of 5,000 employees where 611 employees responded. Internal Microsoft Sales Team data based on 687 sellers of Microsoft 365 Copilot, Jan. – June 2024, as compared with sellers with low usage of Copilot. Regular usage of Copilot means sellers who use Copilot daily at least 50% of the time during the testing period. Internal CSS experiment conducted by Microsoft, 600 participants using Copilot Q&A function, Azure Core team, Nov. – Dec. 2023. These results are statistically significant at the 95th% confidence interval. Internal Microsoft Marketing team data, June – Sept. 2024. Conversion means initiating the free account sign-up process on Azure.com. Internal HR experiment conducted by Microsoft, 33 participants, Oct. 2024. These results are statistically significant at the 95th% confidence interval.

Tags: AI, Copilot, Copilot Studio, Dynamics 365, Microsoft 365 Copilot, Microsoft 365 Graph, Microsoft Dataverse, Microsoft Fabric

We're living through a technological paradigm shift. In a few short years, our computers have learned to speak our languages, see what we see and hear what we hear. Yet technology for its own sake counts for nothing. What matters is how it feels to people and what impact it has on societies. It's about how it changes lives, opens doors, expands minds and relieves pressure. It is perhaps the greatest amplifier of human well-being in history, one of the most effective ways to create tangible and lasting benefits for billions of people. And yet technology is, and must always remain, in service to humanity: an enabler and a path to deepening our common bonds and shared understanding, our energy and imagination, our creativity and our capacity for everything from invention to forming relationships. In the field of AI, we often get caught up in the technical details. We spend our time talking about parameters and compute. The focus is on training runs, datacenters and the latest techniques. This is natural and inevitable when operating on the frontiers of something new, where the details do really matter. But I think it's important that in doing all of this, getting stuck right in the technical weeds, we don't lose sight of not only what we are building, but why we are building it. At Microsoft AI, we are creating an AI companion for everyone. I truly believe we can create a calmer, more helpful and supportive era of technology, quite unlike anything we've seen before. Great technology experiences are about how you feel, not what's under the hood. It should be about what you experience, not what we are building. Copilot will be there for you, in your corner, by your side and always strongly aligned with your interests. It understands the context of your life, while safeguarding your privacy, data and security, remembering the details that are most helpful in any situation. It gives you access to a universe of knowledge, simplifying and decluttering the daily barrage of information, and offering support and encouragement when you want it. Over time it'll adapt to your mannerisms and develop capabilities built around your preferences and needs. We are not creating a static tool so much as establishing a dynamic, emergent and evolving interaction. It will provide you with unwavering support to help you show up the way you really want in your everyday life, a new means of facilitating human connections and accomplishments alike. With your permission, Copilot will ultimately be able to act on your behalf, smoothing life's complexities and giving you more time to focus on what matters to you. It'll be an advocate for you in many of life's most important moments. It'll accompany you to that doctor's appointment, take notes and follow up at the right time. It'll share the load of planning and preparing for your child's birthday party. And it'll be there at the end of the day to help you think through a tricky life decision. Some people worry that AI will diminish what makes us unique as humans. My life's work has been to ensure it does precisely the opposite. We choose what we create. This is something we must do together. Our task is to ensure AI always enriches people's lives and strengthens our bonds with others, while supporting our uniqueness and endlessly complex humanity. This is a new era of technology that doesn't just "solve problems," it's there to support you, teach you and help you. In this sense, Copilot really is different from that last wave of the web and mobile. This is the beginning of a fundamental shift in what's possible for all of us. It's a long journey that will take years. With our latest updates to Copilot, you are seeing only the first careful steps in this direction. Patience and care with our deployments are at the very foundation of our approach. My commitment is to be accountable at every stage, work with you and listen to you. Respect and deep compassion for our users and for society is the core purpose behind everything we do. It comes first. This is a journey we promise to take together. I couldn't be more excited to embark on it with you.

Mustafa Suleyman, Executive Vice President and CEO of Microsoft AI __________ From the Copilot Team Our refreshed Copilot is focused on delivering a more intuitive design with more digestible, speedy and fluent answers. It now adapts to you with a warm tone and a distinct style, providing not only information but encouragement, feedback and advice as you navigate life's everyday challenges — no matter how big or small. For example, in addition to helping you plan that big trip, it can also offer tips on how to keep your composure when, say, the back of your seat is getting kicked by a small child. And we're adding advanced capabilities like Voice and Vision that make it both more useful and feel more natural. These modalities change how we connect with technology, enabling people to seamlessly reflect with, learn from and work with their Copilot companions.

YouTube Video Click here to load media Copilot's next phase Today, we're beginning to roll out the updated Copilot, including new and enhanced features: Copilot Voice: We're making it easier than ever to connect with your companion using Copilot Voice. This is the most intuitive and natural way to brainstorm on the go, ask a quick question or even just vent at the end of a tough day. Your companion will be personal to you, with four voice options to choose from.

YouTube Video Click here to load media Copilot Daily: Copilot Daily helps you kick off your morning with a summary of news and weather, all read in your favorite Copilot Voice, with more options like reminders of what's coming soon. It's an antidote for that familiar feeling of information overload. Clean, simple and easy to digest. Copilot Daily will only pull from authorized content sources. We are working with partners such as Reuters, Axel Springer, Hearst Magazines, USA TODAY Network and Financial Times, and plan to add more sources over time. We'll also add additional personalization and controls in Copilot Daily over time. Personalized Discover: Not sure where to start? Copilot Discover helps you get going, offering both a handy guide to its useful features and conversation starters, making jumping in simpler than ever. With your permission, these jumping off points are customized based on your interactions with other Microsoft services and will be further personalized over time according to your conversation history.

YouTube Video Click here to load media Copilot in Microsoft Edge: Copilot has been built right into your Microsoft Edge browser, quickly helping answer questions, summarize page content, translate text or rewrite a sentence. Now it's even easier to access Copilot directly from the Microsoft Edge browser by simply typing @copilot into the address bar. Copilot Labs: Copilot Labs gives people the opportunity to test out our experimental features that are still in development. It's a chance to give feedback and help shape the experiences we create. We have two features we're bringing to Labs initially, Copilot Vision and Think Deeper. Copilot Vision: This is a profoundly new way of interacting with a computer. Copilot Vision sees what you see and can talk to you about it in real time. It understands the web page you're viewing, both text and images, and can answer questions about its content, suggest next steps and help you without disrupting your workflow. Imagine you're trying to furnish a new apartment, Copilot Vision can help you search for furniture, find the right color palette, think through your options on everything from rugs to throws and even suggest ways of arranging what you're looking at.

YouTube Video Click here to load media Safety and security are our top priority here:

Copilot Vision sessions are entirely opt-in and ephemeral. None of the content Copilot Vision engages with is stored or used for training — the moment you end your session, data is permanently discarded. The experience won't work on all websites because we've taken important steps to put boundaries on the types of websites Copilot Vision can engage. We're starting with a limited list of popular websites to help ensure it's a safe experience for everyone. Copilot Vision won't work on paywalled and sensitive content for this preview. We've created it with both users' and creators' interests top of mind. There is no specific processing of the content of a website you are browsing, nor any AI training. Copilot Vision simply reads and interprets the images and text it sees on the page for the first time along with you. Before we launch broadly, we'll continue to take feedback on all the above from early users in Copilot Labs, refine our safety measures and keep privacy and responsibility at the center of everything we do. Let us know what you think!

Think Deeper: Copilot can now reason through more complex questions. Think Deeper takes more time before responding, allowing Copilot to deliver detailed and step-by-step answers to challenging questions. We've designed it to be helpful for all kinds of practical and everyday challenges, like comparing two complex options side by side. Should I move to this city or that? What type of car best suits my needs? And so on. It's an early Copilot Skill that's still undergoing development, so we've placed it in our experimental Copilot Labs to test and get feedback. We're also making sure Copilot is easily available on all our platforms. On Copilot+ PCs and Windows we're putting Copilot just a click away, unlocking new ways to interact with your PC. Check out today's announcement on the latest with Copilot+ PCs and Windows. Our work toward creating these kinds of experiences goes much further than just Copilot. Bing, Edge and MSN will also be more personalized, visual, cohesive and useful experiences than ever. For example, Bing generative search goes beyond simply finding an answer. Instead, it understands the search query, reviews millions of sources of information, dynamically matches content and generates search results on the fly. While still under development, you'll notice generative search rolling out across Bing starting today. To explore generative search in the US, simply search "Bing generative search" to choose from a selection of demo queries. We are also launching a new experience where users can more easily trigger Bing generative search for informational queries. The refreshed Copilot is rolling out today on iOS and Android, through the Copilot web experience at copilot.microsoft.com and in Windows. We're also excited to start rolling out Copilot to WhatsApp to help users there experience natural and engaging interactions with Copilot. Learn more here. Notes:

Copilot Voice is initially available in English in Australia, Canada, New Zealand, the United Kingdom and the United States. Copilot Voice will expand to more regions and languages soon. Copilot Daily is rolling out from today starting in the United States and the United Kingdom with more countries coming soon. Personalization in Copilot follows our Microsoft Privacy Statement, and people can opt out at any time in Settings. We're still finalizing options for offering personalization to users in the European Economic Area and the United Kingdom. Copilot Vision is rolling out to Copilot Labs soon and will be available to a limited number of Copilot Pro subscribers in the United States. Think Deeper starts to roll out this week through Copilot Labs to a limited number of Copilot Pro users in Australia, Canada, New Zealand, the United Kingdom and the United States.

Tags: AI, Bing, Bing generative search, Copilot, Copilot Daily, Copilot Discover, Copilot Labs, Copilot Pro, Copilot Vision, Copilot Voice, Copilot Windows, Copilot+ PCs, Microsoft AI, Microsoft Edge, MSN

In 2012, a child's drawing of her appointment with a physician was published in the Journal of the American Medical Association (JAMA). In the brightly colored sketch, the girl is sitting on the exam table accompanied by her mother and siblings. To those of us working in healthcare, her doctor is the most notable part of the image. He is sitting at his desk, typing on the computer — with his back to the patient.

This drawing, published alongside an article titled "The Cost of Technology," is emblematic of the unintended consequences of healthcare innovation. With more tools for physicians to use and greater demands on their time, technology can easily become a barrier between the clinician and the patient. For example, in 2023 approximately 53% of physicians surveyed by Medscape reported feeling burned out, and 23% reported feeling depressed. Long working hours, bureaucratic demands and loss of work-life balance due to administrative requirements are leading causes. Our mission has always been to help clinicians turn their chairs around by providing the support they need to fully focus their attention on delivering care.

Fast-forward to today, and we've translated that goal into reality. After establishing the ambient technology category, we introduced DAX Copilot, the first generative AI voice-enabled solution, which has been generally available for one year, and we're seeing remarkable momentum. Part of a proven and extensible platform, DAX Copilot leverages health systems' existing investments in the trusted family of Dragon Medical solutions, which is used by more than 600,000 clinicians worldwide.

DAX Copilot combines proven conversational and ambient AI with generative AI to automatically convert multiparty conversations into specialty-specific standardized draft clinical summaries that integrate with existing workflows. DAX Copilot was the first ambient solution to be integrated into the Epic electronic health record (EHR) workflow and allows clinicians to seamlessly document patient visits directly within the EHR. Across the country, more than 400 organizations have already embraced its revolutionary capabilities, streamlining administrative tasks and lightening clinicians' documentation workloads.

This transformative year has highlighted many of the ways we're helping empower healthcare teams today and bringing joy back to practicing medicine for thousands of clinicians.

DAX Copilot is helping Northwestern Medicine in Chicago improve patient access to care, with physicians using the solution in at least 50% of patient encounters and able to see an average of 11.3 additional patients per month. DAX Copilot users at Northwestern Medicine also report spending an average of 24% less time on notes and a 17% decrease in "pajama time," working on administrative tasks late into the night.

"Northwestern Medicine is committed to providing a superior work environment that promotes well-being, and implementing DAX Copilot will allow our physicians to spend more quality time with our patients, focusing on their needs rather than on paperwork and data entry," says Dr. Gaurava Agarwal, chief wellness executive, Northwestern Medicine.

Overlake Medical Center & Clinics in Bellevue, Washington, deployed DAX Copilot to help reduce after-hours documentation time and equip its clinicians with tools to manage escalating demands. A pilot with 30 Overlake clinicians saw a significant reduction in time spent on notes outside of scheduled hours. Users also reported other important benefits: 81% said DAX Copilot had reduced their cognitive burden and 77% said the solution had improved the quality of their documentation.

At Atrium Health, "DAX Copilot is enabling a better clinician experience," says Dr. Matthew Anderson, senior medical director of primary care at Atrium Health. "Most of our surveyed users are reporting a positive impact on their day-to-day schedule, being able to increase the number of patients they are able to see and even spending more time with them."

Atrium Health isn't alone in finding that the time saved on documentation with DAX Copilot is having a positive impact on clinicians' quality of life. "I finally have weekends back," says Dr. Christy Chan, a family medicine physician at Overlake. "I used to always have to worry that there was something I had to do — get back onto the EMR, log back in — but I actually have some weekends back."

Clinicians at Novant Health are experiencing similar benefits. "DAX Copilot has done an amazing thing for the physicians and clinicians who are currently using it," says Dr. Aram Alexanian, a family physician at Novant Health. "When you hear a comment like 'I am now able to do things with my daughter in the evenings and weekends that I couldn't do before,' nothing satisfies us more than knowing the impact DAX is having on our clinicians."

DAX Copilot can accurately identify different voices in the room during pediatric visits, differentiating between parents and children to accurately capture the patient exam note for the physician to review and approve.

Community Health Network places a major focus on clinician well-being and implementing DAX Copilot has had a major impact on its clinicians' ability to close notes before they leave for the day. "Since we have implemented DAX Copilot, I have not left clinic with an open note," says Dr. Patrick McGill, chief transformation officer for Community Health Network. "We have seen improved access, improved numbers of patients, but overall, it's really the clinician satisfaction that we've seen. In one word, DAX Copilot is transformative. It transforms how we're able to deliver care and how we're able to document it. It also transforms the patient experience."

"DAX Copilot has made my professional life easier. My patients have also benefited from my using Nuance DAX during our appointments. I can be right there with the patient and not furiously writing notes. I cannot thank you enough," said Anita M. Kelsey, M.D., Duke Health.

Dr. Dominick Lanzo, an orthopedic surgeon at Greater Baltimore Medical Center, agrees. "Once I introduced the DAX Copilot program, it completely transformed the patient experience, and it's turned out to be incredibly accurate with regards to the history of present illness and the physical exam," he says. "It's made my practice much more efficient. I can see more patients, my notes are more accurate, and they're done in a timely fashion by the end of the afternoon."

For Dr. Alison Pomykala, an internal medicine specialist at Baptist Medical Group, the integration of DAX Copilot with the Epic EHR is particularly valuable. "The thing I like most about DAX Copilot embedded in the Epic workflow is I'm able to focus more on the patient and I'm spending less time in the exam room typing on the computer," she says.

"I think the interface is wonderful with Epic. It has been great to see the notes coming up basically in real time on the Epic system. That's also helped with other things: where we needed to generate a note quickly for referrals, for insurance pre-certifications, for imaging studies, or to have a complete note ready for a patient that we're sending to the hospital."

"At our academic health system, integrating DAX Copilot has revolutionized patient care," says Dr. Anthony Mazzarelli, co-president and CEO at Cooper University Health Care. "By automating clinical documentation through ambient voice technology, it has significantly reduced administrative workloads. This allows our physicians to focus on real-time patient interactions, leading to better care outcomes and increased job satisfaction. DAX Copilot has not only improved efficiency but has also empowered our team to spend more time where it matters most — caring for patients."

It's a privilege to see this industry-leading solution make a meaningful difference for the clinicians who are already on board — but the work never stops. The Microsoft healthcare team is determined to continue solving some of the industry's most complex challenges, and harnessing the power and potential of AI is how we'll achieve that. We will continue to be leaders in innovation, collaborating across our ecosystem of incredible customers, partners and Microsoft researchers to bring real impact to clinical settings.

DAX Copilot is an innovative solution that goes beyond documentation, offering unique features such as orders, problem-based charting and pre-charting capabilities. For example, recent updates to DAX Copilot include a robust set of features, such as the ability to customize documentation style and formatting, as well as automatically create referral letters, diagnostic evidence, after-visit summaries and encounter summaries. The solution also now offers AI coaching to help users improve the quality and completeness of their notes. Also, the new Summarized Evidence capability offers a comprehensive and sophisticated approach that helps clinicians validate and trust the note output by combining insights that go beyond evidence linking, helping clinicians validate the note.

Importantly, when creating technologies that can change the world, Microsoft believes organizations need to ensure that the technology is used responsibly. Microsoft is committed to creating responsible AI by design that is guided by a core set of principles: fairness, reliability and safety, privacy and security, inclusiveness, transparency and accountability.

DAX Copilot is helping restore the human connection at the heart of medicine. We're excited to drive this solution forward and expand its reach to more clinicians over the coming year and beyond.

Today, the solution can be used across ambulatory specialties, in-office primary care and urgent care, telehealth and emergency medicine. And we are scaling the solution's availability and capabilities to even more care settings, such as nursing and geographies.

If you'd like to learn how DAX Copilot can help transform healthcare at your organization, please visit: the DAX Copilot website.

Tags: AI, Copilot, Dax Copilot, Dragon Medical solutions, Generative AI, Microsoft Partners, Responsible AI

YouTube Video Click here to load media As AI advances, we all have a role to play to unlock AI's positive impact for organizations and communities around the world. That's why we're focused on helping customers use and build AI that is trustworthy, meaning AI that is secure, safe and private. At Microsoft, we have commitments to ensure Trustworthy AI and are building industry-leading supporting technology. Our commitments and capabilities go hand in hand to make sure our customers and developers are protected at every layer. Building on our commitments, today we are announcing new product capabilities to strengthen the security, safety and privacy of AI systems. Security. Security is our top priority at Microsoft, and our expanded Secure Future Initiative (SFI) underscores the company-wide commitments and the responsibility we feel to make our customers more secure. This week we announced our first SFI Progress Report, highlighting updates spanning culture, governance, technology and operations. This delivers on our pledge to prioritize security above all else and is guided by three principles: secure by design, secure by default and secure operations. In addition to our first party offerings, Microsoft Defender and Purview, our AI services come with foundational security controls, such as built-in functions to help prevent prompt injections and copyright violations. Building on those, today we're announcing two new capabilities:

Evaluations in Azure AI Studio to support proactive risk assessments. Microsoft 365 Copilot will provide transparency into web queries to help admins and users better understand how web search enhances the Copilot response. Coming soon.

Our security capabilities are already being used by customers. Cummins, a 105-year-old company known for its engine manufacturing and development of clean energy technologies, turned to Microsoft Purview to strengthen their data security and governance by automating the classification, tagging and labeling of data. EPAM Systems, a software engineering and business consulting company, deployed Microsoft 365 Copilot for 300 users because of the data protection they get from Microsoft. J.T. Sodano, Senior Director of IT, shared that "we were a lot more confident with Copilot for Microsoft 365, compared to other large language models (LLMs), because we know that the same information and data protection policies that we've configured in Microsoft Purview apply to Copilot." Safety. Inclusive of both security and privacy, Microsoft's broader Responsible AI principles, established in 2018, continue to guide how we build and deploy AI safely across the company. In practice this means properly building, testing and monitoring systems to avoid undesirable behaviors, such as harmful content, bias, misuse and other unintended risks. Over the years, we have made significant investments in building out the necessary governance structure, policies, tools and processes to uphold these principles and build and deploy AI safely. At Microsoft, we are committed to sharing our learnings on this journey of upholding our Responsible AI principles with our customers. We use our own best practices and learnings to provide people and organizations with capabilities and tools to build AI applications that share the same high standards we strive for. Today, we are sharing new capabilities to help customers pursue the benefits of AI while mitigating the risks:

A Correction capability in Microsoft Azure AI Content Safety's Groundedness detection feature that helps fix hallucination issues in real time before users see them. Embedded Content Safety, which allows customers to embed Azure AI Content Safety on devices. This is important for on-device scenarios where cloud connectivity might be intermittent or unavailable. New evaluations in Azure AI Studio to help customers assess the quality and relevancy of outputs and how often their AI application outputs protected material. Protected Material Detection for Code is now in preview in Azure AI Content Safety to help detect pre-existing content and code. This feature helps developers explore public source code in GitHub repositories, fostering collaboration and transparency, while enabling more informed coding decisions.

It's amazing to see how customers across industries are already using Microsoft solutions to build more secure and trustworthy AI applications. For example, Unity, a platform for 3D games, used Microsoft Azure OpenAI Service to build Muse Chat, an AI assistant that makes game development easier. Muse Chat uses content-filtering models in Azure AI Content Safety to ensure responsible use of the software. Additionally, ASOS, a UK-based fashion retailer with nearly 900 brand partners, used the same built-in content filters in Azure AI Content Safety to support top-quality interactions through an AI app that helps customers find new looks. We're seeing the impact in the education space too. New York City Public Schools partnered with Microsoft to develop a chat system that is safe and appropriate for the education context, which they are now piloting in schools. The South Australia Department for Education similarly brought generative AI into the classroom with EdChat, relying on the same infrastructure to ensure safe use for students and teachers. Privacy. Data is at the foundation of AI, and Microsoft's priority is to help ensure customer data is protected and compliant through our long-standing privacy principles, which include user control, transparency and legal and regulatory protections. To build on this, today we're announcing:

Confidential inferencing in preview in our Azure OpenAI Service Whisper model, so customers can develop generative AI applications that support verifiable end-to-end privacy. Confidential inferencing ensures that sensitive customer data remains secure and private during the inferencing process, which is when a trained AI model makes predictions or decisions based on new data. This is especially important for highly regulated industries, such as healthcare, financial services, retail, manufacturing and energy. The general availability of Azure Confidential VMs with NVIDIA H100 Tensor Core GPUs, which allow customers to secure data directly on the GPU. This builds on our confidential computing solutions, which ensure customer data stays encrypted and protected in a secure environment so that no one gains access to the information or system without permission. Azure OpenAI Data Zones for the EU and US are coming soon and build on the existing data residency provided by Azure OpenAI Service by making it easier to manage the data processing and storage of generative AI applications. This new functionality offers customers the flexibility of scaling generative AI applications across all Azure regions within a geography, while giving them the control of data processing and storage within the EU or US.

We've seen increasing customer interest in confidential computing and excitement for confidential GPUs, including from application security provider F5, which is using Azure Confidential VMs with NVIDIA H100 Tensor Core GPUs to build advanced AI-powered security solutions, while ensuring confidentiality of the data its models are analyzing. And multinational banking corporation Royal Bank of Canada (RBC) has integrated Azure confidential computing into their own platform to analyze encrypted data while preserving customer privacy. With the general availability of Azure Confidential VMs with NVIDIA H100 Tensor Core GPUs, RBC can now use these advanced AI tools to work more efficiently and develop more powerful AI models.

Achieve more with Trustworthy AI We all need and expect AI we can trust. We've seen what's possible when people are empowered to use AI in a trusted way, from enriching employee experiences and reshaping business processes to reinventing customer engagement and reimagining our everyday lives. With new capabilities that improve security, safety and privacy, we continue to enable customers to use and build trustworthy AI solutions that help every person and organization on the planet achieve more. Ultimately, Trustworthy AI encompasses all that we do at Microsoft and it's essential to our mission as we work to expand opportunity, earn trust, protect fundamental rights and advance sustainability across everything we do. Related: Commitments

Security: Secure Future Initiative Privacy: Trust Center Safety: Responsible AI Principles

Security: Security for AI Privacy: Azure Confidential Computing Safety: Azure AI Content Safety

Tags: AI, Azure AI Content Safety, Azure AI Studio, Azure Confidential Computing, Azure OpenAI Service, Copilot, GitHub, Microsoft 365, Microsoft Defender, Microsoft Purview, Microsoft Trust Center, Responsible AI, Secure Future Initiative, Trustworthy AI

As we mark our 50th year, I've been reflecting on how we have remained a consequential company decade after decade in an industry where there is no franchise value. And it is because — time and time again when tech paradigms have shifted — we have seized the opportunity to reinvent ourselves. And that's what we are doing again today in this AI platform shift.

To continue thriving as a company, we need to raise the bar on our operational excellence, continually improving security, quality, and delivery to our customers, as well as the rigor with which we operate the business. Building this capability is essential, and I want each of us to take as much pride in exceeding customer expectations in our fundamentals as we do in our product innovation. After all, both are mission critical to our customers and our future.

In this context, I'm thrilled to share that Carolina Dybeck Happe is joining Microsoft as EVP and Chief Operations Officer. In this newly created role, she will join the senior leadership team (SLT), reporting to me.

I've come to admire Carolina through her work as a global business leader, including most recently her role in leading GE's historic turnaround. She is recognized for her ability to drive transformational change at scale while delivering improved customer experiences and faster time to value. Carolina will partner with the SLT to help us drive continuous business process improvement across all our organizations and accelerate our company-wide AI transformation, increasing value to customers and partners.

As part of this transition, the Commerce + Ecosystems organization in Cloud + AI, the Microsoft Digital organization in Experiences + Devices, and the Microsoft Business Operations organization in Finance will move to report to Carolina. These teams are doing mission-critical work for us with high ambition plans on how to empower our partners, customers, and employees with world class technology and experiences.

I look forward to seeing the progress we will achieve together as we embrace continuous improvement in all we do.

At Microsoft, we're ushering in a new era of computing on the path to unlocking scientific advantage and tackling some of the world's most pressing challenges. This is why we're building Azure Quantum — to create the first platform for reliable quantum computing and achieve the vision of quantum at scale.

In April, we announced we're entering the next phase for solving meaningful problems with reliable quantum computers by demonstrating the most reliable logical qubits with an error rate 800x better than physical qubits. The main issue with today's noisy intermediate-scale quantum (NISQ) machines is that their physical qubits are too noisy and error-prone, making the machines impractical for real-world applications. That's why we must transition to using reliable logical qubits that combine multiple physical qubits together to protect against noise and to maintain coherence for long-running computations.

But quantum computing doesn't exist in isolation. It requires deep integration with the power of the cloud. We must leverage the best of computing to unlock a new generation of hybrid quantum applications that could solve some of our most pressing challenges — from pioneering more sustainable energy solutions to transforming how we treat disease with the next generation of life-saving therapeutics.

We designed the Azure Quantum compute platform to provide quantum computing across a variety of hardware architectures, enabling the most advanced hybrid quantum applications in the industry — all in a secure, unified and scalable cloud environment — to tackle classically intractable problems. This is our vision for Azure Quantum. Today, we continue to make advances that bring us closer to achieving it with our industry-leading partners, Quantinuum and Atom Computing. With both companies, we want to bring best-in-class solutions to the Azure Quantum platform, and collectively advance and scale resilient quantum capabilities.

In collaboration with Quantinuum, we applied our improved qubit-virtualization system to create and entangle 12 highly reliable logical qubits. This represents the largest number of entangled logical qubits, with the highest fidelity, on record. These results scale logical qubit computation — on ion-trap hardware — within our Azure Quantum compute platform. In addition, advancing toward scalable quantum computing necessitates not only reaching significant hardware milestones, but also proving these improvements can address practical and real-world challenges.

This is why we demonstrated the first end-to-end chemistry simulation that combines reliable logical quantum computation with cloud high-performance computing (HPC) and AI. Today's announcements would not have been possible without Quantinuum's leading quantum machines. This paves the way toward practical solutions at the intersection of these technologies, especially in the domains of chemistry, physics and life sciences.

Lastly, as we expand our Azure Quantum compute platform, we are excited to announce that Microsoft and Atom Computing are coming together to ultimately build the world's most powerful quantum machine. Through this collaboration, we're bringing a new generation of reliable quantum hardware to customers by integrating and advancing Atom Computing's neutral-atom hardware into our Azure Quantum compute platform. With it, we are bringing the best-in-class from Microsoft and our partner ecosystem to provide the commercial offering of a reliable quantum machine.

Combining the capabilities of this reliable quantum hardware with our platform for Science, Azure Elements, we are providing a comprehensive discovery suite to achieve scientific quantum advantage.

At Microsoft, we're pioneering a new computing paradigm by bringing the power of the cloud and AI together with quantum. Our Azure Quantum compute platform enables the seamless execution of quantum applications that leverage hardware across a variety of qubit architectures and chips, while offering integration with cloud HPC and AI. Over this past year, we've continued to announce new breakthroughs and collaborations in pursuit of this platform mission, including offering Generative Chemistry and Accelerated DFT and advancing the industry to reliable quantum computing by demonstrating highly reliable logical qubits.

We are bringing these technologies together in a purpose-built cloud platform that leverages the complementary strengths of both AI for large-scale data processing and quantum for complex calculations and unprecedented accuracy. This strong compute foundation offers a secure, unified and scalable hybrid computing environment that enables innovators to develop best-in-class solutions for tackling problems that are difficult or even intractable on classical computers. We are integrating quantum hardware architectures from our ecosystem partners with our quantum control, processing and error correction software — in addition to capabilities for copilot-assisted workflows, developer tools, classical supercomputing and multimodal AI models. This differentiated computing stack will pave the way for this new generation of hybrid applications. AI co-reasoning will help articulate problems and translate them into workflows, using both classical and scaled quantum tools at the right stages to drive impactful insights in an iterative loop to compress R&D and time-to-solution into days, not years.

Today, in collaboration with Quantinuum, we're proud to announce the demonstration of the best performing logical qubits on record, achieving the largest number of entangled logical qubits. We created 12 logical qubits by improving and optimizing our qubit-virtualization system for Quantinuum's 56-physical-qubits H2 machine.

This progress speaks to the world-class error correction expertise at Microsoft. In less than six months, our improved qubit-virtualization system tripled reliable logical qubit counts. Furthermore, when we entangled all 12 logical qubits in a complex state required for 'deeper' quantum computation, they exhibited a 22X circuit error rate improvement over the corresponding physical qubits.

The ability of our systems to triple the number of logical qubits while less than doubling our physical qubits from 30 to 56 physical qubits is a testament to the high fidelities and all-to-all connectivity of our H-Series trapped-ion hardware. Our current H2-1 hardware combined with Microsoft's qubit-virtualization system is bringing us and our customers fully into Level 2 resilient quantum computing. This powerful collaboration will unlock even greater advancements when combined with the cutting-edge AI and HPC tools delivered through Azure Quantum.

With our improved error correction code and qubit-virtualization system, we've demonstrated a 22X improvement between physical and logical circuit error rates when entangled.

As we continue to strive toward scientific and industrial breakthroughs with quantum computers, noise remains our biggest barrier. In a previous post, I highlighted how increasing the number of physical qubits alone is not enough to make robust quantum error correction possible. As part of the quantum ecosystem, we must remain focused on improving both logical qubit counts and fidelity to have a solid foundation for producing meaningful results. This will be possible through hardware and software advancements that together enable running longer and more reliable quantum applications. Today's announcement demonstrates that it is possible to realize these fundamental capabilities on the path to large-scale quantum computing.

A true computing paradigm shift also requires a focus on practical and commercially relevant applications. Earlier, we successfully completed a chemistry simulation in the first end-to-end workflow that combined HPC, AI and logical qubit computation to predict the ground state energy for a specific catalyst problem. This demonstration marked a critical step toward ushering in a new generation of hybrid applications that will become increasingly impactful as quantum technologies scale. Quantum and AI will have the earliest significant impact on scientific discovery, and researchers at Microsoft have demonstrated the breakthrough potential of this integration. This work was only possible thanks to our long-standing and close collaboration with Quantinuum, a company that remains at the forefront of quantum computing.

You can learn more about today's improved logical qubits and the technical details about this chemistry simulation in our blog Microsoft and Quantinuum create 12 logical qubits and demonstrate a hybrid, end-to-end chemistry simulation.

Lastly, in collaboration with Atom Computing, we are excited to bring a new generation of reliable quantum hardware to customers. Bringing together Microsoft's enhanced qubit-virtualization system with Atom Computing's neutral-atom hardware, we've jointly generated logical qubits and are optimizing the system to enable reliable quantum computation. Together, we believe this new commercial offering will be the world's most powerful quantum machine on record and will scale to scientific advantage and beyond.

Atom Computing's hardware uniquely combines capabilities essential for expanding quantum error correction, including large numbers of high-fidelity qubits, all-to-all qubit connectivity, long coherence times and mid-circuit measurements with qubit reset and reuse. The company is building 2nd generation systems with over 1,200 physical qubits and plans to increase the physical qubit count tenfold with each new hardware generation. By applying Microsoft's state-of-the-art fault-tolerance protocols on a different qubit architecture, our Azure Quantum compute platform can offer a spectrum of best-in-class logical qubits across multiple hardware platforms, providing flexibility and future proofing our customers' investments.

Microsoft and Atom Computing team up to enhance the Azure Quantum compute platform with neutral-atom hardware and tailored qubit virtualization, enabling a commercial discovery suite with continuous upgrade capabilities for additional logical qubits.

Our collaboration with Atom Computing aims to integrate these capabilities with Azure Elements, our purpose-built cloud platform offering differentiated computing scale, state-of-the-art AI models for chemistry and materials science simulations and Copilot. Our goal is to empower governments and organizations to tackle scientifically and commercially relevant problems with today's most advanced computational solutions, including designing and predicting properties of chemicals and materials, exploring molecular interactions and simulating complex chemical reactions. Additionally, we want to help galvanize a quantum-ready ecosystem, providing the critical tools necessary for commercial adoption of these technologies that can help build quantum expertise and create new demand for jobs.

We are excited to accelerate Atom Computing's quantum capabilities with Microsoft as our partner. We believe that this collaboration uniquely positions us to scale and be first to reach scientific quantum advantage. Our neutral-atom technology is an ideal foundation for Microsoft's leading qubit-virtualization capabilities, and we look forward to enabling fault tolerant, cutting-edge quantum applications for global innovators to use the best platform in the world.

— Ben Bloom, PhD, Founder and CEO of Atom Computing

At Microsoft, we want to enable practitioners to unlock a new generation of applications that harness the complementary strengths of quantum, classical supercomputing and AI, all connected in the Azure cloud.

We remain committed to achieving quantum at scale so we can solve commercially significant problems that are far too complex for classical computers. As a platform company, it's critical that we continue investing in the quantum ecosystem and collaborating with industry leaders such as Quantinuum, Atom Computing, Photonic and others to advance and scale quantum capabilities. Alongside our industry collaborations, we're also focused on our own innovation with a topological qubit-based approach.

This approach continues to offer a unique path to scaling up, with fast clock speeds, digital control and more. Furthermore, a topological quantum computer could control over one million physical qubits on a single chip, with the ability to process information faster than other types of qubits. Our Azure Quantum team previously demonstrated the feasibility of this approach, and we look forward to scaling this to the level of quantum supercomputing.

Azure is the place where all this innovation comes together. For more information about today's announcements:

Read the technical blog Microsoft and Quantinuum create 12 logical qubits and demonstrate a hybrid end-to-end chemistry simulation. Register for the upcoming Microsoft Quantum Innovator Series on how quantum and AI can unlock a new generation of hybrid applications for science. Get the latest news and announcements from Azure Quantum.

Tags: AI, Azure Elements, Azure Quantum, Copilot, Microsoft AI, Microsoft Partners

Innovation isn't just a buzzword — it's the heartbeat of thought leadership and a catalyst for growth. And in the past year, no innovation has changed the business landscape quite like AI. At Microsoft, we're always looking for ways to harness emerging technology for the greater good, and it's that desire that led to the creation of the Microsoft AI Tour.

Last year's AI Tour gave senior business leaders and technical practitioners a platform to come together and explore the transformative potential of AI, and we couldn't be happier with the response it received. That's why we're hitting the road again, and this time we've tripled the cities on the itinerary to ensure we can reach more of our customers and partners around the world. We hope you'll join us.

The Microsoft AI Tour is more than an event; it's a global movement that will begin September 24, 2024, and span over 60 cities around the world, from Mexico City to Johannesburg, Mumbai to Sydney, Seoul to Berlin, and many more. This free, one-day and in-person experience offers AI thought leadership, sessions to help build AI skills, hands-on workshops and connection opportunities designed to inspire attendees, while providing practical approaches for using the power of AI to improve productivity and deliver solutions that drive real impact for businesses. We'll also showcase local customer and partner stories at each stop, which makes the AI Tour a great chance for attendees to deepen their connections with local peers.

Whether you're a senior business leader or a technical practitioner, you'll find valuable insights and actionable solutions at these events. We've built the Microsoft AI Tour from the ground up to be a comprehensive AI experience at no cost to you.

For technical practitioners, the Microsoft AI Tour is a platform for learning, collaborating and staying ahead of AI trends. It offers immersive workshops on the latest AI technologies, hands-on experience with Microsoft and partner experts and a vibrant community ready to exchange ideas. We'll also provide insight into the future of AI, helping attendees stay updated on the rapidly evolving tech landscape.

For senior business leaders, the AI Tour is a unique opportunity to understand how AI can drive growth and innovation for their organizations. Through expert-led presentations, attendees will gain insights into how to best leverage AI to solve complex problems and create lasting value. Plus, dive deep into how AI fits into the Microsoft roadmap, which will help those present align their AI strategies with the latest technologies and best practices.

During the event, attendees will have the opportunity to engage with industry-leading experts, partners and members of Microsoft's senior leadership team. Leaders such as Satya Nadella, Chairman and CEO, and Judson Althoff, Executive Vice President and Chief Commercial Officer, will join us at select stops on the tour, where they'll share how this new generation of AI is reshaping how people live and work.

AI innovation continues to grow, and how we conduct business and develop solutions will never be the same because of it. By joining us at the Microsoft AI Tour, you can position yourself at the forefront of that innovation and ensure you are prepared for whatever's next on the horizon.

Ready to join this innovation revolution? Visit the Microsoft AI Tour website and request to attend the Tour when it comes to your city.

Tags: AI, Microsoft AI, Microsoft AI Tour, Microsoft Cloud, Microsoft Copilot, Microsoft Partners

A year ago in July, Microsoft coined the term AI Transformation. It is almost hard to imagine at that time Copilots were not generally available and Azure OpenAI Service had only been available for six months. As Satya Nadella, our Chairman and CEO, stated in last quarter's earnings: 60% of the Fortune 500 have adopted Copilots and 65% use Azure OpenAI Service. Today, Copilots define how AI is empowering human achievement.

Together with our customers and partners, we have established how AI Transformation enriches employee experiences, reinvents customer engagement, reshapes business processes and bends the curve on pragmatic innovation. Our approach to put a Copilot on every desk for every role, identify AI design patterns and build a strong cybersecurity foundation is helping organizations harness AI responsibly, securely and with purpose.

As I looked back on the year, I enjoyed reviewing hundreds of customer and partner examples from around the world that demonstrated how AI Transformation delivered tangible business outcomes and value for their organizations. I am pleased to highlight 20 of those stories that I found to be the most inspirational and illustrative of what we can achieve together.

Audi is taking in-car voice control to the next level by integrating ChatGPT into its MIB 3 infotainment system using Microsoft Azure OpenAI Service, making driving safer and more enjoyable. With this update, Audi's voice assistant will become even more intuitive, handling complex questions and offering a truly conversational experience for drivers and passengers using natural language. Starting this month, nearly two million Audi models made since 2021 are being upgraded with this generative AI-powered experience, allowing drivers to more easily control infotainment, navigation and air-conditioning systems, as well as ask general knowledge questions.

To help address Taiwan's chronic shortage of healthcare workers, Chi Mei Medical Center developed AI copilots built with Microsoft Azure OpenAI Service to help overworked staff lighten their workloads while ensuring patient safety. With copilots implemented throughout its operations and at multiple patient touchpoints, medical center staff are seeing impressive results. Doctors now spend 15 minutes instead of an hour writing medical reports, and nurses can document patient information in under 5 minutes per patient instead of 10 to 20 minutes. Pharmacists are now able to double the number of patients they see per day — from 15 to 30 — by leveraging a copilot for comprehensive clinical summaries across multiple databases. By bending the curve on innovation and deploying these copilots, staff have more time to focus on patient care and outcomes, while reducing burnout and stress.

Coles, a leading Australian supermarket chain, is transforming the retail experience by leveraging AI to deepen its relationships with shoppers and improve efficiency in its stores. Coles has deployed AI models that predict the flow of 20,000 stock keeping units to 850 stores with remarkable accuracy. They are now generating 1.6 billion predictions daily, ensuring every shopper finds exactly what they are looking for when they need it. Using an AI model, Coles is also reinventing customer engagement by identifying customer patterns to provide its more than 4 million loyalty club customers with bespoke product recommendations weekly. The company is also utilizing Microsoft Azure Stack HCI and NVIDIA's A16 GPUs to streamline checkout processes, enhance queue monitoring and elevate customer satisfaction by significantly reducing wait times.

In its quest to transform aquaculture in Indonesia, eFishery harnessed the power of Microsoft Azure OpenAI Service to create Mas Ahya, a generative AI assistant available through a mobile application. Mas Ahya equips farmers with aquaculture expertise right at their fingertips, enabling them to monitor water quality, estimate market prices and manage feeding schedules effectively. It also offers real-time insights and recommendations to maintain ideal pond conditions for more precise feeding — shortening the time from fry to market size fish from four to three-and-a-half months. By utilizing this AI assistant for queries on a range of topics — from plankton levels to remedies for diseases or bacteria affecting their stock — farmers have seen shrimp survival rates soar from 60% to 90%, further boosting their shrimp export capacity.

EY is using Copilot for Microsoft 365 to help clients adopt an AI-powered approach to tackle their unique tax, finance and operational challenges; boost efficiency; and get more value from their data. Reconciling and combining large amounts of data faster and more accurately is a game changer for Finance and Tax, driving a differentiated experience and opportunity to deliver these capabilities through EY's core platforms and solutions. The ability to apply Copilot to vast swaths of data to develop insights helps drive smarter decision-making across the business. For example, EY professionals are seeing productivity gains of up to 14 hours per week using an AI digital assistant to automate routine tasks, freeing up time for more strategic work. The initial rollout of Copilot was so positive that EY is now scaling it to 150,000 of its employees and helping clients drive their own transformations using EY's Customer Zero copilot accelerators.

Global design, engineering and environmental services leader GHD is leveraging Copilot for Microsoft 365 to enrich the employee experience and transform its operations to help tackle some of the world's most complex design and engineering challenges for its clients. GHD employees can now respond to client requests more quickly by using Copilot, reducing the time spent reviewing new requests for proposals from hours to just 15 minutes. More broadly, a recent survey of Copilot users at GHD revealed that 41% are saving time in their workday, with 29% saving more than 30 minutes and 12% saving more than an hour. Additionally, 75% of users feel more efficient and 45% find their work more rewarding when using Copilot.

Hanover Research faced the challenge of efficiently processing vast amounts of data to deliver timely insights to its clients. To tackle this, the custom market research and analytics provider partnered with Neudesic to create the Hanover Intelligent Virtual Engine (HIVE), a customized AI-powered research tool using Azure OpenAI Service that eliminates the need for analysts to manually comb through its vast repository of documents. HIVE helps analysts identify insights up to 10 times faster and creates more opportunities to provide clients with information that would otherwise be buried in data and unavailable. By leveraging Azure AI capabilities to reshape business processes, Hanover has significantly boosted its research efficiency, enabling it to provide more accurate and timely data-driven insights to its clients.

Lumen Technologies was among the first companies to invest broadly in Copilot, making Copilot for Microsoft 365 available last August. Since then, the company has expanded its use of Microsoft Copilot across its organization to empower its people by simplifying workflows and inspiring a forward-thinking mindset. Today, Copilot for Sales is saving its 3,000-plus sellers an average of four hours a week, equating to $50 million annually. Sellers can now spend more time with customers, improve their work-life balance and fundamentally change the way they work.

As an integrated risk assessment firm operating globally, Moody's Corporation has harnessed the power of AI through Azure OpenAI Service, Microsoft Fabric and Microsoft Teams to significantly enhance productivity and insights. Through our co-innovation efforts, we built an internal tool — Moody's Copilot — that enables employees to quickly synthesize vast amounts of research and data, and 94% of users reported increased productivity. Additionally, the launch of Moody's Research Assistant allows customers to generate new insights from extensive credit research, data and analytics, potentially saving users over 25% of their time on typical financial analyst tasks.

OCBC bank is empowering its global workforce with generative AI-powered solutions to boost productivity and enhance customer engagement. Built upon Azure OpenAI, the bank developed a digital assistant for use within a secure and controlled Microsoft Teams environment to help employees in their daily work across customer service, research, product management and marketing roles. Since using the tool, employees have reduced time spent on tasks such as writing, research, translation and ideation by approximately 50%, with 72% of team members reporting significant improvements in their day-to-day productivity and more time to focus on customers.

As one of the largest combined natural gas and electric companies in the US, Pacific Gas & Electric Company (PG&E) had an ambitious goal to reshape its business processes by automating low-value tasks and rededicating employees to focus on high-value work. By implementing Microsoft Power Platform solutions, its Digital Creators and citizen developers created new business solutions, saving nearly 527,000 hours and generating approximately $75 million in savings annually. Using Microsoft Copilot Studio, the company also built a digital assistant that fulfills 25% to 40% of help desk demands, significantly optimizing agent workloads and providing sizeable labor savings and service-level improvements — saving teams 840 hours and the company more than $1.1 million per year.

As a leading insurance and reinsurance company in Peru — and one of the largest in Latin America — Pacífico Seguros partnered with TC1 Labs and leveraged Microsoft AI, Microsoft 365 and Azure AI solutions to reshape business processes, improve data management and enhance customer interactions. Pacífico Seguros reported a 40% increase in operational efficiency and a 30% reduction in response times to customer inquiries. It also implemented Microsoft Copilot for Security with a Zero Trust approach, drastically boosting responses to cyber threats and delivering ongoing protection of critical systems and data. This AI Transformation has improved service quality and enhanced security across its business.

Paysafe, a leader in specialized payments, supports 260 payment types in more than 40 currencies and enables seamless transactions for businesses and consumers worldwide. By leveraging Copilot for Microsoft 365 the company has transformed its operations to ensure secure and efficient processing across different payment methods. Paysafe uses Copilot to address the challenges of a diverse and multilingual global workforce, using it to translate everything into various languages — document policies, standard operating procedures and meetings — to save both time and money. Compared to preparing documentation from scratch or searching for information manually, Paysafe's IT team saves between 10% and 50% of their time with Copilot.

Fintech company Saphyre creates real-time client account reconciliation and management solutions for institutional parties involved in financial trading. The company is using Microsoft Azure to provide intelligent, cloud-based solutions that automate and streamline complex financial trading workflows and modernize time-consuming processes efficiently and securely. Using Azure AI, Saphyre built a solution that helped clients reduce manual paperwork by 75%. It is also sharing data more securely using Microsoft security products so clients can be ready to trade three to five times faster compared to manual onboarding. This results in increased revenue opportunities for clients by completing trades more quickly and at better prices.

Softchoice has partnered with Microsoft to help companies drive efficiencies, reinvent customer engagement and address cybersecurity threats. By implementing Copilot for Microsoft 365, Softchoice has achieved significant productivity gains, reducing time spent summarizing technical meetings by 97%, creating internal training modules by 70% and developing customer-facing technical content by 62% to 67%. With Microsoft Copilot for Security built into daily dashboards, the company's analysts are saving 20% to 30% of their time summarizing pertinent information needed to execute tasks. Analysts can more quickly determine if failed sign-ins are routine or accidental, saving 20-30 minutes each morning and freeing up time to focus on those that require more in-depth analysis. Security incidents can now be audited faster, reducing the time and resources needed to manually review by 50% and improving efficiency by 30% to 40%.

TomTom, a global leader in mapping and navigation technology, faced the challenge of a fragmented development process with too much time spent navigating multiple tools. By centralizing on the GitHub platform and integrating GitHub Copilot into its development processes, TomTom has streamlined workflows to deliver products to customers more quickly than ever. Additionally, 85% of developers report feeling more productive and 70% feel they can focus on more satisfying work by reducing cognitive workloads and enhancing collaboration among development teams.

Australia's Torrens University set a goal to transform its digital learning environment to deliver a superior online student experience. It developed MyLearn — a modern online platform built with Azure OpenAI Service — to provide a consistent and convenient learning environment for students anytime, anywhere. By leveraging generative AI, the university has standardized and improved course curriculums, saving an impressive 20,000 hours and $2.4 million Australian dollars in time and resources. This AI-driven approach not only streamlined its digital learning environment but also set the stage for future savings and agile curriculum updates, creating a more engaging and efficient educational experience for students.

Unilever, a global leader in consumer products across over 190 countries, is working with Microsoft to revolutionize scientific discovery and positively impact the 3.4 billion people it serves daily. With Copilot and the advanced simulation capabilities of Azure Quantum Elements, Unilever can query scientific information using natural language, performing thousands of computational simulations in the time it would take to run tens of laboratory experiments. This technological leap, combined with its vast repository of proprietary data and a century of expertise in personal and household care, enables Unilever's scientists to lead the industry in developing the next generation of eco-friendly household and personal products through sustainable product development.

As a leading provider of workplace benefits and services, Unum Group set out to enrich the employee experience by modernizing the manual and time-consuming process of retrieving policy information for inquiries from its client support center. With Azure OpenAI Service, the company developed an application that searches 1.3 terabytes of data with 95% accuracy, cutting response times to four to five seconds and significantly improving efficiency and customer satisfaction. The AI-generated results address 75% of contract-related questions, freeing up employee time for personalized solutions and interactions with clients. Employees trust the tool and feel it has improved their jobs, and now more than 90% of support center employees are using it.

Visma develops and tests software for over 1.8 million customers across Europe and Latin America through its 188 individual companies. Facing high inflation and tight labor markets, it turned to Microsoft AI technologies such as Azure OpenAI Service, Semantic Kernel and Azure AI Search to securely boost efficiency and customer satisfaction across its companies — each with their own customers and domain. Visma developers are using GitHub Copilot to streamline development processes, automate workflows and enhance collaboration. As a result, it has reported up to 50% reduction in development times. GitHub Copilot has also helped Visma developers unleash their creativity and bend the curve on innovation; the company has seen a marked increase in innovation since adopting the solution, even among those who have been working on the same code for 30 years.

Our mission has never been clearer: to empower every person and every organization on the planet to achieve more. We remain committed to democratizing intelligence and unlocking AI opportunities industry by industry in the year ahead by bringing Copilot to life on every device and across every role. We are focused on our co-innovation efforts to identify AI design patterns that will enable our customers and partners to build out their AI innovation environments. We are committed to helping you fortify your cybersecurity foundation by prioritizing security above all else — securing our products by design, by default and within our own operations as part of our Secure Future Initiative. As we continue leading in AI Transformation, we remain rooted in the fact we are at our best when we serve others, and I look forward to what we will accomplish together in the year ahead as your trusted cloud and AI partner.

Tags: AI, AI Azure, AI Transformation, Azure AI Search, Azure OpenAI Service, Copilot for Microsoft 365, digital transformation, GitHub, Microsoft 365, Microsoft Azure, Microsoft Copilot for Sales, Microsoft Copilot for Security, Microsoft Copilot Studio, Microsoft Fabric, Microsoft Partners, Microsoft Power Platform, Microsoft Teams, Semantic Kernel

On July 18, CrowdStrike, an independent cybersecurity company, released a software update that began impacting IT systems globally. Although this was not a Microsoft incident, given it impacts our ecosystem, we want to provide an update on the steps we've taken with CrowdStrike and others to remediate and support our customers. Since this event began, we've maintained ongoing communication with our customers, CrowdStrike and external developers to collect information and expedite solutions. We recognize the disruption this problem has caused for businesses and in the daily routines of many individuals. Our focus is providing customers with technical guidance and support to safely bring disrupted systems back online. Steps taken have included:

Engaging with CrowdStrike to automate their work on developing a solution. CrowdStrike has recommended a workaround to address this issue and has also issued a public statement. Instructions to remedy the situation on Windows endpoints were posted on the Windows Message Center.

Deploying hundreds of Microsoft engineers and experts to work directly with customers to restore services.

Collaborating with other cloud providers and stakeholders, including Google Cloud Platform (GCP) and Amazon Web Services (AWS), to share awareness on the state of impact we are each seeing across the industry and inform ongoing conversations with CrowdStrike and customers.

Keeping customers informed of the latest status on the incident through the Azure Status Dashboard here.

We're working around the clock and providing ongoing updates and support. Additionally, CrowdStrike has helped us develop a scalable solution that will help Microsoft's Azure infrastructure accelerate a fix for CrowdStrike's faulty update. We have also worked with both AWS and GCP to collaborate on the most effective approaches. While software updates may occasionally cause disturbances, significant incidents like the CrowdStrike event are infrequent. We currently estimate that CrowdStrike's update affected 8.5 million Windows devices, or less than one percent of all Windows machines. While the percentage was small, the broad economic and societal impacts reflect the use of CrowdStrike by enterprises that run many critical services. This incident demonstrates the interconnected nature of our broad ecosystem — global cloud providers, software platforms, security vendors and other software vendors, and customers. It's also a reminder of how important it is for all of us across the tech ecosystem to prioritize operating with safe deployment and disaster recovery using the mechanisms that exist. As we've seen over the last two days, we learn, recover and move forward most effectively when we collaborate and work together. We appreciate the cooperation and collaboration of our entire sector, and we will continue to update with learnings and next steps.

At Microsoft, our vision is to empower scientists with the latest breakthroughs in AI to unlock their full creative potential and tackle some of our most pressing challenges. This vision will require bringing the full power of generative AI together with quantum-classical hybrid computing to augment every stage of the scientific method. Whether expanding knowledge research, creating better hypotheses, or accelerating experimentation and analyses, doing so demands a purpose-built cloud platform for science. This is why we built Azure Quantum Elements for chemistry and materials science. Today, we're announcing Generative Chemistry and Accelerated DFT, which will expand the ways researchers can harness the full power of this platform. These breakthrough capabilities will empower scientists to compress the next 250 years of chemistry into the next 25. With Generative Chemistry, we want to broaden the horizons of scientific exploration. Researchers can generate and explore novel molecules suited for specific industry applications using the latest AI models trained on hundreds of millions of compounds, and then evaluate the steps suggested by the workflow for synthesizing the most promising candidates in a lab more efficiently — all in a matter of days rather than years. With Accelerated DFT, researchers can expedite and scale their chemical discovery pipelines by simulating the quantum-mechanical properties of molecules at an unprecedented speed — an order of magnitude faster compared to other Density Functional Theory (DFT) codes. This brings us closer to a new paradigm for scientific discovery, where advanced AI and digital tools are more accessible than ever to scientists, students, and labs across industries. Below is our vision for how researchers will be able to leverage these breakthrough capabilities to design new molecules and enable the transformation of entire sectors from consumer goods and medicine, to manufacturing and energy, in turn addressing some of our most pressing societal challenges.

YouTube Video Click here to load media We're working towards this vision today. As part of the private preview of Azure Quantum Elements, scientists and developers have the opportunity to explore Accelerated DFT today, with the potential to access Generative Chemistry in the coming weeks. We're already putting our vision into practice by collaborating with Unilever, a global leader in consumer goods, which serves over 3.4 billion people every single day. Unilever is harnessing the power of Microsoft supercomputing and AI services to support their digital R&D transformation and product innovation. Integrating AI into every stage of the scientific method From global ambitions like reversing climate change and pioneering renewable energy sources to personal ones like living more sustainably and using healthier and safer products, we all want to do our part to create a better world. Time is of the essence for many of these goals, with more than 8 million scientists1 around the globe working to pioneer innovative solutions and unlock progress. At Microsoft, we aim to empower them with state-of-the-art digital tools to harness the full collective ingenuity of every researcher and lab around the world. Just as generative AI has unleashed new waves of creativity and improved productivity with collaborative tools like Copilot, we are now bringing AI and natural language processing capabilities to science. Our goal is to integrate AI reasoning into every stage of the scientific method: this requires the power of next-generation AI models to speed up the scientific process from hypothesis to results. It starts with knowledge research and hypothesis generation, connecting the dots by generating millions of potential molecular candidate solutions, then narrowing down candidates with digital experiments and analyzing the outcomes — all in a matter of days. We demonstrated how this approach can land real-world results in our collaboration with PNNL, where we screened over 32 million candidates to discover and synthesize a new material that holds the potential for better batteries — a tangible example of the possibilities in this new era of scientific discovery. When powered by natural language tools, this new paradigm will help create an autonomous reasoning loop with AI at every stage as a scientific assistant. It will redefine how we approach innovation by democratizing these capabilities for breakthrough discoveries. Our goal is to integrate AI into every stage of the scientific method, from initial research and hypothesis generation to experimentation and analysis. Announcing new capabilities in Azure Quantum Elements Generative Chemistry will unleash a new wave of creativity for scientists tasked with discovering and designing new molecules. This will enable breakthrough growth across many industries, whether helping an oil and gas company discover a stronger fuel additive for enhancing the longevity of engine life, or an adhesive firm creating a new chemical for strengthening adhesion while removing unwanted residue. We could compare this discovery process to searching for a small box in a large, crowded and dark warehouse with one small flashlight. We can only focus the light on a small area at a time while the rest of the warehouse remains completely dark and unknown. Generative AI gives us a much smarter light that can point in new directions, providing visibility where we may not have considered – or have been able – to look before. Researchers can ask Generative Chemistry for molecules with desired characteristics, such as the ability to degrade rapidly or be recycled more easily. They can also provide information about their targeted application and let the system help determine relevant molecular properties. After a few more steps, they receive a set of candidates — matching those parameters — for further study. However, simply generating candidates is not sufficient for transforming the discovery process with AI. The essential criteria for computational tools in chemistry are that they help scientists discover molecules that are novel, synthesizable and useful in the real world. This is why I'm excited to see our approach to Generative Chemistry come to life, suggesting molecules that have not been seen before, with useful properties tuned for a specific application, and whose synthesis is feasible in a reasonable number of steps. For this reason, Generative Chemistry will offer researchers potential steps to consider as they develop their "recipe" for synthesizing these molecular candidates in a laboratory. Support for this critical component has been developed from the capabilities of our AutoRXN software, exploring chemical reactions in reverse order, which can help to evaluate synthesis pathways for creating a target molecule. After scientists specify the desired characteristics of a molecule, they receive thousands of molecular candidates that can be further refined through AI inferences and subsequent HPC simulations, arriving at a select few that show the most promise for laboratory synthesis and further experimental exploration. This capability is truly groundbreaking for scientific discovery. Businesses and research groups can look for efficient, cost-effective and innovative methods to develop new molecules in a matter of days, compressing the iterative process of extensive database searches and trial-and-error laboratory experiments. This end-to-end workflow will provide scientists with entirely new compounds that could lead to the next breakthrough in manufacturing, medicine and more. We're also announcing Accelerated DFT to offer a simplified and more powerful quantum chemistry solution for scientists. For the past few decades, DFT has been an extremely popular method used across a variety of molecular simulations, helping researchers to simulate and study the electronic structure of atoms, molecules and nanoparticles, as well as surfaces and interfaces. We can liken molecular systems to traffic systems, where cars moving in various directions at different speeds represent electrons. From a traffic helicopter, we can observe the overall flow of traffic even if we don't know each car's speed and destination. DFT provides this "helicopter view'" of molecular systems, simplifying the complex task of tracking individual electrons by instead mapping out the "density" of them at a higher altitude. Such DFT simulations can be complex to optimize and run, and often require supercomputer-scale resources. This is why our managed DFT service, based on innovation developed by Microsoft Research, enables researchers to perform substantially faster calculations than other DFT codes and offers a 20-fold average increase in speed compared to PySCF, a widely used open-source DFT code. Accelerated DFT is already used by many organizations such as AspenTech, DTU Energy University of Denmark and Unilever. It seamlessly integrates into broader chemistry and materials science workflows, and paves the way for expediting innovations in therapeutics, environmental sustainability and beyond. You can learn more about this announcement in the technical blog, Introducing two powerful new capabilities in Azure Quantum Elements: Generative Chemistry and Accelerated DFT. Pioneering a new scientific discovery paradigm with Unilever Unilever stands at the forefront of the consumer goods industry, with a strong portfolio of household brands that are used by 3.4 billion people every day, including Dove, TRESemmé, Omo, Degree, Hellmann's and Ben & Jerry's. Whether cleaning, beauty or care products, each requires the latest scientific breakthroughs to ensure the best possible consumer experience and enhance daily life. Over the past two and a half years, Unilever has worked with Microsoft to identify new digital capabilities to drive product innovation forward. Unilever is bringing its digital vision to life through the transformational DataLab — its digital counterpart to the company's physical laboratories — with the help of Microsoft Azure. From unlocking the secrets of our skin's microbiome to reducing the carbon footprint of a multi-billion-dollar business, Unilever is redefining what it means to be a consumer goods company in the modern world with leading science.

YouTube Video Click here to load media With Copilot and the advanced simulation capabilities of Azure Quantum Elements, Unilever can query scientific information using natural language, performing thousands of computational simulations in the time it would take to run tens of laboratory experiments. Unilever scientists can use the data gathered from these simulations to fine-tune models that screen tens of thousands of materials at substantial speed or enable the exploration of intricate chemical reactions. For example, R&D teams can expand their search space for novel molecules that restore natural bonds in hair fibers across more hair types, in turn redefining the standards of personalized hair care for brands like Dove and TRESemmé. Furthermore, by placing scaled simulations at the forefront of the discovery funnel, Unilever will be further empowered to expedite the delivery of solutions within their key sustainability focus areas. "Digital tools are unlocking an unprecedented age of scientific discovery. Using advanced computing power and AI, we are able to compress decades of lab work into days, accessing a level of insight we could not previously have imagined. This technological leap, coupled with our vast repository of proprietary data and a century of expertise in personal and household care, means our scientists are able to lead the industry in developing the next generation of consumer goods." — Alberto Prado, Global Head of R&D Digital and Partnerships at Unilever Expanding quantum capabilities in Azure Quantum Elements We stand on the cusp of unprecedented innovation, and at Microsoft, we continue to pioneer state-of-the-art solutions to usher in a new era of scientific discovery. We remain focused on achieving scaled quantum computing and more breakthroughs on our path to engineering our topological qubits with inherent hardware-level stability. Earlier this year, we demonstrated with Quantinuum the most reliable logical qubits on record, further advancing the state-of-the-art for quantum computing. And recently, we simulated a chemical catalyst combining classical supercomputers, AI and logical qubits created with Microsoft's qubit-virtualization system and Quantinuum's H1 hardware. This combination holds the key to unlocking scientific breakthroughs enabled by a new generation of hybrid-computing applications. In the coming months, we will bring advanced logical qubit capabilities using our software and Quantinuum's hardware in private preview in Azure Quantum Elements. As logical qubit capabilities scale to deliver increasingly reliable results, we will unlock simulation accuracy, moving us from scientific advantage to commercial advantage, and ultimately to solving some of the world's most pressing problems. Accelerating scientific discovery, together We're committed to advancing these technologies responsibly, always focusing on innovation, empowerment and trust. That's why we are committed to responsible computing practices and the Microsoft AI principles, to help ensure that safety measures adequately account for the increasing power of AI and quantum. For more information about today's announcements:

Sign up to learn more about Accelerated DFT and Generative Chemistry, as part of the private preview of Azure Quantum Elements. Read the technical blog, Introducing two powerful new capabilities in Azure Quantum Elements: Generative Chemistry and Accelerated DFT. Register for our upcoming ACS Webinar Check out the upcoming Future of Cloud webinar series about our vision to accelerate scientific discovery for business innovation.

Top image: Leaders from Unilever and Microsoft discuss the Azure Quantum Elements program. Sources 1. "Statistics and resources | 2021 Science Report." This translates into 8.854 million full-time equivalent (FTE) researchers by 2018". Tags: Accelerated DFT, AI, Azure Quantum Elements, Generative Chemistry, quantum computing

At the 2024 G7 Summit in Fasano, Italy, President Biden and other G7 leaders met with private sector executives, including Microsoft Chairman and CEO Satya Nadella, to reaffirm their commitment to accelerate sustainable infrastructure investment through the Partnership for Global Infrastructure and Investment. Below are Satya's remarks at the event.

Thank you very much, Prime Minister Meloni and President Biden for the opportunity this afternoon to really participate in this very important discussion about scaling the Partnership for Global Infrastructure and Investment to support very high-quality infrastructure, financing in low-and-middle-income countries. It's encouraging to see the partnership expand and gain momentum.

As this partnership has recognized, we are entering a very new age, driven by AI, which has made clear the critical importance of both digital infrastructure and connectivity in every sector of the economy and every corner of the world, as well as energy.

There is no doubt that AI is one of the most transformative technologies of our time, and we believe it will fundamentally bend the productivity curve for every individual, organization, and industry sector to help us address some of our most pressing global challenges, including in the world's low-income countries.

At Microsoft we have this saying where we say that we will only succeed if the world around is fundamentally succeeding. We are therefore very, very focused on ensuring that every country can benefit from this broad technological shift, while mitigating its risks.

This presents a pretty unique opportunity to help address the economic development and societal priorities, everywhere in the world, whether it's small businesses becoming more productive, whether it's the multinationals in every part of the world becoming more competitive, globally, governments becoming more efficient, improving health outcomes and education outcomes.

Over the past three months, for example, we have invested $5 billion in cloud and AI infrastructure in Indonesia and Malaysia and Thailand, as well as in Kenya, together with our partners G42 of the UAE. This is work designed to drive that broad spectrum of economic growth, innovation, and connectivity to drive productivity.

But we recognize that this requires more than just infrastructure, which is why we have also launched initiatives to provide broad-based AI skilling opportunities, strengthen cybersecurity, and support startups and innovative companies around the world.

We have already seen many examples of this impact in action. I was in Indonesia recently and I saw how Indonesian farmers are improving their yields. To Kenyan street vendors who now have access to credit for the first time.

We also have noted that the infrastructure of the future has to be sustainable, and so this is why we are investing in carbon-free energy, as well as efforts to develop greener steel and concrete in order to build our data centers, and also make our silicon more carbon-free and efficient.

At the same time, this ambition is not something that any one company, or for that matter, any one country can accomplish on its own. This requires broad partnerships across the private and public sectors.

For example, we need regulatory innovation to enable multiple countries to share digital infrastructure when located in neighboring regions. That's what we did in Kenya, where we created a trusted data zone for the East Africa community, in partnership with the government and G42. This ensures governments and companies across the region can store their data in our data center with the assurance that their own privacy and cybersecurity and other legal rules and protections will continue to be applied.

This is just one example of the type of partnership and innovation and regulatory involvement that is required in order to be able to really drive this next generation of infrastructure.

To close, I'm incredibly optimistic about our collective opportunity, and I'm looking forward to working together in helping people and organizations around the world as they apply this new generation of technology to grow and thrive.

Today we are thrilled to announce the latest milestone in our journey towards modernizing customer service: Microsoft Dynamics 365 Contact Center, a Copilot-first contact center solution that delivers generative AI to every customer engagement channel. With general availability on July 1, this standalone Contact Center as a Service (CCaaS) solution enables customers to maximize their current investments by connecting to preferred customer relationship management systems (CRMs) or custom apps.

YouTube Video Click here to load media Modernizing service experiences with generative AI Customer service expectations are higher than ever. It's not only frustrating for customers to deal with long wait times, being transferred to the wrong agent or having to repeat themselves multiple times — it's detrimental to business. When people have poor customer service experiences, over half of them end up spending less or decide to take their business elsewhere (Qualtrics). Generative AI is transforming customer service and revolutionizing the way contact centers operate — from delivering rich experiences across digital and voice channels that enable customers to resolve their own needs, to equipping agents with relevant context within the flow of work, and ultimately unifying operations to drive efficiency and reduce costs. We have experienced the transformational impact of generative AI firsthand with Microsoft's Customer Service and Support (CSS) team, one of the largest customer service organizations in the world. Before the support team migrated to Microsoft's own tools, CSS was previously using 16 different systems and over 500 individual tools — slowing down service, hindering collaboration and producing inefficient workflows. With Copilot as part of the solution, the CSS team achieved a 12 percent decrease in average handle time for chat engagements and 13 percent decrease in agents requiring peer assistance to resolve an incident. And more broadly, CSS has seen a 31 percent increase in first call resolution and a 20 percent reduction in missed routes. Dynamics 365 Contact Center Applying learnings and insights from our own Copilot usage, coupled with multi-year investments in voice and digital channels, Dynamics 365 Contact Center infuses generative AI throughout the contact center workflow — spanning the channels of communication, self-service, intelligent routing, agent-assisted service and operations to help contact centers solve problems faster, empower agents and reduce costs. Additionally, Dynamics 365 Contact Center is built natively on the Microsoft cloud to deliver extensive scalability and reliability across voice, digital channels and routing while at the same time allowing organizations to retain their existing investments in CRM or custom apps. Key Dynamics 365 Contact Center capabilities include:

Next-generation self-service: With sophisticated pre-integrated Copilots for digital and voice channels that drive context-aware, personalized conversations, contact centers can deploy rich self-service experiences. Combining the best of interactive voice response (IVR) technology from Nuance and Microsoft Copilot Studio's no-code/low-code designer, contact centers can provide customers with engaging, individualized experiences powered by generative AI. Accelerated human-assisted service: Across every channel, intelligent unified routing steers incoming requests that require a human touch to the agent best suited to help, enhancing service quality and efficiency. When a customer reaches an agent, Dynamics 365 Contact Center gives the agent a 360-degree view of the customer with generative AI — for example, real-time conversation tools like sentiment analysis, translation, conversation summary, transcription and more are included to help improve service, along with others that automate repetitive tasks for agents such as case summary, draft an email, suggested response and the ability for Copilot to answer agent questions grounded on your trusted knowledge sources. Operational efficiency: Contact center efficiency depends just as much on what happens behind the scenes as it does on customer and agent experiences. We've built a solution that helps service teams detect issues early, improve critical KPIs and adapt quickly. With generative AI-based, real-time reporting, Dynamics 365 Contact Center allows service leaders to optimize contact center operations across all support channels, including their workforce.

"At 1-800-Flowers.com, we pride ourselves on exceptional service and continually raising the bar. With Microsoft Dynamics 365 Contact Center, we're creating a best-in-class solution that furthers our mission and helps inspire people to give more, connect more, and build more and better relationships." — Arnie Leap, CIO, 1-800-FLOWERS.COM, Inc. "MSC has always been known for the personal service that we give to our customers; Microsoft Dynamics 365 Contact Center helps us elevate that customer-centric approach."— Fabio Catassi, CIO, Mediterranean Shipping Company "For our support teams, efficient problem-solving and smooth customer interactions are key to delivering exceptional service. With Dynamics 365 Contact Center and by leveraging its AI capabilities, we see a future where our support teams will deliver that level of service every day."— Stephen Currie, Vice President Support Operations, Synoptek

If you're attending Customer Contact Week in Las Vegas, join me for my main stage panel on Thursday, June 6. Be sure to also stop by the Microsoft booth (#151) during the event to see Dynamics 365 Contact Center in action. Stay tuned for the general availability of Dynamics 365 Contact Center on July 1. Tags: AI, Microsoft Copilot Studio, Microsoft Customer Service, Microsoft Dynamics 365 Contact Center

In the face of adversity, the ability to continue to move forward, inspire and spark change is something to celebrate. Even small change is hugely meaningful because progress often comes from small steps of affirmation, resistance and perhaps, even most importantly, celebration. That's why the LGBTQIA+ communities at Microsoft centered our 2024 Pride campaign on the idea of "Radical joy." The joy that radiates and inspires. The joy that is not dependent on outside forces but is within us and within our control, and when expressed, can help make change for all and inspire others to do the same. "Radical joy" is an anthem to those who thrive against all odds.

This year's campaign is an ode to the strength and power of the LGBTQIA+ communities. The LGBTQIA+ community is the heart of this movement, and employees are the heart of Microsoft. Eight Microsoft employees shared their radical joy stories, reminding us of the why behind Pride and uniting us in furthering its mission.

Microsoft employees share what Pride and "Radical joy" means to them. Open source and spreading joy

"Radical joy" is grounded in inclusion and acceptance, values that are important for us at Microsoft. One of the many ways we are celebrating them at Pride is by highlighting the interconnected and intersectional nature of LGBTQIA+ communities. Our designs unite 74 flags representing different LGBTQIA+ identities through vibrant abstract shapes that nod to the joy and power that radiates from and through the community year-round. This year's graphic expands representation to 25 new communities and is localized for use in 33 languages.

For the third year in a row, Microsoft invites everyone to spread LGBTQIA+ acceptance and inclusion by sharing stories, open sourcing the entire Pride campaign design to remix and share, as we have found time and time again, participation is key to the community and its allies. For the first time, this year, we invite you to leverage AI to create even more Pride by sharing your own stories with fast and easy AI-powered tools like Microsoft Designer and Clipchamp to make your own Pride stickers, social posts and more. In addition, we are making all Pride assets available on Figma and GitHub, where anyone can download and customize the campaign assets to spread joy this Pride. Microsoft and its employees invite everyone to visit Microsoft.com/pride to learn more and share on social by using #MicrosoftPride and #Pride. We look forward to seeing what you come up with!

Everyone can use our Pride graphics to create their own celebrations and designs. Our pride collection Pride lights up across many Microsoft products – including Teams, Edge, Clipchamp, Microsoft 365, Designer and Windows.

We are spreading joy across our products and platforms and have been since 2018. Discover how you can celebrate Pride and the power of LGBTQIA+ communities:

Microsoft Unlocked – Unlock all things Pride at Microsoft. Open source – Microsoft Pride designs from this year and past years are available on GitHub and Figma for you to download, remix and customize. Microsoft Designer – Create and customize this year's radical joy design for your own Pride graphics. Microsoft Designer templates – Discover and customize even more Pride-themed templates. Microsoft Designer sticker creator – Prompt AI to create custom stickers that reflect your joy and use them on your devices! GIPHY – Radical joy should be shared, and that is why we brought this year's graphics to GIPHY, so you can share joyful GIFs across devices! Microsoft Teams – Download custom Teams backgrounds to spread joy during your video calls. Microsoft Edge themes – Light up your Edge experience with the new Pride theme. Windows wallpapers – Update your desktop with radical joy by downloading one of our custom wallpapers. Microsoft 365 apps theming – Turn on your Pride in select Microsoft 365 apps* and celebrate with a special theme inspired by the flags of the LGBTQIA+ communities. Clipchamp – Unleash your creativity on Clipchamp with Pride assets built straight into the product. Xbox – Learn about Xbox's partnership with The Trevor Project, discover custom Pride art, purchase merch and more on Xbox Wire. Microsoft Store – Explore collections of curated games from LGBTQIA+ and transgender and non-binary Inclusion is Innovation – Explore LGBTQIA+ employee stories year-round. Our commitment

We are continuing our longstanding efforts to financially contribute to nonprofits doing on-the-ground work to support LGBTQIA+ communities. This year, we are donating $200,000 to charities across the globe: The Trevor Project, Transconvida, ACCEPT ASOCIATIA and Transgender Law Center. Together with our employees, Microsoft has donated over $16 million to organizations that support LGBTQIA+ communities in the last 30 years, and in the last year alone, we donated over $1.3 million.

Our efforts to support and advocate for LGBTQIA+ people do not start or end at Pride. By supporting our employees through global advocacy, charitable giving and better workplace practices, we hope to make a difference for LGBTQIA+ communities and beyond, as our drive for positive global impact continues and celebrates our employees and what brings them, and their communities' joy.

At Microsoft, we continue to leverage our platform year-round to support and learn from LGBTQIA+ communities and address issues that impact the lived experience, safety, and fundamental rights of employees and their communities around the world.

We know that there is still work to be done, as a brand and as a global society. While we celebrate our successes, we remain committed to making continued progress.

Microsoft introduced sexual orientation into our non-discrimination policies in 1989 – and we have been driving toward LGBTQIA+ inclusion ever since. In 1993, we began offering employee benefits for same-sex domestic partnerships, making us one of the first companies in the world to do so. We stood as a public supporter for marriage equality even before it became legal in the United States – and we continue to advocate for every type of family. Microsoft has attained a 100 score on the Human Rights Campaign Foundation's Corporate Equality Index for 18 years. Since its founding in 1993, GLEAM (Global LGBTQIA+ Employees & Allies at Microsoft) has honored Pride by lifting up LGBTQIA+ voices and used the reach of our products to share the messages of inclusion and support.

We invite everyone to join us in the call for making more Pride. Share your thoughts with us on social by using #MicrosoftPride and #Pride.

* "Show your Pride" theme is available in the following apps – iOS: Outlook, OneNote, Office, Word, Excel and PowerPoint; in Android: Outlook; in Mac OS: Word, Excel, PowerPoint, OneNote. In Outlook, you can find the Pride themes in Settings > Appearance In all other apps, you can find Pride theme in Settings.

In the span of just one year, we've witnessed a remarkable acceleration in AI adoption, opening a new frontier of innovation, discovery and solutions. This surge in AI is not just about technological advancement; it's about harnessing the potential to promote innovation, tackle some of societies' largest challenges and change lives for the better. Microsoft's role and responsibility as a leader in AI is reflected in the company's commitment to ensure broad access to our technology that empowers organizations and individuals to serve the public good. We build and operate a global network of advanced datacenters that enable this technology, and we recognize the crucial role that communities play in this ecosystem. We rely on a vast network of local suppliers, officials, stakeholders and residents to plan, design, construct and operate each of our facilities. To those communities, we owe a commitment to be responsible neighbors and contribute positively to local economies and ecosystems while advancing digital transformation. We recognize the importance of supporting communities, and our datacenters should be a resource that addresses local needs and priorities. With that, I'm delighted to announce our Datacenter Community Pledge that builds upon the deep partnerships we have formed with communities around the world. This is our pledge that Microsoft's datacenters will be not only the backbone of modern technology but also a force for good in the communities they serve. The Datacenter Community Pledge focuses on three core areas: Contributing to a sustainable future We will design and operate our datacenters to support society's climate goals and become carbon negative, water positive and zero waste before 2030. Our sustainability work starts with taking accountability for our operational footprint. We remain unwavering in our commitment to achieve carbon negativity by 2030, and our datacenters play a pivotal role in this comprehensive decarbonization strategy. By 2025, we will procure 100% renewable energy on a global scale, both significantly expanding and decarbonizing local electricity grids. Our datacenter designs are more water efficient than traditional enterprise datacenters, and our plan by 2030 is to replenish more water than we consume locally. Another critical aspect of our work is achieving zero waste. In 2020, we introduced the first of its kind Microsoft Circular Center in Amsterdam designed to reuse and repurpose servers and hardware in our datacenters. Since then, we have six operational Circular Centers in key locations that help us divert waste from landfills, and our plan by 2030 is to achieve zero waste through a combination of waste reduction, reuse, recycling and composting. This pledge minimizes our dependency on local resources and ensures that our operations contribute positively to the community. Read more about this work:

Building world-class sustainable datacenters and investing in solar power in Arizona Opening doors to a sustainable future with Microsoft's Circular Center in Singapore Progress on our commitment to sustainable backup power in datacenters by 2030

Advancing community prosperity and well-being We will deliver significant local, economic, social and environmental benefits to the communities where we operate. Local people know their community better than anyone else, and that is why we partner with local stakeholders to ensure our datacenters provide support in the form of educational programs, grants and other initiatives aligned to local needs and priorities. We partner with local governments, businesses, schools and nonprofits to create well-paid jobs and apprenticeship opportunities, and we provide essential digital skills training and STEM education for adults and children. Our goal is to empower community members with future-ready skills necessary for the digital age, ensuring they are prepared for the opportunities of tomorrow. We invest in initiatives to enhance local sustainability. We restore sensitive watershed corridors and invest in projects to conserve trees, enhance native flora and protect ecological habitats, providing benefits to the surrounding ecosystems and honoring the natural beauty of the area. Our datacenters generate public infrastructure improvements and tax revenue that serve as a catalyst for enhancing the quality of life in these areas. That translates into new and improved social services, such as parks where families can come together, better-equipped schools that empower children's education and well-maintained roads and digital infrastructure that connect us all. Read more about this work:

How a small city in Iowa became an epicenter for advancing AI Microsoft opens Asia's first Datacenter Academy in Singapore with ITE Microsoft to establish its first datacenter region in Taiwan as a part of its "Reimagine Taiwan" initiative

Operating responsibly as a good neighbor We will partner closely with communities and operate in a way that respects the local environment. In all we do, we strive to be a responsible neighbor. We listen, learn, build relationships and follow through on our commitments — guided by what is important locally. We will design our datacenters with the environment and communities in mind. From sustainable development practices and building materials to noise and light reduction measures, we will work with, and support, the environment and the communities where we are based. We will collaborate with neighbors and communities throughout our development and operations, while partnering with nonprofits and suppliers who share our commitment to expand opportunities for all. Read more about this work:

Microsoft and Fortum join forces – Microsoft is building a data center area in Finland that will produce emission-free district heat for Fortum's customers in the Helsinki metropolitan area Blending datacenters into Wieringermeer nature with biomimicry Partnering with the City of Quincy to open Washington's first industrial water reuse center

We approach this work to create positive change, uplift the communities where we operate and to make a significant contribution to the digital transformation. Through partnerships, we are crafting a future both at the forefront of technology and also deeply rooted in social responsibility and environmental sustainability. Our investments serve not only the countries in which we operate but embody our pledge to be a good neighbor. When we build a datacenter in your community, we are here for all of us.

Editor's note: This post has been updated to reflect that Microsoft has six operational circular centers. .

Groundbreaking advances in AI are having a profound impact on Microsoft and on the developers using our technology to improve efficiency, enhance customer experiences and make new breakthroughs.

Over the past year, we've built Microsoft Copilot and released more than 150 updates to it, and we've developed the Copilot stack, which takes everything we've learned so far and lets developers build their own copilots.

In addition, over the past two years, GitHub Copilot has become the most widely adopted AI developer tool, with 1.8 million paid subscribers.

And just a day ago, we introduced a new category of Copilot+ PCs, the fastest, most AI-ready PCs ever built.

We are the industry leader in AI, and that's why, as we begin our flagship event for developers, Microsoft Build, there's one question on everyone's mind: What's next?

On Monday, we introduced the world to a new class of Windows PCs, Copilot+ PCs. These devices are designed to allow developers to deliver differentiated AI experiences on the edge, and, together with the great features that we are announcing at Build, make Windows the most open platform for AI and the best place for developers.

At Build, we also are announcing further branches of this technology revolution, including:

How Microsoft Fabric is helping developers and customers leverage data in motion, or digital information being transported within or between computer systems, to build intelligent apps. New frontier models that allow developers to explore multimodal capabilities supporting text, images, video and other types of data in their AI applications, evolving beyond text prompts and completions. Partnerships that show how AI can impact the future of myriad industries, including education. And how an open and flexible cloud-based platform, built on a foundation of safe and responsible AI, can help developers innovate.

Microsoft Build is always an exciting time for us. About 200,000 people have registered to join us for three days of technical learning and community connection, with 4,000 attending in person in Seattle. Attendees can choose from more than 300 sessions, demos and expert and instructor-led labs from Microsoft and our partners. For those not able to attend the live event, most content will be available on demand. In total, we are announcing about 60 new products and solutions at the event.

With that introduction to Build in mind, let's explore some of the news and announcements.

For the most efficient AI apps, businesses need to be able to qualify, analyze and organize data at ingestion. This has proven to be a difficult step. The new Real-Time Intelligence within Microsoft Fabric provides an end-to-end Software as a Service (SaaS) solution that empowers customers to act on high-volume, time-sensitive and highly granular data to make faster and more informed business decisions.

Real-Time Intelligence, now in preview, can be helpful for analysts with simple low/no-code experiences, and it also can benefit pro developers with code-rich user interfaces. For example, racing team Dener Motorsport has been using Microsoft Fabric to support real-time analytics, storage and reporting, enabling them to sustain optimal performance and maintain the cars in good repair, which can keep drivers safer. Dener plans to use Real-Time Intelligence as part of their winning strategy to gain insights at ingestion during races.

Building apps requires a level of flexibility, customization and efficiency to make it feasible for developers. The new Microsoft Fabric Workload Development Kit makes this possible by enabling independent software vendors (ISVs) and developers to extend applications within Fabric, creating a unified user experience.

GitHub is introducing the first set of GitHub Copilot extensions, developed by Microsoft and third-party partners, in private preview. These additions allow developers and organizations to customize their GitHub Copilot experience with their preferred services like Azure, Docker, Sentry and more directly within GitHub Copilot Chat.

GitHub Copilot for Azure, one of the extensions from Microsoft, showcases how building in natural language with a broader range of capabilities can propel development velocity. Using the extension through Copilot Chat, developers can explore and manage Azure resources, while also troubleshooting issues and locating relevant logs and code.

GPT-4o, OpenAI's newest flagship model, is now available in Azure AI Studio and as an API. This groundbreaking multimodal model integrates text, image and audio processing to set a new standard for generative and conversational AI experiences.

We also announced that Phi-3-vision, a new multimodal model in the Phi-3 family of AI small language models (SLMs) developed by Microsoft, is now available in Azure. Phi-3 models are powerful, cost-effective and optimized for personal devices. Phi-3-vision offers the ability to input images and text and receive text responses. For example, users can ask questions about a chart or ask an open-ended question about specific images.

Developers can experiment with these state-of-the-art frontier models in the Azure AI Playground, and they can start building with and customizing with the models in Azure AI Studio.

Microsoft and Khan Academy are announcing a multi-faceted partnership to turn the transformative potential of AI into reality.

To start, Microsoft is enabling Khan Academy to offer all K-12 U.S. educators free access to Khanmigo for Teachers, an AI-powered teaching assistant that frees up teachers' time so they can focus on what matters most – engaging with and supporting their students. Microsoft is donating access to Azure AI-optimized infrastructure to increase the availability of Khanmigo for Teachers, which will now be powered by Azure OpenAI Service.

Khan Academy is collaborating with Microsoft to explore opportunities to improve math tutoring in an affordable, scalable and adaptable manner with a new version of Phi-3, a family of SLMs developed by Microsoft. They also plan to bring more Khan Academy content into Copilot and Teams for Education, expanding resources for learners.

Microsoft and Cognition will bring Cognition's autonomous AI software agent, Devin, to customers to help them with complex tasks such as code migration and modernization projects. As part of the agreement, Devin will be powered by Azure. Cognition AI is an applied AI lab building end-to-end software agents to help developers achieve more.

Microsoft has a unique systems approach to AI infrastructure, which includes hardware and software from Microsoft and our partners, all optimized to run AI workloads at scale and fine-tuned for customer needs. We are the first cloud provider to bring AMD's leading MI300X AI accelerator chip to power customers' AI training and inferencing needs, with the general availability of the Azure ND MI300X v5 virtual machine series optimized for demanding AI and high-performance computing (HPC) workloads like Azure OpenAI Service.

Following the launch of Azure Cobalt 100, Microsoft's first custom-designed compute processor, the company is announcing a preview of new Cobalt 100 Arm-based virtual machines (VMs), based on the company's custom silicon series announced in November 2023. Cobalt 100 Arm-based VMs are the first generation of VMs to feature Microsoft's new Cobalt processor, custom-built on an Arm architecture, and optimized for efficiency and performance when running general-purpose and cloud-native workloads. Customers can expect up to 40% improved performance compared to comparable Azure VMs.

Copilot has been a game-changer for many people since it was first released. Using modern AI and large language models (LLMs) like Open AI's GPT-4, copilots across Microsoft products have assisted people with complex tasks, serving as a personal, behind-the-scenes AI assistant.

Now, we are introducing Team Copilot, the expansion of Copilot for Microsoft 365 from a behind-the-scenes, personal AI assistant to a new, valuable member of your team. You will be able to invoke Copilot where you collaborate – in Teams, Loop, Planner and more. Team Copilot can be a meeting facilitator in meetings, managing the agenda, tracking time and taking notes. It can act as a collaborator in chats by surfacing important information, tracking action items and addressing unresolved issues. It can serve as a project manager to help ensure every project runs smoothly and notify the team when their input is needed. These initial experiences, coming in preview later this year, will enable us to learn, iterate and refine as we enter a new phase of innovation where Copilot begins to take more action on behalf of individuals and teams.

Microsoft Copilot Studio is introducing new agent capabilities, empowering developers to build copilots that can proactively respond to data and events, tailored to specific tasks and functions. Copilots built with this new category of capabilities can now independently manage complex, long-running business processes by leveraging memory and knowledge for context, reason over actions and inputs, learn based on user feedback and ask for help when they encounter situations that they don't know how to handle. Users can now put Copilot to work for them – from IT device procurement to customer concierge for sales and service.

Copilot extensions, including plugins and connectors, allow customers to enhance Microsoft Copilot by connecting it to new data sources and applications, expanding its functionality.

We're excited about these announcements and all the other updates and features being unveiled at Build. For more information, today you can watch keynotes from Microsoft Chairman and CEO Satya Nadella, Executive Vice President of Experiences and Devices Rajesh Jha and Chief Technology Officer Kevin Scott.

On Wednesday, you can watch keynotes from Executive Vice President of Cloud and AI Scott Guthrie and friends. Additionally, you can explore all the news and announcements in the Book of News.

Tags: AI, Copilot+ PC, Microsoft Build, Microsoft Fabric, Phi-3, Real-Time Intelligence, Team Copilot

An on-demand recording of our May 20 event is available. Today, at a special event on our new Microsoft campus, we introduced the world to a new category of Windows PCs designed for AI, Copilot+ PCs. Copilot+ PCs are the fastest, most intelligent Windows PCs ever built. With powerful new silicon capable of an incredible 40+ TOPS (trillion operations per second), all–day battery life and access to the most advanced AI models, Copilot+ PCs will enable you to do things you can't on any other PC. Easily find and remember what you have seen in your PC with Recall, generate and refine AI images in near real-time directly on the device using Cocreator, and bridge language barriers with Live Captions, translating audio from 40+ languages into English. These experiences come to life on a set of thin, light and beautiful devices from Microsoft Surface and our OEM partners Acer, ASUS, Dell, HP, Lenovo and Samsung, with pre-orders beginning today and availability starting on June 18. Starting at $999, Copilot+ PCs offer incredible value. This first wave of Copilot+ PCs is just the beginning. Over the past year, we have seen an incredible pace of innovation of AI in the cloud with Copilot allowing us to do things that we never dreamed possible. Now, we begin a new chapter with AI innovation on the device. We have completely reimagined the entirety of the PC – from silicon to the operating system, the application layer to the cloud – with AI at the center, marking the most significant change to the Windows platform in decades.

YouTube Video Click here to load media The fastest, most secure Windows PCs ever built We introduced an all-new system architecture to bring the power of the CPU, GPU, and now a new high performance Neural Processing Unit (NPU) together. Connected to and enhanced by the large language models (LLMs) running in our Azure Cloud in concert with small language models (SLMs), Copilot+ PCs can now achieve a level of performance never seen before. They are up to 20x more powerful[1] and up to 100x as efficient[2] for running AI workloads and deliver industry-leading AI acceleration. They outperform Apple's MacBook Air 15" by up to 58% in sustained multithreaded performance[3], all while delivering all-day battery life. With incredible efficiency, Copilot+ PCs can deliver up to 22 hours of local video playback or 15 hours of web browsing on a single charge.[4] That is up to 20% more battery in local video playback than the MacBook Air 15".[5] Windows now has the best implementation of apps on the fastest chip, starting with Qualcomm. We now offer more native Arm64 experiences than ever before, including our fastest implementation of Microsoft 365 apps like Teams, PowerPoint, Outlook, Word, Excel, OneDrive and OneNote. Chrome, Spotify, Zoom, WhatsApp, Adobe Photoshop, Adobe Lightroom, Blender, Affinity Suite, DaVinci Resolve and many more now run​ natively on Arm to give you great performance with additional apps, like Slack, releasing later this year. In fact, 87% of the total app minutes people spend in apps today have native Arm versions.[6] With a powerful new emulator, Prism, your apps run great, whether native or emulated. Every Copilot+ PC comes secured out of the box. The Microsoft Pluton Security processor will be enabled by default on all Copilot+ PCs and we have introduced a number of new features, updates and defaults to Windows 11 that make it easy for users to stay secure. And, we've built in personalized privacy controls to help you protect what's important to you. You can read more about how we are making Windows more secure here. Entirely new, powerful AI experiences Copilot+ PCs leverage powerful processors and multiple state-of-the-art AI models, including several of Microsoft's world-class SLMs, to unlock a new set of experiences you can run locally, directly on the device. This removes previous limitations on things like latency, cost and even privacy to help you be more productive, creative and communicate more effectively. Recall instantly We set out to solve one of the most frustrating problems we encounter daily – finding something we know we have seen before on our PC. Today, we must remember what file folder it was stored in, what website it was on, or scroll through hundreds of emails trying to find it. Now with Recall, in preview starting June 18, you can access virtually what you have seen or done on your PC in a way that feels like having photographic memory. Copilot+ PCs organize information like we do – based on relationships and associations unique to each of our individual experiences. This helps you remember things you may have forgotten so you can find what you're looking for quickly and intuitively by simply using the cues you remember. [7]

YouTube Video Click here to load media You can scroll across time to find the content you need in your timeline across any application, website, document, or more. Interact intuitively using snapshots with screenray to help you take the next step using suggested actions based on object recognition. And get back to where you were, whether to a specific email in Outlook or the right chat in Teams. Recall leverages your personal semantic index, built and stored entirely on your device. Your snapshots are yours; they stay locally on your PC. You can delete individual snapshots, adjust and delete ranges of time in Settings, or pause at any point right from the icon in the System Tray on your Taskbar. You can also filter apps and websites from ever being saved. You are always in control with privacy you can trust. Cocreate with AI-powered image creation and editing, built into Windows Since the launch of Image Creator, almost 10 billion images have been generated, helping more people bring their ideas to life easily by using natural language to describe what they want to create. Yet, today's cloud offerings may limit the number of images you can create, keep you waiting while the artwork processes or even present privacy concerns. By using the Neural Processing Units (NPUs) and powerful local small language models, we are bringing innovative new experiences to your favorite creative applications like Paint and Photos.

YouTube Video Click here to load media Combine your ink strokes with text prompts to generate new images in nearly real time with Cocreator. As you iterate, so does the artwork, helping you more easily refine, edit and evolve your ideas. Powerful diffusion-based algorithms optimize for the highest quality output over minimum steps to make it feel like you are creating alongside AI. Use the creativity slider to choose from a range of artwork from more literal to more expressive. Once you select your artwork, you can continue iterating on top of it, helping you express your ideas, regardless of your creative skills. Restyle app screenshot Take photo editing and image creation to the next level. With Restyle Image, you can reimagine your personal photos with a new style combining image generation and photo editing in Photos. Use a pre-set style like Cyberpunk or Claymation to change the background, foreground or full picture to create an entirely new image. Or jumpstart your next creative project and get visual inspiration with Image Creator in Photos. On Copilot+ PCs you can generate endless images for free, fast, with the ability to fine tune images to your liking and to save your favorites to collections. Innovative AI experiences from the creative apps you love We are also partnering with some of the biggest and most-loved applications on the planet to leverage the power of the NPU to deliver new innovative AI experiences. Adobe Together with Adobe, we are thrilled to announce Adobe's flagship apps are coming to Copilot+ PCs, including Photoshop, Lightroom and Express – available today. Illustrator, Premiere Pro and more are coming this summer. And we're continuing to partner to optimize AI in these apps for the NPU. For Adobe Creative Cloud customers, they will benefit from the full performance advantages of Copilot+ PCs to express their creativity faster than ever before. Adobe Photoshop screenshot DaVinci Resolve Studio Effortlessly apply visual effects to objects and people using NPU-accelerated Magic Mask in DaVinci Resolve Studio. DaVinci Resolve Studio CapCut Remove the background from any video clip in a snap using Auto Cutout running on the NPU in CapCut. CapCut Cephable Stay in your flow with faster, more responsive adaptive input controls, like head movement or facial expressions via the new NPU-powered camera pipeline in Cephable. Cephable LiquidText Make quicker and smarter annotations to documents, using AI features that run entirely on-device via NPU, so data stays private in LiquidText. LiquidText djay Pro Have fun breaking down and remixing any music track, with a new, higher-quality version of NeuralMix™ that's exclusive to NPU in Algoriddim's djay Pro. djay NeuralMix Connect and communicate effortlessly with live captions In an increasingly connected and global world, Windows wants to bring people closer together. Whether catching up on your favorite podcast from a different country, or watching your favorite international sports team, or even collaborating with friends and colleagues across the world, we want to make more content accessible to more people. Live Captions now has live translations and will turn any audio that passes through your PC into a single, English-language caption experience, in real time on your screen across all your apps consistently. You can translate any live or pre-recorded audio in any app or video platform from over 40 languages into English subtitles instantly, automatically and even while you're offline. Powered by the NPU and available across all Copilot+ PCs, now you can have confidence your words are understood as intended. New and enhanced Windows Studio Effects Look and sound your best automatically with easily accessible controls at your fingertips in Quick Settings. Portrait light automatically adjusts the image to improve your perceived illumination in a dark environment or brighten the foreground pixels when in a low-light environment. Three new creative filters (illustrated, animated or watercolor) add an artistic flare. Eye contact teleprompter helps you maintain eye contact while reading your screen. New improvements to voice focus and portrait blur help ensure you're always in focus. Copilot, your everyday AI companion Copilot Every Copilot+ PC comes with your personal powerful AI agent that is just a single tap away on keyboards with the new Copilot key.[8] Copilot will now have the full application experience customers have been asking for in a streamlined, simple yet powerful and personal design. Copilot puts the most advanced AI models at your fingertips. In the coming weeks, get access to the latest models including GPT-4o from our partners at OpenAI, so you can have voice conversations that feel more natural. Advancing AI responsibly At Microsoft, we have a company-wide commitment to develop ethical, safe and secure AI. Our responsible AI principles guided the development of these new experiences, and all AI features are aligned with our standards. Learn more here. New Copilot+ PCs from Microsoft Surface and our partners We have worked with each of the top OEMs — Acer, ASUS, Dell, HP, Lenovo, Samsung — and of course Surface, to bring exciting new Copilot+ PCs that will begin to launch on June 18. Starting at $999, these devices are up to $200 less than similar spec'd devices[9]. Surface plays a key role in the Windows ecosystem, as we design software and hardware together to deliver innovative designs and meaningful experiences to our customers and fans. We are introducing the first-ever Copilot+ PCs from Surface: The all-new Surface Pro and Surface Laptop.

Surface Pro and Surface Laptop The new Surface Laptop is a powerhouse in an updated, modern laptop design with razor-thin bezels, a brilliant touchscreen display, AI-enhanced camera, premium audio, and now with a haptic touchpad. Choose between a 13.8" and 15" display and four stunning colors. Enjoy up to 22 hours of local video playback on Surface Laptop 15" or up to 20 hours on Surface Laptop13.8" on top of incredible performance and all-new AI experiences. The new Surface Pro is the most flexible 2-in-1 laptop, now reimagined with more speed and battery life to power all-new AI experiences. It introduces a new, optional OLED with HDR display, and ultrawide field of view camera perfect for Windows Studio Effects. The new Surface Pro Flex Keyboard is the first 2-in-1 keyboard designed to be used both attached or detached. It delivers enhanced stability, with Surface Slim Pen storage and charging integrated seamlessly, as well as a quiet, haptic touchpad. Learn more here. New Copilot+ PCs from the biggest brands available starting June 18:

Acer: Acer's Swift 14 AI 2.5K touchscreen enables you to draw and edit your vision with greater accuracy and with color-accurate imagery. Launch and discover AI-enhanced features, like Acer PurifiedVoice 2.0 and Purified View, with a touch of the dedicated AcerSense button. ASUS: The ASUS Vivobook S 15 is a powerful device that brings AI experiences to life with its Snapdragon X Elite Platform and built-in Qualcomm® AI. It boasts 40+ NPU TOPS, a dual-fan cooling system, and up to 1 TB of storage. Next-gen AI enhancements include Windows Studio effects v2 and ASUS AiSense camera, with presence-detection capabilities for Adaptive Dimming and Lock. Built for portability, it has an ultra-slim and light all-metal design, a high-capacity battery, and premium styling with a single-zone RGB backlit keyboard. Dell: Dell is launching five new Copilot+ PCs, including the XPS 13, Inspiron 14 Plus, Inspiron 14, Latitude 7455, and Latitude 5455, offering a range of consumer and commercial options that deliver groundbreaking battery life and unique AI experiences. The XPS 13 is powered by Snapdragon X Elite processors and features a premium, futuristic design, while the Latitude 7455 boasts a stunning QHD+ display and quad speakers with AI noise reduction. The Inspiron 14 and Inspiron 14 Plus feature a Snapdragon X Plus and are crafted with lightweight, low carbon aluminum and are energy efficient with EPEAT Gold rating. HP: HP's OmniBook X AI PC and HP EliteBook Ultra G1q AI PC with Snapdragon X Elite are slim and sleek designs, delivering advanced performance and mobility for a more personalized computing experience. Features include long-lasting battery life and AI-powered productivity tools, such as real-time transcription and meeting summaries. A 5MP camera with automatic framing and eye focus is supported by Poly Studio's crystal-clear audio for enhanced virtual interactions. Lenovo: Lenovo is launching two AI PCs: one built for consumers, Yoga Slim 7x, and one for commercial, ThinkPad T14s Gen 6. The Yoga Slim 7x brings efficiency for creatives, featuring a 14.5" touchscreen with 3K Dolby Vision and optimized power for 3D rendering and video editing. The T14s Gen 6 brings enterprise-level experiences and AI performance to your work tasks, with features including a webcam privacy shutter, Wi-Fi 7 connectivity and up to 64GB RAM. Samsung: Samsung's new Galaxy Book4 Edge is ultra-thin and light, with a 3K resolution, Dynamic AMOLED 2X display and Wi-Fi 7 connectivity. It has a long-lasting battery that provides up to 22 hours of video playback, making it perfect for work or entertainment on the go.

Learn more about new Copilot+ PCs and pre-order today at Microsoft.com and from major PC manufacturers, as well as other leading global retailers. Start testing for commercial deployment today Copilot+ PCs offer businesses the most performant Windows 11 devices with unique AI capabilities to unlock productivity, improve collaboration and drive efficiency. As a Windows PC, businesses can deploy and manage a Copilot+ PC with the same tools and processes used today including IT controls for new features and AppAssure support. We recommend IT admins begin testing and readying for deployment to start empowering your workforce with access to powerful AI features on these high-performance devices. You can read more about our commercial experiences here. Neural Processing Units (NPU) AI innovation across the Windows ecosystem Like we've always done with Windows, we have built a platform for our ecosystem partners to build on. The first Copilot+ PCs will launch with both the Snapdragon® X Elite and Snapdragon® X Plus processors and feature leading performance per watt thanks to the custom Qualcomm Oryon™ CPU, which delivers unrivaled performance and battery efficiency. Snapdragon X Series delivers 45 NPU TOPS all-in-one system on a chip (SoC). The premium integrated Qualcomm® Adreno™ GPU delivers stunning graphics for immersive entertainment. We look forward to expanding through deep partnerships with Intel and AMD, starting with Lunar Lake and Strix Point. We will bring new Copilot+ PC experiences at a later date. In the future we expect to see devices with this silicon paired with powerful graphics cards like NVIDIA GeForce RTX and AMD Radeon™, bringing Copilot+ PC experiences to reach even broader audiences like advanced gamers and creators. We are at an inflection point where the PC will accelerate AI innovation. We believe the richest AI experiences will only be possible when the cloud and device work together in concert. Together with our partners, we're setting the frame for the next decade of Windows innovation. Editor's note: This blog has been updated to note that Recall is launching in preview on June 18. Editor's note – June 17, 2024: Information about the Recall feature was updated.

[1] Based on snapshot of aggregated, non-gaming app usage data as of April 2024 for iGPU-based laptops and 2-in-1 devices running Windows 10 and Windows 11 in US, UK, CA, FR, AU, DE, JP. [2] Tested April 2024 using Phi SLM workload running 512-token prompt processing in a loop with default settings comparing pre-release Copilot+ PC builds with Snapdragon Elite X 12 Core and Snapdragon X Plus 10 core configurations (QNN build) to Windows 11 PC with NVIDIA 4080 GPU configuration (CUDA build). [3] Tested May 2024 using Cinebench 2024 Multi-Core benchmark comparing Copilot+ PCs with Snapdragon X Elite 12 core and Snapdragon X Plus 10 core configurations to MacBook Air 15" with M3 8 core CPU / 10 Core GPU configuration. Performance will vary significantly between device configuration and usage. [4] *Battery life varies significantly by device and with settings, usage and other factors. See aka.ms/cpclaims* [5] *Battery life varies significantly based on device configuration, usage, network and feature configuration, signal strength, settings and other factors. Testing conducted May 2024 using the prelease Windows ADK full screen local video playback assessment under standard testing conditions, with the device connected to Wi-Fi and screen brightness set to 150 nits, comparing Copilot+ PCs with Snapdragon X Elite 12 core and Snapdragon X Plus 10 core configurations running Windows Version 26097.5003 (24H2) to MacBook Air 15" M3 8-Core CPU/ 10 Core GPU running macOS 14.4 with similar device configurations and testing scenario. [6] Based on snapshot of aggregated, non-gaming app usage data as of April 2024 for iGPU-based laptops and 2-in-1 devices running Windows 10 and Windows 11 in US, UK, CA, FR, AU, DE, JP. [7] Recall is optimized for select languages (English, Chinese (simplified), French, German, Japanese, and Spanish.) Content-based and storage limitations apply. Learn more here. [8] Copilot key functionality may vary. See aka.ms/keysupport [9] Based on MSRPs; actual savings may vary

One year ago, generative AI burst onto the scene and for the first time since the smartphone, people began to change the way they interact with technology. People are bringing AI to work at an unexpected scale — and now the big question is, how's it going? As AI becomes ubiquitous in the workplace, employees and businesses alike are under extreme pressure. The pace and intensity of work, which accelerated during the pandemic, has not eased, so employees are bringing their own AI to work. Leaders agree AI is a business imperative — and feel the pressure to show immediate ROI — but many lack a plan and vision to go from individual impact to applying AI to drive the bottom line. At the same time, the labor market is set to shift and there's a new AI economy. While some professionals worry AI will replace their job, the data tells a more nuanced story — of a hidden talent shortage, more employees eyeing a career change, and a massive opportunity for those willing to skill up. "AI is democratizing expertise across the workforce," said Satya Nadella, Chairman and Chief Executive Officer, Microsoft. "Our latest research highlights the opportunity for every organization to apply this technology to drive better decision-making, collaboration — and ultimately business outcomes." For our fourth annual Work Trend Index, out today, we partnered with LinkedIn for the first time on a joint report so we could provide a comprehensive view of how AI is not only reshaping work, but the labor market more broadly. We surveyed 31,000 people across 31 countries, identified labor and hiring trends from LinkedIn, analyzed trillions of Microsoft 365 productivity signals and conducted research with Fortune 500 customers. The data points to insights every leader and professional needs to know — and actions they can take — when it comes to AI's implications for work. 1. Employees want AI at work — and won't wait for companies to catch up. Three in four knowledge workers (75%) now use AI at work. Employees, overwhelmed and under duress, say AI saves time, boosts creativity and allows them to focus on their most important work. While 79% of leaders agree AI adoption is critical to remain competitive, 59% worry about quantifying the productivity gains of AI and 60% worry their company lacks a vision and plan to implement it. While leaders feel the pressure to turn individual productivity gains into organizational impact, employees aren't waiting to reap the benefits: 78% of AI users are bringing their own AI tools to work. The opportunity for every leader is to channel this momentum into ROI. 2. For employees, AI raises the bar and breaks the career ceiling. We also see AI beginning to impact the job market. While AI and job loss are top of mind for some, our data shows more people are eyeing a career change, there are jobs available, and employees with AI skills will get first pick. The majority of leaders (55%) say they're worried about having enough talent to fill open roles this year, with leaders in cybersecurity, engineering, and creative design feeling the pinch most. And professionals are looking. Forty-six percent across the globe are considering quitting in the year ahead — an all-time high since the Great Reshuffle of 2021 — a separate LinkedIn study found U.S. numbers to be even higher with 85% eyeing career moves. While two-thirds of leaders wouldn't hire someone without AI skills, only 39% of users have received AI training from their company. So, professionals are skilling up on their own. As of late last year, we've seen a 142x increase in LinkedIn members adding AI skills like Copilot and ChatGPT to their profiles and a 160% increase in non-technical professionals using LinkedIn Learning courses to build their AI aptitude. In a world where AI mentions in LinkedIn job posts drive a 17% bump in application growth, it's a two-way street: Organizations that empower employees with AI tools and training will attract the best talent, and professionals who skill up will have the edge. 3. The rise of the AI power user — and what they reveal about the future. In the research, four types of AI users emerged on a spectrum — from skeptics who rarely use AI to power users who use it extensively. Compared to skeptics, AI power users have reoriented their workdays in fundamental ways, reimagining business processes and saving over 30 minutes per day. Over 90% of power users say AI makes their overwhelming workload more manageable and their work more enjoyable, but they aren't doing it on their own. Power users work for a different kind of company. They are 61% more likely to have heard from their CEO on the importance of using generative AI at work, 53% more likely to receive encouragement from leadership to consider how AI can transform their function and 35% more likely to receive tailored AI training for their specific role or function. "AI is redefining work and it's clear we need new playbooks," said Ryan Roslansky, CEO of LinkedIn. "It's the leaders who build for agility instead of stability and invest in skill building internally that will give their organizations a competitive advantage and create more efficient, engaged and equitable teams." The prompt box is the new blank page We hear one consistent piece of feedback from our customers: talking to AI is harder than it seems. We've all learned how to use a search engine, identifying the right few words to get the best results. AI requires more context — just like when you delegate work to a direct report or colleague. But for many, staring down that empty prompt box feels like facing a blank page: Where should I even start? Today, we're announcing Copilot for Microsoft 365 innovations to help our customers answer that question.

If you've got the start of a prompt, Copilot will offer to auto-complete it to get to a better result, suggesting something more detailed to help ensure you get what you're looking for. That not only speeds things up, it offers you new ideas for how to leverage Copilot's power.

Other times, you know exactly what you want — you're just not sure how to ask. With its new rewrite feature, Copilot turns a basic prompt into a rich one with the click of a button, turning everyone into a prompt engineer.

Catch Up, a new chat interface that surfaces personal insights based on your recent activity, provides responsive recommendations, like "You have a meeting with the sales VP on Thursday. Let's get you prepared — click here to get detailed notes." We also know that every role, team and function has unique needs and ways of working. To help create prompts for exactly the work you do, you'll soon be able to create, publish and manage prompts in Copilot Lab that are expressly tailored to your closest teams.

These features will be available in the coming months, and in the future, we'll take it a step further, with Copilot asking you questions to get to your best work yet. LinkedIn has also made free over 50 learning courses to empower professionals at all levels to advance their AI aptitude. Head to WorkLab for the full Work Trend Index Report, and head to LinkedIn to hear more from LinkedIn's Chief Economist, Karin Kimbrough, on how AI is reshaping the labor market. And for all the blogs, videos and assets related to today's announcements, please visit our microsite. Tags: AI, LinkedIn, Microsoft Copilot, Work Trend Index

Satya Nadella, Chairman and CEO, shared the below communication with Microsoft employees. In light of the significant attention and discussion this announcement has garnered, it has been made publicly available as an official record.

Today, I want to talk about something critical to our company's future: prioritizing security above all else.

Microsoft runs on trust, and our success depends on earning and maintaining it. We have a unique opportunity and responsibility to build the most secure and trusted platform that the world innovates upon.

The recent findings by the Department of Homeland Security's Cyber Safety Review Board (CSRB) regarding the Storm-0558 cyberattack, from summer 2023, underscore the severity of the threats facing our company and our customers, as well as our responsibility to defend against these increasingly sophisticated threat actors.

Last November, we launched our Secure Future Initiative (SFI) with this responsibility in mind, bringing together every part of the company to advance cybersecurity protection across both new products and legacy infrastructure. I'm proud of this initiative, and grateful for the work that has gone into implementing it. But we must and will do more.

Going forward, we will commit the entirety of our organization to SFI, as we double down on this initiative with an approach grounded in three core principles:

Secure by Design: Security comes first when designing any product or service. Secure by Default: Security protections are enabled and enforced by default, require no extra effort, and are not optional. Secure Operations: Security controls and monitoring will continuously be improved to meet current and future threats.

These principles will govern every facet of our SFI pillars as we: Protect Identities and Secrets, Protect Tenants and Isolate Production Systems, Protect Networks, Protect Engineering Systems, Monitor and Detect Threats, and Accelerate Response and Remediation. We've shared specific, company-wide actions each of these pillars will entail — including those recommended in the CSRB's report — which you can learn about here. Across Microsoft, we will mobilize to implement and operationalize these standards, guidelines, and requirements and this will be an added dimension of our hiring and rewards decisions. In addition, we will instill accountability by basing part of the compensation of the senior leadership team on our progress towards meeting our security plans and milestones.

We must approach this challenge with both technical and operational rigor, and with a focus on continuous improvement. Every task we take on – from a line of code, to a customer or partner process – is an opportunity to help bolster our own security and that of our entire ecosystem. This includes learning from our adversaries and the increasing sophistication of their capabilities, as we did with Midnight Blizzard. And learning from the trillions of unique signals we're constantly monitoring to strengthen our overall posture. It also includes stronger, more structured collaboration across the public and private sector.

Security is a team sport, and accelerating SFI isn't just job number one for our security teams – it's everyone's top priority and our customers' greatest need.

If you're faced with the tradeoff between security and another priority, your answer is clear: Do security. In some cases, this will mean prioritizing security above other things we do, such as releasing new features or providing ongoing support for legacy systems. This is key to advancing both our platform quality and capability such that we can protect the digital estates of our customers and build a safer world for all.

A Copilot on every desk, every device and across every role is core to Microsoft's mission to empower every person and every organization on the planet to achieve more. By bolstering the services customers know and love with our Copilot capabilities across the Microsoft Cloud, we are enabling zero-shot innovation — the ability to effectively gain value out of the box — for their businesses to increase productivity, creativity and inclusive collaboration. ISVs, digital natives, startups and the rest of our partner ecosystem are leveraging our Copilot stack to build AI solutions that are reshaping business processes across industries. We continue co-innovating directly with customers on our open cloud platform to bend the curve on innovation by identifying AI design patterns; then integrating and applying data to solve their most pressing challenges faster and more efficiently than ever before. Underpinning it all is the need for a strong cybersecurity foundation, and the work we are doing to ensure this critical imperative is met. No one is immune from bad actors, and we remain committed to transparency and trust in our approach to protecting customer and partner data. By empowering organizations with industry-leading cloud and responsible AI solutions, paired with our focus on security, we are helping them unlock opportunities that deliver pragmatic business outcomes. I am proud of the work we have done this past quarter that exemplifies our approach to enabling AI Transformation, and the success of our customers and partners across industries who have embraced it.

Just this week we announced our strategic partnership with The Coca-Cola Company to accelerate AI Transformation enterprise-wide as well as across its global network of independent bottlers, and shared news of our partnership with Cognizant to drive enterprise AI adoption for millions of users. We are expanding our work with G42 to accelerate responsible AI innovation in the United Arab Emirates and beyond while accelerating digital transformation securely across the Middle East, Central Asia and Africa with expanded access to services and technologies. We are also building on our relationship with Cloud Software Group to bring joint cloud solutions and generative AI capabilities to more than 100 million people.

We continue to shape the future of industry alongside our customers to help them differentiate their businesses. At CES, we showcased how our customers and partners are innovating across the automotive and mobility industry with generative AI to solve deep business problems and create new opportunities, and revealed the Copilot key for Windows 11 PCs. At NRF, we shared new copilot templates to help retailers incorporate generative AI across the shopper journey while enhancing the experience for store associates and making AI implementation more accessible. Just recently at HIMSS, we announced Microsoft's role as the technology enabling partner for the Trustworthy & Responsible AI Network, a consortium of healthcare leaders aimed at operationalizing responsible AI principles to improve the quality, safety and trustworthiness of AI in healthcare.

Air India harnesses AI for operational excellence with Copilot for Microsoft 365. Enabling pragmatic AI innovation with Copilot capabilities to deliver immediate value

Companies like Amadeus, AAMI, TotalEnergies and Cushman & Wakefield are leveraging Copilot for Microsoft 365 to enrich customer interactions and advance high-priority projects. Visa employees are transforming how they work to better serve the needs of their clients and Banca Transilvania is boosting efficiency and innovation while delivering the highest quality of customer service. Emirates NBD engineers are solving their most complex problems while remaining within their development environments, and 96% of early adopters at CommBank have shared that Copilot is making them more productive. Colombia-based Cenit is experiencing rapid adoption of Copilot to empower employees to be more creative, productive and collaborative. U.K.-based law firm Clifford Chance is helping employees automate daily tasks like meeting notes, email drafts and inbox management with AI and natural language processing, while Air India built a plugin to gain real-time access to flight and operations data to empower better decision-making.

With Copilot in Dynamics 365 Customer Insights, Northrop & Johnson achieved a 250% increase in charter bookings while helping team members save time, boost productivity and enrich the quality of engagement with their customers. Using Power Platform and Microsoft Copilot Studio, Cineplex is saving employees over 30,000 hours annually in manual processing times and reducing ticket refund handling time from 15 minutes to under a minute. Northern Trust is using Microsoft Copilot for Service to modernize its client relations organization, streamline employee processes and elevate the client experience. Teams at EPAM Systems are leveraging Microsoft Copilot for Sales to improve sales processes and better support its business strategies, while freeing up time to spend with customers.

Blue Yonder optimizes supply chain orchestration with Azure OpenAI Service. Building AI design patterns that bend the curve on innovation and intelligently reason over data to solve business challenges

Icertis is leveraging Azure OpenAI Service and its proprietary AI models in conjunction with its extensive data lake to uncover cost savings, enhance compliance and reduce risk across millions of legal contracts. Insurance company FWD Group is using the service to enhance customer experiences and operations across its lines of business, and Singapore-based CapitaLand Investment has saved 1 million Singapore dollars and 10,000 work hours per year with data-driven AI models. With Azure OpenAI Service as the secure foundation for its supply chain platform, Blue Yonder is harnessing AI and machine learning to provide real-time decision making for businesses across 78 countries. Homes & Villas by Marriott Bonvoy and Publicis Sapient are making it easier for travelers to find the right vacation homes through natural language search powered by large language models within Azure OpenAI Service, and Miral is curating some of the most sought-after leisure, entertainment and tourism activities with a 24/7 AI-powered concierge. Using several AI models, Australian retailer Coles developed an edge computing platform that makes 1.6 billion informed predictions each day so customers can find exactly what they are looking for across its 850 stores.

With Microsoft Fabric, Melbourne Airport is synchronizing flight bookings and ground transportation data to project demand while maintaining efficiency, reliability and safety of its operations. Rail freight operator Aurizon is deriving better data insights from 400 sensor-equipped locomotives to enhance cost efficiency, scalability and predictive maintenance. Seair Exim Solutions is increasing export-import trade data ingestion speeds by 90% to empower its global shipping industry customers with improved insights more quickly with help from Mandelbulb Technologies, while Dener Motorsport is using Fabric and real-time analytics to help automotive engineers detect and resolve car issues within minutes — down from nearly half an hour. By hosting its data within Azure Kubernetes, Windstream is improving its custom GPT platform to help employees find the information they need faster across 100,000 indexed documents. To promote financial inclusion, Trusting Social focused on a strong data foundation paired with AI services to enable the development and deployment of solutions that assist more than 130 financial institutions across Asia. Sasfin Bank worked with Legal Interact to implement a unified document management system to analyze contract clauses and streamline legal operations and workflows using Azure Cognitive Services and Azure SQL. TomTom has deployed GitHub Copilot to its developers, resulting in 85% of the company's engineers feeling more productive and 70% saying it enables them to focus on more satisfying work. Meesho — a leading online marketplace in India — is using a generative AI chat agent to increase inquiry resolution by 40% and customer satisfaction by 25%, while leveraging GitHub Copilot to streamline code delivery and testing for its 200+ developers.

Grupo Bimbo bakes in end-to-end data security and compliance with Microsoft Purview. Strengthening cybersecurity foundations with industry-leading cloud and responsible AI solutions

Microsoft's Secure Future Initiative (SFI) reflects our commitment to continually advancing the built-in security of our products and services. We recently announced the general availability of Microsoft Copilot for Security — the industry's first generative AI solution designed to help security and IT professionals do their work with greater speed and accuracy. In our latest Copilot for Security economic study, security analysts using the Copilot were 22% faster and 7% more accurate across all tasks, with 97% saying they want to use it again. Our ability to offer this solution to customers is bolstered by more than 100 partners such as Capgemini, Cisco and Darktrace who are committed to working with us to deliver trusted, responsible AI solutions. In collaboration with TC1 Labs, Pacifico Seguros has become the first company in Latin America to implement Copilot for Security with a Zero Trust approach, strengthening its security defenses and confidently preparing the company for future security challenges. Security professionals at Enverus are saving hours per day on low-value tasks with Copilot, allowing them to focus on strategic work and help elevate the impact of their security team. Wipro is using the solution to help employees save time and act quickly to improve outcomes across its business.

Using the Microsoft Defender security stack, Del Monte Foods increased security visibility and coverage across its IT environment, reducing security risks by 50% and improving its security response and remediation capabilities. To streamline its security operations, Jones Lang LaSalle implemented Microsoft Defender for Endpoint with help from Tanium, reducing cybersecurity spending by 20% and enhancing protection for nearly 90,000 endpoints. Türkiye-based Demirören has adopted a Zero Trust security model by unifying its security solutions to protect essential business data and enhance threat detection and response capabilities. Paytronix, a provider of customized digital experiences for restaurant and convenience store guests, improved its security profile by seamlessly migrating data for 250 million user accounts to Azure without interruption. The Audi Group's IT department is simplifying endpoint management and safeguarding its hybrid workforce across 12 countries with enhanced security using Microsoft Intune. To ensure robust data protection and seamless access for authorized personnel, SLT-MOBITEL is using Microsoft's AI-powered security products to fortify end-user devices and services while simplifying information security and compliance. Oregon State University and Grupo Bimbo are utilizing Microsoft data security solutions to safeguard their environments and confidently prepare for their continued adoption and deployment of AI.

Embracing AI Transformation has become a priority for organizations around the world seeking to unlock AI opportunities. Whether helping our customers adopt and scale AI with our Copilot capabilities or building unique AI strategies leveraging our open platform and partner ecosystem, we are humbled to work with you to drive pragmatic business outcomes and differentiated industry leadership. Our commitment to your success is unwavering. We will continue building upon our trusted relationships and our focus on responsibility and security so you can move forward with confidence in your AI journey. I remain inspired by what we have done — and what we will do — to help our customers and partners achieve more.

Tags: AI, Azure OpenAI Service, Copilot for Dynamics 365, Copilot for Microsoft 365, Microsoft Cloud, Microsoft Copilot for Sales, Microsoft Copilot for Security, Microsoft Copilot for Service, Microsoft Copilot Studio, Microsoft Defender, Microsoft Fabric, Microsoft Partners

After years of uncertainty from supply chain disruption and increased customer expectations, to changes in consumer demand and workforce shortages, manufacturing remains one of the most resilient and complex industries. Today, we are witnessing the manufacturing industry enter a transformative era, fueled by AI and new AI-powered industrial solutions. This AI-driven shift is prompting many organizations to fundamentally alter their business models and re-evaluate how to address industry-wide challenges like data siloes from disparate data estates and legacy products, supply chain visibility issues, labor shortages, and the need for upskilling employees, among others. AI is more than just an automation tool, it's a catalyst for innovation, efficiency and sustainability. AI innovation creates an opportunity to help manufacturers enhance time-to-value, bolster operations resilience, optimize factory and production costs and produce repeatable outcomes.

Ahead of Hannover Messe, one of the world's largest manufacturing innovation events, Microsoft is announcing new AI and data solutions for manufacturers to help unlock innovation, enable intelligent factories, optimize operations and enhance employee productivity. The manufacturing industry has been incredibly resilient over the last decade and the infusion of new AI solutions signifies a critical transformation in this vital industry.

Unlock innovation and fuel the next generation of intelligent factories with data and AI

Manufacturing is one of the most data-intensive industries, generating an average of 1.9 petabytes worldwide every year, according to McKinsey Global Institute. And most of this data goes unused, leaving many valuable insights untapped. According to Gartner® Research, "Generative AI will transform the manufacturing industry to a level previously not available, by providing new insights and recommendations based on data and actionable information."[1] In this era of AI, the importance of data continues to grow as organizations realize they are only scratching the surface of what's possible.

To help customers leverage their data and insights, today, we are announcing the private preview of manufacturing data solutions in Microsoft Fabric, and copilot template for factory operations on Azure AI. These solutions help manufacturers unify their operational technology and information technology (IT) data estate and accelerate and scale data transformation for AI in Fabric, our end-to-end analytics SaaS-based platform. Copilot template for factory operations helps manufacturers to create their own copilots for front-line workers utilizing the unified data. Front-line employees can use natural language to query the data for knowledge discovery, training, issue resolution, asset maintenance and more. For example, if a factory plant manager wants to understand why a machine is breaking, they can query the copilot to get insights and resolve the issue in just days, instead of weeks.

As part of our private preview, Intertape Polymer Group (IPG) uses Sight Machine's Manufacturing Data Platform to continuously transform data generated by its factory equipment into a robust data foundation for analyzing and modeling its machines, production processes and finished products. IPG is now using Sight Machine's Factory CoPilot, a generative AI platform with an intuitive natural language chat interface, powered by the Microsoft Cloud for Manufacturing and the copilot template for factory operations on Azure AI. This tool facilitates the team's ability to rapidly gather insights and direct work on production lines which previously operated like black boxes. Instead of working through manual spreadsheets and inaccessible data, all teammates including production, engineering, procurement and finance have better information to drive decisions on products and processes throughout the plant improving yield and reducing inventory levels.

Also in private preview, Bridgestone is partnering with Avanade to confront production challenges head-on, focusing on critical issues related to production disruptions and scheduling inefficiencies, like yield loss, which can escalate into quality issues. As a private preview customer collaborating with Avanade, Bridgestone aims to harness the power of manufacturing data solutions in Fabric and the copilot template for factory operations. Their goal is to implement a natural language query system that enables front-line workers, with different levels of experience, with insights that lead to faster issue resolution. The team is excited to establish a centralized system that efficiently gathers and presents critical information from various sources and facilitates informed decision-making and enhances operational agility across Bridgestone's production ecosystem.

A robust data strategy must span from cloud to the shop floor to enable the level of scale and integration that will help manufacturers accelerate industrial transformation across all operations. However, gathering OT data and integrating the data into multiple solutions is not an easy task for manufacturers. Production is complex, and their sensors, machines and systems are highly varied. Each site is unique and ensuring the right data is being shared with the right person at the right time is onerous and costly. Unfortunately, these scale and integration hurdles also block the enterprise from scaling AI solutions across every shop floor or gaining global visibility across all their sites.

With this in mind, Microsoft recently launched the adaptive cloud approach, including Azure IoT Operations. Our adaptive cloud is a framework to modernize edge infrastructure across operations, like factories, to take advantage of a modern, composable and connected architecture for your applications. Our adaptive cloud approach creates the level of scale needed to repeat AI solutions across production lines and sites. Putting the adaptive cloud approach into practice, Azure IoT Operations leverages open standards and works with Microsoft Fabric to create a common data foundation for IT and OT collaboration. To find out more about our adaptive cloud approach and Azure IoT Operations, visit our Azure Blog.

Looking to increase global operational efficiency, Microsoft's customer Electrolux Group, developed a single platform to build, deploy and manage several key manufacturing use cases. Their platform's goal is to capture all manufacturing data, contextualize it and make it available for real time decision-making across all levels of the organization within a scalable infrastructure. To enable this, Electrolux Group is adopting a full stack solution from Microsoft that leverages the adaptive cloud approach, including Azure IoT Operations. Using this approach, Electrolux Group is looking to reduce overhead from multiple vendors, a consistent and simple way to deploy and manage multiple use cases at a site, and then the ability to scale those solutions to multiple sites with simple and consistent fleet management.

Supply chain disruption is not new; however, its complexity and the rate of change are outpacing organizations' ability to address issues. Manufacturers are under pressure to prevent and minimize disruptions, and as a result, almost 90% of supply chain professionals plan to invest in ways to make their supply chains more resilient. To support our customers, we're announcing the upcoming preview of a traceability add-in for Dynamics 365 Supply Chain Management that will allow businesses to increase visibility into their product genealogy through the different steps of the supply chain. Traceability will also help businesses track events and attributes throughout supply chain processes and will provide an interface to query and analyze data.

Empowering front-line workers with AI tools to improve productivity and job satisfaction

To enable intelligent factory operations, an empowered and connected workforce is key. According to the latest Work Trend Index, 63% of front-line workers do repetitive or menial tasks that take time away from more meaningful work. Additionally, 80% of front-line workers think AI will augment their ability to find the right information and the answers they need. From the office to the factory floor to the field, we are building solutions to address the unique challenges manufacturers face — by helping streamline front-line operations, enhance communication and collaboration, improve employee experience and strengthen security across shared devices.

Today we're introducing new capabilities for Copilot in Dynamics 365 Field Service that help service managers and technicians efficiently find information, resolve issues while keeping customers updated at every step, and help summarize their work. Generally available, field service managers can interact with Copilot to find pertinent information about work orders using natural language in their flow of work in the Dynamics 365 Field Service web app. Additionally, available in public preview, front-line workers can configure and customize the fields Copilot uses to generate summaries within Dynamics 365 Field Service.

To further streamline collaboration among field service managers, technicians, and remote experts, Dynamics 365 Field Service users with the Field Service app in Teams can now share links to work orders that automatically expand to provide key details. This capability is generally available starting today. Should technicians need additional assistance from remote experts to resolve issues, they can simply access Dynamics 365 Remote Assist capabilities in the flow of work in Microsoft Teams with anchored spatial annotations even if the camera moves.

These new industry innovations in data and AI are strengthened through the Microsoft Cloud for Manufacturing, which enables organizations to accelerate their data and AI journey by augmenting the Microsoft Cloud with industry-relevant data solutions, application templates and AI services. The Microsoft Cloud for Manufacturing brings the best of Microsoft and our partners to jointly accelerate the digital transformation in manufacturing.

Microsoft is a trusted co-innovation partner committed to working with enterprises to unlock the true potential of AI solutions and transform the industry.​ Our offerings can also be customized by an unmatched global ecosystem of trusted partners. This year, we're proud to have the following valued partners demonstrate at our Hannover Messe booth: Accenture, Annata, Ansys, Avanade, AVEVA, Blue Yonder, Bosch, CapGemini, Cognite, Connected Cars DK, DSA, HERE Technologies, Hexagon, Netstar, NVIDIA, o9 Solutions, PTC, Rockwell Automation, SAP, Syntax, Sight Machine, Siemens, SymphonyAI, Tata Consultancy Services (TCS), Threedy, ToolsGroup and Tulip Interfaces.

We look forward to seeing you at the Microsoft Booth in Hall 17 Stand G06, where you can join guided tours, and speak with manufacturing and industrial experts from around the world.

[1] Gartner®, GenAI use-case prism for manufacturing, By Ellen Eichhorn, Sohard Aggarwal, July 2023. GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved.

Strategic partnership highlights: Expansion of partnership between Microsoft and G42 to deliver advanced AI solutions with Microsoft Azure across various industries and markets. Microsoft will invest $1.5 billion in G42 for a minority stake in G42 and join its board of directors. Companies will support the establishment of a $1 billion fund for developers to boost AI skills in the United Arab Emirates (UAE) and broader region. Expanded strategic partnership:

Today, we announced a strategic investment in G42, a leading AI company in the UAE, to co-innovate and deliver advanced AI solutions with Microsoft Azure for various industries and markets across the Middle East, Central Asia and Africa.

Microsoft will invest $1.5 billion in G42 for a minority stake in the company with Brad Smith, Microsoft Vice Chair and President, joining G42's board of directors — strengthening the long-standing collaboration and mutual synergies between the two companies. With the breadth of the Microsoft Cloud and its differentiated AI capabilities, the deal significantly advances G42's strategy of delivering generative AI and next-generation infrastructure and services for a range of customers across financial services, healthcare, energy, government and education.

The commercial partnership is backed by assurances to the U.S. and UAE governments through a first-of- its-kind binding agreement to apply world-class best practices to ensure the secure, trusted, and responsible development and deployment of AI. Microsoft and G42 will work closely together to elevate the security and compliance framework of their joint international infrastructure. Both companies will move forward with a commitment to comply with U.S. and international trade, security, responsible AI, and business integrity laws and regulations. The work on these topics is governed by a detailed Intergovernmental Assurance Agreement between G42 and Microsoft that was developed in close consultation with both the UAE and U.S. governments.

Foundational to the partnership is G42's trust and commitment in Microsoft's cloud platform. G42 will expand its existing commitment to deploying Microsoft Cloud offerings, demonstrating confidence in Microsoft as its preferred partner to enhance services and deliver value-added solutions to its customers. With the partnership, G42's data platform and other essential technology infrastructure will migrate to Microsoft Azure to benefit from industry-leading performance, scalability and security capabilities. Migrating to Azure will also support AI product development that allows G42 to create services that can scale to achieve faster delivery times for its customers globally. Together, we look forward to accelerating AI transformation in emerging markets and advancing equitable growth in AI globally.

G42 brings an excellent track record as a leader actively driving global progress and accessibility in AI technologies, and Microsoft and G42 have worked closely together to help optimize Cloud and AI solutions for the Middle East.

Last year, G42 was one of the first partners to commit to implementing Microsoft Cloud for Sovereignty offering to UAE-based organizations. G42 is helping public sector and regulated industries to use new platform capabilities for securing sensitive data, providing access to the latest cloud and AI features available on Azure public cloud, and ensuring they comply with local privacy and regulatory requirements. G42's deep understanding of UAE sovereignty requirements and technical capabilities are central to customizing Microsoft Cloud for Sovereignty to help address customer's specific needs.

Microsoft also announced that Jais, G42's Arabic Large Language Model (LLM), will be available in the Azure AI Model Catalog. This model represents a significant advancement for the Arabic world in AI, offering over 400 million Arabic speakers the opportunity to explore the potential of generative AI. Jais is the world's first Arabic LLM developed by G42 in collaboration with Cerebras, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), and Med42 LLM, a generative AI model to streamline medical reporting. The expanded partnership with Microsoft will help accelerate the adoption of G42's groundbreaking AI products and services, such as Jais, making them available through Microsoft Azure.

Announced in March of this year, First Abu Dhabi Bank (FAB), the UAE's largest bank, will collaborate with Core42, a subsidiary of G42, to accelerate its digital transformation journey leveraging Microsoft Azure trusted cloud platform for enterprises. FAB will move its datacenter and workload to Azure, enabling the bank to use Core42's sovereign controls platform, which is built on Azure and ensures the highest standards of data sovereignty and compliance with UAE regulations.

One of the leading examples of precision medicine in action is the collaboration between G42 subsidiary M42, a global health care company, the Broad Institute of MIT and Harvard, Microsoft, and the International Center for Genetic Disease (ICGD). The partners are using Terra, a scalable and secure platform for biomedical research, to enable data sharing and analysis across different institutions and countries. Terra, powered by Microsoft Azure, allows researchers to access and analyze anonymized genomic data from the Emirati Genome Program, which has completed over 500,000 whole genome sequences to date. By applying AI technologies to this rich data source, the collaborators aim to advance clinical genomic research and disease prevention, as well as support precision medicine and life science strategies globally.

Accelerating access to digital innovation in UAE and the region

Along with providing advanced AI capabilities, the partnership will benefit regions beyond the UAE in ways that will improve how enterprises experience cloud computing. By bringing expanded low latency datacenter infrastructure to emerging markets, Microsoft and G42 will help accelerate digital transformation across key industries in those regions. This will provide countries across the Middle East, Central Asia and Africa with expanded access to services and technologies that will allow them to address the most challenging business concerns while ensuring the highest standards of security and privacy.

Furthermore, the partnership will also support the development of a skilled and diverse AI workforce and talent pool that will drive innovation and competitiveness for the UAE and broader region with the investment of $1 billion in a fund for developers.

Microsoft recently announced the creation of Microsoft AI, a newly formed organization to help advance our consumer AI products and research, including Copilot. Building on that news, I'm thrilled to share that Microsoft AI is opening a new AI hub in the heart of London. Microsoft AI London will drive pioneering work to advance state-of-the-art language models and their supporting infrastructure, and to create world-class tooling for foundation models, collaborating closely with our AI teams across Microsoft and with our partners, including OpenAI.

The new AI hub will be led by Jordan Hoffmann, an exceptional AI scientist and engineer. Prior to joining Microsoft AI, Hoffmann distinguished himself as an AI pioneer at Inflection and DeepMind, based in London. Hoffmann will be joined by a talented group of Microsoft AI team members based in our London Paddington office.

There is an enormous pool of AI talent and expertise in the U.K., and Microsoft AI plans to make a significant, long-term investment in the region as we begin hiring the best AI scientists and engineers into this new AI hub.

In the coming weeks and months, we will be posting job openings and actively hiring exceptional individuals who want to work on the most interesting and challenging AI questions of our time. We're looking for new team members who are driven by impact at scale, and who are passionate innovators eager to contribute to a team culture where continuous learning is the norm.

This is great news for Microsoft AI and for the U.K. As a British citizen, born and raised in London, I'm proud to have co-founded and built a cutting-edge AI business here. I'm deeply aware of the extraordinary talent pool and AI ecosystem in the U.K., and I'm excited to make this commitment to the U.K. on behalf of Microsoft AI. I know – through my close work with thought leaders in the U.K. government, business community and academia – that the country is committed to advancing AI responsibly and with a safety-first commitment to drive investment, innovation and economic growth. Our decision to open this hub in the U.K. reflects this ambition.

The Microsoft AI London hub adds to Microsoft's existing presence in the U.K., including the Microsoft Research Cambridge lab, home to some of the foremost researchers in the areas of AI, cloud and productivity. At the same time, it builds off Microsoft's recently announced £2.5 billion investment to upskill the U.K. workforce for the AI era and to build the infrastructure to power the AI economy, including our commitment to bring 20,000 of the most advanced GPUs to the country by 2026.

Stay tuned for more updates as we continue to push the boundaries of what's possible with AI and extend the benefits to every person and organization across the U.K.

Quantinuum scientists making adjustments to a beam line array used to deliver laser pulses in H-Series quantum computers. Photo courtesy of Quantinuum. Today signifies a major achievement for the entire quantum ecosystem: Microsoft and Quantinuum demonstrated the most reliable logical qubits on record. By applying Microsoft's breakthrough qubit-virtualization system, with error diagnostics and correction, to Quantinuum's ion-trap hardware, we ran more than 14,000 individual experiments without a single error. Furthermore, we demonstrated more reliable quantum computation by performing error diagnostics and corrections on logical qubits without destroying them. This finally moves us out of the current noisy intermediate-scale quantum (NISQ) level to Level 2 Resilient quantum computing. This is a crucial milestone on our path to building a hybrid supercomputing system that can transform research and innovation across many industries. It is made possible by the collective advancement of quantum hardware, qubit virtualization and correction, and hybrid applications that take advantage of the best of AI, supercomputing, and quantum capabilities. With a hybrid supercomputer powered by 100 reliable logical qubits, organizations would start to see scientific advantage, while scaling closer to 1,000 reliable logical qubits would unlock commercial advantage. Advanced capabilities based on these logical qubits will be available in private preview for Azure Quantum Elements customers in the coming months.

YouTube Video Click here to load media A purpose-built computing platform for science Many of the hardest problems facing society, such as reversing climate change, addressing food insecurity and solving the energy crisis, are chemistry and materials science problems. However, the number of possible stable molecules and materials may surpass the number of atoms in the observable universe. Even a billion years of classical computing would be insufficient to explore and evaluate them all. That's why the promise of quantum is so appealing. Scaled quantum computers would offer the ability to simulate the interactions of molecules and atoms at the quantum level beyond the reach of classical computers, unlocking solutions that can be a catalyst for positive change in our world. But quantum computing is just one layer for driving these breakthrough insights. Whether it's to supercharge pharma productivity or pioneer the next sustainable battery, accelerating scientific discovery requires a purpose-built, hybrid compute platform. Researchers need access to the right tool at the right stage of their discovery pipeline to efficiently solve every layer of their scientific problem and drive insights into where they matter most. This is what we built with Azure Quantum Elements, empowering organizations to transform research and development with capabilities including screening massive data sets with AI, narrowing down options with high-performance computing (HPC) or improving model accuracy with the power of scaled quantum computing in the future. We continue to advance the state-of-the-art across all these hybrid technologies for our customers, with today's quantum milestone laying the foundation for useful, reliable and scalable simulations of quantum mechanics. Moving toward resilience In an article I wrote on LinkedIn, I used a 'leaky boat' analogy to explain why fidelity and error correction are so important to quantum computing. In short, fidelity is the value we use to measure how reliably a quantum computer can produce a meaningful result. Only with good fidelity will we have a solid foundation to reliably scale a quantum machine that can solve practical, real-world problems. For years, one approach used to fix this leaky boat has been to increase the number of noisy physical qubits together with techniques to compensate for that noise but falling short of real logical qubits with superior error correction rates. The main shortcoming of most of today's NISQ machines is that the physical qubits are too noisy and error-prone to make robust quantum error correction possible. Our industry's foundational components are not good enough for quantum error correction to work, and it's why even larger NISQ systems are not practical for real-world applications. The task at hand for the entire quantum ecosystem is to increase the fidelity of qubits and enable fault-tolerant quantum computing so that we can use a quantum machine to unlock solutions to previously intractable problems. In short, we need to transition to reliable logical qubits — created by combining multiple physical qubits together into logical ones to protect against noise and sustain a long (i.e., resilient) computation. We can only obtain this with careful hardware and software co-design. By having high-quality hardware components and breakthrough error-handling capabilities designed for that machine, we can get better results than any individual component could give us. Today, we've done just that. "Breakthroughs in quantum error correction and fault tolerance are important for realizing the long-term value of quantum computing for scientific discovery and energy security. Results like these enable continued development of quantum computing systems for research and development." Dr. Travis Humble, Director, Quantum Science Center, Oak Ridge National Laboratory A breakthrough for handling quantum errors That's why today is such a historic moment: for the first time on record as an industry, we're advancing from Level 1 Foundational to Level 2 Resilient quantum computing. We're now entering the next phase for solving meaningful problems with reliable quantum computers. Our qubit-virtualization system, which filters and corrects errors, combined with Quantinuum's hardware demonstrates the largest gap between physical and logical error rates reported to date. This is the first demonstrated system with four logical qubits that improves the logical over the physical error rate by such a large order of magnitude. We've been able to demonstrate the largest gap between physical and logical error rates yet detected — far below the break-even point, now within a regime where quantum error correction is valuable and works. As importantly, we're also now able to diagnose and correct errors in the logical qubits without destroying them — referred to as "active syndrome extraction." This represents a huge step forward for the industry as it enables more reliable quantum computation. With this system, we ran more than 14,000 individual experiments without a single error. You can read more about these results here. "Quantum error correction often seems very theoretical. What's striking here is the massive contribution Microsoft's midstack software for qubit optimization is making to the improved step-down in error rates. Microsoft really is putting theory into practice." Dr. David Shaw, Chief Analyst, Global Quantum Intelligence A long-standing collaboration with Quantinuum Since 2019, Microsoft has been collaborating with Quantinuum to enable quantum developers to write and run their own quantum code on ion-trap qubit technology which includes high-fidelity, full connectivity and mid-circuit measurements. Multiple published benchmark tests recognize Quantinuum as having the best quantum volumes, making them well-positioned to enter Level 2. "Today's results mark a historic achievement and are a wonderful reflection of how this collaboration continues to push the boundaries for the quantum ecosystem. With Microsoft's state-of-the-art error correction aligned with the world's most powerful quantum computer and a fully integrated approach, we are so excited for the next evolution in quantum applications and can't wait to see how our customers and partners will benefit from our solutions especially as we move towards quantum processors at scale." Ilyas Khan, Founder and Chief Product Officer, Quantinuum Quantinuum's hardware performs at physical two-qubit fidelity of 99.8%. This fidelity enables application of our qubit-virtualization system, with diagnostics and error correction, and makes today's announcement possible. This quantum system, with co-innovation from Microsoft and Quantinuum, ushers us into Level 2 Resilient. Pioneering quantum supercomputing, together At Microsoft, our mission is to empower every individual and organization to achieve more. We've brought the world's best NISQ hardware to the cloud with our Azure Quantum platform so our customers can embark on their quantum journey. This is why we've integrated artificial intelligence with quantum computing and cloud HPC in the private preview of Azure Quantum Elements. We used this platform to design and demonstrate an end-to-end workflow that integrates Copilot, Azure compute and a quantum algorithm running on Quantinuum processors to train an AI model for property prediction. Today's announcement continues this commitment by advancing quantum hardware to Level 2. Advanced capabilities based on these logical qubits will be available in private preview for Azure Quantum Elements in the coming months. Lastly, we continue to invest heavily in progressing beyond Level 2, scaling to the level of quantum supercomputing. This is why we've been advocating for our topological approach, the feasibility of which our Azure Quantum team has demonstrated. At Level 3, we expect to be able to solve some of our most challenging problems, particularly in fields like chemistry and materials science, unlocking new applications that bring quantum at scale together with the best of classical supercomputing and AI — all connected in the Azure Quantum cloud. We are excited to empower the collective genius and make these breakthroughs accessible to our customers. For more details on how we achieved today's results, explore our technical blog, and register for the upcoming Quantum Innovator Series with Quantinuum. Tags: AI, Azure Quantum Elements, quantum computing

During the past year, the pace of AI adoption has accelerated significantly, ushering in groundbreaking advances, discoveries and solutions with the potential to help address humanity's biggest problems. We see this as a massive platform shift, akin to the printing press, which was not just an invention, but a technology that shaped a new economy. Alongside the incredible promise and benefits of AI, we recognize the resource intensity of these applications and the need to address the environmental impact from every angle.

In line with our commitment to responsible AI and our ambitious sustainability commitments, we're determined to tackle this challenge so the world can harness the full benefits of AI. There are three areas where we're deeply invested and increasing our focus. The first is optimizing datacenter energy and water efficiency. The second is advancing low-carbon materials, creating global markets to help advance sustainability across industries. And the third is improving the energy efficiency of AI and cloud services, empowering our customers and partners with tools for collective progress.

Over the past decade, our quest to innovate across every part of our cloud infrastructure to deliver more sustainable cloud services has led to many changes across how we design, build and operate our datacenters. As we continue this work, two of the biggest challenges we're addressing are energy management and water intensity.

The energy intensity of advanced cloud and AI services has driven us to accelerate our efforts to drive efficiencies and energy reductions. In addition, we have expanded our support to grow the availability of renewable energy, both for our own operations and for the communities in which we operate.

To continue driving improvements in datacenter energy management, we work to reduce peak power, safely harvest unused power, increase server density in existing datacenters through intelligent utilization and power-aware virtual machine allocation, and drive efficiency all the way to our chips and code.

With recognition of the need to continue bringing more renewable energy online, we currently have more than 135 renewables projects in our power purchase agreement (PPA) portfolio globally, a powerful mechanism to support the global energy transition. In the way we design, build and operate our datacenters, we're focused on the path to 100% zero-carbon electricity 100% of the time.

We're also working on solutions that enable datacenters to provide energy back to the grid to contribute to local energy supply during times of high demand. For example, in Ireland we built batteries into wind turbines for a wind energy project to capture energy when the turbines over-perform and deliver that energy to the local grid. In Denmark, excess heat created in a Microsoft datacenter will provide heat to the local community, producing enough heat to warm around 6,000 local homes. Both are examples of our work to use our data centers as a source of electricity to relieve pressure on local electric grids.

Currently, many datacenters rely on water for two reasons: directly for cooling, and indirectly for electricity generation. Although at a global scale total water consumption by datacenters is relatively small, weighing in about 0.1% of national water use in the U.S.1 we recognize the impact of datacenter operations on water-stressed areas, and are working to reduce this impact and design solutions that advance our progress on the road to water positive.

We take a holistic approach to water reduction across our business, from design to efficiency, looking for immediate opportunities through operational usage and, in the longer term, through design innovation to reduce, recycle and repurpose water. We've found success in using direct air instead of water to cool datacenters, harvesting rainwater, and procuring reclaimed water from utilities to reduce our dependence on fresh water. For example, in our Sweden datacenters, we will use a process called free cooling, a simple, cost-effective method that results in a 30% reduction in energy costs and 90% less water usage than standard systems.

For our future datacenters and to help drive progress industry-wide, another way we can advance progress is by helping to accelerate markets for low-carbon building materials. As a sector, building materials such as steel and cement are currently some of the highest contributors to the carbon cost of new construction, together producing an estimated 13.5% of global carbon emissions.2

Innovations in green steel3 and lower-carbon cement are rapidly emerging, however, these markets are still nascent and need significant investment to scale up and bring supply online.

With our $1 billion Climate Innovation Fund, we're investing to hasten the development and deployment of new climate innovations, especially for underfunded sectors and supply-constrained markets like lower-carbon building materials. For example, we are investing in solutions such as H2 Green Steel to expand market supply of near-zero carbon steel4 which can deliver up to 95% lower CO2 emissions than conventional steel. We are also evaluating use of near-zero carbon steel in our own building materials and equipment supply chains.

Similarly, we're working to broaden availability of low-carbon concrete and other construction materials through commercial projects and collaboration with the largest datacenter companies in the world. In Washington state, our pilot program utilizes concrete alternatives like biogenic limestone and fly ash and slag with the goal of lowering the embodied carbon in concrete by more than 50% compared to traditional concrete mixes. With these investments, we aim to facilitate the commercialization of materials that can make an outsized impact on carbon reduction, for our own construction and the broader industry.

Reducing the energy needed to power AI and cloud services up front is another critical component of the solution. We're working to support developers and IT professionals with tools to optimize models and code, exploring ways to reduce the energy requirements of AI, and harnessing the power of these advanced technologies to drive energy breakthroughs.

As a founding member of the Green Software Foundation, we collaborate with other industry-leading organizations to help grow the field of green software engineering, contribute to standards for the industry and work together to reduce the carbon emissions of software. Across our cloud services, we're working to ensure IT professionals have the information they need to better understand and reduce the carbon emissions associated with their cloud usage.

As AI scenarios increase in complexity, we're empowering developers to build and optimize AI models that can achieve similar outcomes while requiring fewer resources. Over the past few months, we've released a suite of small language models (SLMs) called "Phi" that achieve remarkable performance on a variety of benchmarks, matching or outperforming models up to 25x larger. Now available in the Azure AI Studio model catalog, Phi-2 offers a compact model for research and development or fine-tuning experimentation on a variety of tasks.

We've learned that the complex sustainability challenges we face today are best addressed through multidisciplinary, multi-sector collaboration, and energy breakthroughs are no exception. We recently collaborated with the Department of Energy's Pacific Northwest National Laboratory (PNNL) using advanced AI models to find new materials that can reduce reliance on traditional battery materials such as lithium. The team screened over 32 million materials, discovered 500,000 stable candidates, and synthesized one promising candidate to a working prototype, shortening a process that can take years to a matter of days.

These highlights provide a glimpse into our work to build and operate cloud services more sustainably, advancing solutions that can reduce the future impact of AI. Our ambitious 2030 targets to become carbon negative, water positive, zero waste and to protect biodiversity require continued innovation across every aspect of our operations, and we're committed to sharing what we learn along the way. Stay tuned for more on this topic in the months ahead.

To learn more, visit the Microsoft Sustainability website and read the whitepaper Accelerating Sustainability with AI. To learn how we're integrating AI into our sustainability solutions, watch the digital event This is AI . . . for Sustainability.

Read the other stories in the Sustainable by design series:

Sustainable by design: Advancing the sustainability of AI​ Sustainable by design: Transforming datacenter water efficiency​ Sustainable by design: Innovating for energy efficiency in AI, part 1 Sustainable by design: Innovating for energy efficiency in AI, part 2

2Cement and steel — nine steps to net zero (nature.com)

3What is green steel and how can it help us reach net zero? | World Economic Forum

Artificial intelligence is a defining technology of our time. Over the past year we've begun to see how it can unlock profound possibilities for individuals, organizations and society — and it's clear that we have only scratched the surface.

For Microsoft and our partners across industries, AI offers a generational moment to reimagine the capabilities that software and services can provide. Our goal at Microsoft is to innovate and democratize our breakthroughs in AI — and the opportunity for partners is far greater than the sum of AI technologies themselves.

For this State of the Partner Ecosystem moment, we'll share how partners are seizing the opportunities that AI offers and making its promise real for organizations across the world. We'll highlight partners who are building AI-powered solutions to innovate across industries, including three partners who are:

Helping people and organizations achieve, in months, what once might have been a multi-year endeavor. Innovating in higher education by streamlining work in marketing, admissions and student advising. Empowering real-time decision making at the point of impact across the supply chain.

First, we'll look at how Microsoft is empowering our partners to innovate, build and differentiate with AI through a combination of technology advancements, an industry leading partner program and true market differentiation. During this time of AI transformation, there has never been a greater time to be a Microsoft partner, and our partners are fully embracing this new opportunity.

Microsoft is driving innovation in generative AI, with new Azure OpenAI Services and an expanding portfolio of Copilot offerings. Today, we offer the most comprehensive commercial portfolio in the market with AI infused everywhere from the cloud to the edge, and across every solution area.

Partners can tap into all this innovation through the Microsoft AI Cloud Partner Program. Through research, partnership, and new investments, we are bringing partners the capabilities and benefits to help them empower customers of every size, in every industry.

Just as AI capabilities are infused throughout our technology portfolio, AI benefits are wired throughout the partner program. And today we are announcing enhancements to enable partners to create an AI-powered future, including:

The expansion of AI skilling, boot camps and events New benefits packages with product enhancements like Microsoft 365 developer, increased Azure credits, and Visual Studio enterprise, to support AI practice growth New designations for partners to establish certified AI solutions or ways to differentiate themselves.

This builds on the incredible momentum we've seen since we announced the Microsoft AI Cloud Partner Program last July. In fact, the Microsoft AI Transformation Partner Playbook outlining new possibilities with AI has been downloaded over 35,000 times. Our free go-to-market asset, the Era of AI marketing campaign-in-a-box, has been downloaded over 13,250 times.

Today more than 13,000 partners building solutions with Microsoft Azure AI, serving more than 53,000 customers with generative AI capabilities alone. Our partner ecosystem is rising to meet demand with a nearly 250 percent increase in the number of generative AI-related partners in the past eight months.

When you add that up, it points to a year of exponential innovation and growth for partners, and we are continuing to develop new ways to support this growth curve. Here are some specifics on the updates we are making to the Microsoft AI Cloud Partner Program in the coming months.

Solutions partner designations and specializations are one way we help partners differentiate themselves in the market. Designations and certifications help to verify skills and validate the effectiveness of solutions so customers can find the partner and technology that's right for their needs.

To obtain designations and specializations, partners must meet skilling and performance requirements, and provide customer evidence to demonstrate their capabilities — and partners have embraced this opportunity with Microsoft AI. We have seen 58 percent growth since last July in partner designations for Azure Data and AI solutions, and 172 percent growth in partner specializations for the Build and Modernize AI apps with Azure and AI and Machine Learning designations.

To build on this momentum, today we are announcing several new designations for partners across software, services and training.

A new set of designations for partners who develop software: Solutions Partner with certified software

Today, we are pleased to announce the general availability of Solutions Partner[i] with certified software[ii] designations. Certified software designations provide an opportunity to better connect with customers and to unlock additional benefits from Microsoft.

By becoming a Solutions Partner with certified software, partners can differentiate their solution, increase their discoverability in the Microsoft Commercial Marketplace, connect with more customers, and take advantage of marketing and branding resources designed to help accelerate revenue growth in a rapidly expanding market.

There are two pathways to become a Solutions Partner with certified software: solution areas and Industry AI:

Solution areas. To become a Solutions Partner[i] with certified[ii] software for solution areas, a partner's software must meet readiness and technical requirements and demonstrate a track record of customer success. Solution area designations include Azure, Business Applications, Modern Work and Security. Industry AI. To become a Solutions Partner with certified software for Industry AI, in addition to the requirements above, a partner's solution must also meet an industry-specific challenge aligned with the Microsoft Industry Cloud scenarios and include a Microsoft AI capability. Currently, partners can attain the following certified software designations for Industry AI: Healthcare AI, Retail AI, Financial Services AI, Manufacturing AI and Sustainability AI.

More than 75 partners with solutions across all five industries and from around the world have participated in the public preview of the designation, including Netherlands, Iceland, Germany, United Kingdom, France, Norway, Ireland, India, Canada and the United States. There are 49 solutions currently certified with an additional 27 in progress.

We are also releasing new designations for partners who provide training services. Starting April 10, partners who qualify for Solutions Partner for Training Services designations will be able to complete enrollment.

Qualified partners will receive designation-specific badges distinguishing their services from other partner types within the Microsoft AI Cloud Partner Program. These badges greatly enhance customer discoverability, making it easier for them to identify partners with unique expertise and a proven track record of delivery quality training.

New Small and Medium Business (SMB) paths for Azure and Security Solutions Partner designations

Based on partner feedback, we are investing in our small- and mid-sized business ecosystem and creating a pathway for partners who focus on delivering solutions for SMB customers across Azure Data & AI, Digital & App Innovation, Infrastructure and Security. Later this year, we will introduce dedicated SMB paths for Solutions Partner designations for Azure​ and Security, adding to our existing SMB paths for Business Applications and Modern Work.

For partners who serve smaller businesses, this creates an exciting new opportunity — a new way to qualify for existing Solutions Partner designations, giving more partners access to the products, go-to-market resources, support, and advisory benefits that come with these designations. The new SMB paths will differentiate partners who can meet customer needs and drive customer success by validating those partners' capabilities in our Azure and Security solution areas.

Additional details for the SMB paths for Azure and Security will be available in the coming weeks.

Today we are announcing that partners with a Solutions Partner for Support Services designation will soon receive exclusive new benefits, including paid support agreement pricing and access to a knowledge-based library of curated materials designed to help partners resolve incidents quickly and efficiently, in addition to customer-facing badging. General availability and more details will come later this year.

Rapid innovation with AI technology has created an increased demand for partner skilling for both pre-sales and technical roles. Responding to partner feedback, we have launched a breadth of Microsoft AI-focused skilling workshops, boot camps and training events. Since July 2023, more than 350,000 partner personnel have also become skilled pre-sales and technical depth learners for Microsoft AI, Copilot and Fabric.

In March, we launched an in-person training series called the Microsoft AI Partner Training Roadshow to target six cities globally through June, across India, the Bay Area, Germany, Japan and Brazil. Our inaugural event in India had 1,500 attendees from 40 partner organizations with 96 percent of participants agreeing that the event was valuable and 94 percent indicating they gained new skills to be successful in their roles. The roadshow is focused on in-person training on AI, targeting 270 partner organizations, and includes a keynote session highlighting the unique value of Microsoft AI, Fabric and Copilot for Microsoft 365, followed by a sales excellence and technical excellence tracks.

To support our partners and customers in securing their businesses, we are excited to announce the general availability of Microsoft Copilot for Security in all commerce channels, including Cloud Solution Provider (CSP) channel, on April 1, 2024. We initiated an active Copilot for Security partner community at Inspire last year, which has since grown to more than 1,000 participants. Over the past six months, Microsoft has engaged with more than 100 Managed Security Service Providers (MSSP) and Independent Software Vendors (ISVs) in a Copilot for Security Partner private preview. Upcoming security skilling events include:

Microsoft Copilot for Security Partner Boot Camp – Helps participants understand how Microsoft Copilot for Security enables security analysts to move at the speed and scale of AI by augmenting the human experience. Starting April 16. Threat Protection and Incident Response with Microsoft Sentinel Workshop – Participants will learn how to deploy and connect this SIEM and SOAR solution to different data sources and use intelligent security analytics and threat intelligence capabilities for attack detection, threat vulnerability, proactive hunting and threat response. April 22-25.

Azure OpenAI Workshop – Designed to help participants deepen their understanding of Azure OpenAI Services, this is tailored for developers and data scientists aiming to do more with less. Starting April 16. Microsoft Fabric Workshop – Participants will learn more about this comprehensive suite of services, including data lake, data engineering and data integration, all in one place. And learn how to implement this easy-to-use product that is designed to simplify analytics needs of enterprises. Starting April 8. Copilot for Microsoft 365 Pre-Sales, Deployment and Adoption Bootcamp – Participants will explore how Copilot for Microsoft 365 provides real-time intelligent assistance, enabling users to enhance their creativity, productivity, and skills. Starting March 26.

On Jan. 22, Microsoft launched three new benefits packages: Partner Launch Benefits, Partner Success Core Benefits and Partner Success Expanded Benefits. Purpose-built to meet the varied needs of partners at different stages of growth and help partners scale their business through a portfolio of key Microsoft benefits, the packages are available for purchase at a significant discount for partners.

These refreshed benefits follow partners through every element of the partner journey: from creating their solutions, to go-to-market, to differentiating their organizations in the marketplace. Beyond the three new packages, partners can continue to pursue Solutions Partner designations and specializations to differentiate their organizations in the Microsoft Commercial Marketplace and access additional product and support benefits. Partners who develop software IP can also tap into ISV Success, a collection of resources that help accelerate software solution development and amplify sales.

The feedback on the new packages has been incredibly positive, and we continue to listen to partner feedback and explore new ways to invest in our partners.

On Jan. 16, Copilot for Microsoft 365 became generally available across all sales channels, including through our Cloud Solution Providers. General availability of Copilot for M365 opened a wealth of immediate opportunities for partners to deliver end-to-end services to help customers achieve a smooth transition into an AI-powered organization.

One Microsoft distribution partner, TD SYNNEX, has already engaged more than 2,000 partners in its enablement journey for Copilot for Microsoft 365, demonstrating the industry's eagerness to harness the potential of generative AI. With more than 500 individuals certified in the program, TD SYNNEX provides its partners with the technical and operational expertise, solutions and resources to help partners leverage Copilot's capabilities and gain a competitive edge.

From vision to reality: partners are delivering AI solutions around the world

We are committed to building AI capabilities across the Microsoft cloud, guided by our Responsible AI principles. Our partners enable us to scale and deliver those technologies to every organization and every person on the planet. We anticipated some remarkable innovation in this area, and in just over a year we have been amazed by the results and impact partners are achieving around the world.

Here are some of the ways Microsoft partners are developing differentiated solutions and helping customers transform their business processes across industries.

Accenture, together with Avanade, is working with Amadeus to develop a generative AI-powered interactive assistant using Microsoft technologies, including GPT models from Azure Open AI Service, Microsoft 365 and Teams, to allow corporate travelers to orchestrate complex travel-related tasks using only natural language.

Anthology Inc is providing AI-powered innovation for higher education through its student success and lifecycle engagement solution that is built on Microsoft Dynamics 365.

The Atlassian product suite — including Confluence, Trello, Jira and Atlas — improves productivity and collaboration. As part of the Microsoft Copilot Early Access Program, Atlassian enhanced its apps to empower users to track tasks, resolve issues and manage projects seamlessly.

Companies use Blue Yonder for end-to-end supply chain management. Blue Yonder, built on Azure, uses AI and ML to provide real-time decision-making power and workflows that help businesses fulfill orders and respond quickly to shifting market conditions.

CallMiner's AI-driven platform empowers organizations to ingest and analyze every conversation that happens in the contact center and beyond to uncover insights, act, and drive transformational change.

Datadog integrates with all major Azure services, including Azure OpenAI Service. Now customers can better optimize costs, troubleshoot issues and monitor the performance of their AI-powered applications.

EY worked with Microsoft to create a generative AI chatbot to answer payroll questions from employees across the 159 countries and 49 languages that EY clients encompass.

Icertis Contract Intelligence Copilots are providing customers with assistive and generative natural language capabilities that cut through legalese and turn contracts into interactive assets that deliver insights and automation to drive businesses forward.

Intapp is enabling thousands of financial and professional services firms to harness the intelligence of their professionals through AI-powered software solutions.

LTI Mindtree is leveraging generative AI, powered by GitHub Copilot and Azure OpenAI Service, to accelerate the migration of legacy apps from on-premises to the cloud. Their methodology automates the creation of new cloud architecture, code conversion and documentation generation, resulting in an average reduction of 46 percent in migration time.

Sight Machine works with manufacturers to make their businesses stronger, more sustainable and more resilient. To better support manufacturers' data accessibility needs, Sight Machine used Azure OpenAI Service to harness generative AI (GPT-4) and to develop Factory Copilot.

Veeam Software, recently announced an extended, five-year strategic partnership with Microsoft to innovate new solutions for protecting customers by integrating Veeam's product family and Microsoft Copilot and AI services.

We have seen tremendous progress as partners seize the moment and turn the promise of AI into a reality around the world. Today, we have moved from talking about AI to applying AI at scale in ways no one could have imagined just a few months ago.

Microsoft is infusing AI across every layer of its tech stack, creating new ways to power organizations and industries, and unlocking new opportunities to deliver transformational value to customers.

Key to making all this a reality are the capabilities, services, product innovation and depth of industry knowledge delivered by our partners. Microsoft's mission is to empower every person and every organization on the planet to achieve more. Our partners enable us to deliver on that commitment, in every customer segment, every geography, every day.

[i] "Solutions Partner" refers to a company that is a member of the Microsoft AI Cloud Partner Program and may offer software, services, and/or solutions to customers. Reference to "Solutions Partner" in any content, materials, resources, web properties, etc. and any associated designation should be not interpreted as an offer, endorsement, guarantee, proof of effectiveness or functionality, a commitment or any other type of representation or warranty on the part of Microsoft. All decisions pertaining to and related to your business needs including but not limited to strategies, solutions, partner selection, implementation, etc. rest solely with your business.

[ii] A certification is (A) specific to the solution's interoperability with Microsoft products and (B) based on self-attestation by the solution owner. Solutions are only certified as of the date the solution is reviewed. Solution functionality and capability are controlled by the solution owner and may be subject to change. The inclusion of a solution in marketplace and any such designations should not be interpreted as an offer, endorsement, guarantee, proof of effectiveness or functionality, a commitment or any other type of representation or warranty on the part of Microsoft. All decisions pertaining and related to your business needs including but not limited to strategies, solutions, partner selection, implementation, etc. rest solely with your business.

Tags: AI, Microsoft AI Cloud Partner Program, Microsoft Copilot for Security, Microsoft Partners, Responsible AI

Satya Nadella, Chairman and CEO, shared the below communication today with Microsoft employees.

I want to share an exciting and important organizational update today. We are in Year 2 of the AI platform shift and must ensure we have the capability and capacity to boldly innovate.

There is no franchise value in our industry and the work and product innovation we drive at this moment will define the next decade and beyond. Let us use this opportunity to build world-class AI products, like Copilot, that are loved by end-users! This is about science, engineering, product, and design coming together and embracing a learning mindset to push our innovation culture and product building process forward in fundamental ways.

In that context, I'm very excited to announce that Mustafa Suleyman and Karén Simonyan are joining Microsoft to form a new organization called Microsoft AI, focused on advancing Copilot and our other consumer AI products and research.

Mustafa will be EVP and CEO, Microsoft AI, and joins the senior leadership team (SLT), reporting to me. Karén is joining this group as Chief Scientist, reporting to Mustafa. I've known Mustafa for several years and have greatly admired him as a founder of both DeepMind and Inflection, and as a visionary, product maker, and builder of pioneering teams that go after bold missions.

Karén, a Co-founder and Chief Scientist of Inflection, is a renowned AI researcher and thought leader, who has led the development of some of the biggest AI breakthroughs over the past decade including AlphaZero.

Several members of the Inflection team have chosen to join Mustafa and Karén at Microsoft. They include some of the most accomplished AI engineers, researchers, and builders in the world. They have designed, led, launched, and co-authored many of the most important contributions in advancing AI over the last five years. I am excited for them to contribute their knowledge, talent, and expertise to our consumer AI research and product making.

At our core, we have always been a platform and partner-led company, and we'll continue to bring that sensibility to all we do. Our AI innovation continues to build on our most strategic and important partnership with OpenAI. We will continue to build AI infrastructure inclusive of custom systems and silicon work in support of OpenAI's foundation model roadmap, and also innovate and build products on top of their foundation models. And today's announcement further reinforces our partnership construct and principles.

As part of this transition, Mikhail Parakhin and his entire team, including Copilot, Bing, and Edge; and Misha Bilenko and the GenAI team will move to report to Mustafa. These teams are at the vanguard of innovation at Microsoft, bringing a new entrant energy and ethos, to a changing consumer product landscape driven by the AI platform shift. These organizational changes will help us double down on this innovation.

Kevin Scott continues as CTO and EVP of AI, responsible for all-up AI strategy, including all system architecture decisions, partnerships, and cross-company orchestration. Kevin was the first person I leaned on to help us manage our transformation to an AI-first company and I'll continue to lean on him to ensure that our AI strategy and initiatives are coherent across the breadth of Microsoft.

Rajesh Jha continues as EVP of Experiences & Devices and I'm grateful for his leadership as he continues to build out Copilot for Microsoft 365, partnering closely with Mustafa and team.

There are no other changes to the senior leadership team or other organizations.

We have been operating with speed and intensity and this infusion of new talent will enable us to accelerate our pace yet again.

We have a real shot to build technology that was once thought impossible and that lives up to our mission to ensure the benefits of AI reach every person and organization on the planet, safely and responsibly. I'm looking forward to doing so with you.

Each quarter, I share a collection of customer and partner stories from around the world that highlight the incredible technological advancements shaping the future of industry. As a partner of choice, we are committed to helping organizations accelerate AI Transformation to unlock opportunities and realize material business value. Our ability to bend the curve on AI innovation is allowing us to live truer to our mission than ever before: to empower our customers and partners to achieve more.

I am sharing an additional blog this quarter because we are seeing so much inspiring AI Transformation and pragmatic innovation with our customers. Their impressive results are benefiting employees, businesses, and industry, and I would like to expand on a handful of stories that crystallize the tangible impact AI is having on organizations that embrace it:

Digital financial services firm Ally Financial is leveraging Microsoft Azure and Azure OpenAI Service to reduce manual tasks for its customer service associates, freeing up time for them to reinvent how they engage with customers. Previously, associates needed to take notes and write summaries after every call, which competed with the amount of time and attention they could give to their customers. By using generative AI to automate manual tasks, Ally empowered its associates to deliver a better, more personal experience while protecting vital customer data through a production-ready solution in only eight weeks. The new solution has cut associates' post-call effort by 30% while capturing critical information automatically with over 85% accuracy — even finding details they may have previously missed in their conversations.

Global travel technology provider Amadeus deployed Copilot for Microsoft 365 to help empower its workforce, beginning with early adoption across a subset of employees. More than 90% of these employees are regularly using Copilot to save time drafting emails, summarize meetings and actions, and more effectively search for information across the web and internal assets. Now, the company is rolling out the solution across its wider workforce so employees around the world can benefit from its time-saving options to focus on more strategic tasks.

Global life sciences leader Bayer is using Copilot for Microsoft 365 to deliver productivity and collaboration benefits across its Crop Science, Pharmaceutical and Consumer Health divisions. With more than 700 use cases identified, they are already seeing a big impact from its implementation. Copilot is helping to summarize communications and expedite the search for information, saving employees hundreds of hours. Bayer also developed the Model Store — a Copilot plugin leveraging the Microsoft Teams platform — to search for information using natural language to close communication gaps between data scientists and laboratory researchers, locate the source of data faster, reduce barriers to finding information and help collaborate with the expert source. For example, Copilot helped a U.S. researcher identify a predictive model already developed by a researcher in Germany, preventing a duplicate model from being developed and saving two to three months of work.

Multinational brewer Carlsberg unified its developer organization around an integrated platform using GitHub Enterprise — reducing their toolchain from six tools to a single platform. This allowed for the seamless and effective integration of GitHub Copilot, which enhanced the synergy in their workflow and removed development roadblocks by providing valuable coding suggestions. Copilot adoption has been overwhelmingly positive, enhancing productivity across teams and proving to be a valuable learning tool for cloud developers by easing the learning curve required when working with new languages and platforms.

One of the top media companies in the world, dentsu, is using Copilot for Microsoft 365 to revolutionize workflows, support its talent and help rethink its services across a growing number of channels. By reducing and removing tactical tasks, employees can focus on the strategic part of their work to collaborate creatively. Employees are saving 30-40 minutes a day using Copilot for tasks such as summarizing chats, generating presentations and building executive summaries. They are reinvesting their time to focus on more thoughtful work without interruption, while having content served up within their workflows. Employees are now averaging 30-60 minutes less time on first drafts of messaging and 80% say they have a very positive view of Copilot.

Food industry giant Grupo Bimbo turned to Microsoft's Azure AI technologies — including Azure OpenAI Service, Form Recognizer and Cognitive Search — to develop a copilot solution in two weeks that helped employees make queries about company policies. The copilot for internal control and risk management employs advanced AI technology to seamlessly convey information in a synthesized format, complemented by readily accessible reference links that pinpoint the exact sources of global policy. It can also respond in all languages where the company has collaborators and operations, no matter what language the query is originally made in. The primary benefit for the company has been its ability to promote compliance across 34 countries. Now, they are replicating the technology to build a copilot product that empowers all individuals and departments. For example, the copilot is helping their communications team draft emails, craft content for social media, propose images for marketing products, translate text and more.

U.S. healthcare organization Providence faced a deluge of incoming messages that required triage and interfered with the time providers needed to spend with their patients. Providence clinicians, informaticists and AI specialists developed a tool based on Azure OpenAI Service named ProvARIA to classify messages, direct them to the appropriate caregiver and free providers to focus on patient care. Providence piloted the tool with four separate clinics representing 27 different doctors and nurse practitioners, funneling all electronic communications into a centralized inbox in a single office. A group of medical assistants worked together to address the messages, processing about 10,000 messages in one month with a 35% improvement in turnaround time. Other groups within Providence enthusiastically joined the pilot, introducing efficiencies that allowed clinics to manage messages with fewer medical assistants — a boon when the limited number of qualified medical assistants is in high demand. Providence medical assistants are now processing 5,000 messages per day, covering 145 Providence clinics and 650 providers. The most profound outcome is one that cannot be measured, and one that had not been anticipated: caregivers have gained peace of mind knowing they have the time and focus to help all patients who need it.

One of the largest digital telco companies in Asia, Telkomsel, needed a way to ensure its customer support team was not overwhelmed by sharply rising demand. The company upgraded a virtual assistant within two weeks with Azure OpenAI Service to improve interactions across its 159 million mobile subscriber base and 8.5 million fixed broadband customers. The virtual agent is helping customer support agents deal with more complex issues without the distraction of routine inquiries and is reducing burnout from facing long call queues. By infusing light-hearted humor and emoticons into the conversation, the new virtual assistant "Veronika" intelligently combines the latest information from multiple FAQs to supply more insightful answers with a personal touch. Telkomsel has scaled the solution to handle up to 5 million transactions per month while delivering the robust security it requires. Since its introduction, self-service interactions have risen from 19% to 45%, with a 140% increase in average messages per user. Customer service agents who used to handle 8,000 calls a day now handle 1,000, freeing up time to increase their cross-selling efforts.

Leading telecommunications and technology company Telstra has scaled its AI adoption following promising pilots of generative AI solutions with front-line team members. Leveraging the Microsoft Cloud and Azure OpenAI Service capabilities, two pilot programs were developed: One Sentence Summary and Ask Telstra. Using Azure OpenAI Service's large language models, One Sentence Summary transforms customer notes, interactions and transactions into a concise summary of a customer's recent history and status. The solution enhances the efficiency and quality of interactions, reducing the need for customers to repeat information. Ninety percent of employees using the tool saved time and increased effectiveness, resulting in 20% less follow-up contact. Ask Telstra enables employees to search the company's extensive internal knowledge base quickly and easily for information, powered by Azure OpenAI Service and Azure AI Search. Employees were able to gather information faster and more easily for customers, with over 80% agreeing the technology had a positive impact on customer interactions. Now the company is rolling out the pilots across all contact centers and store teams throughout 2024.

To pursue its digital transformation efforts, global multi-energy company TotalEnergies launched Microsoft Copilot to provide employees with a secure AI chat solution based on internal data. After a successful test phase across 300 employees, the company also launched Copilot for Microsoft 365 for their employees, resulting in improved operational efficiency and user adoption. The company is also focused on a new program to support and enhance employee skilling to get the most out of these new AI tools, including implementation of low code-no code solutions with Microsoft Power Platform.

To alleviate an overwhelming workload for doctors and medical staff in major cities across Vietnam, VinBrain introduced an AI platform into hospital workflows: DrAid™. Operating on an Electronic Medical Record (EMR) data system, DrAid™ harnesses Microsoft technologies —including Azure OpenAI Service —to aid physicians in diagnostic processes, treatment planning and the efficient management of burgeoning medical records. This platform is the first and only AI-driven X-ray diagnostic tool in Southeast Asia to receive U.S. Food and Drug Administration (FDA) certification, propelling Vietnam into the top six nations globally with FDA-approved AI products for chest X-ray diagnostics. Comprehensive screening is achieving an accuracy rate of up to 95%, and advanced imaging capabilities allow for the detection of liver cancer tumors as small as 5mm to help facilitate early-stage intervention and curative treatment. The tool is also reducing initial screening times by 80-85% and lowering time to analyze cases from 30 minutes to 5 minutes per case.

To scale digital platforms to more than 300 million businesses, public sector organizations and consumers across Europe and Africa, Vodafone is leveraging Microsoft's generative AI to provide highly personalized and differentiated customer experiences across multiple channels. Their investment is already reinventing how Vodafone engages with customers and accelerating digital transformation. Early testing shows that AI-powered assistants are reducing average handling times and providing AI-generated responses with greater accuracy. Thanks to faster and more accurate responses, the company is also improving customer satisfaction scores.

Leading manufacturer Volvo Group needed the ability to extract data from images — such as photographs, stamps and printed text with handwritten notes over it — and translate documents to and from multiple languages to help its workers streamline invoices and claims document processing. Built on Microsoft Azure, Volvo created a six-week pilot program with a solution using Microsoft Azure AI services and AI Document Intelligence. After a four-month production timeline, they launched a solution that simplified document processing and meets the objectives of data extraction that has saved employees more than 10,000 manual hours — about 850-plus manual hours per month. Now, employees are enjoying their work more, with additional time to focus on innovation and tasks related to their specific skill sets.

I hope you find these stories of AI Transformation as promising and inspiring as I do. I invite you to read my January pre-earnings blog to learn more about how customers and partners are unlocking AI opportunity across industries: Embracing AI Transformation: How customers and partners are driving pragmatic innovation to achieve business outcomes with the Microsoft Cloud. I remain grateful for the opportunity we have at Microsoft to help our customers and partners realize pragmatic business value with AI, and look forward to finding ways we can help your organization achieve more.

Tags: AI, Azure, Azure OpenAI Service, Copilot for Microsoft 365, copilots, Microsoft Copilot

Just over a year ago, the healthcare industry was energized by the debut of generative AI and the promise this new technology held for delivering real-world outcomes that positively impact clinicians, patients, health systems, and the broader health and life sciences ecosystem. Since then, it has been a catalyst for the development of new use cases, opening possibilities for an entirely new era of innovation — and this shows no signs of slowing down. We continue to see AI adoption within healthcare grow, with 79% of healthcare organizations reporting that they're currently using AI technology, according to a Microsoft-commissioned study through IDC[i]. AI also has a demonstrable business value, with healthcare organizations realizing a return on their AI investments within 14 months, along with an average return of $3.20 for every $1 they invest in AI[ii].

Working alongside healthcare organizations, Microsoft is making the promise of AI real by empowering the industry to tackle its biggest challenges and create a real difference in the lives of clinicians and patients. At the 2024 HIMSS Global Health Conference & Exhibition, we are highlighting how providers are adopting generative AI solutions and the impact the technology is making.

Stanford Medicine and Microsoft announced the enterprise-wide deployment of Nuance Dragon Ambient eXperience Copilot (DAX Copilot), providing conversational, ambient and generative AI to Stanford Medicine's clinicians. This deployment aligns with Stanford Medicine's mission to alleviate physician burnout and enhance patient care by streamlining administrative tasks. Stanford Medicine's commitment to innovation, coupled with DAX Copilot's ability to automate clinical documentation, has led to significant improvements in efficiency and patient-focused care. DAX Copilot enables healthcare organizations to adopt AI-powered clinical documentation applications at scale, leveraging existing investments in our trusted and extensible Dragon solutions. Stanford Health Care clinicians who used DAX Copilot reported through a preliminary survey that 96% of physicians stated that it was easy to use, and 78% reported that it expedited clinical notetaking. About two-thirds reported that DAX Copilot saved time. WellSpan Health announced its widespread adoption of DAX Copilot, enhancing patient-physician interactions during exam room and telehealth visits. Leveraging generative AI, DAX Copilot automates clinical note drafting, allowing physicians to focus entirely on patients without the distraction of manual documentation. WellSpan's decision to adopt DAX Copilot builds upon its successful use of Nuance solutions to streamline clinical workflows and improve patient care. Surveys indicate high satisfaction among physicians and patients, with DAX significantly improving the quality of interactions and reducing documentation burdens. This initiative reflects WellSpan's commitment to delivering exceptional care experiences and addressing physician burnout by providing innovative tools to enhance efficiency and personalize care delivery. Providence and Microsoft announced a strategic collaboration aimed at accelerating AI innovation in healthcare. Leveraging Microsoft Cloud for Healthcare and Azure as a standard platform, the collaboration focuses on delivering AI-powered applications to improve interoperability, generate clinical insights and enhance care delivery. Past successes from this relationship include Providence's migration to cloud solutions and the adoption of AI-powered applications like Nuance's DAX Copilot. By leveraging their combined expertise, the collaboration aims to rapidly scale existing solutions and create more personalized experiences for patients and clinicians. Through this initiative, Providence aims to transform healthcare delivery and improve outcomes by harnessing the power of the cloud and advanced AI technologies.

As incredible as AI – and all its potential – is, the important role clinicians play in determining its use and enabling responsible AI guidelines is vital. That's why we remain steadfast in our commitment to our Responsible AI principles, which help to ensure safe, fair and responsible use of the technology. As part of this ongoing commitment, Microsoft has joined a consortium of healthcare leaders to announce the formation of the Trustworthy & Responsible AI Network (TRAIN), creating one of the first health AI networks aimed at operationalizing responsible AI principles to improve the quality, safety and trustworthiness of AI in health.

Serving as the technology-enabling partner for TRAIN, Microsoft is working with members that include AdventHealth, Advocate Health, Boston Children's Hospital, Cleveland Clinic, Duke Health, Johns Hopkins Medicine, Mass General Brigham, MedStar Health, Mercy, Mount Sinai Health System, Northwestern Medicine, Providence, Sharp HealthCare, University of Texas Southwestern Medical Center, University of Wisconsin School of Medicine and Public Health and Vanderbilt University Medical Center – to share best practices and provide tools to enable measurement of outcomes associated with the implementation of AI. Additionally, OCHIN, which serves a national network of community health organizations with solutions, expertise, clinical insights and tailored technologies, and TruBridge, a partner and conduit to community healthcare, will work with TRAIN to help ensure that every organization, regardless of resources, has access to the benefits the network offers.

Additionally, we continue to take the necessary steps to ensure healthcare organizations can implement technology in compliance with the highest levels of security and privacy in mind. We recently announced the preview of healthcare data solutions in Microsoft Fabric, which enables healthcare organizations to break down data silos and harmonize their disparate healthcare data in a single unified store where analytics and AI workloads can operate at-scale. We are also pleased to share that Fabric now supports HIPAA (Health Insurance Portability and Accountability Act) compliance, allowing our U.S. healthcare industry customers and partners to compliantly use Fabric to store, process and analyze data.

Microsoft's unmatched global ecosystem of trusted partners is one of the key components that helps drive our innovation forward. This week, Cognizant announced that its TriZetto Assistant on Facets will leverage Microsoft Azure OpenAI Service and Semantic Kernel to provide access to generative AI within the TriZetto user interface. This new collaboration will help increase productivity and efficiency for healthcare payers and providers, while ensuring timely responses and improved care for patients.

Additionally, Microsoft for Startups announced a new collaboration with the American Medical Association's (AMA) Physician Innovation Network. The Physician Innovation Network is a powerful match-making tool developed by the AMA to connect physicians, care team members, business liaisons and entrepreneurs in a shared mission to enhance healthcare. The collaboration extends the reach of the Physician Innovation Network to all startup founders in the Microsoft for Startups Founders Hub, so whether they're driven to improve healthcare, collaborate with industry leaders or learn from healthcare experts, they will have access to a unique space for connection and innovation.

Without a doubt, these are incredibly exciting times, and we are proud to see our customers and partners adopting Microsoft's generative AI solutions and putting them to use in the real world to make a meaningful impact in the lives of clinicians and patients. We look forward to continuing to play a leading role in fostering innovation with generative AI, and empowering healthcare providers and partners across the entire health and life sciences industries with leading-edge and responsible AI technologies that contribute to better experiences and outcomes in healthcare.

[i] IDC InfoBrief, sponsored by Microsoft, The Business Opportunity of AI: How Leading Organizations Around the World Are Using AI to Drive Impact Across Every Industry, IDC #US51364223, Nov. 2023.

[ii] IDC Resource Map Document: IDC Business Value of AI Survey, sponsored by Microsoft, IDC #US51331223, Nov. 2023.

Tags: AI, healthcare, Microsoft Copilot, Microsoft Fabric, Microsoft for Startups, Responsible AI

Today we kick off the 14th Microsoft Ability Summit, an annual event to bring together thought leaders to discuss how we accelerate accessibility to help bridge the Disability Divide. There are three key themes to this year's summit: Build, Imagine, and Include. "Build" invites us to explore how to build accessibly and inclusively by leaning on the insights of disabled talent. "Imagine" dives into best practices for architecting accessible buildings, events, content and products. And "Include" highlights the issues and opportunities AI presents for creators, developers and engineers. Katy Jo Wright and Dave McCarthy discuss Katy Jo's journey living with the complex disability, Chronic Lyme Disease. Get insights from deaf creator and performer Leila Hanaumi; international accessibility leaders Sara Minkara, U.S. Special Advisor on International Disability Rights, U.S. Department of State; and Stephanie Cadieux, Chief Accessibility Officer, Government of Canada. And we'll be digging into mental health with singer, actor and mental health advocate, Michelle Williams. We'll also be launching a few things along the way. Advancing accessible technology Accessible technology is crucial to empowering the 1.3 billion-plus people with disabilities globally. With this new chapter of AI, the possibilities are growing, as is the responsibility to get it right. We are learning where AI can be impactful, from the potential to shorten the gap between thoughts and action, to making it easier to code and create. But there is more to do, and we will continue to leverage every tool in the technology toolbox to advance accessibility. Today we'll be highlighting the latest technology and tools from Microsoft to help achieve this goal including:

Copilot for Windows, making accessibility easier. Beginning late March 2024, new accessibility skills are being enabled in Windows. Ask Copilot for Windows to launch Live Captions, Narrator and other accessibility features. More: Windows Blog M365 empowering creators to build accessible content. Including Accessibility Assistant, a set of tools to help creators produce accessible content available today in Insider preview for Word and sharing that Outlook and PowerPoint are coming soon! Ales Holecek will also be sharing some examples and early research on neurodiversity and M365 Copilot which reduces time to create content and shortens the gap between thoughts and action. Azure AI for Accessibility, today sharing six new examples including:

Seeing AI is now available in an additional 14 languages (total 33) and will be rolling out the latest generative AI models, including richer descriptions of images and chat capability for photos and documents. Azure AI Audio Description solution accelerator, new upcoming solution accelerator that uses Azure GPT-4 Turbo with Vision to describe videos for people who are blind or low vision. WPP will be demonstrating how this works, and the solution accelerator is now open for sign-ups.

Technology can also help tackle long enduring challenges, like finding a cure for ALS (Motor Neuron Disease). With Azure, we are proudly supporting ALS Therapy Development Institute (TDI) and Answer ALS to almost double the clinical and genomic data available for research. In 2021, Answer ALS provided open access to its research through an Azure Data Portal, Neuromine. This data has since enabled over 300 independent research projects around the world. The addition of ALS TDI's data from the ongoing ALS Research Collaborative (ARC) study will allow researchers to accelerate the journey to find a cure. We will also be previewing some of our ongoing work to use Custom Neural Voice to empower people with ALS and other speech disabilities to have their voice. We have been working with the community including Team Gleason for some time and are committed to making sure this technology is used for good and plan to launch later in the year.

YouTube Video Click here to load media Accessibility as a fundamental right To build inclusively in an increasingly digital world, we need to protect fundamental rights and will be sharing partnerships advancing this across the community throughout the day. This includes:

A discussion with Ilene Arenberg and Tanya Harris on access to competitive integrated employment and economic equity, talking about the workplace and fair compensation. The British Association for Supported Employment (BASE) will share their work to support disabled, neurodivergent and disadvantaged people in the U.K. to find and maintain careers. They have been growing digital skills using Microsoft Teams and community training to modernize the supported employment sector. The Hunger Project will talk about the importance of connectivity as a foundation to accessibility. It is working with community partners in Malawi and around the world to provide access to connectivity and support for accessible spaces, training materials and digital accessibility. Rijksmuseum will discuss their partnership with the blind and low vision community to bring detailed text descriptions to over 1 million pieces of art using Azure AI Computer Vision and Azure OpenAI. This video brings the experience to life.

Next steps, accelerating your accessibility journey All through the Ability Summit, industry leaders will be sharing their learnings and best practices. Today we are posting four new Microsoft playbooks, sharing our learnings from working on our physical, event and digital environment. This includes a new Mental Health toolkit, with tips for product makers to build experiences that support mental health conditions, created in partnership with Mental Health America. And "Accessible and Inclusive Workplace Handbook," with best practices for building an accessible campus from our Global Workplace Services team, responsible for our global building footprint including the new Redmond headquarters campus. Please join us to watch content on demand via www.aka.ms/AbilitySummit. Technical support is always available via Microsoft's Disability Answer Desk. Thank you for your partnership and commitment to build a more accessible future for people with disabilities around the world. Tags: accessibility, AI, AI for Accessibility

Today we're announcing the public preview of Microsoft Copilot for Finance, the newest Copilot offering designed for business functions that extends Microsoft Copilot for Microsoft 365 and revolutionizes how finance teams approach their daily work. Copilot for Finance joins Copilot for Sales and Copilot for Service, now generally available, to provide AI-powered, role-based workflow automation, recommendations and guided actions in the flow of work. Finance departments are critical partners in strategic decisions impacting the direction of a company. Eighty percent of finance leaders and teams face challenges to take on more strategic work outside the operational portions of their roles[1]. However, 62% of finance professionals say they are stuck in the drudgery of data entry and review cycles [2]. Copilot for Finance can help free up time for finance to play more of a strategic role in delivering counsel and insights to the business by streamlining financial tasks, automating workflows and providing insights in the flow of work. Copilot for Finance includes Copilot for Microsoft 365, which means it supercharges Excel, Outlook and other widely used productivity apps with workflow and data-specific insights for the finance professional. Copilot for Finance draws on essential context from your existing financial data sources, including traditional Enterprise Resource Planning (ERP) systems, such as Microsoft Dynamics 365 and SAP, and the Microsoft Graph. In public preview today, Copilot for Finance introduces several key features to enhance financial operations:

Helps financial analysts quickly conduct a variance analysis in Excel using natural language prompts to review data sets for anomalies, risks and unmatched values. This type of analysis helps finance provide strategic insights to business leaders about where it is meeting, exceeding or falling short of planned financial outcomes and why. Simplifies the reconciliation process in Excel with automated data structure comparisons and guided troubleshooting to help move from insight to action, which helps ensure the reliability and accuracy of financial records. Provides a complete summary of relevant customer account details in Outlook, such as balance statements and invoices, to expedite the collections process. Enables customers to turn raw data in Excel into presentation-ready visuals and reports ready to be shared across Outlook and Teams.

Customers transforming business operations with Microsoft Copilot The Copilot offerings designed for business functions help workers tackle a common problem: getting from insights to impact – with the relevant data and workflows specific to their roles. The latest Work Trend Index survey revealed that people are drowning in data. Roughly a quarter of their day is spent searching for information – roughly 50% of the information they consume each day is deemed necessary for their job, and a recent survey found roles like sales, finance and supply chain have role-specific needs from their data. Copilot helps break down information and application silos while actively deriving insights, recommendations and guidance from a variety of data sources — all in accordance with Microsoft's responsible AI principles. With Microsoft Copilot Studio, businesses can further customize Copilot for business processes inside of Copilot for Microsoft 365 and its role-based extensions. Copilot for Sales is already helping sellers at more than 30,000 organizations. Companies including dentsu, Lumen Technologies, Northern Trust, Schneider Electric, Visa and hundreds more are empowering their employees with Copilot across their sales, service and finance departments. Here is what a few of the companies had to say:

"Artificial intelligence is transforming the way businesses operate and thrive. At dentsu, we are constantly searching for ways to bring the power of generative AI to all our employees with a framework defined on ethical and responsible AI principles. Building on the existing use cases we've defined to empower our workforce with Microsoft Copilot for Microsoft 365 and Microsoft Copilot for Sales, we are excited to participate in the preview of Microsoft Copilot for Finance. We see potential for Copilot for Finance to accelerate the impact of our finance professionals by optimizing routine processes, and we anticipate efficiency gains will free up finance capacity to focus on performance across our organization." – Carolyn Isaacs, Global Director Finance Services, dentsu "Northern Trust's digital workplace transformation is rooted in empowering our employees with technology that enhances and optimizes the services that they provide our clients. Deploying Microsoft Copilot for Service is a milestone in this transformation journey and we are excited for the potential of this AI-powered solution to help modernize our client relations organization, streamline processes for our employees, and elevate our client experience." – Shaelyn Otikor SVP, Head of Global Digital Workplace Strategy, Asset Servicing, Northern Trust "Building on our 30-year history of embracing AI, Visa is on a journey to roll out generative AI across our entire company to empower our employees and develop new solutions to serve and protect our cardholders, merchants and the broader ecosystem. We've seen our employees embrace the broad rollout of Microsoft Copilot for Microsoft 365, and we're excited to continue to bring employees new ways to take advantage of the technology, transforming the ways in which we work and how we service our clients." – Don Hobson, Chief Information Officer, Visa

At Microsoft, we are also an AI-powered organization, leveraging Copilot for Sales and Copilot for Service to improve seller and agent workflows and transform customer experiences:

Microsoft Copilot for Sales empowers sellers to close deals faster with AI-assisted insights and recommendations. Our study of Microsoft sellers who use Copilot for Sales at least weekly found it makes them more productive, saving an average of 90 minutes per week – and 67% reported it allowed them to spend more time with customers.

"We have seen firsthand that an AI-powered sales organization is a more successful sales organization. Not only has Copilot for Sales helped our global sales team simplify tasks and save time, but it has also strengthened our customer relationships with AI-supported insights and recommendations that are personalized and tailored to each customer." – Judson Althoff, Microsoft EVP and Chief Commercial Officer

Microsoft Copilot for Service is modernizing the contact center with AI to enhance service experiences and boost agent productivity. In Microsoft's customer service department – one of the largest in the world – there has been a 12% reduction in average case handling times (the time actively spent on resolving customer cases via chat) in two different customer support business areas while using similar capabilities in Copilot in Dynamics 365 Customer Service. The benefits and use cases from our own Copilot deployment will continue to shape Copilot for Service and its capabilities.

"Generative AI has been a game-changer for our own contact center at Microsoft. Agents spend less time searching for information, allowing them to focus more time on helping customers solve complex challenges. Moreover, newer agents experience significant benefits, feeling more confident and capable in their roles. This has led to reduced onboarding times and increased job satisfaction." – Mala Anand, Microsoft CVP Customer Experience & Success

Microsoft Copilot for Finance streamlines financial processes and surfaces insights for better-informed decision making. Microsoft's world-class finance organization has long prioritized adoption of AI and automation tools to modernize operations, reduce financial risk and support the company's priorities with strategic insights. The team has helped inform the Copilot for Finance product capabilities and roadmap.

"Our finance organization is just like any other – looking for technology to help us do our work in a more efficient and impactful way – and we're excited to track our journey as customer zero of Microsoft Copilot for Finance" – Cory Hrncirik, Modern Finance Lead, Microsoft

Companies of all sizes are moving beyond AI experimentation and embracing Microsoft Copilot to strategically empower those closest to their customer interactions and critical operations to create new business value. To get started with the new Copilot for Finance, visit: aka.ms/CopilotforFinancePreview.

[1] Future of Finance Trends | Microsoft Dynamics 365 [2] Metric of the Month: Time Allocation in Finance | CFO Tags: AI, Copilot for Finance, Copilot for Microsoft 365

2023 was a year of exciting growth and innovation here at Microsoft. This year's focus is to empower our customers and partners through AI transformation, and we're excited to share what will be an impactful lineup of events for 2024. Attending any of these events provides you with the opportunity to learn, grow and make defining connections with experts from around the world.

Expect to see enhancements in some of this year's events. Azure AI-powered natural language assistants will provide personalized session recommendations, summarize content and answer your event-related questions. To meet the needs of a global audience, we are also offering options to participate in person, online and on-demand so you can choose what format works best for you. We structure these events to support the goals of our audiences and ensure that anyone attending has a great experience.

Visit our events site to find out which ones are right for you.

It's not too late to register for this ongoing series of one-day, in-person experiences around the world. These events bring together those on the cutting edge of innovation — including decision-makers, industry experts, thought leaders and developers — to focus on how AI will revolutionize work. So far, we've welcomed thousands of senior leaders and developers in six locations around the globe with keynotes highlighting the latest innovations in AI. Tour stops remain in Berlin, Paris, São Paulo and Seoul, where you can attend interactive workshops and learn how you can unlock the power of AI. Go to the Microsoft AI Tour site to sign up.

In-demand experts, distinguished engineers and developers are gathering in Seattle for our annual Microsoft Build. This celebration of technology is a chance to hear the latest announcements and get hands-on with new technology. Learn how to create new features and opportunities with AI and copilots, dive deep into the latest tech, and develop the skills that are needed for tomorrow — today.

We are evolving the event previously known as Microsoft Inspire. In July, we will kick off our fiscal year with partners in tandem with our Microsoft sellers by providing a digital engagement to share strategic priorities, investments and key program changes. We look forward to sharing more details soon. Online | July 2024

With this change, we will also welcome partners to join us at Microsoft Ignite for an in-person experience in November to see the latest Microsoft innovations, network and celebrate the Partner of the Year Award winners.

Our biggest event of the year is getting even bigger for customers and partners, and we're returning to Chicago!

Join IT professionals, implementers, developers, architects and more in checking out the latest tech Microsoft has to offer. With demos and firsthand access to new AI solutions and copilots, this is your chance to explore the latest tools, receive deep technical training and get questions answered by Microsoft experts. We're bringing the best of our customer and partner event experiences to the Windy City and online so you can participate in the festivities and discover how AI can enhance your organization.

In addition to seeing the latest technology firsthand, senior leaders and decision-makers are invited to learn more about how to lead in the era of AI and find robust networking opportunities.

If you're looking to expand your AI knowledge, create connections and push the boundaries of what we can accomplish together, there's no better event than Microsoft Ignite.

Visit our full global events catalog for a complete list of events, including some that could even be in your area. There, you can filter events by product, role or industry to find something specific to your needs or interests.

It's very exciting to bring you opportunities that showcase the growth and innovation that's being done at Microsoft to help you do more with AI. Whether you're a customer, partner, IT professional, decision-maker or developer, if you're looking to achieve more ― there's an event for you.

Tags: Microsoft AI Tour, Microsoft Build, Microsoft Ignite, Microsoft Inspire

AI is redefining digital transformation for every industry, including telecommunications. Every operator's AI journey will be distinct. But each AI journey requires cloud-native transformation, which provides the foundation for any organization to harness the full potential of AI, driving innovation, efficiency and business value.

This new era of AI will create incredible economic growth and represent a profound shift as a percentage impact on global GDP, which is just over $100 trillion. So, when we look at the potential value driven by this next generation of AI technology, we may see a boost to global GDP of an additional $7 trillion to $10 trillion.

Embracing AI will help operators unlock new revenue streams, deliver superior customer experiences and pioneer future innovations for growth.

Operators can now leverage cloud services that are adaptive, purpose-built for telecommunications and span from near edge on-premises environments to the far edges of Earth and space to monetize investments, modernize networks, elevate customer experiences and streamline business operations with AI.

Our aim is to be the most trusted co-innovation partner for the telecommunications industry. We want to help accelerate telco transformation and empower operators to succeed in the era of AI, which is why we are committed to working with operators, enterprises and developers on the future cloud.

At MWC in Barcelona this week, we are announcing updates to our Azure for Operators portfolio to help operators seize the opportunity ahead in a cloud- and AI-native future.

AI opens new growth opportunities for operators. The biggest potential is that operators, as they embrace this new era of cloud and AI, can also help their customers in their own transformation.

For example, spam calls and malicious activities are a well-known menace and are growing exponentially, and often impact the most vulnerable members of society. Besides the annoyance, the direct cost of those calls adds up. For example, in the United States, FTC data for 2023 shows $850 million in reported fraud losses stemming from scam calls.

Today, we are announcing the public preview of Azure Operator Call Protection, a new service that uses AI to help protect consumers from scam calls. The service uses real-time analysis of voice content, alerting consumers who opt into the service when there is suspicious in-call activity. Azure Operator Call Protection works on any endpoint, mobile or landline, and it works entirely through the network without needing any app installation.

In the U.K., BT Group is trialing Azure Operator Call Protection to identify, educate and protect their customers from potential fraud, making it harder for bad actors to take advantage of their customers.

We are also announcing the public preview of Azure Programmable Connectivity (APC), which provides a unified, standard interface across operators' networks. APC provides seamless access to Open Gateway for developers to create cloud and edge-native applications that interact with the intelligence of the network. APC also empowers operators to commercialize their network APIs and simplifies their access for developers and is available in the Azure Marketplace.

AI opens incredible opportunities to modernize network operations, providing new levels of real-time insights, intelligence and automation. Operators, such as Three UK, are already using Azure Operator Insights to eliminate data silos and deliver actionable business insights by enabling the collection and analysis of massive quantities of network data gathered from complex multi-vendor network functions. Designed for operator-specific workloads, operators tackle complex scenarios with Azure Operator Insights, such as understanding the health of their networks and the quality of their subscribers' experiences.

Azure Operator Insights uses a modern data mesh architecture for dividing complex domains into manageable sub-domains called data products. These data products integrate large datasets from different sources and vendors to provide data visibility from disaggregated networks for comprehensive analytical and business insights. Using this data product factory capability, operators, network equipment providers and solution integrators can create unique data products for one customer or published to the Azure Marketplace for many customers to use.

Today, we are also announcing the limited preview of Copilot in Azure Operator Insights, a groundbreaking, operator-focused, generative AI capability helping operators move from reactive to proactive and predictive in tangible ways. Engineers use the Copilot to interact with network insights using natural language and receive simple explanations of what the data means and possible actions to take, resolving network issues quickly and accurately, ultimately improving customer satisfaction.

Copilot in Azure Operator Insights is delivering AI-infused insights to drive network efficiency for customers like Three UK and participating partners including Amdocs, Accenture and BMC Remedy. Three UK is using Copilot in Azure Operator Insights to unlock actionable intelligence on network health and customer experience quality of service, a process that previously took weeks or months to assess, is now possible to perform in minutes.

Additionally, with our next-generation hybrid cloud platform, Azure Operator Nexus, we offer the ability to future-proof the network to support mission-critical workloads, and power new revenue-generating services and applications. This immense opportunity is what drives operators to modernize their networks with Azure Operator Nexus, a carrier-grade, hybrid cloud platform and AI-powered automation and insights — unlocking improved efficiency, scalability and reliability. Purpose-built for and validated by tier one operators to run mission-critical workloads, Azure Operator Nexus enables operators to run workloads on-premises or on Azure, where they can seamlessly deploy, manage, secure and monitor everything — from the bare metal to the tenant.

E& UAE is taking advantage of the Azure Operator Nexus platform to lower total cost of ownership (TCO), leverage the power of AI to simplify operations, improve time to market and focus on their core competencies. And operations at AT&T that took months with previous generations of technology now take weeks to complete with Azure Operator Nexus.

We continue to build robust capabilities into Azure Operator Nexus, including new deployment options giving operators the flexibility to use one carrier-grade platform to deliver innovative solutions on near-edge, far-edge and enterprise edge.

Read more about the latest Azure for Operator updates here.

Operators are creating differentiation by collaborating with us to improve customer experiences and streamline their business operations with AI. Operators are leveraging Microsoft's copilot stack and copilot experiences across our core products and services, such as Microsoft Copilot, Microsoft Copilot for M365 and Microsoft Security Copilot to drive productivity and improve customer experiences.

Last year AT&T launched Ask AT&T, a generative AI tool designed to help employees be more effective, creative and innovative. An early use case for Ask AT&T was to enable software developers to write and refine code — in many cases, AT&T is seeing a 25-50% productivity gain for their developers. Since its launch, the company has identified many other use cases for the solution. Today, approximately 68,000 employees have access to Ask AT&T and are using it for anything from vulnerability remediation in IT to analysis of the vast data flows the company manages on its network. The company is currently training Ask AT&T on contracts and financial materials to assist with HR questions and making our care representatives even more effective at supporting our customers. Teams at Lumen use Copilot to surface relevant policies, summarize tickets and easily find repair instructions from manuals. They can quickly create presentations, new business proposals and statements of work. Giving their workforce the digital tools they need to deliver dramatically improved customer experiences with greater ease is an essential part of the company's transformation. Telefónica evolves Kernel, its digital framework for the creation of advanced services, to the next level by adding new Microsoft Azure AI services to power every relevant use case with generative AI capabilities at scale. New Kernel 2.0 integrates all the digital services from the Telefónica Group to enable private-first, secure and standardized access to data and APIs, providing a foundation for value-added services that can be developed faster and with higher customer impact. Indonesian-based Telkomsel introduced Veronika — a virtual assistant that integrates Microsoft Azure OpenAI Service. Veronika is rooted in natural language processing and machine learning, and recommends telco packages based on customers' needs, and it can quickly and accurately address customer concerns. Since deploying Veronika, Telkomsel has been able to address rising call volumes without increasing agents. They have seen a significant drop in call volumes for their agents who are now handling 1,000 calls a day (down from 8,000 previously) and can now focus on increased cross-selling. To increase customer satisfaction, Vodafone will apply the power of Microsoft Azure OpenAI to deliver frictionless, real-time, proactive and hyper-personalized experiences across all Vodafone customer touchpoints, including its digital assistant TOBi (available in 13 countries). Vodafone employees will also be able to leverage the AI capabilities of Microsoft Copilot to transform working practices, boost productivity and improve digital efficiency.

An average operator spends 20% of annual revenue on capital expenditures. However, this investment does not translate into an equivalent increase in revenue growth. Operators need to empower their service teams with data-driven insights to increase productivity, enhance care, use conversational AI to enable self-service, expedite issue resolution and deliver frictionless customer experiences at scale.

Together with our partner ecosystem, we are investing in creating a comprehensive set of solutions for the telecommunications industry. This includes the Azure for Operators portfolio — a carrier-grade hybrid cloud platform, voice core, mobile core and multi-access edge compute, as well as our suite of generative AI solutions that holistically address the needs of network operators as they transform their networks.

As customers continue to embrace generative AI, we remain committed to working with operators and enterprises alike to future-proof networks and unlock new revenue streams in a cloud- and AI-native future.

Tags: AI, Azure for Operators, Azure Operator Call Protection, Azure Operator Insights, Azure Operator Nexus, Copilot in Azure Operator Insights

As global sustainability progress becomes more pressing, organizations need tangible, transformative solutions to help them move forward faster. Most have not progressed as much as they'd like. While 85% of executives report that sustainability is strategically important to their company, only 16% have integrated sustainability into their business strategies. Today, we're introducing new data and AI solutions in Microsoft Cloud for Sustainability that provide capabilities organizations need to progress in their sustainability journey. These include faster environmental, social and governance (ESG) data analytics and insights, an AI assistant to help accelerate impactful decision-making and reporting, and other advanced capabilities — all designed to help organizations move the needle faster.

YouTube Video Click here to load media Microsoft Fabric: A single analytics platform for sustainability data In May 2023, we unveiled Microsoft Fabric, an end-to-end, analytics SaaS platform that brings together all of an organization's enterprise data, including ESG data that the company may not have had access to before. With Microsoft Fabric, your teams can connect to data from anywhere, uncover insights using AI-powered analytics tools in a unified experience, and embed those insights in the applications your people use every day to drive action and improve decision-making. Now in preview, sustainability data solutions in Microsoft Fabric allows organizations to accelerate their time to insights and sustainability progress by providing out-of-the-box ESG data model, connectors and reporting. By connecting your ESG data with Microsoft Fabric you can turn volumes of sustainability data into meaningful insights and progress. Copilot and intelligent insights: AI assistance for better, faster decisions With centrally managed data and Copilot in Microsoft Sustainability Manager, now in preview, organizations can use powerful generative AI and natural language queries to gain faster lines of sight into and answers out of data. Organizations can ask Copilot questions, and Copilot will work across Microsoft Sustainability Manager to:

Quickly analyze environmental data and surface reduction opportunities in areas like carbon and water use, saving work and time. Talk to sustainability data — ask questions and retrieve answers quickly. Draft sustainability preparatory reports, reducing time and freeing up resources to focus on other priorities.

With intelligent insights in Microsoft Sustainability Manager, also in preview, organizations get an integrated and interactive AI model enabling deeper analysis of calculated emissions data. Using it, they can:

See where their data needs cleaning or where more complete data is needed. Scan the organization's data to identify both short- and long-term reduction opportunities. Fine-tune decisions based on historical trends, seasonality and data anomalies.

Södra, Sweden's largest forest-owner association with 52,000 owners of responsibly managed forests, processes sustainable raw materials from members' forests into renewable products. Using Microsoft Sustainability Manager with new AI capabilities, Södra has improved sustainability practices like reporting through automated flows that enable employees to more easily collect detailed information. "With the AI capabilities in Microsoft Sustainability Manager, we can be more productive, which is our ultimate goal," shares Cristian Brolin, Chief Digital Officer at Södra. "We found the Copilot feature very easy to use and my vision and ambition is that this will help accelerate the way we work. We are excited to see how this solution helps accelerate our sustainability progress." ESG value chain solution: Working with suppliers to drive reductions For most organizations, supplier activities can account for upward of 80-90% of their emissions footprint. Getting a handle on this data is critical to meeting reduction targets and reporting requirements. ESG value chain solution in Microsoft Sustainability Manager, now generally available, allows organizations to simplify data collection and provides a seamless and more secure process for suppliers. Organizations can run advanced analytics on ESG value chain data in one place and find emissions reduction opportunities within their value chain. At Microsoft, our procurement team has found that getting data out of siloed reports, spreadsheets, and systems into one data estate and sharing learnings with others is key to advancing our own progress as well as others' progress. Through more automated data gathering and management, such as through ESG value chain solution, we can not only more easily share data but also respond more nimbly to changing regulatory requirements. Driving sustainable business toward new growth opportunities According to research by the United Nations and Accenture, 98% of CEOs see sustainability as a core to their role, and two out of three are forming partnerships and taking action on sustainability. Many are also translating these actions into new business models and growth. Allegiant Stadium, home to the Las Vegas Raiders, is using Microsoft Sustainability Manager to support a state-of-the art and energy efficient facility that hosted the 2024 Super Bowl. Allegiant obtained LEED Gold certification, is 100% powered by renewable energy from local solar farms and is using Microsoft solutions to help organize data to inform decisions about resource utilization. Microsoft Cloud for Sustainability is designed to be extensible by partners, including experts operating at the intersections of sustainability and AI. For example, Avanade's copilot for ESG takes the friction out of regulatory compliance reporting, generating audit-ready reports against new and existing ESG frameworks. We are proud to have a growing, global ecosystem of sustainability solution, advisory and implementation partners working closely with us, to build and deploy capabilities that can address the broad, multifaceted and complex challenges in sustainability. At the intersection of sustainability and AI, the possibilities are vast for any organization to refocus business around transforming operations, reinventing brands and exploring new business opportunities. Microsoft AI delivers on this potential and the needs of our customers with a responsible approach to AI that's based on robust evaluation, verification and validation. Using the power of data and AI, sustainability progress isn't just possible but tangible. Organizations can gain efficiency and accuracy in decision-making and reporting as well as new ways of doing and growing business — on an exciting, shared path to a transformative future. Tags: Microsoft Cloud for Sustainability, Microsoft Copilot, Microsoft Fabric, Microsoft Sustainability Manager

As we approach Super Bowl weekend, we're thrilled to be a part of the festivities for the first time in four years. This year, we're proud to celebrate the transformative power of AI and Microsoft Copilot, showcasing peoples' "watch me" moments with Copilot enabling people to do things previously unattainable. With a simple sentence or two, you will see a budding entrepreneur turn a fledgling idea for a new product into an actionable business plan, a filmmaker's concept into a rich set of storyboards, and a fantasy football player's team come to life with a mascot image they can edit inline. Coincident with the launch of our Super Bowl ad, we are also launching a significant new update to our Microsoft Copilot experience on copilot.microsoft.com and our Copilot app on iOS and Android app stores. Today when you visit Copilot, you will see a more streamlined look and feel designed to help you bring your ideas to life and more easily gain understanding about the world. We have introduced a cleaner, sleeker look and feel for answers and a fun new carousel of suggested prompts to showcase the power of Copilot. Today marks exactly one year since our entry into AI-powered experiences for people with Bing Chat. In that year we have learned so many new things and seen the use of our Copilot experiences explode with over 5 billion chats and 5 billion images created to date which have led to sustained growth in Edge and Bing share. Now with Copilot as our singular experience for people looking to get more out of AI creation, we are today introducing further image creation capabilities. With Designer in Copilot, you can go beyond just creating images to now customize your generated images with inline editing right inside Copilot1, keeping you in the flow of your chat. Whether you want to highlight an object to make it pop with enhanced color, blur the background of your image to make your subject shine, or even reimagine your image with a different effect like pixel art,2 Copilot has you covered, all for free. If you're a Copilot Pro subscriber, in addition to the above, you can also now easily resize and regenerate images between square and landscape without leaving chat. Lastly, we will soon roll out our new Designer GPT inside Copilot, which offers an immersive, dedicated canvas inside of Copilot where you can visualize your ideas.

YouTube Video Click here to load media Copilot is free to use and works on Microsoft Edge, Chrome, Firefox and Safari. Or download the Copilot mobile app on iOS or Android. AI is the defining technology of our time. Microsoft's advancements in AI align with our company mission to empower every person and organization on the planet to achieve more. With Copilot, we're democratizing our breakthroughs in AI to help make the promise of AI real for everyone.

1Available in English in the United States, United Kingdom, Australia, India and New Zealand. 215 daily boosts included in Copilot, 100 daily boosts with a Copilot Pro subscription to be used for creative needs, faster image generation, and more detailed images.

This past year was one of technology's most exciting with the emergence of generative AI, as leaders everywhere considered the possibilities it represented for their organizations. Many recognized its value and are eager to continue innovating, while others are inspired by what it has unlocked and are seeking ways to adopt it. At Microsoft, we are focused on developing responsible AI strategies grounded in pragmatic innovation and enabling AI Transformation for our customers. As I talk to customers and partners about the outcomes they are seeing — and rationalize those against Microsoft's generative AI capabilities — we have identified four areas of opportunity for organizations to empower their AI Transformation: enriching employee experiences, reinventing customer engagement, reshaping business processes and bending the curve on innovation. With these as a foundation, it becomes easier to see how to bring pragmatic AI innovation to life, and I am proud of the impact we have made with customers and partners around the world. From developing customer-focused AI and cloud services for millions across Europe and Africa with Vodafone, to empowering customers and employees with generative AI capabilities with Walmart, I look forward to what we will help you achieve in the year ahead.

Dentsu drives creativity and growth for brands, supported by Microsoft Copilot.

Enriching employee experiences and shaping the future of work with copilot technology

Bayer employees are collaborating better on worldwide research projects and saving time on daily tasks with Copilot for Microsoft 365, while Finnish company Elisa is helping knowledge workers across finance, sales and customer service streamline routine tasks. Banreservas is driving employee productivity and enhancing decision-making, and Hong Kong's largest transportation companies — Cathay and MTR — are streamlining workflows, improving communications, and reducing time-consuming administrative tasks. Across professional services, KPMG has seen a 50% jump in employee productivity, Dentsu is saving hundreds of employees up to 30 minutes per day on creative visualization processes, and EY is making it easier to generate reports and access insights in near real-time with Copilot for Microsoft 365. In Malaysia, financial services organization PNB is saving employees time searching through documents and emails and AmBank employees are enhancing the quality and impact of their work. At Hargreaves Lansdown, financial advisers are using Copilot for Microsoft 365 and Teams to drive productivity and make meetings more inclusive. Avanade is helping sellers save time updating contact records and summarizing email threads with Copilot for Dynamics 365, while HSO Group, Vixxo, and 9altitudes are streamlining work for field and service teams.

Organizations are creating their own Generative AI assistants to help employees improve customer service.

Reinventing customer engagement with generative AI to deliver greater value and increased satisfaction

MECOMS is making it possible for utility customers to ask questions and get suggestions about how to reduce power consumption using Microsoft Fabric and copilot on their Power Pages portal. Schneider Electric has built a Resource Advisor copilot to equip customers with enhanced data analysis, visualization, decision support and performance optimization. California State University San Marcos is finding ways to better understand and personalize the student journey while driving engagement with parents and alumni using Dynamics 365 Customer Insights and Copilot for Dynamics 365. With Azure OpenAI Service, Adecco Group is bolstering its services and solutions to enable worker preparedness as generative AI reshapes the workforce, UiPath has already helped one of its insurance customers save over 90,000 hours through more efficient operations, and Providence has developed a solution for clinicians to respond to patient messages up to 35% faster. Organizations are building generative AI assistants to help employees save time, improve customer service and focus on more complex work, including Domino's, LAQO and OCBC. Within a few weeks of introducing its copilot to personalize customer service, Atento has increased customer satisfaction by 20% and team productivity by 30% while reducing operational errors by nearly 20%, and Turkey-based Setur is personalizing travel planning with a chatbot to customize responses in multiple languages for its 60,000 daily users. In the fashion industry, Coats Digital launched an AI assistant in six weeks to make customer onboarding easier. Greece-based ERGO Insurance partnered with EBO to provide 24/7 personalized assistance with its virtual agent, and H&R Block introduced AI Tax Assist to help individuals and small business owners file and manage their taxes confidently while saving costs.

Novo Nordisk is building out GitHub Copilot integration to decrease repetitive research and engineering tasks.

Reshaping business processes to uncover efficiencies, improve developer creativity and spur AI innovation

Siemens built its own industrial copilot to simplify virtual collaboration of design engineers and front-line workers, accelerate simulation times and reduce tasks from weeks to minutes. With help from Neudesic, Hanover Research designed a custom AI-powered research tool to streamline workflows and identify insights up to 10 times faster. With Microsoft Fabric, organizations like the London Stock Exchange Group and Milliman are reshaping how teams create more value from data insights, while Zeiss is streamlining analytics workflows to help teams make more customer-centric decisions. Volvo Group has saved more than 10,000 manual hours by launching a custom solution built with Azure AI to simplify document processing. By integrating GitHub Copilot, Carlsberg has significantly enhanced productivity across its development team; and Hover, SPH Media, Doctolib and CloudZero have improved their workflows within an agile and secure environment. Mastery Logistics Systems and Novo Nordisk are using GitHub Copilot to automate repetitive coding tasks for developers, while Intertech is pairing it with Azure OpenAI Service to enhance coding accuracy and reduce daily emails by 50%. Swiss AI-driven company Unique AG is helping financial industry clients reduce administrative work, speed up existing processes and improve IT support; and PwC is simplifying its audit process and increasing transparency for clients with Azure OpenAI Service. By leveraging Power Platform, including AI and Copilot features, Epiq has automated employee processes, saving over $500,000 in annual costs and 2,000 hours of work each month, PG&E is addressing up to 40% of help desk demands to save more than $1 million annually, and Nsure is building automations that reduce manual processing times by over 60% and costs by 50%. With security top of mind, WTW is using Microsoft Copilot for Security to accelerate its threat-hunting capabilities by making it possible for cyber teams to ask questions in natural language, while LTIMindtree is planning on using it to reduce training time and strengthen security analyst expertise.

VinBrain is harnessing Microsoft's cutting-edge AI technologies to transform healthcare in Vietnam.

Bending the curve on innovation across industries with differentiated AI offerings

To make disaster response more efficient, nonprofit Team Rubicon is quickly identifying and engaging the right volunteers in the right locations with the help of Copilot for Dynamics 365. Netherlands-based TomTom is bringing the benefits of generative AI to the global automotive industry by developing an advanced AI-powered voice assistant to help drivers with tasks like navigation and temperature control. In Vietnam, VinBrain has developed one of the country's first comprehensive AI-powered copilots to support medical professionals with enhanced screening and detection processes and encourage more meaningful doctor-patient interactions. Rockwell Automation is delivering industry-first capabilities with Azure OpenAI Service to accelerate time-to-market for customers building industrial automation systems. With a vision to democratize AI and reach millions of users, Perplexity.AI has brought its conversational answer engine to market in six months using Azure AI Studio. India's biggest online fashion retailer, Myntra, is solving the open-ended search problem facing the industry by using generative AI to help shoppers figure out what they should wear based on occasion. In Japan, Aisin Corp has developed a generative AI app to empower people who are deaf or hard of hearing with tasks like navigation, communication and translation; and Canada-based startup Natural Reader is making education more accessible on-the-go for students with learning differences by improving AI voice quality with Azure AI. To solve one of the most complex engineering challenges — the design process for semiconductors — Synopsys is bringing in the power of generative AI to help engineering teams accelerate time-to-market.

As organizations continue to embrace AI Transformation, it is critical they develop clarity on how best to apply AI to meet their most pressing business needs. Microsoft is committed to helping our customers and partners accelerate pragmatic AI innovation and I am excited by the opportunities before us to enrich employee experiences, reinvent customer engagement, reshape business processes and bend the curve on innovation. As a technology partner of choice — from our differentiated copilot capabilities to our unparalleled partner ecosystem and unique co-innovation efforts with customers — we remain in service to your successful outcomes. We are also dedicated to preserving the trust we have built through our partnership approach, responsible AI solutions and commitments to protecting your data, privacy and IP. We believe this era of AI innovation allows us to live truer to our mission than ever before, and I look forward to continuing on this journey with you to help you achieve more.

Tags: AI, Azure, Azure AI, Azure OpenAI Service, Copilot for Dynamics 365, Copilot for Microsoft 365, Innovation, Microsoft AI, Microsoft Cloud, Microsoft Copilot for Security, Microsoft Partners, Microsoft Power Platform

As we kick off a new year, we're thrilled to see people increasingly using and loving Microsoft Copilot for work and life. Our goal is to empower every person and every organization on the planet to achieve more by bringing Copilot, the everyday AI companion, to millions of people around the world. We have reached another milestone in this mission with more than 5 billion chats and more than 5 billion images to date. As Copilot continues to earn preference and usage, we're receiving valuable feedback on how to improve. Two examples: First, there are a set of Copilot power users like creators, researchers, programmers and others who want more rapid access to the very latest we have to offer. And second, our Microsoft 365 customers want access to Copilot in the Microsoft 365 apps for personal use. To help address those needs, today we're delighted to announce more options for power users, creators and anyone looking to take their Copilot experience to the next level. This begins with the introduction of Copilot Pro, a new premium subscription for individuals that provides a higher tier of service for AI capabilities, brings Copilot AI capabilities to Microsoft 365 Personal and Family subscribers, and new capabilities, such as the ability to create Copilot GPTs. We are also announcing the general availability of our Copilot app for iOS and Android phones. Finally, we're excited to bring Copilot for Microsoft 365 to more commercial customers by expanding the availability to businesses of all sizes, including small- and medium-sized businesses, starting today. Introducing Copilot Pro: Supercharge your creativity and productivity Today we're announcing the availability of Copilot Pro, a new subscription that delivers the most advanced features and capabilities of Microsoft Copilot to individuals looking to supercharge their Copilot experience. Whether you need advanced help with writing, coding, designing, researching or learning, Copilot Pro brings greater performance, productivity and creativity. Copilot Pro provides:

A single AI experience that runs across your devices, understanding your context on the web, on your PC, across your apps and soon on your phone to bring the right skills to you when you need them.

Access to Copilot in Word, Excel[i], PowerPoint, Outlook, and OneNote on PC, Mac and iPad for Microsoft 365 Personal and Family subscribers. Priority access to the very latest models – starting today with OpenAI's GPT-4 Turbo. With Copilot Pro you'll have access to GPT-4 Turbo during peak times for faster performance and, coming soon, the ability to toggle between models to optimize your experience how you choose. Enhanced AI image creation with Image Creator from Designer (formerly Bing Image Creator) – ensuring it's faster with 100 boosts per day while bringing you more detailed image quality as well as landscape image format. The ability to build your own Copilot GPT – a customized Copilot tailored for a specific topic – in our new Copilot GPT Builder (coming soon) with just a simple set of prompts.

You can subscribe to Copilot Pro today for $20 per month/per user.

YouTube Video Click here to load media Expanding Copilot for Microsoft 365 to businesses of all sizes While Copilot Pro is our best experience for individuals, Copilot for Microsoft 365 is our best experience for organizations. Copilot for Microsoft 365 became generally available for enterprises in November. As we said earlier this year, 40% of the Fortune 100 participated in our Early Access Program, and since GA for enterprise, customers like Visa, BP, Honda, Pfizer, and partners like Accenture, KPMG and PwC are already using Copilot — which means thousands of people across industries and sectors have started working in new ways, with an AI-powered copilot at their side. Today we are excited to announce that Copilot for Microsoft 365 is now available for organizations of all sizes — with no seat minimum. We are also enabling our partners to help every business become AI-powered. Today's updates include:

Copilot for Microsoft 365 is now generally available for small businesses with Microsoft 365 Business Premium and Business Standard Customers can purchase between one and 300 seats for $30 per person per month. We're removing the 300-seat purchase minimum for commercial plans and making Copilot available for Office 365 E3 and E5 customers (A Microsoft 365 license was previously required). Commercial customers can now purchase Copilot for Microsoft 365 through our amazing network of Microsoft Cloud Solution Provider partners. Last month, we also announced eligibility of Copilot for Microsoft 365 for education faculty and staff.

Copilot for Microsoft 365 is even more powerful for organizations because it works across your entire universe of data at work — including emails, meetings, chats, documents and more, plus the web. With natural language prompts like "Tell my team how we updated the product strategy," Copilot can generate a status update based on the morning's meetings, emails and chat threads. Copilot is also integrated into the apps millions of people use every day, including Microsoft Teams (which is not available with Copilot Pro). Copilot jump-starts your creativity in Word, analyzes data in Excel, designs presentations in PowerPoint, triages your Outlook inbox, summarizes meetings in Teams – whether you attended or not – and so much more. Backed by enterprise-grade security, privacy, and compliance, and Microsoft's Customer Copyright Commitment, we can't wait to see how businesses of all sizes achieve more using AI. Learn more on the Microsoft 365 blog.

Introducing new features in Copilot As we expand the availability of Copilot to even more people, we continue to offer a great free experience for anyone interested in exploring how Copilot can transform productivity and creativity using AI. Today we're excited to share additional updates to Copilot. You can get started by visiting copilot.microsoft.com.

Copilot GPTs – Today we're announcing Copilot GPTs. Copilot GPTs let you customize the behavior of Microsoft Copilot on a topic that is of particular interest to you. A handful of Copilot GPTs will start to roll out beginning today with specific purposes such as fitness, travel, cooking and more. Soon, Copilot Pro users will also be able to create their own Copilot GPTs using Copilot GPT Builder. Stay tuned for more on this experience as we get closer to availability. Copilot mobile app – The Copilot mobile app is now available for Android and iOS. The Copilot app gives you the power of Copilot on the go as your Copilot queries and chats will roam across your phone and PC. The Copilot mobile app includes the same capabilities of Copilot on your PC including access to GPT-4, Dall-E 3 for image creation, and the ability to use images from your phone when chatting with Copilot. Download the app from the Google Play Store or the Apple App Store. Copilot in the Microsoft 365 mobile app – We're also adding Copilot to the Microsoft 365 mobile app for Android and iOS for individuals with a Microsoft account. This new feature is rolling out over the next month. Access Copilot right inside the app and easily export the content you create to a Word or PDF document. Download the app from the Google Play Store or the Apple App Store.

With today's announcements, we continue to bring Copilot to more customers with more options that work for them. Whether you're looking to get started with Copilot for free, want to supercharge your Copilot experience with Copilot Pro or are an SMB or Enterprise customer looking to increase your productivity in new ways with Copilot for Microsoft 365, there's a Copilot experience for everyone.

[i] Currently in preview, English only Tags: AI, Copilot Pro, Microsoft 365 Copilot

Time and money are two of the most valuable resources people have. And one of the most popular ways for Americans to save on both, is shopping online. According to the U.S. Bureau of Labor Statistics, American families spend six hours per week on household planning and shopping. Many of those families do so digitally at Walmart. The world's largest retailer, famous for its low prices, generated over $82 billion in e-commerce sales in fiscal year 2023 and is steadily growing its active digital customers. In 2024, the company is further improving the digital shopping experience, building generative AI into its search function to deliver customers a helpful and intuitive browsing experience. Using a combination of Walmart proprietary data and technology and large language models, including those available in Microsoft Azure OpenAI Service, as well as retail-specific models built by Walmart, the new design serves up a curated list of the personalized items a shopper is looking for. At CES 2024, Walmart President and CEO Doug McMillon and Microsoft Chairman and CEO Satya Nadella introduced the new AI innovations to benefit Walmart customers.

YouTube Video Click here to load media Study after study shows that AI is driving impact and value across businesses.

McKinsey reports AI will create $400-660 billion in value for the retail and consumer goods industry. FTI Consulting data shows nearly 80% of online shoppers believe AI personalization can enhance their online experience. An IDC study commissioned by Microsoft found that for every $1 a retail and consumer packaged goods company invests in AI, it is seeing a return of $3.45.

Generative AI-powered search Walmart's new generative AI-powered search function will be available across iOS, Android and the company's website. Photo courtesy of Walmart. Walmart built an all-new generative AI-powered search function across iOS, Android and its own website. The new capability is specifically designed to understand the context of a customer's query and generate personalized responses. Soon, customers will have a more interactive and conversational experience, get answers to specific questions, and receive personalized product suggestions. For example, a parent planning a birthday party for a child that loves unicorns. Instead of multiple searches for unicorn-themed balloons, napkins, streamers, etc., the parent can simply ask the question "Help me plan a unicorn-themed party for my daughter." One of the reasons why Walmart and other leading retailers are choosing Azure OpenAI Service is the ability to access the most advanced AI models in the world while backed by the enterprise-grade capabilities found in Microsoft Azure including security, compliance and regional availability. Generative AI in retail is particularly exciting as it can help usher in a new way of shopping; shifting from "scroll searching" to "goal searching," which makes the digital shopping experience more seamless and intuitive. Don't forget about the associates! Walmart's generative AI-powered "My Assistant" app helps associates with a range of tasks, from summarizing long documents to assisting in the creation of new content. Photo courtesy of Walmart. Both Walmart and Microsoft share an aligned vision around how AI can help organizations and their people be more productive, more satisfied in their work and go on to solve the most pressing problems. Over the summer, Walmart launched a new tool, giving its 50,000 non-store associates access to its new "My Assistant" app, which was created and built by Walmart and leverages a unique build of Walmart proprietary data, technology and large language models in Azure OpenAI Service. In just a few months, the app has already made a big impact for associates, assisting with a range of tasks, from summarizing long documents to assisting in the creation of new content. A history of success This is just the latest evolution in the Walmart and Microsoft relationship. In 2018, the companies established a strategic partnership to drive Walmart's digital transformation. As one of Walmart's preferred and strategic cloud providers, Microsoft has supported Walmart on a variety of different cloud projects. From powering access to electronic health records at its in-store clinics to centralizing, democratizing and optimizing data, the companies have and will continue to collaborate to accelerate innovation. Other ways to AI While Walmart's approach to generative AI uses a combination of large language models, retail-specific models, and their own proprietary data built on top of Azure OpenAI Service, other retailers may have different scenarios that require an alternative approach. That's why Microsoft offers a broad portfolio of prebuilt, low-code and pro-code solutions that can benefit all retailers and shoppers, no matter where they are on their respective journey. Visit the Microsoft Cloud for Retail page to learn more. Tags: AI, Azure OpenAI Service, Microsoft Cloud for Retail

Today, we're announcing general availability of Microsoft Cloud for Sovereignty across all Azure regions. Microsoft Cloud for Sovereignty helps governments meet their compliance, security, and policy requirements while utilizing the cloud to provide superior value to their citizens.

Governments around the world must meet a wide range of national and regional compliance requirements for applications and workloads, including governance, security controls, privacy, and in some cases data residency and sovereign protections. Until now, most solutions for addressing these regulatory requirements have relied upon private cloud and on-premises environments, creating a lag in adoption of scalable, secure and resilient cloud capabilities.

Microsoft Cloud for Sovereignty — offering governance, security, transparency and sovereign technology — supports the digital transformation of government customers unlike any other cloud provider in the world. Customers gain automation of best practices to address regulatory requirements while benefiting from the rapid pace of innovation in the hyperscale cloud.

When we talk with government customers, we hear common questions in terms of achieving data sovereignty in the cloud. These often include questions around (1) residency, security and compliance of the hyperscale cloud; (2) controls for data access; and (3) the complexity of addressing regulations that vary by country.

First, Microsoft Cloud for Sovereignty is built on the foundation of more than 60 cloud regions, providing industry-leading cybersecurity along with the broadest compliance coverage. Microsoft offers the most regions of any cloud provider. Customers can implement policies to contain their data and applications within their preferred geographic boundary, in alignment with national or regional data residency requirements.

Second, Microsoft Cloud for Sovereignty provides sovereign controls to protect and encrypt sensitive data and control access to that data, enabled by sovereign landing zones and Azure Confidential Computing.

A sovereign landing zone is a type of Azure landing zone designed for organizations that need government-regulated privacy, security and sovereign controls. Organizations can leverage landing zones as a repeatable best-practice for secure and consistent development and deployment of cloud services. As many government organizations face a complex and layered regulatory landscape, utilizing sovereign landing zones makes it much easier to design, develop, deploy and audit solutions while enforcing compliance with defined policies.

Customers can also leverage Azure Confidential Computing to secure sensitive and regulated data even while it's being processed in the cloud. Azure Confidential Computing protects data in memory in hardware-based trusted execution environments, helping prevent unwanted data access by the cloud provider, administrators and users. Customers can benefit from this capability on Azure without having to change existing applications.

Third, customers can adopt specific, sovereignty-focused Azure policy initiatives to address the complexity of compliance with national and regional regulatory requirements. These help customers create cloud guardrails and enforce specific regulations more easily. Customers can layer these policy initiatives to form a complete solution for their specific needs, and use deployment automation to ensure consistency, best practices and save time.

For example, our customers typically start with Azure Cloud Security Benchmark then layer the new Sovereignty Policy Baseline to ensure best practices for digital sovereignty. Then, customers can leverage a regional specific layer, such as Baseline informatiebeveiliging overheid (BIO), a foundational standards framework for information security within the Netherlands government, or the strategic guidelines for migration to the cloud of data and digital services of the Italian Public Administration National Cybersecurity Agency (ACN).

Additionally, the new Cloud Security Alliance Cloud Controls Matrix (CSA CCM v4) policy initiative is oriented toward a global standard that many other regional standards build upon.

Through our product development lifecycle, we've had the privilege of collaborating with customers around the world and their trusted partners. These collaborations continue to inform both our technical roadmap as well as solution design and delivery.

In the Netherlands, the National Cyber Security Center (NCSC-NL) has achieved significant traction in their work to establish a cloud center of excellence, deploying Microsoft Cloud for Sovereignty to accelerate innovation in the cloud while still managing sovereign controls. Arnoud van Petersen, CIO & Head of IT Services at NCSC-NL, explains: "Our innovation teams are exploring ways to use advanced data and AI capabilities to reduce the time required to interpret signal data and produce advisories from days to minutes, delivering an EU leading rapid response and information sharing in a dynamic cyber threat landscape."

The municipality of Amsterdam is partnering with InSpark (a subsidiary of Royal KPN) to bring on-premises datacenter workloads to the cloud in compliance with the relevant classification within the Dutch BIO regulation. Patrick Scholte, Director Platforms & Development, shares: "Municipality of Amsterdam opted for Cloud for Sovereignty primarily due to privacy considerations, specifically in compliance with GDPR. Beyond its effective implementation of innovative security technologies, Microsoft Cloud for Sovereignty plays a crucial role in facilitating significant internal advancement for us.

"As an organization, it is imperative for us to address the establishment of processes, policies, and guidelines, as an addition to the use of innovative technologies that pertain to data sovereignty," Scholte adds. "These measures are essential in mitigating evolving risks, and the use of Microsoft Cloud for Sovereignty allows us to encode and audit them effectively."

In Italy, Leonardo is now migrating their public and private customers to a secure public cloud service on the new Italy North Azure region, leveraging sovereign landing zones and advanced technologies such as Azure Confidential Computing. "We have chosen Microsoft Cloud for Sovereignty to deliver solutions for our customers in compliance with digital sovereignty attributes and regulations, leveraging the economies of scale, security levels and pace of innovation that today are only available with hyperscale clouds," says Massimo Tedeschi, Senior Vice President Cyber & Security Engineering, Leonardo.

In Belgium, Proximus has selected Microsoft Cloud for Sovereignty in its work with customers across Belgium, Luxembourg and the Netherlands to deliver cloud capabilities along with advanced security layers and sovereignty controls to protect sensitive data in the cloud.

"Our customers expect us to provide them with the highest data protection and sovereignty, says Anne-Sophie Lotgering, Enterprise Market Lead at Proximus. "That is exactly why we collaborate with Microsoft to offer advanced cloud capabilities together with the highest security and privacy controls to our Benelux customers."

As the Microsoft Cloud for Sovereignty enters general availability today, we're also announcing new capabilities in preview. These solutions underscore our continued investment in a rapid pace of innovation to advance sovereignty in the hyperscale cloud:

Drift analysis capabilities: Ongoing administration and maintenance can potentially introduce changes that don't comply with policies, resulting in the deployment beginning to drift out of compliance over time. The new drift analysis tool inspects your deployment and generates a list of non-compliant settings, as well as a severity rating, making it easier to identify any discrepancies to remediate and verify the compliance of specific environments. Transparency logs: Gives eligible customers visibility into the instances where Microsoft engineers have accessed customer resources through Just-In-Time (JIT) access, most commonly in response to a customer support request. With this update, customers can now request access to the preview feature via the Azure portal. New configuration tools in the Azure portal: Allows customers to create a new tailored sovereign landing zone in two simple steps using a guided experience. Learn more and get started today

Get started with Microsoft Cloud for Sovereignty today to advance government innovation in the cloud.

Visit microsoft.com/sovereignty for the latest news, visit Microsoft Learn, and reach out to your Microsoft representative to learn more.

This year will be remembered as the moment that we, as individuals, began to harness the power of AI in our daily lives. The last 10 months reflect years of AI research, close partnerships, and breakthrough innovations coming together. This culmination is now unifying our product vision to empower every person and every organization on the planet to achieve more. We started with the introduction of Bing Chat, and the feedback was incredible! Right away, people began to change how they search on the Internet, shop, code, prepare for job interviews, improve their gaming skills, and create beautiful documents and images. We proceeded to incorporate these capabilities into Microsoft 365, Microsoft Edge and Windows, unlocking new scenarios with ever greater context and intelligence for people. Two weeks ago, we took the significant step to bring together all of this under one brand and one experience that we call Microsoft Copilot, launching and making it accessible to anyone on any device. We're thrilled by the impact it is having on the industry and, more importantly, on the lives of hundreds of millions of people. Recent survey data shows that Copilot for Microsoft 365 makes people more productive and faster overall in tasks like searching and writing. What's next? As we set our sights on 2024, we're committed to bringing more innovation and advanced capabilities to Copilot to provide you with the leading way to benefit from AI. Here are some incredible new features that we have begun testing that you will see roll out soon: GPT-4 Turbo – Soon, Copilot will be able to generate responses using OpenAI's latest model, GPT-4 Turbo, enabling you to tackle more complex and longer tasks. This model is currently being tested with select users and will be widely integrated into Copilot in the coming weeks. New DALL-E 3 Model – You can now use Copilot to create images that are even higher quality and more accurate to the prompt with an updated DALL-E 3 model. These capabilities are available to you now by visiting bing.com/create or by prompting Copilot to create an image. Inline Compose with rewrite menu – With Copilot, Microsoft Edge users can easily write from most websites. Just select the text you want to change and ask Copilot to rewrite it for you. Coming to all Edge users soon. Multi-Modal with Search Grounding Multi-Modal with Search Grounding – We are combining the power of GPT-4 with vision with Bing image search and web search data to deliver better image understanding for your queries. This new capability will be available soon.

Code Interpreter – We are developing a new capability that will enable you to perform complex tasks such as more accurate calculations, coding, data analysis, visualization, math and more. We are gathering feedback on these capabilities from a select set of users and plan to make it widely available soon. Code Interpreter

Deep Search – Coming soon to Bing, Deep Search harnesses the power of GPT-4 to deliver optimized search results for complex topics. Activating Deep Search expands search queries into more comprehensive descriptions to deliver more relevant results. More information can be found on our Bing Blog.

To share a sense of the breadth of what Copilot can do for you, we've created a list of some of our favorite use cases for you to try right now – inspired* by our community of fans and preview testers.

YouTube Video Click here to load media Try one of the prompts below and follow your curiosity. Just copy and paste into Copilot at copilot.microsoft.com, in Copilot in Bing or directly from the Copilot tab in Windows 11.

Education How can I design and improve activity plans for elementary school students to experience and learn about different plants?

Gaming What are the best methods and tools for stealth and hacking in Starfield?

Shopping Explain how to choose a mattress based on firmness level and sleeping preferences. What are the different firmness levels and how do they affect comfort and support?

Travel Plan a 4-day itinerary for visiting Paris and Amsterdam. What are the must-see attractions, activities, and restaurants in each city?

Culture & Art Analyze Michelangelo's legacy as an artist and a person. What are the achievements and influences on Michelangelo's art and life and how did he shape society?

Education Explain like I'm five the structure of city-states and the characteristics of civilizations throughout Mesopotamia.

History Are depictions of pirates in movies and novels accurate?

Design How can I make my living room look more open and refreshing? What are the tips and tricks for choosing and arranging furniture, colors, lighting, and accessories?

Education Identify and describe the flowers and plants that attract bees and how I can grow them in my garden. How do you pollinate a garden by hand?

Culture & Art Create an image I can use as inspiration for a tattoo. It should be minimalist in design and feature a sun, moon and golden retriever.

Jobs How can I get into the semiconductor industry and what are some related fields of study? What are the skills, qualifications, and experiences required?

Travel What advice do you have for hiking and sightseeing in Big Four Ice Caves?

Science Explain the theory of parallel universes. What is the concept and evidence of parallel universes?

Shopping Recommend some DAC (digital-to-analog converter) options for audiophiles. What are the features, benefits, and drawbacks of each option?

Culture & Art Who were some of the most influential female musicians of the 1960s and how did they impact culture?

Food Suggest some recipe ideas for a vegetarian dinner on a low-carb diet. What are the ingredients and recipes of each dish?

Health Create an upper body workout routine for beginners focused on chest and arms using body weight techniques at home. What are the exercises, sets, reps, and rest periods for each session?

Tech & Software Design an algorithm to find the majority book preference among a group of friends for a book club. What are the inputs and outputs of the algorithm?

Work Discuss how to work effectively with others in a professional work environment. What are the skills and qualities of a good team player?

Food What are the best places to eat and drink in Renton, Washington? What are the specialties, prices, and ratings of each place?

Jobs What are some tips for someone who has been out of the workforce for a few years but is looking to get back into it?

Jobs Compare and contrast the MBA and MHA degrees for a nurse executive. What are the career opportunities and outcomes of each degree?

Travel Plan a 5-day summer trip to Hawaii that includes a visit to Diamond Head.

Writing Write a congratulatory message for a small gift store's fifth anniversary. Praise their service and atmosphere and express your gratitude and loyalty as a customer.

Writing Write a story about a cat named Babbi who escapes outside and gets scared. How does Babbi escape and how does he return home?

Shopping Research and summarize the internet's reviews about Meta's Quest 3. What are the features, specs, and pros and cons of the headset?

Images Create an image I can put on a holiday card featuring 10 English bulldogs.

*Prompts seen in this post showcase some of our favorite use cases of the consumer version of Copilot, formerly known as Bing Chat, since entering preview in February. All prompts seen below are inspired by popular conversation topics and were generated by Microsoft. No real user prompts are shown above. Tags: AI, Copilot for Microsoft 365, Copilot in Bing, Microsoft Copilot

At Microsoft, we've asked our communications team to be innovators of AI. To explore and experiment with how we can all use it in our everyday work. We've urged the team not to forget their unique expertise and value — the art of communications — but to welcome some science into the process, to be a leader in this new stage of communications.

So, for the last few months we've been experimenting with Copilot for Microsoft 365, which is now generally available for enterprises. We're using Copilot in Teams to story mine with spokespeople and anticipate coverage after interviews. We're using Copilot in Word to start blogs and draft plans. We're using chat to test messages, brainstorm and expand our thinking. And Copilot in PowerPoint is helping us create decks that drive clarity and communicate our vision.

We've been feeling the impact in our day-to-day but wanted to quantify it — so we did a bit of research ourselves. Using the same methodology featured in our latest Work Trend Index, we surveyed 80 people from our comms and marketing organization and found that— like most knowledge workers —many employees suffer from meeting (70%) and information overload (86%) daily, so it's not surprising that 80% struggle with having enough focus time each day.

The good news? AI is helping. According to the survey:

86% say Copilot makes them more productive and 81% say it helps them complete tasks faster. 89% say Copilot helps jump-start the creative process and 78% say Copilot improves the quality of their work. 70% say Copilot reduces time spent on tasks they don't enjoy; specifically, 76% say Copilot reduces mental effort on mundane tasks and 71% say Copilot reduces time spent searching for information.

And the most exciting one for me: 84% say they don't want to go back to working without Copilot. And I feel the same way. 😊

Now, it's still early days and the tools aren't perfect. But to help other communicators consider how AI can improve their work days, here are five ways the team and I are using AI to be more productive, creative, and to make the work we do a little more fun.

1. Meetings and interviews That meeting overwhelm we feel isn't just constant back-to-backs; it's also frequent double-booking and conflicts (if your calendar is anything like mine). Copilot in Teams makes it easier to prioritize your most important meetings and catch up on those you've missed — without having to watch the full recording. Some prompts:

Summarize what was discussed in the meeting in short bullet points. Were there any action items for me? Were there any unresolved decisions?

During or after a meeting, Copilot can also help you recall what was discussed more accurately, note a great quote or detail for a story, evaluate complex topics, or anticipate coverage following an interview. For example, you can ask:

What was the exact feedback shared about the comms plan that was presented? The engineer we spoke with shared an interesting anecdote about that I'd like to include in a story I'm working on. Can you remind me exactly what she said? Put the pros and cons of what the team is discussing about the announcement strategy in a table and recommend a course of action. After a press interview, ask, "What do you think the reporter will take away from this conversation? Use the transcript as a base document.

2. Message testing Clarity is key in communications of course, but it's perhaps even more critical in another setting: the military. Napoleon, the famous general, is known for bringing a corporal to meetings when his generals were presenting war plans. He would ask the corporal, "Having heard these plans, would you know what to do?" If the answer was no, he would ask the generals to re-write their plans.

What does this have to do with AI? I've been using Copilot as my corporal to provide fresh perspective. Some examples:

I'll feed Copilot a blog and ask it to articulate the three key messages; if they're off, I'll refine. I'll ask Copilot to poke holes in a statement we're making on a tricky topic, like sustainability or responsible AI. I'll ask it things like, "If you wanted to disrupt this argument, what would you say?"

With Copilot as my brainstorming partner, I can identify blind spots, sharpen our positions, and expand my thinking so we're clear and thoughtful in our positionin

3. Preparing spokespeople To help our spokespeople prepare for press interviews, we're using Copilot to anticipate questions and develop FAQs. Some example prompts:

Pretend you are a tech reporter. Share the top five questions you would have for me if presented the information in this narrative document: /[X]. Pretend you are a PR person preparing an executive for an interview with a top tech reporter, at . She's been writing about AI quite a bit. What questions do you think the reporter will ask a tech executive in an interview about how AI will change work? I'm doing a press interview about what are the top three questions someone unfamiliar with the matter would have on the topic? Help me draft responses to this list of questions . Answers should be brief, but conversational in tone.

4. Writing comms plans and blogs Now, many communicators are hired because of their unique ability to write. And as a writer myself, I can tell you that skill isn't going anywhere — but I've found Copilot to be useful in helping me get started, keep going, and refine my content. It's helping me do what I do best — even better.

In Word, I'll highlight a paragraph in a plan or blog that I'm getting hung up on and ask Copilot to suggest a few other ways to write it. Others on my team use Copilot to draft a plan for an upcoming announcement. They'll give it a brain dump of the elements they want to see in the plan such as what they're announcing, key messages, timelines, press targets, and spokespeople. Now, it doesn't produce a full plan end-to-end, but it gives them a solid start. When I'm at the end of a blog and am stuck for an ending, I like to ask Copilot to write a short, but compelling closing paragraph. And because we're using Copilot for Microsoft 365, sensitive information, like launch plans and dates, remain protected. Finally, when the blog or LinkedIn post is done and I've used up all my creativity and still need a title or headline, I can ask for five headlines, then jump off from there.

5. Creating presentations Sometimes the way we present our ideas is just as important as the actual content. So, it's no surprise that one of my team's favorite Copilot use cases is turning a Word document, like a PR plan, into a PowerPoint presentation.

Simply open a new presentation in PowerPoint, select the Copilot icon in the ribbon and once the side pane pops up, click "Create presentation from file…" Select the document and let Copilot go to work building a deck.

From there, you can ask Copilot to add an end slide, make all the titles a certain font and color, or ask it to update one of the images to a photograph that's more to your liking. It's massively cut down the time it takes my team to build decks — meaning they can focus more on content and less on copy-pasting.

Like any new tool or habit the key is practice, practice, practice. If you already have Copilot for Microsoft 365, you can start experimenting with the prompts above today — or test out some of my communications prompts for Copilot in Bing for free. You can also check out this article on WorkLab that shows you how to write a great prompt.

I'll keep practicing too — and sharing along the way.

This blog was updated at 10:15 p.m. on Nov. 21 to reflect the latest statements from Sam Altman and Satya Nadella.

Sam Altman: i love openai, and everything i've done over the past few days has been in service of keeping this team and its mission together. when i decided to join msft on sun evening, it was clear that was the best path for me and the team. with the new board and w satya's support, i'm looking forward to returning to openai, and building on our strong partnership with msft.

Satya Nadella: We are encouraged by the changes to the OpenAI board. We believe this is a first essential step on a path to more stable, well-informed, and effective governance. Sam, Greg, and I have talked and agreed they have a key role to play along with the OAI leadership team in ensuring OAI continues to thrive and build on its mission. We look forward to building on our strong partnership and delivering the value of this next generation of AI to our customers and partners.

Posted Nov. 19, 2023 Satya Nadella: We remain committed to our partnership with OpenAI and have confidence in our product roadmap, our ability to continue to innovate with everything we announced at Microsoft Ignite, and in continuing to support our customers and partners. We look forward to getting to know Emmett Shear and OAI's new leadership team and working with them. And we're extremely excited to share the news that Sam Altman and Greg Brockman, together with colleagues, will be joining Microsoft to lead a new advanced AI research team. We look forward to moving quickly to provide them with the resources needed for their success.

Posted Nov. 17, 2023 Satya Nadella: As you saw at Microsoft Ignite this week, we're continuing to rapidly innovate for this era of AI, with over 100 announcements across the full tech stack from AI systems, models, and tools in Azure, to Copilot. Most importantly, we're committed to delivering all of this to our customers while building for the future. We have a long-term agreement with OpenAI with full access to everything we need to deliver on our innovation agenda and an exciting product roadmap; and remain committed to our partnership, and to Mira and the team. Together, we will continue to deliver the meaningful benefits of this technology to the world.

As we reach the end of 2023, nearly every industry is undergoing a collective transformation – discovering entirely new ways of working due to AI advancements.

Microsoft Ignite is a showcase of the advances being developed to help customers, partners and developers achieve the total value of Microsoft's technology and reshape the way work is done.

As we round out the year, there are strong signals of AI's potential to transform work. Take our latest Work Trend Index. Eight months ago, we introduced Copilot for Microsoft 365 to reduce digital debt and increase productivity so people can focus on the work that is uniquely human. What everyone wants to know now is: Will Copilot really change work, and how? Our research, using a combination of surveys and experiments, shows the productivity gains are real:

70% of Copilot users said they were more productive and 68% said it improved the quality of their work; 68% say it helped jumpstart the creative process. Overall, users were 29% faster at specific tasks (searching, writing and summarizing). Users caught up on a missed meeting nearly 4x faster. 64% of users said Copilot helps them spend less time processing email. 87% of users said Copilot makes it easier to get started on a first draft. 75% of users said Copilot "saves me time by finding whatever I need in my files." 77% of users said once they use Copilot, they don't want to give it up.

Today, we will make about 100 news announcements that touch on multiple layers of an AI-forward strategy, from adoption to productivity to security. We'll zoom in on a few key areas of impact below.

Rethinking cloud infrastructure Microsoft has led with groundbreaking advances like partnerships with OpenAI and the integration of ChatGPT capabilities into tools used to search, collaborate, work and learn. As we accelerate further into AI, Microsoft is rethinking cloud infrastructure to ensure optimization across every layer of the hardware and software stack.

At Ignite we are announcing new innovations across our datacenter fleet, including the latest AI optimized silicon from our industry partners and two new Microsoft-designed chips.

Microsoft Azure Maia, an AI Accelerator chip designed to run cloud-based training and inferencing for AI workloads such as OpenAI models, Bing, GitHub Copilot and ChatGPT. Microsoft Azure Cobalt, a cloud-native chip based on Arm architecture optimized for performance, power efficiency and cost-effectiveness for general purpose workloads. Additionally, we are announcing the general availability of Azure Boost, a system that makes storage and networking faster by moving those processes off the host servers onto purpose-built hardware and software.

Complementing our custom silicon, we are expanding partnerships with our silicon providers to provide infrastructure options for customers.

We'll be adding AMD MI300X accelerated virtual machines (VMs) to Azure. The ND MI300 VMs are designed to accelerate the processing of AI workloads for high range AI model training and generative inferencing, and will feature AMD's latest GPU, the AMD Instinct MI300X. The preview of the new NC H100 v5 Virtual Machine Series built for NVIDIA H100 Tensor Core GPUs, offering greater performance, reliability and efficiency for mid-range AI training and generative AI inferencing. We're also announcing plans for the ND H200 v5 Virtual Machine Series, an AI-optimized VM featuring the upcoming NVIDIA H200 Tensor Core GPU.

Extending the Microsoft Copilot experience Over the past year we have continued to refine our vision for Microsoft Copilot, a set of tools that help people achieve more using AI. To go beyond individual productivity, we are extending Microsoft Copilot offerings across solutions to transform productivity and business processes for every role and function – from office workers and front-line workers to developers and IT professionals.

Microsoft is the Copilot company, and we believe in the future there will be a Copilot for everyone and for everything you do. Some of our Copilot-related announcements and updates include:

Microsoft Copilot for Microsoft 365: This month, Copilot for Microsoft 365 became generally available for enterprises. Already customers like Visa, BP, Honda and Pfizer and partners like Accenture, EY, KPMG, Kyndryl and PwC are using Copilot. We continue to bring new value, based on learnings from our Early Access Program and other research channels. The new Microsoft Copilot Dashboard shows customers how Copilot is impacting their organization – with insights like those found in our Work Trend Index. We're introducing new personalization capabilities that help Copilot offer responses that are tailored to your unique preferences and role. To empower teamwork, new features for Copilot in Outlook help you prep for meetings, and during meetings, new whiteboarding and note-taking experiences for Copilot in Microsoft Teams keep everyone on the same page. And customers who need it can now use Copilot during a meeting without transcription retention. When you give Copilot a seat at the table, it goes beyond being your personal assistant to helping the entire team – check out the Microsoft 365 blog for updates across the suite including PowerPoint, Excel, Microsoft Viva and more. Microsoft Copilot Studio: AI transformation begins by tapping into an organization's unique data and workflows. Microsoft Copilot Studio is a low-code tool designed to customize Microsoft Copilot for Microsoft 365 by integrating business-critical data and build custom copilots for internal or external use. Copilot Studio works with connectors, plugins and GPTs, allowing IT teams to steer Copilot to the best data sources for specific queries. Microsoft Copilot for Service: The newest copilot to provide role-based support helps businesses accelerate their AI transformation of customer service. Copilot for Service includes Microsoft Copilot for Microsoft 365 and helps extend existing contact centers with generative AI. In customer interactions, agents can ask Copilot for Service questions in natural language and receive relevant insights based on data sources from knowledge repositories, leading to faster and smarter resolutions. Copilot in Microsoft Dynamics 365 Guides: Combining the power of generative AI and mixed reality, this copilot helps frontline workers complete complex tasks and resolve issues faster without disrupting workflow. Available first on HoloLens 2, the hands-free copilot will help service industry professionals use natural language and human gestures to offer interactive guidance through content and holograms overlaid on the equipment. Microsoft Copilot for Azure: This is an AI companion for IT that simplifies day-to-day IT administration. More than just a tool, it is a unified chat experience that understands the user's role and goals, and enhances the ability to design, operate and troubleshoot apps and infrastructure. Copilot for Azure helps IT teams gain new insights into their workloads, unlock untapped Azure functionality and orchestrate tasks across both cloud and edge. Bringing Copilot to everyone: Our efforts to simplify the user experience and make Copilot more accessible to everyone starts with Bing, our leading experience for the web. Bing Chat and Bing Chat Enterprise will now simply become Copilot. With these changes, when signed in with a Microsoft Entra ID, customers using Copilot in Bing, Edge and Windows will receive the benefit of commercial data protection. Over time, Microsoft will also expand the eligibility of Copilot with commercial data protection to even more Entra ID (formerly Azure Active Directory) users at no additional cost. Copilot (formerly Bing Chat and Bing Chat Enterprise) will be out of preview and become generally available starting Dec. 1. Learn more here.

Reinforcing the data and AI connection AI is only as good as the data that fuels it. That's why Microsoft is committed to creating an integrated, simplified experience to connect your data to our AI tools.

Microsoft Fabric is part of that solution. Available now, Microsoft Fabric reshapes how teams work with data by bringing everyone together on a single, AI-powered platform that unifies all those data estates on an enterprise-grade data foundation.

Copilot in Microsoft Fabric also integrates with Microsoft Office and Teams to foster a data culture to scale the power of data value creation throughout the organization. We've made more than 100 feature updates since Build and expanded our ecosystem with industry leading partners, and have over 25,000 customers including Milliman, Zeiss, London Stock Exchange and EY using it today.

Unlocking more value for developers with Azure AI We continue to expand choice and flexibility in generative AI models to offer developers the most comprehensive selection. With Model-as-a-Service, a new feature in the model catalog we announced at Microsoft Build, pro developers will be able to easily integrate the latest AI models, such as Llama 2 from Meta and upcoming premium models from Mistral, and Jais from G42, as API endpoints to their applications. They can also customize these models with their own data without needing to worry about setting up and managing the GPU infrastructure, helping eliminate complexity.

With the preview of Azure AI Studio, there is now a unified and trusted platform to help organizations more easily explore, build, test and deploy AI apps – all in one place. With Azure AI Studio, you can build your own copilots, train your own, or ground other foundational and open models with data that you bring.

And Vector Search, a feature of Azure AI Search, is now generally available, so organizations can generate highly accurate experiences for every user in their generative AI applications.

The new GPT-3.5 Turbo model with a 16K token prompt length will be generally available and GPT-4 Turbo will be in public preview in Azure OpenAI Service at the end of November 2023. GPT-4 Turbo will enable customers to extend prompt length and bring even more control and efficiency to their generative AI applications.

GPT-4 Turbo with Vision is coming soon to preview and DALL·E 3 is now available in public preview in Azure OpenAI Service, helping fuel the next generation of enterprise solutions along with GPT-4, so organizations can pursue advanced functionalities with images. And when used with our Azure AI Vision service, GPT-4 Turbo with Vision even understands video for generating text outputs, furthering human creativity.

Enabling the responsible deployment of AI Microsoft leads the industry in the safe and responsible use of AI. The company has set the standard with an industry-leading commitment to defend and indemnify commercial customers from lawsuits for copyright infringement – the Copilot Copyright Commitment (CCC).

Today, Microsoft takes its commitment one step further by announcing the expansion of the CCC to customers using Azure OpenAI Service. The new benefit will be called the Customer Copyright Commitment. As part of this expansion, Microsoft has published new documentation to help Azure OpenAI Service customers implement technical measures to mitigate the risk of infringing content. Customers will need to comply with the documentation to take advantage of the benefit.

And Azure AI Content Safety is now generally available, helping organizations detect and mitigate harmful content and create better online experiences. Customers can use Azure AI Content Safety as a built-in-safety system within Azure OpenAI Service, for open-source models as part of their prompt engineering in Azure Machine Learning, or as a standalone API service.

Introducing new experiences in Windows to empower employees, IT and developers We continue to invest in and build Windows to empower people to navigate the platform shift to AI. We are thrilled to introduce new experiences in Windows 11 and Windows 365 for IT and employees that unlock new ways of working and make more AI accessible across any device. To further our mission of making Windows the home for developers and the best place for AI development, we announced a host of new AI and productivity tools for developers, including Windows AI Studio.

Announcing NVIDIA AI foundry service Aimed at helping enterprises and startups supercharge the development, tuning and deployment of their own custom AI models on Microsoft Azure, NVIDIA will announce their AI foundry service running on Azure. The NVIDIA AI foundry service pulls together three elements – a collection of NVIDIA AI Foundation models, NVIDIA NeMo framework and tools, and NVIDIA DGX Cloud AI supercomputing and services – that give enterprises an end-to-end solution for creating custom generative AI models. Businesses can then deploy their models with NVIDIA AI Enterprise software on Azure to power generative AI applications, including intelligent search, summarization and content generation.

Strengthening defenses in the era of AI The threat landscape has evolved dramatically in recent years, and at Microsoft Ignite we are introducing new technologies across Microsoft's suite of security solutions to help defenders make the world a safer place.

Microsoft Sentinel and Microsoft Defender XDR (previously Microsoft 365 Defender) will be combined to create the industry's first Unified Security Operations Platform, with embedded Security Copilot experiences. With built-in generative AI, it's a single, powerful experience focused on protecting threats at machine speed and aiding defenders by simplifying the complexity of their environment.

Additionally, the expansion of Security Copilot embedded within Intune, Purview and Entra will help IT administrators, compliance units and identity teams simplify complex scenarios. In Entra, identity administrators can quickly troubleshoot identity access. In Purview, data security alerts deliver rich context to help resolve problems faster. In Intune, IT administrators can use "what if" analysis to keep business running while improving governance and compliance.

And that's just a snapshot of what we'll be announcing at Ignite. As a reminder, you can view keynote sessions from Satya Nadella, Rajesh Jha and Jared Spataro, Charlie Bell and Vasu Jakkal, and Scott Guthrie live or on-demand.

Plus, you can get more on all these announcements by exploring the Book of News, the official compendium of all today's news, and the product blogs below.

Watch the keynotes and get all the latest photos, videos and more from Microsoft Ignite

With a systems approach to chips, Microsoft aims to tailor everything 'from silicon to service' to meet AI demand

Introducing new Copilot experiences to boost productivity and elevate customer experiences across the organization

Simplify IT management with Microsoft Copilot for Azure – save time and get answers fast

Introducing Microsoft Copilot Studio and new features in Copilot for Microsoft 365

Announcing general availability of vector search and semantic ranker in Azure AI Search

How Azure AI Content Safety helps protect users from the classroom to the chatroom

Elevating the developer experience on Windows with new AI tools and productivity tools

Microsoft unveils expansion of AI for security and security for AI at Microsoft Ignite

Tags: AI, Azure AI Content Safety, Azure AI Studio, Microsoft 365, Microsoft Copilot, Microsoft Fabric, Microsoft Ignite 2023, Microsoft Security Copilot, Model-as-a-Service

Today Microsoft is updating its startup program to include a free Azure AI infrastructure option for high-end GPU virtual machine clusters, for use in training and running large language models and other deep learning models. Y Combinator (YC) and its community of startup innovators will be the first to access this offering in private preview to a limited cohort. YC has an unmatched reputation as a pioneering startup accelerator helping launch transformative companies including Airbnb, Coinbase and Stripe. Now YC startups will have the technical resources they need to quickly prototype and bring to market cutting-edge AI innovations. Our close collaboration with YC provides valuable insights into the infrastructure needs of early-stage AI companies, ensuring our offering delivers optimal value to additional startups going forward. "With the overwhelming infrastructure requirements needed to do AI at scale, we believe that providing startups with high-performance capabilities tailored for demanding AI workloads will empower our startups to ship faster," said Michael Seibel, Managing Director of Y Combinator. We are also working with M12, Microsoft's venture fund, and the startups in M12's portfolio which will gain access to these dedicated supercomputing resources to further empower their AI innovations. Over time, our vision is to partner with additional startup investors and accelerators, with a goal of working with the ecosystem to lower the barrier to training and running AI models for any promising startup. Microsoft Azure offers cloud-based scalable AI infrastructure, built for and with the world's most sophisticated AI workloads, from delivering the largest and most complex AI models including GPT-4 and ChatGPT through Azure OpenAI Service to developers to infuse AI capabilities into many apps. Azure AI infrastructure is fueling groundbreaking innovations. Infrastructure requirements to do AI at scale are often overwhelming, but with Azure's global infrastructure of AI-accelerated server offerings with networked graphics processing units (GPUs), startups building advanced AI systems will be able to leverage these high-performance capabilities to accelerate innovation. On top of world-class infrastructure, we will also provide tools to simplify deployment and management through Azure Machine Learning. This enables easy low-code or code-based training of custom models and fine-tuning of frontier and open-source models, simplified deployment and optimizations like Low Rank Adaptation, DeepSpeed and ONNX Runtime (ORT). Further, startups can deploy AI solutions with peace of mind knowing all deployments are secure and backed by Microsoft's principles for Responsible AI. Empowering startups to build transformative solutions powered by AI AI is transforming industries and startups are leading that innovation, creating new business and societal value quicker than many thought possible. According to a recent KPMG survey, the near-term demand is real, with 75% of U.S. CEOs stating that generative AI is a top investment priority, 83% anticipating an increase in generative AI investment by more than 50% in the next year, and 45% saying investment will at least double. For startups, this represents a once-in-a-generation opportunity to bring groundbreaking impact to a market hungry for change. To help startups meet this opportunity, last year we introduced Microsoft for Startups Founders Hub – designed to help founders speed development with free access to GitHub and the Microsoft Cloud as well as unique benefits including free access to $2,500 of OpenAI credits to experiment and up to $150,000 in Azure credits that startups can apply to Azure OpenAI Service. Startups also receive 1:1 advice from Microsoft AI experts to help guide implementation. The Microsoft Pegasus Program, an extension of Founders Hub, links enterprise customers with startup solutions for immediate deployment. Seventy-five percent of Pegasus startups have landed deals with Fortune 1000 companies via increased reach across Azure Marketplace. Startups using Azure AI to develop cutting-edge solutions for today's problems Whether you have a product in market or just an idea, Microsoft provides startups with the tools they need to rapidly build and scale AI solutions. Already, we are seeing the results of empowering startups to innovate with AI to improve customer support, detect and address health conditions and advance immersive gaming experiences. Here are just a few examples of the cutting-edge innovation happening now: Commerce.AI dramatically increases call center productivity with Azure OpenAI Service Commerce.AI uses Azure OpenAI Service and Azure AI Services to make call centers more efficient. Azure Cognitive Services uses a Commerce.AI model to transcribe interactions in real time, including into multiple languages. After the call, Azure OpenAI Service creates a summary with customer contact information, topics of conversation and embedded sentiment analysis. The system selects next steps and follow-up action items from pre-generated options, and the customer service agent exports the information to Microsoft Dynamics 365 in one quick step. Inworld: The next-generation AI character engine for immersive gaming Inworld, a Silicon Valley startup, is a fully integrated character engine that goes beyond language models to give users complete control over AI non-player characters (NPCs). With Inworld, users can customize their characters' knowledge, memory, personality, emotions and narrative role. Inworld uses Azure AI technologies like Azure OpenAI Service to power its advanced natural language understanding and generation. BeeKeeperAI is helping catch rare childhood conditions early AI tooling company BeeKeeperAI enables AI algorithms to run in a private and compliant way in healthcare environments. The company is pioneering an effort to leverage confidential computing to train an algorithm for predicting a rare childhood condition using real patient health records. By encrypting both the data and algorithm and using Microsoft Azure's confidential computing, the company has enabled the algorithm to analyze identifiable health information in a secure, sightless manner.

YouTube Video Click here to load media Calling all startup founders — Start building the future todayThe AI landscape is developing at breakneck speed, and Microsoft is ready to assist startups in seizing this opportunity. If you're a startup founder evaluating partners, we invite you to join us at Microsoft for Startups Founders Hub and discover how we can accelerate your immediate success. Tags: AI, Azure AI, Azure OpenAI Service, M12, Microsoft for Startups Founders Hub, startups

Microsoft honors service members and their families today and every day.

Since 1996, the month of November has been commemorated by presidential proclamation as National Veterans and Military Families Month in the United States with the observance of Veterans Day on the 11th as a central moment of celebration and appreciation. Throughout the month, you can expect to see American flags, camouflage and other patriotic or military decorations everywhere you look as communities celebrate one of our greatest national treasures: Our military veterans and those who make their service possible.

As a leader at Microsoft with deep personal appreciation for the military community that goes beyond my role as executive sponsor of the Military at Microsoft Employee Resource Group, I am grateful that this month is dedicated to honoring those who have answered the call to serve. The month provides an intentional reminder to step back and think about our veterans and all they contribute to our community and to our business. At Microsoft, we recognize the intrinsic value that veterans bring every day. The entrepreneurial mindset, team spirit and creativity that are cultivated out of necessity in ambiguous and often-challenging military operational environments are the same fundamental characteristics required to drive rapid innovation in technology. Across career stages and disciplines, the contributions and capabilities of our military veterans are clear.

While we are celebrating living veterans in the United States, other nations mark Nov. 11 as a moment for remembrance of those lost in war. That date in 1918 marked the end of hostilities and an initial step toward ending the First World War. In the years following the war, Armistice Day served as a moment for nations and people around the world to remember the sacrifice of their fallen service members and the heavy price that each nation paid with the hope of establishing an enduring peace. Great Britain, the Commonwealth and many countries across Europe continue to observe Nov. 11 as Remembrance Day or Poppy Day, a day dedicated to honoring fallen service members.

At Microsoft, the week leading up to Veterans Day is filled with events focused on honoring, celebrating and expressing gratitude for veterans and military communities around the world both inside and outside of Microsoft. Along with these celebrations, I believe one of the best ways Microsoft continues to honor the service and sacrifice our veterans and their families willingly make on our behalf is to do its part — to provide the support they need to thrive while still in uniform and after they have hung up the uniform for the final time.

Meet a few of our remarkable veterans who continue to make a difference and embody Microsoft's mission to empower every person and every organization on the planet to achieve more.

Shannon during an exercise with the Coastal Riverine Forces (now the Maritime Expeditionary Security Forces) around the Port of Djibouti. Photo courtesy of Shannon Llenza.

Shannon joined the United States Navy in March 2000 while she was in law school. Upon graduation in May 2001, she began active-duty service as a Judge Advocate General (JAG) officer.

After completing nearly five years of active-duty service, Shannon transitioned into the Navy Reserves in 2006 – where she continues to serve as a commander. She plans to retire in February 2024, after completing 20 years of service.

During her time in service, Shannon deployed to Camp Lemonier in Djibouti, Africa — the only permanent U.S. military base in Africa which supports a variety of missions and operations in the region.

She served as the legal advisor to the commanding officer of the base, providing legal counsel on a wide range of issues.

She shared that one highlight of the deployment was the opportunity to participate in the Grand Bara, which is a run across the Grand Bara Desert, Djibouti, with about 200 other service members.

Shannon was a full-time employee at Microsoft during her deployment to Africa. She shared that "Microsoft has a good program set up for service members who deploy and for reservists who must fulfill their commitment of drilling two weeks a year with their unit. Microsoft supports and values the contributions of its military employees, which allows them to fulfill their service without having to think about their employment while they are away."

Shannon participating in the Grand Bara 15-kilometer run in Djibouti. Photo courtesy of Shannon Llenza.

She has also been involved in the Corporate, External, and Legal Affairs (CELA) Military Employee Network since she started at Microsoft.

She said, "it has been a great way to meet people in the community and the network has been instrumental in providing resources and education on all things Microsoft for its members."

For Shannon, her experience in the military has shaped how she approaches issues in her civilian career. "The military teaches you how to get things done, how to work in ambiguous environments, and how to step out of your comfort zone."

Chas joined the U.S. Air Force in 1991, retiring on Oct. 29, 2023 after 30 years of service and achieving the rank of major general.

Entering service in the computer/communications officer career field, he served for six years on active duty in the Air Force before joining Microsoft and transitioning to the Air National Guard.

Chas, center, at his change of command ceremony in 2017. Photo by Airman 1st Class Timothy Tweet. The appearance of U.S. Department of Defense (DoD) visual information does not imply or constitute DoD endorsement.

Chas said, "the experience I gained in the Air Force gave me a foundation in software engineering that enabled me to transition to civilian employment at a company like Microsoft."

Joining Microsoft as a full-time employee in July 2000, Chas began serving in the Washington Air National Guard's Information Warfare Team and continued to serve in the Air National Guard while working at Microsoft for 23 years.

Over the years, Chas experienced an incredible synergy between his roles at Microsoft and in the military. In both staff roles and in command, Chas has taken his knowledge of high-performing teams in the military and applied it to his civilian role.

Chas was part of the "start-up" team that would eventually form the foundation for Cyberspace Ops Squadrons and a Cyberspace Ops Group in the Washington Air National Guard. While serving in these units, Chas gained training and experience in information dominance and cybersecurity operations which he was able to directly apply to his work helping Microsoft build and launch sovereign clouds.

Chas speaking at a 2023 cyber executive seminar. Photo by Sgt. Matthew Lucibello. The appearance of U.S. Department of Defense (DoD) visual information does not imply or constitute DoD endorsement.

In addition to the experiences that he gained through leadership roles and operational roles, Chas said, "I've also benefited from a wealth of executive leadership training afforded to me. I've had the opportunity to attend Department of Defense-sponsored executive leadership courses at Harvard and MIT on topics including national cybersecurity policy and AI."

Chas was deployed with the Air National Guard to Afghanistan in 2008 and with U.S. Forces Korea for multiple 30-day deployments between 2014 and 2017. When asked about the ways Microsoft has supported him as he continued to serve while working as a fulltime employee, Chas said that "Microsoft is an industry leader in its support of military members who chose to continue to serve." For Chas, the robust Military at Microsoft Employee Resource Group community has provided him with tremendous support.

He also recognizes the incredible impact of Microsoft's Military Affairs organization which provides an invaluable resource to military veterans and current service members. Like many teams at Microsoft, Chas has benefited directly by hiring amazing veterans from the hiring and skilling programs the company supports.

As a company, we endeavor to honor the commitment to service that Shannon, Chas and others like them demonstrate through our commitment to train, hire and support our veterans and their families. In addition to supporting employees who continue to serve in reserve components, Microsoft also dedicates resources to help service members transition into careers in tech through mentorship, skilling programs and fellowships.

This year, the Military Affairs team at Microsoft is celebrating the 10th anniversary of the Microsoft Software and Systems Academy (MSSA) by featuring 10 tech professionals who got their start with MSSA and continue to make an incredible impact in the industry today. MSSA is a 17-week training program for veterans and transitioning service members for high-demand careers in cloud development or cloud administration. Program participants have an opportunity to engage directly with Microsoft recruiters, connect with alumni and receive resources and education on Microsoft roles, how to apply and the overall hiring process. Historically, this program made a tremendous impact with over 3,600 graduates and 98% of those graduates are employed or pursuing continuing education. The Military Affairs team continues to look for ways to best serve the community and recently announced new benefits for alumni and a resource center for MSSA alumni as part of their 10th anniversary celebration. I look forward to seeing how MSSA and other programs Microsoft offers for the military community will grow and continue making an impact for service members and their families.

As an executive sponsor of the Military at Microsoft Employee Resource Group, I have the distinct honor of serving our incredible global military community within Microsoft. In this capacity as well as in my business leadership role, I am fortunate to work alongside members of the military community and bear witness to their incredible ongoing impact within our organizations. The unique qualities and values that compel individuals like Shannon and Chas to serve their country are the same that make them invaluable members of the Microsoft team. Selfless service is one of the most noticeable traits of the ethos that follows veterans wherever they go, and you can see it in their dedication to team and mission above self.

As AI becomes more woven into society, its economic impact will be significant, and organizations are just starting to understand the extent of what's possible. For companies to invest in AI though, it must make good business sense. Business leaders and decision makers need to understand the industry and line-of-business use cases that are best positioned to drive value within their organizations, what the return on investment will be, what time to value to expect, and how to get started. In short, they need help demystifying the business case for AI.

To help companies understand the opportunities AI can unlock, Microsoft commissioned a study through IDC that provides unique insights into how AI is being used to drive economic impact for organizations. IDC surveyed over 2,000 business leaders and decision makers from around the world who are responsible for bringing AI transformation to life within their organizations. The study, which builds on the results from Microsoft's Work Trend Index focused on workplace productivity, examines how companies are monetizing their AI investments, from generating new revenue streams to delivering differentiated customer experiences, to modernizing internal processes. Key findings from this study show*:

71% of respondents say their companies are already using AI 92% of AI deployments are taking 12 months or less Organizations are realizing a return on their AI investments within 14 months For every $1 a company invests in AI, it is realizing an average return of $3.5X 52% report that a lack of skilled workers is their biggest barrier to implement and scale AI

The study illustrates that AI has demonstrable business value, and we are seeing this surface in core use cases within areas like employee experience, customer engagement and internal business processes, and how AI can help bend the curve on innovation. With generative AI, that value gets exponentially greater, as we've seen in the past year with generative AI technologies from OpenAI such as ChatGPT.

"IDC is projecting that generative AI will add nearly $10 trillion to global GDP over the next 10 years.** Calculating the value of new investments in GenAI requires building the business case by simulating potential cost and responsible value realization," said Ritu Jyoti, Group Vice President AI and Automation for IDC.

This wave of innovation has accelerated the pace of AI adoption in ways that are changing and augmenting how we work and live, and Microsoft customers are increasingly embracing AI opportunities for business transformation.

Enrich employee experiences: Employees in every industry are dealing with an increasing volume of digital debt and administrative burdens that slow down productivity and get in the way of meaningful work. To address this challenge, AI is being used to bring together unstructured data like social media, product details and customer engagement to better tailor communications, enable more intelligent insights and solve problems faster. Additionally, employees are using Azure OpenAI and Microsoft Copilot in Microsoft 365 to augment their copywriting capabilities for things like presentations, website content, case studies, blogs, press releases, search engine optimization and digital art.

Reinvent customer engagement: With the heavy competition for customer acquisitions and retention, organizations have struggled to keep pace with the increasing amount of customer signals, and to deliver personalized service to customers in real-time. To drive greater customer loyalty, organizations are applying the AI capabilities of Dynamics 365 in contact centers for real-time assistance and guidance on suggested responses. Employees are also using AI to summarize conversations, guide on next steps, and get coaching feedback. Azure-powered virtual assistants are being used to deliver all kinds of hyper-personalized experiences across different verticals like healthcare for processing claims and entertainment for sports fans. Salespeople are using Viva Sales to help nurture leads and close deals.

Reshape business processes: Companies have pockets of valuable information scattered throughout their organization that can be difficult for employees to locate and use holistically. By finding and making connections across this information, AI can surface integrated insights that help to predict and accelerate workloads. This is particularly evident in cybersecurity, where employees are using AI insights to identify bad actors more quickly and better protect both employees and intellectual property. AI is also being used in manufacturing and operations, to create digital replicas of their supply chain environments so they can run simulations and optimize workflow management, resulting in enhanced supply chain efficiency.

Bend the curve on innovation: This is an exciting concept as companies in every industry look to regain an edge. Organizations can deploy AI to stay ahead of changing business dynamics, and to exceed customer expectations. By not having to modernize every underlying system to achieve these results, and by putting AI directly in the hands of developers with GitHub Copilot, organizations can operate with agility and accelerate innovation. Teams can leverage AI to help scale production and speed to market while being able to focus on higher-value activities.

IDC survey data confirms that businesses are eager to adopt AI technology, with 71% of survey respondents currently using AI tools in their organizations, and 22% planning to do so within the next 12 months. However, even with this momentum and positive outlook for what AI can help them achieve, organizations are facing challenges when it comes to implementation. A shortage of skilled employees is holding companies back from accelerating their AI-based innovations, with 52 percent of those surveyed reporting a lack of skilled workers needed to implement and scale AI initiatives across business functions as the top blocker.

To help address the skilling gap, Microsoft has already engaged over 6 million people globally in learning activities in the last 12 months and has ambitions to provide skills to everyone using our AI technology. We have also empowered our ecosystem of more than 400,000 partners worldwide with the skills needed to implement AI technology responsibly and to deliver greater customer value.

No matter where you are in your cloud and AI transformation journey, Microsoft can help. To learn more about how customers across industries are shaping their AI transformation with Microsoft, please visit the Microsoft Cloud blog which shares real-world examples of business impact being driven by AI, as well as resources and skilling opportunities you can use to build your readiness to lead the era of AI.

About the study The IDC study, commissioned by Microsoft, is based on results from 2,109 enterprise organizations totaling more than 13 million employees worldwide across 16 countries globally. Through the questionnaire, respondents were identified as the decision maker for AI within their organization.

Source: *IDC Infographic, sponsored by Microsoft, The Business Opportunity of AI, IDC #US51315823, November 2023. **Generative Artificial Intelligence: A New Chapter for Enterprise Business Applications, IDC Perspective #US50471523, March 2023.

Today, I am sharing Microsoft's 2023 Global Diversity & Inclusion Report — our fifth consecutive annual report and the 10th year of releasing our global workforce demographic data. As we mark this milestone, a couple of key aspects about our company's work on diversity and inclusion (D&I) stand out: Our journey is ever evolving, and our focus and progress are consistent, both of which are vital to delivering on Microsoft's mission to empower every person and every organization on the planet to achieve more. This year's report shows that we continue to be a more diverse Microsoft today than we have ever been. Looking at this year's data as well as our cumulative efforts, it's clear that we are driving positive change. The data we share is also a powerful tool for us to understand with precision where we need to concentrate and accelerate our work. This year, amid an evolving macroeconomic environment, our company — like many others — made intentional organizational and workforce adjustments to meet the strategic demands of the business, which impacted our rate of progress in some areas. While there's more work to be done, I am motivated by our ongoing progress and sustained efforts on increasing representation and strengthening a culture of inclusion, as detailed in this year's report. Key highlights

The representation of women and most racial and ethnic minority groups (Asian, Black and African American, Hispanic and Latinx, and multiracial employees) has increased at all levels over the past five years. We maintained or grew representation within several leadership levels for women and U.S. racial and ethnic minority groups year over year.

The representation of women in Executive roles is 29.1%, a 3.2 percentage point increase year over year. This was the highest year-over-year Executive representation growth across women, men, and U.S. racial and ethnic groups in 2023. The number of Black and African American Directors, Partners and Executives (including People Managers and Individual Contributors) rose to 107.8% of our 2025 Racial Equity Initiative commitment, up from 92.0% in 2022. The number of Hispanic and Latinx Directors, Partners and Executives (including People Managers and Individual Contributors) increased to 74.8% of our 2025 Racial Equity Initiative commitment, up from 57.6% in 2022.

While hiring volume slowed, hiring representation was greater than or equal to representation for women and all racial and ethnic minority groups except Native American and Alaska Native. At Microsoft, we are committed to the principle of pay equity, which accounts for factors that legitimately influence total pay including things like job title, level and tenure. As of September 2023:

Inside the U.S., all racial and ethnic minority groups who are rewards-eligible combined earn $1.007 total pay for every $1.000 earned by U.S. rewards-eligible white employees with the same job title and level and considering tenure. Inside the U.S., women who are rewards-eligible earn $1.007 total pay for every $1.000 earned by rewards-eligible employees who are men and have the same job title and level, and considering tenure; outside the U.S., women who are rewards-eligible earn $1.003 total pay for every $1.000 earned by rewards-eligible employees who are men and have the same job title and level, and considering tenure.

Last year, we began voluntarily disclosing median unadjusted pay analysis, which gathers the total pay amounts for all employees across a company — regardless of factors such as job title, level or tenure — sorts those amounts by value, and then identifies the number that's in the middle, or median, of all of those data points. The difference between that median pay amount for any two employee groups is referred to as a median unadjusted pay gap.

As of September 2023, our analysis shows that we have made progress in narrowing the median unadjusted pay gap for women in the U.S., women outside of the U.S., and Asian, Black and African American, and Hispanic and Latinx employees in the U.S. As we continue to increase representation for women and racial and ethnic minority groups at more senior levels, and continue to ensure pay equity for all, the gap between the medians will continue to reduce.

New data Hires data: As one of the most transparent companies of our size when it comes to the diversity and inclusion data we share, we are continually evaluating where we are now and where we aim to be. That is why, in addition to the extensive data we already share, we're reporting on external hires representation for women and men globally and race and ethnicity in the U.S. for the first time. This data reflects the hires of members of a particular group as a percentage of total employee hires within the respective fiscal year. Hires representation being higher than headcount representation is one of the factors that could increase a group's representation in the workforce. This past fiscal year, hires representation was greater than headcount representation for women as well as Asian, Black and African American, Hispanic and Latinx, and multiracial employees. Self-ID data: At Microsoft, Self-ID helps us recognize the different identities, experiences and needs of the entire workforce. Through voluntary Self-ID, employees can help Microsoft make better-informed, more inclusive decisions about meaningful benefits and programs that meet their needs in various stages of life, flex to their interests, and enrich their lives. Self-ID is available globally in 46 markets with some variation, as dictated by local laws, practices and customs. We continue to evolve self-identification options for employees to be as inclusive as possible. This year, we're sharing more self-identification (Self-ID) data on Asian sub-identities in the U.S. to further highlight the importance of identity and the impact of self-identification. This comes after expanding the options for Asian employees in the U.S. who want to identify their backgrounds in additional detail last year. The Asian community is the single largest racial and ethnic minority group within our company, with more than 20 sub-identities. Employee survey data In addition to our demographic data, we share employee survey data in our D&I report each year, which helps us assess the impact of our D&I efforts so we can better understand how to close the gap between the culture of inclusion we aspire to and the lived experiences of everyone at Microsoft. We have continued to invest in experiences, behavior and organization changes, as well as prioritization of retention and development. This year, meaningful insights include:

This year, 96.4% of employees reported some level of awareness of the concept of allyship, which is a cornerstone of our growth mindset approach to D&I. This is up from 90.3% in 2022 and 65.0% in 2019, when we first started asking employees about their awareness. The average score for the survey question asking employees if they understand what is expected of them to contribute to a more diverse and inclusive environment increased from 82 to 84 globally year over year, and from 80 to 83 in the U.S. Additionally, the average score increased year over year for men, women and every racial and ethnic group.

Inclusion spotlights While data is important, it does not tell the whole story. Through six Inclusion Spotlights, this year's report shares more details on some of the people, programs and initiatives that demonstrate how we invest in and innovate for D&I.

Global strategy, local implementation: We explore how the global Microsoft workforce activated around D&I this past year in ways relevant and meaningful to local employees and communities to drive positive change. Inclusion from the start: We shine a spotlight on New Employee Orientation (NEO) as well as the Nuance acquisition and explore ways we introduce a culture of inclusion to new employees or integrate companies we acquire into our inclusive culture. Self-expression in our products: We connect how technical and D&I expertise come together to inform new self-expression tools, including profile videos, pronouns and name pronunciation. Innovative learning: We delve into some of our learning offerings, informed by a range of communities and experts, that enable employees to deepen their understanding and take intentional action for meaningful progress. D&I Core Priority: We share the evolution and impact of the D&I Core Priority, an accountability approach that sets Microsoft apart. AI & D&I: We look at how we build trust through our responsible AI strategy and inclusive AI solutions.

As we look ahead, we are unwavering in our focus to attract, develop and retain a workforce that reflects a diversity of backgrounds, skills and experiences. We support employees' careers through intentional talent management, access and career mobility across all levels of our organization. To further support this, our team and my role have recently evolved to include talent development efforts in addition to global D&I work. This organizational alignment allows us to further embed D&I into all our talent practices in an effort to accelerate representation progress. We believe our continued work to build diverse workforces and strengthen our culture of inclusion helps foster innovation and serve our business and customer needs. I am confident that our combined momentum and commitment will only fuel additional ways for us to leverage our resources with intention, driving progress toward a more diverse and inclusive Microsoft. Tags: Diversity and Inclusion Report

Marketing excellence at Microsoft is key to how we drive business growth and for the past ten years, Chris Capossela has done a terrific job as our CMO driving revenue and brand love for the company. The numerous accolades Microsoft continues to receive in terms of brand recognition and marketing awards are impressive and a testament to the strong team he has built. Chris and I have been working on his succession plan for some time, and as this new era of AI is upon us, we've decided this is the right time to put that plan into action.

I am pleased to announce that Takeshi Numoto is being promoted to Executive Vice President, Chief Marketing Officer, reporting to me. Takeshi has been at the heart of our Cloud transformation, he's a fantastic systems thinker who works end-to-end across all functions from engineering to finance to operations to sales, and he's built a great team of marketing leaders. I'm thrilled for him to step into the CMO role for Microsoft and drive our vision forward.

I'm also excited that Yusuf Mehdi is being promoted to Executive Vice President, Consumer Chief Marketing Officer. He will join the senior leadership team and report to Takeshi. Yusuf will serve as the champion of our end-user experiences and build on his work launching several of our AI-powered services to lead Microsoft Copilot product marketing. He will also continue to lead our Search, Ad, & News and Devices & Creativity Customer Solution Areas (CSAs).

And with our recently closed acquisition of Activision Blizzard King, we are doubling down on our Gaming business. To realize our growth potential, Ami Silverman and her Consumer Sales Organization will move to report to Phil Spencer, CEO of Gaming. Ami will continue to drive all our consumer sales including our Devices & Creativity CSA through our direct and partner sales channels.

After 32 years of dedicated service to our company, employees, and customers, Chris is leaving Microsoft. Over the many years we've worked together, I've known and respected Chris as a leader who has exemplified a complete, unwavering commitment to our mission and our culture. As a leader and a colleague, he's always shown how deeply he cares about both the "what" and the "how" of driving our business forward. I'm grateful to have had the opportunity to work closely with him and for the significant impact he's had on our company through marketing leadership, championing our culture and D&I, and developing world class talent that will serve us well into the future.

Please join me in congratulating Takeshi and Yusuf on their new roles and extending a big thank you to Chris for all his contributions to Microsoft.

For the past decade, we have been on a journey with our customers to help them achieve digital transformation, which is business transformation empowered by cloud technology. With increasing excitement around generative AI — both for its potential and the impact it is already having — organizational leaders are eager to prioritize adoption that takes advantage of this next wave of AI transformation. At Microsoft, we are working with our customers to pragmatically assess and develop responsible, secure AI strategies focused on maximizing their investment while yielding desired business outcomes. With our differentiated copilot capabilities, customers can take advantage of the latest advancements in generative AI across the services they already know and love; and with the confidence they are building upon the most integrated, comprehensive and trusted cloud in the industry. Together with our unmatched partner ecosystem — from the ISVs helping shape industries to the digital natives that are disrupting them — we are building generative AI solutions that will unlock productivity and innovation opportunities for organizations everywhere. In many cases, we are co-innovating and co-developing custom AI solutions directly with our customers. As I look back on the past quarter, I am humbled by what we have achieved together and the incredible effort by organizations across industries to embrace AI transformation.

Grupo Bimbo developed an AI solution with an integrated copilot to help employees save time when asking about company policies.

Organizations like 3M, Prada Group and Campari have been testing copilot features in Dynamics 365 and Power Platform firsthand, while General Motors, Visa, KPMG, AGL, Data #3, Bupa, NAB, Powerlink Queensland, Rest Super and Suncorp are now part of the Microsoft 365 Copilot Early Access Program. Our collaboration with IBM is helping clients accelerate the deployment of generative AI with its new Azure OpenAI Service offering, and we are enabling thousands of organizations like PepsiCo, Vodafone and Voya Financial with Azure cloud services through our expanded partnership with Oracle. To empower employees across 34 countries, Grupo Bimbo developed a solution with an integrated copilot in two weeks to help prevent non-compliance and save time when asking questions about company policies. All of Microsoft's AI-powered copilots are backed by our Copilot Copyright Commitments to extend IP and legal protection to enterprise customers who use these services, building upon our AI Customer Commitments.

Global healthcare company Sanofi is improving training and collaboration for employees in different locations with HoloLens 2.

Helping healthcare professionals and providers focus on patient care and support through secure data automation and training

By securely centralizing its data in an AI-powered intelligent data platform on Azure, Mercy is able to use generative AI to help patients schedule appointments more efficiently and better understand their lab results. Mayo Clinic is deploying Microsoft 365 Copilot to ease the burden of administrative demands on healthcare providers so they can focus on patient care, while Duke Health is partnering with Microsoft to responsibly and ethically harness the potential of generative AI and the Microsoft Cloud to redefine the healthcare landscape. To speed up care and improve patient experiences, MultiCare has automated its medication reconciliation process and increased efficiency by 175% with help from Microsoft and partner 3Cloud. By consolidating data such as lab results, medications and plan coverage in the cloud, Blue Shield of California is making services more accessible to members while reducing the cost of care. With help from Healthanea, France-based insurance company AXA is connecting hospitals, pharmaceuticals, health tech solutions and insurers to provide patients with reliable and accurate health information while protecting sensitive health data through the Microsoft Cloud for Healthcare. Paige and Microsoft are collaborating to transform patient care by building the world's largest image-based AI to detect cancer with higher accuracy. To ensure patient confidentiality and automate threat reporting, Hamad Medical Corporation is working with Mannai to proactively detect and respond to cyber threats while providing real-time visibility to its executives. Sanofi is improving training and collaboration for employees in different locations with HoloLens 2 — saving two weeks of training time for its production line operators and meeting increased demand for pharmaceuticals.

In Canada, the Government of Alberta created an AI-powered tool to help firefighting duty officers make decisions and use resources more strategically.

Empowering government workers, non-profit volunteers, students and teachers to expand social impact and save costs with data and AI

Working with AltaML, the Government of Alberta has built an AI-powered tool to help firefighting duty officers become more confident in predicting and sustainably managing the risk of wildfires, creating the potential to save up to $3 million in annual operating costs. In the U.K., Aberdeen City Council is making it easier for workers to access social care data within minutes to provide faster and more accurate service to vulnerable citizens while saving more than $2 million annually on manual data collection. To help ensure seamless service to citizens and prevent cyberattacks to critical infrastructure like traffic light systems and utilities, the City of Brampton is helping its IT team improve its cybersecurity maturity and reduce attack alerts by 70% with help from partner Difenda. To quickly scale the creation of thousands of audiobooks and reduce volunteer labor, nonprofit Project Gutenberg is making its e-books more accessible to people with visual impairments by using text-to-speech capabilities in Azure AI and Microsoft Fabric. German-based Nationalpark Bayerischer Wald is using AI to help research students pre-classify wildlife images in two days instead of 30 — saving 95% on the cost of wildlife monitoring and distance measurement. Working with teachers to offer more individualized support to students, New York City Public Schools has developed a secure, custom AI-powered teaching assistant on Azure OpenAI to answer questions and give real-time feedback.

AI is being used to help make the financial services industry more efficient in a number of areas.

Providing financial services employees and customers with AI capabilities for more meaningful and personalized interactions

Ally Financial launched a proprietary platform as its foundation for generative AI innovation across the company, including live summarization of customer service calls so employees can spend more time focusing on customer interactions. With Azure OpenAI, Swiss investment firm Vontobel is boosting employee productivity in programming and data analysis, while automatic payments company Sem Parar is responding intelligently and contextually to customer inquiries to help them resolve their questions more quickly. Emirates NBD is empowering more than a thousand developers with its coding assistant leveraging the capabilities of GitHub Copilot X, while automating repetitive tasks and content generation for employees using Microsoft 365 Copilot. MetLife is helping pet parents access health records and connect with veterinary technicians more easily through its app built with AI-powered search and machine learning capabilities. With a focus on making financial services more accessible in Qatar, CWallet has saved $1 million by building its secure and compliant fintech platform on Azure to meet data regulations in the Middle East, and now plans to explore AI capabilities to simplify customer transactions.

Swedish steel manufacturer Epiroc is helping automate complex processes for its global employees.

Equipping manufacturing workers with AI-driven data and insights to improve customer support and streamline complex operations

Using Azure OpenAI, electrical equipment manufacturer ABB is integrating generative AI into its platform and applications to provide industry executives, specialists and engineers with real-time insights for better decision making and increased productivity. To reduce waiting time for customers, Dubai-based BMW Group importer AGMC is building an AI solution to more quickly locate and transport vehicles from its largest service center. By creating a scalable AI platform in Azure, Swedish steel manufacturer Epiroc is helping automate complex processes for its global employees to ensure consistent quality of its products, increase efficiency and reduce waste. Printer and imaging company Lexmark has improved its technician support efficiency by 20% with AI-driven predictive modelling, contributing to a 20% increase in customer satisfaction ratings. Xiaomi has developed an AI service bot with Power Virtual Agents to handle customer questions more efficiently and is better using Azure Translator to quickly generate product manuals in multiple languages. Indian textile company Arvind is leveraging the integrated AI capabilities of Power Apps to reduce sampling errors by 90%, production errors by 70% and order turnaround time by 30%.

INSPIRE Environmental is using Azure AI and machine learning to reduce costs and client turnaround time for image analysis of ocean data.

Enhancing employee productivity and providing customers with AI-enhanced solutions across professional services

Lumen Technologies is empowering employees across organizations with Microsoft 365 Copilot, making it easier for customer service teams to access repair manuals and helping sales teams summarize actionable steps from customer communications. In France, consulting firm Arthur D. Little is unifying its complex, unsorted data while maintaining data confidentiality and maximizing human capital — helping its consultants prepare for client meetings faster and curate presentation content in 50% less time. Jacobs Solutions is making it easier for staff to access and share high-quality data by integrating Microsoft Fabric with its end-to-end platform for seamless data management and advanced AI capabilities. INSPIRE Environmental is reducing costs and client turnaround time for image analysis of complex ocean data using Azure AI and machine learning. By leveraging Azure OpenAI, Orca Security is speeding up response times to customers and increasing data security while meeting regulatory compliance, Amadeus is working with Microsoft and Accenture to assist corporate travelers with trip logistics through its interactive assistant — as well as building a plug-in for Microsoft 365 Copilot, and professional services firm Atera is helping IT technicians focus on high value work while improving efficiency by 10x with its AI-powered platform.

Today's AI advancements have also generated new opportunities for digital natives to build upon their cloud-first strategies and shape the future of industry. To combat and identify cyber financial crime behaviors, SymphonyAI developed an AI assistant to help investigators securely and automatically collect, collate and summarize financial and third-party information. By applying AI capabilities across thousands of interactions each day, CallMiner is unlocking insights that empower organizations with a better understanding of the customer journey. Striving to be a leader in empathetic personal intelligence, Inflection AI is developing an AI chatbot that helps people accelerate development while reducing downtime. Building upon its cloud-based solution on Azure, Elastic is giving customers more focused answers to search queries by combining internal company knowledge with context from actual documents. With Azure OpenAI and Azure Cognitive Services, DeepBrain AI is cutting down chatbot development time and reducing manual processes while increasing language understanding and translation capabilities; while Commerce.AI has already started driving up to 50% increases in productivity for clients with its consumer sentiment analytics solutions.

We have a collective opportunity to put generative AI to work to help solve the biggest challenges facing organizations, industries and society today. Microsoft is helping customers and partners prioritize work and pragmatically innovate with generative AI to meet their most pressing business needs. From our unwavering commitment to building products responsibly and securely to how we engage with our customers and partners, I am proud of the trust we have forged along the way. Whether you are leveraging our copilot capabilities across the Microsoft Cloud, working with our partners to apply industry-specific cloud and AI solutions or seeking to co-innovate and co-develop to build custom solutions, I look forward to working alongside you to accelerate your AI transformation.

Tags: AI, AI Customer Commitments, Azure, Azure AI, Azure OpenAI Service, Copilot Copyright Commitments, Digital Natives, HoloLens 2, ISV, Microsoft 365 Copilot, Microsoft AI, Microsoft Cloud, Microsoft Cloud for Healthcare, Microsoft Cloud Partner Program, Microsoft Copilot, Microsoft Dynamics 365 Copilot, Microsoft Fabric, Microsoft Partners, Microsoft Power Platform, Microsoft Security, Power Virtual Agents

Today, Microsoft officially completed the acquisition of Activision Blizzard, publishers of some of the most played and most beloved franchises in gaming history across console, PC and mobile. From Pitfall to Call of Duty, World of Warcraft to Overwatch, Candy Crush Saga to Farm Heroes Super Saga, their studios have pushed the boundaries of gaming for players around the world. For the millions of fans who love Activision, Blizzard and King games, we want you to know that today is a good day to play. Together, we'll learn, innovate and continue to deliver on our promise to bring the joy and community of gaming to more people. Visit Xbox Wire to learn more. Tags: XBOX

Kathleen Hogan, Executive Vice President and Chief Human Resources Officer, shared the below communication today with Microsoft employees.

I am profoundly saddened by the horrific terrorist attacks in Israel this weekend and the atrocities we continue to watch unfold. Together with the senior leadership team (SLT), I want to express my deepest sympathy for those who have been killed, injured, lost loved ones, and all those impacted by this violence. Together we condemn this hatred and brutality. To everyone impacted, please know we are all with you in our hearts and in our actions, and we will continue to do everything we can to support you and keep you safe.

The outpouring of support from so many of you worldwide is heartfelt and needed now, and many of you are asking how you can help. Since the attacks unfolded on Saturday, we have been focused on the safety of our employees and their families in Israel and their overall wellbeing. Many teams across Microsoft — including our Crisis Management Team — have been activated and are working to assist employees. Our SLT is in direct contact with our local leaders in Israel and our employee communities, to offer support and to understand how best to help employees and their families during this unimaginable time.

We have nearly 3,000 employees in Israel directly impacted. We have Jewish employees around the world who are also experiencing grief, fear and anxiety as hatred and vitriol increase. We have Palestinian employees globally who are deeply concerned for the safety of their loved ones in the region and stand against these acts of terrorism.

We have many resources to help. Microsoft CARES provides mental and emotional wellbeing programs. In addition, we are responding quickly to support those impacted by these tragic events, assessing their needs and providing benefits such as adding virtual support groups. We will continue to listen and determine what other resources are needed.

Microsoft Philanthropies has activated a giving opportunity for eligible employees who wish to support relief efforts. Additionally, Microsoft's Disaster Response is connecting with local organizations to support first responders with digital technology and services and will help as they receive further requests for assistance.

Microsoft employees show great care, empathy and allyship for each other. Thank you for checking in on your colleagues, listening, and showing support. Thank you to the many employees in Israel volunteering to address urgent community needs and leveraging their AI, cybersecurity, data and research expertise to help. And thank you to local leadership in Israel for coordinating efforts with the SLT and being there for our employees.

Across the company, let us stand together in our shared humanity.

Every industry depends on unique insights to achieve their goals, and unlocking the power of data is the key to an organization's success. This is especially true in healthcare, where data has the potential to do so much good – from improving health outcomes and enhancing patient and clinician experiences to driving better organizational performance for healthcare systems. When a patient's or the population's health is at the center of the decision-making process, having the right data strategy in place can have a life-changing impact on people's lives.

Healthcare data continues to grow rapidly, and organizations are struggling to keep up with higher volume, greater variety and increased velocity. According to the World Economic Forum, hospitals produce 50 petabytes of siloed data per year – that's equivalent to approximately 10 billion music files. Ninety-seven percent of this data goes unused, leaving many valuable insights locked away. Putting all this data to good use is the key to unlocking clinical and operational breakthroughs that can make a meaningful difference in the lives of patients and their healthcare journey. And in the new era of AI, the importance of data continues to grow as organizations realize that without a solid data strategy, they are only scratching the surface of what's possible with AI.

At the HLTH 2023 conference, we're introducing new data and AI solutions and capabilities that will help healthcare organizations stay focused on improving patient and clinician experiences while delivering quality care more efficiently and at a lower cost. Together, these new solutions offer healthcare organizations a unified, safe and responsible approach to their data and AI strategy and enable them to take advantage of the breadth and scale of Microsoft Cloud for Healthcare.

Unifying data analytics to drive business value and better patient care In May of this year, we unveiled Microsoft Fabric – an end-to-end, unified analytics platform that brings together all the data and analytics tools that organizations need to unlock the potential of their data and lay the foundation for the era of AI. Today, through the power of Microsoft Cloud for Healthcare, we are introducing the first industry-specific data solutions in Fabric that unify data and insights through one common architecture and experience. Now available in preview, the healthcare data solutions in Fabric eliminate the costly, time-consuming process of stitching together a complex set of disconnected, multimodal health data sources – text, images, video, etc. – and provides a secure and governed way for organizations to access, analyze and visualize data-driven insights across their organization.

The ability to combine data from previously siloed sources across their organization, such as electronic health records (EHRs), Picture Archiving and Communication Systems (PACS), labs systems, claims systems and medical devices. The solution brings structured, unstructured, imaging and medical device data into the Fabric data lake with open data standards using FHIR, DICOM and MedTech services, providing customers with one common architecture. Additionally, connectors and converters make it easier to transform FHIR, DICOM and MedTech data from one format to another or build pipelines for specific use cases. A multimodal data foundation that allows them to build standardized, scalable solutions that help accelerate the process of uncovering impactful clinical and operational insights and ultimately drive better patient care. Fabric helps create a single data estate where health data can live and be used to build and run AI models, as well as derive insights. Standard capabilities like Observational Medical Outcomes Partnership (OMOP) analytics enable clinical research and patient outreach analytics help provide more personalized engagement with patients. A new de-identification service will allow organizations to de-identify clinical data, keeping patient-protected health information (PHI) private by using machine learning models to extract, redact or surrogate identifiers while unlocking insights from unstructured data, such as doctor's notes, medical documents and clinical trial studies. In addition, through healthcare-specific pre-built classification rules, labels and data glossaries in Microsoft Purview (preview), healthcare organizations can govern, protect and manage their entire data estate.

Organizations across the healthcare spectrum can benefit from Microsoft Fabric, with early adopters already planning to leverage the analytics platform to help advance some of their most prominent use cases:

Northwestern Medicine, Chicago's premier integrated academic health system, will leverage the healthcare data solutions in Fabric to integrate clinical data across a variety of sources, meet regulatory mandates for information exchange, and unlock insights with data and AI, helping further their patients-first mission with high quality and timely care. Arthur Health plans to use Fabric to power predictive care stage models in partnership with Quisitive for the Ontario Workers Network (OWN). OWN is a provincial network of hospitals, including Ottawa Hospital, and has experienced clinicians that provide world-class care for workers in their own communities. SingHealth, Singapore's largest network of public healthcare institutions, aims to harness the power of Fabric's healthcare data solutions for its underlying data infrastructure. This will help to transform the delivery of healthcare to provide excellent care and services to the population and patients, empowering them to take care of their own health and healthcare.

Microsoft Fabric is a step forward in solving the healthcare industry's longstanding disparate data problem. Learn more about the healthcare data solutions in Fabric here.

New AI capabilities that empower patients and simplify medical jargon Within Azure AI services, we are releasing new healthcare capabilities that will help organizations maximize the value of AI to increase positive impact on patient outcomes:

Helping clinicians and researchers make informed decisions – Azure AI Health Insights is a cognitive service that provides prebuilt models that perform analysis and provide inferences that can be reviewed and used by clinicians and researchers to facilitate patient care during important healthcare scenarios.

We are launching three new models in preview, including patient timeline, which uses generative AI to extract key events from unstructured data, such as medications, diagnosis and procedures, and organizes them chronologically to give clinicians a more accurate view of a patient's medical history to better inform care plans. Clinical report simplification uses generative AI to give clinicians the ability to take medical jargon and convert it into simple language while preserving the full essence of the clinical information so that it can be shared with others, including patients. Radiology insights provides quality checks through feedback on errors and inconsistencies. The model also identifies follow-up recommendations and clinical findings within clinical documentation with measurements (sizes) documented by the radiologist.

Bringing generative AI to healthcare chatbots and virtual assistants – This new preview capability in Azure AI Health Bot provides out-the-box healthcare intelligence that can be customized and connected into existing workflows, using answers from a healthcare organization's own content sources, as well as leveraging generative AI to provide answers from credible sources like the National Institutes of Health and the U.S. Food and Drug Administration. Extracting and labeling medical data to identify meaningful insights – Text Analytics for health, an Azure AI Language service, applies machine learning intelligence to extract and label essential medical information from a variety of unstructured texts. Newly released industry open source templates include population health, patient population Q&A using Azure OpenAI Service, clinical trials patient cohorts and mass historic data processing.

AI-powered solutions empower clinicians to deliver quality, personalized care U.S. health systems are turning to AI-powered solutions to alleviate administrative burden and the resulting clinician burnout, which rose to 53% among physicians in 2023 compared to 42% in 2018, according to Medscape's 2023 survey. To address this burnout, and empower clinicians to focus on delivering high-quality personalized care, we announced the general availability of Dragon Ambient eXperience (DAX™) Copilot, formerly known as DAX Express.

DAX Copilot, part of the larger Nuance Dragon family of solutions used by more than 550,000 users worldwide, allows clinicians to create draft clinical summaries automatically and securely in seconds from exam room or telehealth conversations for immediate review and entry in the EHR.

Atrium Health was the first to deploy Nuance DAX Copilot to its primary care physicians with plans for a broad rollout across its footprint. Physicians are already reporting saving meaningful time in their documentation tasks for each patient visit. In particular, Atrium Health clinicians are reporting that physicians are already saving up to 40 minutes per day with this advanced documentation technology. Additionally, 68% have recognized an improved experience providing care..

Microsoft Cloud for Healthcare in the era of data and AI These new industry innovations in data and AI are strengthened through Microsoft Cloud for Healthcare, which enables healthcare organizations to accelerate their data and AI journey by augmenting the Microsoft Cloud with industry relevant data solutions, application templates and AI services. Our offerings can also be customized by an unmatched global ecosystem of trusted partners. We work with leading ISVs and system integrators so that our healthcare customers have complete solutions that address their unique business challenges.

Our healthcare solutions are built on a foundation of trust and Microsoft's Responsible AI principles. Through these innovations, we are making it easier for healthcare organizations to create connected experiences at every point of care, provide tools that foster collaboration, empower the healthcare workforce, and unlock the value from clinical and operational data using data standards that are important to the healthcare industry.

Tags: AI, Azure AI, Azure AI Health Insights, healthcare, Microsoft Cloud for Healthcare, Microsoft Fabric

I recently highlighted how generative AI technology is opening doors for organizations to imagine new ways to solve challenges, while unlocking innovation and delivering greater business value for those who adopt it. Organizations with modern data estates fortified with the Microsoft Cloud are already accelerating their AI transformation; and as many business leaders seek to keep pace they are turning to Microsoft and our partner ecosystem for industry expertise, scale and copilot capabilities.

Microsoft has built a foundation of successfully delivering AI solutions for many years. Now, we are infusing generative AI capabilities across the services customers know and love, while remaining committed to helping them apply it responsibly and securely. With so many compelling AI opportunities to discover, it is important leaders across industries understand how to successfully use this groundbreaking technology to meet their business imperatives.

At our kickoff experience in New York City last month, we highlighted Microsoft's copilot technology and learned from customers about the business results and productivity gains they are realizing today. I am pleased to announce additional dates for Microsoft Envision | The Tour — an in-person experience bringing together senior decision makers and industry experts in AI. Attendees will leave feeling equipped with the latest information to begin — or build upon — their AI transformation.

Please join us on Oct. 18 as we continue the tour in London, where Satya Nadella will discuss the opportunity for organizations around the world to transform every aspect of their business with AI. To register and learn more about upcoming dates across Mumbai, Sydney, Mexico City, Paris and Tokyo, visit: Microsoft Envision | The Tour.

I look forward to connecting with you as we begin one of the most exciting and pivotal years for technology in our lifetimes.

The Microsoft Cloud for Sovereignty represents a significant advancement at the intersection of global demand for innovation and evolving national policies regarding data privacy and control. It ushers in a new era for governments worldwide, providing a secure avenue for modernizing their technology infrastructure and workflows.

Today, we are announcing public preview of the Microsoft Cloud for Sovereignty. Furthermore, we are announcing our planned general availability of the Microsoft Cloud for Sovereignty for this December. This solution will enable governments to meet their compliance, security and policy requirements while harnessing the cloud to deliver value to their citizens.

Since the inception of the cloud, government customers have faced limitations with digital transformation, in part because of the need for controls to meet specific national and regional requirements. Over recent years, innovators within global governments have asked for alternatives to the heavy capital expense and operational costs of a legacy approach, both to reduce the overhead of owning and operating datacenters and to facilitate modernization. Additionally, evolving policy decisions such as the European Commission's EU-U.S. Adequacy Decision in July 2023 under the EU General Data Protection Regulation (GDPR) now provide legal support for continued adoption of hyperscale cloud computing by government customers within the EU.

With these two trends converging in the public sector, and with specific requirements varying by country, Microsoft Cloud for Sovereignty is grounded in a repeatable best-practice approach that can be leveraged to assist with complex regulation achievements. This solution features industry-leading data sovereignty and encryption controls, enabling governments to easily create solutions tailored to help address regional and national requirements.

At Microsoft, we believe in transparency so that people and organizations can control their data and have meaningful choices in how it's used. We empower and defend the data privacy choices of every person who uses our products and services.

Our principled approach to digital sovereignty in the cloud builds on our principled approach to data privacy in the cloud. We understand that sovereignty can mean different things for different scenarios, and as we work with customers and partners around the world the common thread is a need to determine for themselves where their data resides and how it's protected, including who has access to that data.

For most government needs, the security, privacy and compliance capabilities of the Microsoft Cloud already deliver on their requirements, regulations and standards — the additional capabilities we're providing with the Microsoft Cloud for Sovereignty are designed specifically for countries with jurisdictional requirements around sensitive data. With all Microsoft Cloud solutions, customers benefit from industry-leading cybersecurity, along with the broadest compliance and more regions than any other cloud provider.

Our collaboration with partners who deeply understand national requirements enables us to provide a global solution for local requirements. These requirements are often complex due to a layered landscape of evolving policy, trends and regulations. The solutions Microsoft Cloud for Sovereignty delivers have been validated by governments and their partners as a preferred way to unlock cloud value. I've had the opportunity to talk with many of our pilot customers and wanted to share a glimpse into how two of them are using this solution.

In the Netherlands, the National Cyber Security Centre (NCSC), a leader in cyber security guidelines and advisories, including those related to secure cloud adoption, is working to establish a cloud center of excellence enabling compliant and rapid cloud adoption. To this end, NCSC is currently piloting Microsoft Cloud for Sovereignty with sovereign landing zones and built-in policy initiatives to meet the Dutch BIO regulation. In the NCSC's own words,

"The National Cyber Security Centre (NCSC) is piloting avenues to adopt and use secure, robust public cloud offerings," says Arnoud van Petersen, CIO & Head of IT Services at NCSC-NL. "Microsoft Cloud for Sovereignty, with its specialized features like sovereign landing zones, and well-developed security and AI capabilities provide a solution that fits our strategy of enabling cloud innovation without compromising sovereign controls."

InSpark, (a subsidiary of Royal KPN), is a fully dedicated Microsoft Cloud Incubator for mission-critical infrastructures for the government and enterprises. InSpark is working to provide a compliant and repeatable approach to meet the demands of Dutch public sector customers, such as the Municipality of Amsterdam, with Microsoft Cloud for Sovereignty.

"At Municipality of Amsterdam, Cloud for Sovereignty will enable us to leverage cloud capabilities for processes that use or create sensitive information. By migrating from an on-premises datacenter to the cloud in compliance with the relevant classification within the Dutch BIO regulation, this enables us to efficiently offer modern services and ultimately improve the experiences of our citizens," says Patrick Scholte, Director Platforms & Development, Municipality of Amsterdam.

In Sweden, Atea is at the forefront of exploring new potentials of leveraging technology and functionality through the public cloud while incorporating additional security measures provided by Microsoft Cloud for Sovereignty. With these enhanced capabilities for managing sensitive data in the cloud, Atea is empowering public customers to deliver digital services and assisting government agencies in improving citizens' digital experiences. Additionally, Atea supports customer use cases enabling advanced predictive healthcare by utilizing Microsoft Cloud for Sovereign technology to analyze highly sensitive data. In the United Arab Emirates, G42 enables the UAE public sector and regulated industries clients to use new platform capabilities for securing data and workloads, providing access to the latest cloud and AI features available on Azure and helping them comply with local privacy and regulatory requirements. In Italy, Leonardo is part of the consortium that delivers the Polo Strategico Nazionale (PSN), which translates as "National Strategic Hub," that aims to provide a centralized, secure and compliant set of cloud services for public administrations in Italy, ranging from central entities like the ministries, down to regional and local governments such as health agencies and Leonardo is using Microsoft Cloud for Sovereignty to deliver solutions for multiple customers that leverage the economies of scale, security levels and pace of innovation that today are only available with hyperscale clouds. According to recent research from Accenture, European enterprises are increasingly embracing sovereign cloud, with 37% already invested and 44% planning to invest in the next two years. With 137 countries enacting some form of data protection and sovereignty laws, Accenture can use Microsoft Cloud for Sovereignty to support governments with a sound sovereign cloud strategy to establish control of their data while unlocking new sources of value in the digital realm. Landing tailored policies with sovereign controls

Our intent with the Microsoft Cloud for Sovereignty is to unlock cloud innovation for governments through tailored sovereign controls, and our technical approach is grounded in repeatable best practices designed to help customers achieve their regional and national requirements.

With the start of the public preview today, we're releasing:

The Sovereign Landing Zone and policy initiative now available on GitHub, which instantiates guardrails for sovereign cloud environments for customer workloads, enabling customers to leverage best practices for secure and consistent environments while supporting their efforts to meet evolving local regulations. Support for Italy's ACN requirements and Netherlands BIO regulation which help customers more easily monitor, guard and report on their compliance in Azure. Transparency Logs, available to eligible customers, provides customers with visibility into key operational activities of Microsoft engineers to support customer service and service reliability issues. Automated workload templates for Azure Confidential Computing and Azure Lighthouse as examples for building workloads using these technologies for sovereign environments to speed learning and adoption. Technical documentation on Microsoft Learn.

Start exploring the new capabilities today and get started with the Sovereign Landing Zone. Stay tuned for more updates on Microsoft Cloud for Sovereignty by bookmarking microsoft.com/sovereignty.

As we turn the corner into a new era for government innovation in the cloud, we look forward to continuing to serve customer needs for preferred solutions that facilitate innovation while helping meet their various compliance, security and policy requirements.

Updated November 15, 2023: To simplify the user experience and make Copilot more accessible to everyone, Bing Chat and Bing Chat Enterprise will now simply become Microsoft Copilot. For more information: We are entering a new era of AI, one that is fundamentally changing how we relate to and benefit from technology. With the convergence of chat interfaces and large language models you can now ask for what you want in natural language and the technology is smart enough to answer, create it or take action. At Microsoft, we think about this as having a copilot to help navigate any task. We have been building AI-powered copilots into our most used and loved products – making coding more efficient with GitHub, transforming productivity at work with Microsoft 365, redefining search with Bing and Edge and delivering contextual value that works across your apps and PC with Windows. Today we take the next step to unify these capabilities into a single experience we call Microsoft Copilot, your everyday AI companion. Copilot will uniquely incorporate the context and intelligence of the web, your work data and what you are doing in the moment on your PC to provide better assistance – with your privacy and security at the forefront. It will be a simple and seamless experience, available in Windows 11, Microsoft 365, and in our web browser with Edge and Bing. It will work as an app or reveal itself when you need it with a right click. We will continue to add capabilities and connections to Copilot across to our most-used applications over time in service of our vision to have one experience that works across your whole life. Copilot will begin to roll out in its early form as part of our free update to Windows 11, starting Sept. 26 — and across Bing, Edge, and Microsoft 365 Copilot this fall. We're also announcing some exciting new experiences and devices to help you be more productive, spark your creativity, and to meet the everyday needs of people and businesses.

With over 150 new features, the next Windows 11 update is one of our most ambitious yet, bringing the power of Copilot and new AI powered experiences to apps like Paint, Photos, Clipchamp and more right to your Windows PC. Bing will add support for the latest DALL.E 3 model from OpenAI and deliver more personalized answers based on your search history, a new AI-powered shopping experience, and updates to Bing Chat Enterprise, making it more mobile and visual. Microsoft 365 Copilot will be generally available for enterprise customers on Nov. 1, 2023, along with Microsoft 365 Chat, a new AI assistant that will completely transform the way you work. Additionally, we introduced powerful new Surface devices that bring all these AI experiences to life for you, and they are available for pre-order beginning today.

New Windows 11 Update delivers over 150 new features, including bringing the power of Copilot to the PC Today, we're thrilled to share our next step toward making Windows the destination for the best AI experiences – with a new update that delivers our most personal experience yet coming on Sept. 26. Here's a look at some of what's new in the latest update for Windows 11:

Copilot in Windows (in preview) empowers you to create faster, complete tasks with ease and lessens your cognitive load – making once complicated tasks, simple. We've made accessing the power of Copilot seamless as it's always right there for you on the taskbar or with the Win+C keyboard shortcut providing assistance alongside all your apps, on all screen sizes at work, school or at home. Paint has been enhanced with AI for drawing and digital creation with the addition of background removal and layers as well as a preview of Cocreator that brings the power of generative AI to the Paint app. Photos has also been enhanced with AI including new features to make editing your photos a breeze. With Background Blur you can make the subject of your photo stand out quickly and easily. The Photos app automatically finds the background in the photo, and with a single click, instantly highlights your subject and blurs out the background. We've improved search, with photos stored in OneDrive (home or personal) accounts, you can now quickly find the photo you're looking for based on the content of the photo. You can also now find photos based on the location where they were taken. Snipping Tool now offers more ways to capture content on your screen – with this update you can now extract specific text content from an image to paste in another application or, you can easily protect your sensitive information with text redaction by using text actions on the post capture screen. And, with the addition of sound capturing using audio and mic support, it's easier to create compelling videos and content from your screen. Clipchamp, now with auto compose, helps you with scenes suggestions, edits and narratives based on your images and footage automatically so you can create and edit videos to share with family, friends, and social media like a pro. Notepad will start automatically saving your session state allowing you to close Notepad without any interrupting dialogs and then pick up where you left off when you return. Notepad will automatically restore previously open tabs as well as unsaved content and edits across those open tabs. With the new Outlook for Windows, you can connect and coordinate your various accounts (including Gmail, Yahoo, iCloud, and more) in one app. Intelligent tools help you write clear, concise emails and seamlessly attach important documents and photos from OneDrive. To learn more, visit this link. Modernized File Explorer, we are introducing a modernized File Explorer home, address bar and search box all designed to help you more easily access important and relevant content, stay up to date with file activity and collaborate without even opening a file. Also coming to File Explorer is a new Gallery feature designed to make it easy to access your photo collection. New text authoring experiences to voice access and new natural voices in Narrator, continuing our ongoing commitment to making Windows 11 the most accessible version of Windows yet. Windows Backup makes moving to a new Windows 11 PC easier than ever. With Windows Backup, transitioning most files, apps and settings from one PC to another, is seamless so everything is right where you left it, exactly how you like it.

These experiences, including Copilot in Windows and more will start to become available on Sept. 26 as part of our latest update to Windows 11, version 22H2. Bing and Edge are redefining how we interact with the web Today, we're announcing new features in Bing and Edge to supercharge your day powered by the latest models delivering the most advanced capabilities for AI available. You can use Bing Chat today with Microsoft Edge or at bing.com/chat. Features will begin to roll out soon.

Personalized answers. Now, your chat history can inform your results. For example, if you've used Bing to track your favorite soccer team, next time you're planning a trip it can proactively tell you if the team is playing in your destination city. If you prefer responses that don't use your chat history, you can turn this feature off in Bing settings. Copilot in Microsoft Shopping. From Bing or Edge, you can now more quickly find what you're shopping for online. When you ask for information on an item, Bing will ask additional questions to learn more, then use that information to provide more tailored recommendations. And you can trust you're getting the best price – in fact, in the last 12 months, shoppers have been offered more than $4 billion in savings on Microsoft Edge. Soon, you'll also be able to use a photo or saved image as the starting point for shopping.

DALL.E 3 model from OpenAI in Bing Image Creator. DALL.E 3 delivers a huge leap forward with more beautiful creations and better renderings for details like fingers and eyes. It also has a better understanding of what you're asking for, which results in delivering more accurate images. We're also integrating Microsoft Designer directly into Bing to make editing your creations even easier. Content Credentials. As we continue to take a responsible approach to generative AI, we're adding new Content Credentials which uses cryptographic methods to add an invisible digital watermark to all AI-generated images in Bing – including time and date it was originally created. We will also bring support for Content Credentials to Paint and Microsoft Designer. Bing Chat Enterprise Updates. Since its introduction just two months ago, more than 160 million Microsoft 365 users now have access to Bing Chat Enterprise at no additional cost and the response has been incredible. Today we're announcing that Bing Chat Enterprise is now available in the Microsoft Edge mobile app. We're also bringing support for multimodal visual search and Image Creator to Bing Chat Enterprise. Boost your creativity at work with the ability to find information using images and creating them.

YouTube Video Click here to load media Transforming work with Microsoft 365 Copilot, Bing Chat Enterprise and Windows In March, we showed you what Microsoft 365 Copilot can do in the apps millions of people use every day across work and life – Word, Excel, PowerPoint, Outlook and Teams – using just your own words. After months of learning alongside customers like Visa, General Motors, KPMG and Lumen Technologies, we're excited to share that Microsoft 365 Copilot will be generally available for enterprise customers on Nov. 1.

YouTube Video Click here to load media Today, we're also introducing a new, hero experience in Microsoft 365 Copilot: Microsoft 365 Chat. You saw a glimpse of Microsoft 365 Chat in March, then called Business Chat — but rapid advancements over the last few months have taken it to a whole new level. Microsoft 365 Chat combs across your entire universe of data at work, including emails, meetings, chats, documents and more, plus the web. Like an assistant, it has a deep understanding of you, your job, your priorities and your organization. It goes far beyond simple questions and answers to give you a head start on some of your most complex or tedious tasks — whether that's writing a strategy document, booking a business trip, or catching up on emails. Over the past few years, the pace and volume of work have only increased. On a given workday, our heaviest users search for what they need 18 times, receive over 250 Outlook emails and send or read nearly 150 Teams chats.[1] Teams users globally are in three times more meetings each week than they were in 2020.[2] And on Windows, some people use 11 apps in a single day to get work done. [3] Microsoft 365 Chat tames the complexity, eliminates the drudgery and helps you reclaim time at work. Preview customers can access it today on Microsoft365.com, Teams, or in Bing when signed in with their work account. In the future you'll be able to access it wherever you see the Copilot icon when signed in with your work account. To empower you at work, we're also introducing new capabilities for Copilot in Outlook, Word, Excel, Loop, OneNote and OneDrive. Bing Chat Enterprise —the first entry point into generative AI for many companies — is getting a few upgrades. And as part of our big Windows 11 update, Windows 365 Switch and Windows 365 Boot will be generally available making it even easier to access your Windows Cloud PC. This will help employees achieve more, while making it easier for IT to deploy, manage and secure. Check out the Microsoft 365 blog to learn more about how Microsoft 365, Bing Chat Enterprise and Windows are transforming the way we work. Unleashing personal productivity and creativity with Designer and Copilot in Microsoft 365 Designer, the newest addition to our family of Microsoft 365 consumer apps, helps you quickly create stunning visuals, social media posts, invitations, and more using cutting-edge AI. Today, we're showing some powerful new features, many of which will be powered by OpenAI's Dall.E 3. Generative expand uses AI to extend your image beyond its borders, generative fill adds a new object or background, and generative erase can remove unwanted objects.[4] Dall.E 3 will also soon power the image generation experience in Designer, making it easy to add original, higher quality images to your design in seconds.

YouTube Video Click here to load media We're also integrating Designer into Microsoft 365 Copilot for consumers — starting with Word. Designer uses the context of your document to propose visuals to choose from; you can make it more personal by uploading your own photos too. And within moments, you can transform a text-heavy document with custom graphics. We're starting to test Microsoft 365 Copilot with a small group of Microsoft 365 consumer subscribers and look forward to expanding the preview to more people over time. Seventy percent of creators tell us one of the most difficult parts of the creation process is just getting started.[5] With creative tools like Designer, plus Bing Image Creator, Clipchamp and Paint, you can now have an immediate visual draft of almost anything — with a few simple prompts. Introducing new Surface devices available for pre-order beginning today for people and businesses There is no better stage to bring to life all of the incredible AI experiences from across Microsoft than our new Surface devices. Surface is at the forefront of device performance and processor technology. We have been investing in silicon advancements to augment this next wave of AI innovation, unlocking experiences like Windows Studio Effects in Surface Pro 9 with 5G and continuing to increase performance to run the latest AI models with powerful devices like the new Surface Laptop Studio 2.

The new Surface Laptop Studio 2 is the most powerful Surface we've ever built. Turbocharged with the latest Intel® Core processors and cutting-edge NVIDIA® Studio tools for creators-with up to 2x more graphics performance than MacBook Pro M2 Max, [6] Surface Laptop Studio brings together the versatility to create and the power to perform — a stunning 14.4″ PixelSense Flow touchscreen display and flexible design with three unique postures. And with new customizations brought to the haptic touchpad to improve accessibility – we're proud to call it the most inclusive touchpad on any laptop today.

The new Surface Laptop Go 3 will turn heads with its balance of style and performance. It's our lightest and most portable Surface Laptop, with a touchscreen display, and packed with premium features like an incredible typing experience and a Fingerprint Power Button, and it comes in four stylish colors. With Intel® Core i5 performance, all-day battery life, and robust RAM and storage options, it's the perfect everyday laptop and stage for the latest AI tools from Microsoft.

Surface Go 4 for Business is our most portable Surface 2-in-1. This fall, the new Surface Go will be available exclusively for organizations to meet the growing needs of frontline workers and educators. We can't wait to see how it will help businesses modernize and make their users more productive. Surface Hub 3 is the premier collaboration device built for hybrid work, designed end-to-end by Microsoft. The Microsoft Teams Rooms on Windows experience is familiar and intuitive on a brilliant 50" or 85" screen. The 50" Surface Hub 3 brings entirely new ways to co-create with Portrait, Smart Rotation and Smart AV. AI-enhanced collaboration tools – like Cloud IntelliFrame and Copilot in Whiteboard – shine on Surface Hub 3.

3D printable Adaptive Pen Grips for Surface Pen have been added to our lineup of adaptive accessories enabling more people to engage in digital inking and creation than before. They are available for purchase through Shapeways or as downloadable plans for 3D printing. To hear more about how we're taking steps to close the disability divide, check out our video.

To pre-order one of our incredible new Surface devices, visit Microsoft.com, Bestbuy.com, and our Surface for Business page and blog to learn more about all of today's new products. The new era of AI with Copilot from Microsoft is here – and it's ready for you We believe that Microsoft is the place where powerful, useful AI experiences come together – simply, securely and responsibly – into the products you use most. Today, we showed you how we are not only increasing the usefulness of these experiences, but we are expanding them​. From Windows 11 as the destination for the best AI experiences to empower people using it at work, school and home​. To Microsoft 365, the most trusted productivity suite on the planet​. To Bing and Edge, the most innovative search engine and browser available​. All of it coming together on Windows 11 PCs like Surface​. And with Copilot helping you get things done, helping you create and connect to people you care about or the world around you​. We can't wait to see what you can do with these experiences. Learn more on the Microsoft 365 blog and the Security blog. And for all the blogs, videos and assets related to today's announcements, please visit our microsite. [1] Data represents top 20% of users by volume of searches across M365 services, emails received, and sent and read chats in Teams, respectively. [2] Microsoft annual Work Trend Index 2023- Work Trend Index | Will AI Fix Work? (microsoft.com) [3] Data reflects the top 20% Windows devices by app volume per day. [4] Generative erase in Microsoft Designer is generally available to try today, with generative expand and fill coming soon. [5] Survey of 941 creators commissioned by Microsoft in June 2022. [6] Tested by Microsoft in September 2023 using CineBench 2024 GPU benchmark comparing Surface Laptop Studio 2 with RTX 2000 Ada Generation to MacBook Pro14" with M2 Max 19 12 core / 30 core configuration. Tags: AI, Bing, Designer, Microsoft 365, Microsoft Copilot, Microsoft Edge, Surface, Windows 11

The vast amount of data in today's enterprises provides a great opportunity to get valuable insights that enhance an organization's products, goods and services. At the same time, many companies are migrating to the cloud to take advantage of its flexibility, efficiency and scale.

With the recent advances in generative AI, businesses face new urgency to bring cloud services like analytics and machine learning to one of their most valuable technology environments: data. According to Gartner, the database market grew 14.4% this year and is on course to be a more than $100 billion market by the end of 2023. The overwhelming contributor to that growth is cloud database as a service.

For customers moving their critical data environments from within the walls of their business to the cloud, one key consideration is the ability to seamlessly connect data sources to new and massive scale cloud services like Azure OpenAI and Azure Kubernetes Service. That's why we are expanding our partnership with Oracle, enabling Oracle's 430,000 customers[i] to apply the unique services of the Microsoft cloud to Oracle's mission-critical databases. With this new offering, Azure is the only cloud provider other than Oracle Cloud Infrastructure to host Oracle services, including Oracle Exadata Database Service and Oracle Autonomous Database on Oracle Cloud Infrastructure in Azure datacenters.

Our deep partnership with Oracle continues to focus on reducing the common hurdles customers face when migrating workloads to the public cloud. Oracle Database@Azure is our new offering to bring Oracle Database services inside Azure. With the ability for customers to migrate Oracle databases "as is" to OCI and deploy them in Azure alongside their current workloads in the Microsoft Cloud, organizations can create new solutions and further competitive differentiation.

Customers across industries have expressed interest in the solution, including Fidelity, PepsiCo, Vodafone and Voya Financial.

Oracle Database@Azure opens new growth potential for the 97% of Fortune 100 companies who use Oracle databases.[i] From financial services to telecommunications and retail to transportation, businesses across every industry are eager to integrate mission-critical data from line of business apps with cloud services to realize higher efficiency, reduce customer churn and create new business applications. Oracle Database@Azure will improve the customer experience with higher availability, enhancing security and compliance on their transactions, improving logistics and supply chain management, and more.

To make this seamless, our engineering teams worked closely to colocate Oracle database services on OCI, such as Exadata and related OCI hardware, into Microsoft datacenters. The combination of Oracle Exadata hardware with OCI and Azure infrastructure means Oracle Database@Azure customers can migrate existing databases to OCI and deploy in Azure, enhance security by keeping apps and data on a single network, gain optimal performance with the same fully managed Exadata Database Service that runs in OCI, and then innovate with the comprehensive services offered in the Microsoft cloud. The tight integration also ensures that Azure customers can use existing skills to build and operate Oracle databases from within the Azure Portal and APIs.

Customers choose the Microsoft cloud for many reasons, including cutting-edge AI capabilities, multilayered security and ecosystem support with hundreds of products and thousands of partners. The high performance and availability of Oracle Exadata has been a critical component of enterprise operations for more than 15 years, running critical transaction processing, analytics, machine learning and mixed workloads. By bringing together the best of the Microsoft cloud and Oracle technology, Oracle Database@Azure helps enterprises harness the power of data in the cloud.

At Microsoft, we believe customers should have the freedom and flexibility to choose the services, infrastructure and clouds that best serve their unique needs. With our passion for helping customers create new value in the cloud, today's expanded partnership with Oracle marks a new chapter in meeting organizations' evolving needs.

Interested customers can learn more about the Oracle Database@Azure here or reach out to their Microsoft account team.

Today, the promise of technology to help us solve some of the biggest challenges we face has never been more tangible, and nowhere is generative AI more needed, and possibly more impactful, than in healthcare. Epic and Microsoft have been paving the way to bring generative AI to the forefront of the healthcare industry. Together, we are working to help clinicians better serve their patients and are addressing some of the most urgent needs, from workforce burnout to staffing shortages.

We combined Microsoft's large-scale cloud and AI technologies with Epic's deep understanding of the healthcare industry and clinical workflows to address many current issues affecting clinicians. Today, we are announcing the expansion of our strategic initiative to bring AI to healthcare at scale, integrating conversational, ambient and generative AI technologies across the Epic electronic health record (EHR) ecosystem. Intended to speed development of solutions for healthcare's most critical needs, the initiative will expand secure access to AI-powered clinical insights and administrative tools within a wide range of Epic modules to enhance patient care, increase operational efficiency, improve healthcare experiences, and support the financial integrity of health systems globally.

We are working together to rapidly deploy dozens of copilot solutions that securely unlock the potential value that the Microsoft Cloud and our AI technologies enable as health systems strive to overcome the urgent staffing, financial and clinical access challenges they face today. Epic will showcase many of these new capabilities that build on our Azure OpenAI Service and Nuance DAX Express solutions at its annual Users Group Meeting today, including:

Enhancing clinician productivity with note summarization: Building on the previously announced AI-assisted Epic In Basket, the new solutions are targeted at increasing clinical efficiency for physicians and nurses, helping them become more productive in their daily clinical workflow. The solutions will help support faster documentation through suggested text and rapid review with in-context summaries. Enhancing clinician productivity with embedded ambient clinical documentation: Leveraging Nuance's Dragon Ambient eXperience (DAX) technology, which is already deployed with hundreds of Epic customers and currently supporting thousands of physicians, Epic will showcase this DAX Express AI technology embedded into the native Epic Hyperdrive platform and Haiku mobile application, further enhancing a seamless workflow experience for users. In addition, Nuance has been named by Epic as one of the first Partners in Epic's Partner and Pals third-party vendor program. Driving administrative efficiencies through reduction in manual, labor intensive processes: Revenue cycle management is one of many areas where generative AI can meaningfully improve efficiency. For example, Epic will demonstrate an AI-powered solution that provides medical coding staff with suggestions based on clinical documentation in the EHR to improve accuracy and streamline the entire coding and billing processes. Advancing medicine for better patient outcomes: By using Azure OpenAI Service, Epic is now delivering generative AI exploration for an initial set of users via SlicerDicer to fill gaps in clinical evidence using real-world data and to study rare diseases and more.

Our work to integrate Azure OpenAI Service and Nuance ambient technologies within the Epic ecosystem shows that broader strategic collaborations can rapidly accelerate the availability of actionable AI-driven solutions for healthcare organizations and the patients they serve.

By 2025, the U.S. Department of Health and Human Services predicts that there will be a nationwide shortage of 90,000 physicians. Additionally, 40% to 60% of clinicians report they are experiencing burnout. On top of these challenges, healthcare providers are facing financial pressures while also trying to efficiently and effectively deliver quality care. According to McKinsey & Company, nearly a quarter of U.S. national health expenditure goes toward administrative costs, which could be reduced through technology.

Additionally, the urgent need to improve operational and clinical efficiency was highlighted again in a recent UPMC Center for Connected Medicine/KLAS Research survey of 58 executives at provider and payor organizations. The survey found that health systems are prioritizing investments over the next two years in AI solutions focusing on operational optimization, health/disease management and prediction, diagnostic imaging, population health management, value-based care, patient engagement and clinical research.

Epic and Microsoft's expanded collaboration will build upon our recently announced integrations, including Azure OpenAI Service into Epic's EHR to automatically draft message responses, as well as a solution that will bring natural language queries and interactive data analysis to SlicerDicer, Epic's self-service reporting tool. Microsoft and Nuance also recently collaborated to integrate Nuance® Dragon® Ambient eXperience™ Express (DAX Express™) solution into the Epic platform with a comprehensive approach to incorporating a broader array of AI-powered capabilities for clinical and administrative users.

Epic's approach to leveraging Microsoft's technology and infrastructure is unprecedented in time and scope. Together, we are bringing generative AI to healthcare at scale as quickly as possible, responsibly and in partnership with providers, in order to address the ongoing issues affecting healthcare.

Frontline workers represent the face of organizations and make up the lion's share of the workforce. Gartner estimates that there are 2.7 billion frontline workers — more than twice the number of desk-based workers.i The current macroeconomic climate highlighted by labor and supply chain shortages has put a lot of pressure on these workers to carry more work as organizations drive efficiency across business operations. The recent Work Trend Index shows that there is an opportunity for digital tools to help ease the burden on these essential workers. Over 60% of frontline workers struggle with having to do repetitive or menial tasks that take time away from more meaningful work and not having enough of the necessary resources to get their work done efficiently.ii In addition, 1 in 2 frontline workers cite being burned out in their jobs, and 45% note they are likely to consider changing employers in the next year.ii Investment in technology that enables frontline workers to thrive is a huge opportunity for business leaders – one that will drive positive outcomes for employees, customers, and the bottom line if solved correctly. Today, over 60% of the Fortune 500 use Microsoft 365 to empower frontline workers. Microsoft is committed to investing in innovative solutions to help frontline workers thrive. With AI transforming productivity across most segments of the workforce, our survey found that 65% of frontline workers are optimistic that AI will help them in their jobs.ii. We are excited to introduce new tools and integrations including bringing the power of next-generation AI to the frontline across three key areas:

These innovations will improve efficiency, enhance customer experiences, and enable faster decision making.

YouTube Video Click here to load media Intelligent operations Whether it's for an inspection, installation or a maintenance request, frontline service managers want to spend their time helping their team deliver exceptional service operations and not on inefficient tasks like copying and pasting information from one system to another. We seek to infuse the productivity applications frontline managers use everyday with the robust data and intelligence of underlying business applications. This helps streamline the frontline experience and drive efficiency with operations.

Copilot in Dynamics 365 Field Service with Outlook and Microsoft Teams integrations brings the power of next-generation AI to service professionals on the frontline. Frontline service managers who receive customer escalations in Outlook or Microsoft Teams can use Copilot in Dynamics 365 to streamline work order creation with relevant details pre-populated from emails or chats, optimize technician scheduling with data-driven recommendations based on factors such as travel time, availability and skillset, as well as generate draft responses to customer messages summarizing next steps without switching apps. Dynamics 365 Field Service app in Microsoft Teams will enable frontline technicians to access key work order functionality in their flow of work. Technicians will now be able to see upcoming work orders at a glance in their home experience in Teams, share full work order details and easily access Dynamics 365 Remote Assist in one click to troubleshoot with remote experts in real time if they need additional support to complete jobs.

Maintaining end-to-end visibility on operations can be time-consuming for frontline managers with fluctuating team schedules and often a large, dispersed team. Soon Microsoft 365 Copilot can ground prompts and retrieve insights for frontline managers, leveraging data from the Shifts app with a new Shifts plugin for Microsoft 365 Copilot, in addition to user and company data it has access to such as Teams chat history, SharePoint, emails and more. This will enable frontline managers to quickly get a list of important items specific to their team and location to speed up time-consuming tasks like covering shifts and onboarding new employees. Effortless communication Workplace culture is built upon a connection to the company mission, and it all starts with strong lines of communication. When companies establish an easy way to access consistent communications, frontline workers feel informed and connected. With a single communications platform to reach their entire workforce, corporate communicators don't have to switch out of their digital workspace to connect to the frontline.

Announcements in Viva Connections enables corporate communicators to draft, schedule and target important announcements like urgent communications, role-specific updates and safety policy changes to frontline workers. Communicators can quickly send messages directly from Viva Connections without having to leave Teams. Messages are delivered to the frontline through push notifications on their mobile devices and announcement cards in their Teams home experience. To build a sense of belonging and purpose across your entire workforce, Targeted Campaigns in Viva Engage enables communicators to create campaigns that promote company-wide initiatives targeted to frontline audiences.

Trusted experiences Many on the frontline work across multiple devices and often hand them over after a shift ends. To enable shift and part-time workers to be productive from the moment they log in, Windows 365 Frontline makes it easy and affordable to extend the power of Cloud PCs to employees on the frontline so they can securely access their personalized Windows experience on any device, no matter where they work. Securing data across shared devices, while still ensuring a seamless end-user experience can be challenging. A digital identity allows frontline workers to access and move between the technology needed to do their work, whether they are on a shared device or a dedicated one.

For organizations using Intune, they can use Microsoft Entra ID (formerly Azure Active Directory) to enable a single sign-in and sign-out experience for Teams, Outlook, Power Apps and more with shared device mode for Android and iOS devices. This allows frontline workers to wipe their device quickly and compliantly for the next shift. And organizations using SOTI or VMware Workspace ONE as their endpoint management solution can now also enroll Android devices in Microsoft Entra ID with shared device mode.

At Microsoft, we believe that technology can be a powerful force to reimagine how work gets done. By investing in innovative solutions for the frontline workforce, we are helping to drive positive change for frontline employees, customers and the bottom line. Frontline innovations across Dynamics 365, Microsoft 365, Windows 365 Frontline, Intune and partner endpoint management solutions push the boundaries of what is possible and work toward a brighter future for all.

i Gartner, Hype Cycle™ for Digital Workplace Applications, 2022, August 2022 ii The Work Trend Index survey was conducted by an independent research firm, Edelman Data x Intelligence, among 31,000 full-time employed or self-employed workers across 31 markets, 6,019 of which are frontline workers, between February 1, 2023, and March 14, 2023. This survey was 20 minutes in length and conducted online, in either the English language or translated into a local language across markets. One thousand full-time workers were surveyed in each market, and global results have been aggregated across all responses to provide an average. Each market is evenly weighted within the global average. Each market was sampled to be representative of the full-time workforce across age, gender, and region; each sample included a mix of work environments (in-person, remote vs. non-remote, office settings vs. non-office settings, etc.), industries, company sizes, tenures, and job levels. Markets surveyed include: Argentina, Australia, Brazil, Canada, China, Colombia, Czech Republic, Finland, France, Germany, Hong Kong, India, Indonesia, Italy, Japan, Malaysia, Mexico, Netherlands, New Zealand, Philippines, Poland, Singapore, South Korea, Spain, Sweden, Switzerland, Taiwan, Thailand, United Kingdom, United States, and Vietnam. Tags: AI, Microsoft 365 Copilot, Microsoft Dynamics 365 Copilot, Work Trend Index

Andriy Onufriyenko/Getty Images Over the past six months, I have witnessed the staggering speed and scale of generative AI technology adoption, and how it has opened doors for organizations to imagine new ways to solve business, societal, and sustainability challenges. For many with modernized data estates fortified with the Microsoft Cloud, advanced AI technology is already unlocking innovation and delivering greater business value. At our annual partner conference Microsoft Inspire, I had the chance to speak with our partner community about the critical role they play in helping customers accelerate their AI transformation. As leaders across industries seek to keep pace with today's advancements, they turn to Microsoft — and our partner ecosystem — to co-innovate on our trusted platform and make their AI strategy a reality. This is further reinforced by launching the Microsoft AI Cloud Partner Program, designed to support every partner as they deliver customer value across our cloud and AI offerings. Together with our partners, we are dedicated to bringing comprehensive industry expertise, scale and copilot capabilities to organizations seeking to transform every role and business function within their industry. From Canadian Tire delivering more seamless and personalized shopper experiences to AT&T easing employee tasks while reducing IT costs, AI transformation is happening now.

In recent months, the remarkable strides made in AI innovation have ignited a wave of transformative possibilities, captivating our collective imagination with the promise of reshaping industries and the way we work.

Today, at Microsoft Inspire, Meta and Microsoft announced support for the Llama 2 family of large language models (LLMs) on Azure and Windows. Llama 2 is designed to enable developers and organizations to build generative AI-powered tools and experiences. Meta and Microsoft share a commitment to democratizing AI and its benefits and we are excited that Meta is taking an open approach with Llama 2. We offer developers choice in the types of models they build on, supporting open and frontier models and are thrilled to be Meta's preferred partner as they release their new version of Llama 2 to commercial customers for the first time.

Now Azure customers can fine-tune and deploy the 7B, 13B, and 70B-parameter Llama 2 models easily and more safely on Azure, the platform for the most widely adopted frontier and open models. In addition, Llama will be optimized to run locally on Windows. Windows developers will be able to use Llama by targeting the DirectML execution provider through the ONNX Runtime, allowing a seamless workflow as they bring generative AI experiences to their applications.

Meta and Microsoft have been longtime partners on AI, starting with a collaboration to integrate ONNX Runtime with PyTorch to create a great developer experience for PyTorch on Azure, and Meta's choice of Azure as a strategic cloud provider. Today's announcement builds on our partnership to accelerate innovation in the era of AI and further extends Microsoft's open model ecosystem and position as the world's supercomputing platform for AI.

Azure's purpose-built AI supercomputing platform is uniquely designed from the facility, hardware and software to support the world's leading AI organizations to build, train and deploy some of the most demanding AI workloads. The availability of the Llama 2 models with Azure AI enables developers to take advantage of Azure AI's powerful tooling for model training, fine-tuning, inference, and particularly the capabilities that support AI safety.

The inclusion of the Llama 2 models in Windows helps propel Windows as the best place for developers to build AI experiences tailored for their customers' needs and unlock their ability to build using world-class tools like Windows Subsystem for Linux (WSL), Windows terminal, Microsoft Visual Studio and VS Code.

Llama 2 is the latest addition to our growing Azure AI model catalog. The model catalog, currently in public preview, serves as a hub of foundation models and empowers developers and machine learning (ML) professionals to easily discover, evaluate, customize and deploy pre-built large AI models at scale.

The catalog eliminates the need for users to manage all infrastructure dependencies when operationalizing Llama 2. It provides turnkey support for model fine-tuning and evaluation, including powerful optimization techniques such as DeepSpeed and ONNX Runtime, that can significantly enhance the speed of model fine-tuning.

Windows developers will be able to easily build new experiences using Llama 2 that can be accessed via GitHub Repo. With Windows Subsystem for Linux and highly capable GPUs, developers can fine tune LLMs to meet their specific needs right on their Windows PCs.

Responsible AI is at the heart of Microsoft's approach to AI and how we partner. For years we've invested heavily in making Azure the place for responsible, cutting-edge AI innovation, whether customers are building their own models or using pre-built and customizable models from Microsoft, Meta, OpenAI and the open-source ecosystem.

At Microsoft, we mitigate potential risks presented by the use of large language models through an iterative, layered approach that includes experimentation and measurement. Azure AI customers can test Llama 2 with their own sample data to see how it performs for their particular use case. Then, customers can use prompt engineering and retrieval augmented generation (RAG) techniques to develop, evaluate and optimize meta-prompts for their app and deliver safer and more reliable experiences for end users.

Services like Azure AI Content Safety add another layer of protection, helping ensure a safer online experience with AI apps. Part of our collaboration with Meta led to combining Meta's safety techniques with Azure AI Content Safety so that by default, the deployments of the Llama 2 models in Azure AI come with a layered safety approach.

Today's expansion of our model catalog with Llama 2 and our partnership with Meta is a big step forward in achieving a responsible, open approach to AI.

Visit the Azure AI model catalog and start using Llama 2 today.

Tags: Azure, Azure AI, large language models, Llama 2, Meta, Microsoft Inspire

Updated November 15, 2023: To simplify the user experience and make Copilot more accessible to everyone, Bing Chat and Bing Chat Enterprise will now simply become Microsoft Copilot. For more information: At Microsoft, we are working to provide a copilot for every person in their lives and at work. Earlier this year, we introduced the new AI-powered Bing, your copilot for the web, fundamentally reinventing search as a category with complete answers, a full chat experience, and features to unlock creativity. We also introduced Microsoft 365 Copilot, which combines the power of large language models (LLMs) with your data in the Microsoft Graph and Microsoft 365 apps to usher in a whole new way of working, using just your own words. Since then, we've seen millions of people incorporate Bing into their lives for the first time in new ways to unlock their creativity and gain a better understanding of the world. And feedback from customers in our Microsoft 365 Copilot Early Access Program, is that Copilot promises to be a game changer for productivity. Today at Microsoft Inspire, we're excited to unveil the next steps in our journey: First, we're significantly expanding Bing to reach new audiences with Bing Chat Enterprise, delivering AI-powered chat for work, and rolling out today in Preview – which means that more than 160 million people already have access. Second, to help commercial customers plan, we're sharing that Microsoft 365 Copilot will be priced at $30 per user, per month for Microsoft 365 E3, E5, Business Standard and Business Premium customers, when broadly available; we'll share more on timing in the coming months. Third, in addition to expanding to more audiences, we continue to build new value in Bing Chat and are announcing Visual Search in Chat, a powerful new way to search, now rolling out broadly in Bing Chat. Bing Chat Enterprise — AI-powered chat for work Employees are looking to use AI tools to help them unlock creativity and productivity at work — 70% say they would delegate as much work as possible to AI according to our Work Trend Index. But using AI tools that aren't built for the enterprise inadvertently puts sensitive business data at risk. As organizations adopt AI, they want to be confident their data is protected. Bing Chat Enterprise gives your organization AI-powered chat for work with commercial data protection. With Bing Chat Enterprise, user and business data are protected and will not leak outside the organization. What goes in — and comes out — remains protected. Chat data is not saved, and Microsoft has no eyes-on access – which means no one can view your data. And, your data is not used to train the models. Whether researching industry insights, analyzing data, or looking for inspiration, Bing Chat Enterprise gives people access to better answers, greater efficiency and new ways to be creative.

YouTube Video Click here to load media Just like Bing Chat, Bing Chat Enterprise is grounded in web data and provides complete, verifiable answers with citations, along with visual answers that include graphs, charts and images, and is designed in line with our AI principles. Bing Chat Enterprise is rolling out in preview today and is included at no additional cost in Microsoft 365 E3, E5, Business Standard and Business Premium. And in the future, it will be available as a stand-alone offering for $5 per user, per month. You can access Bing Chat Enterprise using your work account wherever Bing Chat is supported — Bing.com/chat and the Microsoft Edge sidebar. And, in the future, Bing Chat Enterprise will also be accessible from Windows Copilot.

Announcing Microsoft 365 Copilot pricing for commercial customers Bing Chat Enterprise unlocks generative AI for work. And Microsoft 365 Copilot brings a whole new way of working – reasoning over all your business data in the context of your enterprise, including the ability to ask questions and get answers from the web. Microsoft 365 Copilot will be available for commercial customers for $30 per user per month for Microsoft 365 E3, E5, Business Standard and Business Premium customers when broadly available. Microsoft 365 Copilot is built on Microsoft's trusted and comprehensive approach to enterprise-grade security, privacy, identity, compliance and responsible AI — so you know it's enterprise ready. This means:

Copilot inherits your existing Microsoft 365 security, privacy, identity and compliance policies. Your data is logically isolated and protected within your Microsoft 365 tenant, and always within your control. At the tenant level, Copilot respects individual and group permission policies.

While some generative AI apps focus on a single capability, like real-time transcription or copywriting, Microsoft 365 Copilot is in a class all its own. It has all the capabilities of Bing Chat Enterprise, plus so much more. Copilot puts thousands of skills at your command and can reason over all your content and context to take on any task. It's grounded in your business data in the Microsoft Graph — that's all your emails, calendar, chats, documents and more. So, Copilot can generate an update from the morning's meetings, emails and chats to send to the team; get you up to speed on project developments from the last week; or create a SWOT analysis from internal files and data from the web. Microsoft 365 Copilot is incredible on its own, and it's also integrated into the apps millions of people use every day. Copilot jump-starts your creativity in Word, analyzes data in Excel, designs presentations in PowerPoint, triages your Outlook inbox, summarizes meetings in Teams – whether you attended or not – and so much more. In May, we announced the expansion of our Microsoft 365 Copilot paid Early Access Program to 600 enterprise customers worldwide, including companies like KPMG, Lumen, and Emirates NBD. We're learning that the more customers use Copilot, the more their enthusiasm for Copilot grows. Soon, no one will want to work without it. Microsoft 365 Copilot: Thousands of skills. All your data. Infinite possibilities. Learn more here.

YouTube Video Click here to load media Search with images — not just words — using Visual Search in Chat In addition to unlocking the power of generative AI to people at work, we continue to deliver new features and experiences in Bing Chat to help people make the most of this technology. Part of this work is a focus on building visual features in Bing Chat. And today we're pleased to announce we're rolling out multimodal capabilities via Visual Search in Chat. Leveraging OpenAI's GPT-4 model, Visual Search in Chat lets anyone upload images and search the web for related content. Take a picture, or use one you find elsewhere, and prompt Bing to tell you about it — Bing can understand the context of an image, interpret it, and answer questions about it. Whether you're traveling to a new city on vacation and asking about the architecture of a particular building or at home trying to come up with lunch ideas based on the contents of your fridge, upload the image into Bing Chat and use it to harness the web's knowledge to get you answers. Visual Search in Chat is beginning to roll out now via desktop and the Bing mobile app and we are working to bring this to Bing Chat Enterprise over time.

YouTube Video Click here to load media There is incredible opportunity for our customers and partners to realize the promise of AI in life and at work – and do so in a way that meets the highest standards for enterprise, security, privacy, compliance and responsible AI. We're excited about what the future holds – and we're just getting started. Editor's note: As of Aug. 21, we have extended Bing Chat Enterprise eligibility to Microsoft 365 A3 and A5 licenses for faculty. Read the official announcement at Tags: AI, Bing Chat Enterprise, Microsoft 365 Copilot, Microsoft Inspire, Visual Search

Collaboration is a key component of Microsoft's success. Our partner ecosystem consists of more than 400,000 partners worldwide, and they play a key role in making new technology available to customers, especially in today's AI-focused world. Microsoft Inspire is a chance to acknowledge the role our partners play in customer success and to share new opportunities and ways to engage with Microsoft products.

To recognize the impressive achievements of our collaborators, we kicked off Microsoft Inspire by celebrating the finalists and winners in the 2023 Microsoft Partner of the Year Awards, which were announced in late June. The awards highlight partner success and innovation in an array of categories, across solution areas, industries, business transformation and social impact.

This year's Microsoft Inspire continues our push to make AI a transformative tool for our customers and partners. We're excited to share even more AI-powered solutions and show how Microsoft partners can apply these AI innovations across their organizations in a variety of ways, from expansion of AI skilling to new products and services that drive customer success. Read on for some of the top announcements at this year's event.

Introducing Bing Chat Enterprise Since launching the new Bing in February, we've heard from many corporate customers who are excited to empower their organizations with powerful new AI tools but are concerned that their companies' data will not be protected. That's why today we're announcing Bing Chat Enterprise, which gives organizations AI-powered chat for work with commercial data protection. What goes in – and comes out – remains protected, giving commercial customers managed access to better answers, greater efficiency and new ways to be creative.

Bing Chat Enterprise will start rolling out today in preview to organizations licensed for Microsoft 365 E5, E3, Business Premium and Business Standard at no additional cost. We will also make Bing Chat Enterprise available as a stand-alone subscription in the future for $5 per user, per month. Learn more and find out how to get started with Bing Chat Enterprise.

Announcing Microsoft 365 Copilot Pricing Today, we're also pleased to announce pricing for Microsoft 365 Copilot. It will be available for $30 per user per month for Microsoft 365 E3, E5, Business Standard and Business Premium customers when generally available.

While some generative AI apps focus on a single capability, like real-time transcription or copywriting, Microsoft 365 Copilot puts thousands of skills at your command. By grounding answers in business data like your documents, emails, calendar, chats, meetings and contacts, and combining them with your working context – the meeting you're in now, the emails you've exchanged on a topic, the chats you had last week – Copilot delivers richer, more relevant and more actionable responses to your questions.

And, Microsoft 365 Copilot is integrated into the apps millions of people use every day. Copilot jump-starts your creativity in Word, analyzes data in Excel, designs presentations in PowerPoint, triages your Outlook inbox, summarizes meetings in Teams – whether you attended or not – and so much more.

Empowering sellers and customer service agents with AI Sellers need to have as many options in their toolboxes as possible. So, we're adding more functionality to Microsoft Sales Copilot directly within Dynamics 365 Sales, such as AI-generated opportunity summary, contextualized email drafts and meeting preparations. This empowers sellers to improve productivity and close more deals with Customer Relationship Management (CRM) task automation (including Salesforce), actionable real-time insights and AI-assisted content and recommendations to personalize customer interactions at scale. These add to AI capabilities already available in Microsoft Sales Copilot such as Teams calls summaries and email thread summaries. Viva Sales, announced in June 2022, kickstarted our work of transforming seller experiences, and these capabilities are now part of Sales Copilot. Read more about Microsoft Sales Copilot.

At Microsoft Inspire, we're also highlighting how customers like Virgin Money are empowering their customer service departments with tailored chatbots built with Power Virtual Agents. With Copilot in Power Virtual Agents, within minutes, businesses can train a chatbot using natural language to reference internal and external knowledge sources, customer service applications and web data via Bing Search. Virgin Money developed their chatbot in two weeks, and it addresses more than 195,000 customer interactions a month, helping their service agents focus on more complex customer inquiries.

Process Mining in Power Automate Organizations often have a difficult time identifying blockages in their workflows and how to clear them. To help, Microsoft is announcing the general availability of next-generation AI features within Power Automate Process Mining, providing customers with AI-powered insights to optimize existing processes and drive efficiencies through low-code automation. With Process Mining, users can understand what is happening across their business, use AI that generates insights, app and automation suggestions, and use Power Platform to quickly build the solutions they need. Learn more about Process Mining in Power Automate.

Azure OpenAI expanded availability We've been thrilled to see the enthusiasm and business adoption of Azure OpenAI Service, with more than 4,500 customers using the product. Witnessing customers do amazing things, like building chatbots using organizational data, summarizing text and generating content, is exciting to watch develop.

Now, we're bringing the service to more organizations around the world. Last week, we expanded access to Azure OpenAI Service, increasing its availability in North America and Western Europe, while making it available for Asia for the first time.

New Azure capabilities and investments We are also going to announce a substantial investment to increase the scale and availability of Azure Migrate & Modernize, and to launch Azure Innovate, an all-new dedicated investment we are making in response to the heightened demands for analytics and AI. These new offerings have expanded scenario coverage and offer richer incentives and support for everything from fast, frictionless migrations to building new AI-powered apps.

Meta and Microsoft partnership Meta and Microsoft have announced support for the Llama family of large language models on Azure and Windows. As part of this announcement, Microsoft will be Meta's preferred partner as they release their new version of Llama 2 to commercial customers for the first time. This announcement means that Azure customers will be able to easily fine-tune and deploy the 7B-parameter, 13B-parameter, and 70B-parameter Llama 2 models easily and safely on Azure, In addition, Llama 2 will be optimized to run locally on Windows – enabling Windows developers to take advantage of Llama 2 by targeting the Direct ML execution provider through the ONNX runtime. More on this announcement can be found here.

Expanded strategic collaboration with Epic We are excited to highlight an expansion of our strategic collaboration with Epic, a leading healthcare software company, where we are using the power of AI to help clinicians spend less time on administrative functions and more time on providing quality care. Epic has integrated Azure OpenAI Service into its electronic health record (EHR) software to provide multiple solutions, from helping clinicians explore clinical data in a conversational and intuitive way to helping them more efficiently reply to patient messages. And with Nuance DAX Express, we are embedding our AI-powered clinical documentation capabilities directly into Epic workflows to help providers further lessen the administrative workloads that lead to burnout, expand access to care for patients and enhance healthcare outcomes.

Additionally, Epic customers are now utilizing Azure Large Instances to achieve the scale needed to run large Epic EHR databases – up to 50 million database accesses per second. This allows Epic customers to scale beyond the previous limits of shared public cloud infrastructure solutions.

The new Microsoft AI Cloud Partner Program In another milestone, Microsoft Inspire marks the launch of the Microsoft AI Cloud Partner Program, the next generation of our partner program, which empowers every partner to deliver customer value while leveraging Microsoft AI and the Microsoft Cloud. Through the Microsoft AI Cloud Partner Program, we're providing partners with a comprehensive portfolio of investments for all partner business models, at every stage of maturity.

The Microsoft AI Cloud Partner Program utilizes the entire partner lifecycle, including onboarding, skilling, go-to-market, incentives and co-selling. Partners get the value and benefits of the previous program plus access to new offerings and benefits specific to AI. And there is no action for a partner to take to move to the new program – we've moved all existing partners into the new program effective immediately and partners maintain their existing benefits and designations. Read more about the Microsoft AI Cloud Partner Program.

Additional partner updates We have several updates with new opportunities for partners to go-to-market and scale their businesses, including:

ISV Success updates: Announced in preview at last Microsoft Inspire, ISV Success is the pathway for ISV partners within the AI Cloud Partner Program and is now generally available. ISV Success offers product and cloud benefits, demo and sandbox environments, technical consults to build and publish applications, and once published, sales and marketing benefits to help accelerate deals through Microsoft commercial marketplace. In addition, partners participating in ISV Success will also get access to GitHub Copilot as part of ISV Success benefits at the end of 2023. Multiparty private offers: As part of our continued investments in the Microsoft commercial marketplace, we also announced multiparty private offers, which empower partners to work together to sell customized deals through the Microsoft commercial marketplace. New Solutions Partner designations for more partners: We are introducing new designations as part of the AI Cloud Partner Program to provide opportunities for additional partners to differentiate their technical capabilities and demonstrate customer success. This includes a new training services designation for learning partners, ISV designations for partners building solutions aligned with the Microsoft Cloud and our industry clouds, and a support services designation.

We are excited about today's announcements and our commitment to accelerating AI transformation, driving customer success, and fueling partner business growth and profitability. This is just a snapshot of the updates being announced at Microsoft Inspire – for a more comprehensive review, please see the additional resources at the end of this post.

And for more information on today's announcements, be sure to register for Microsoft Inspire and tune into the Day 1 keynotes from Satya Nadella, Judson Althoff and Nicole Dezen, or watch them on demand.

Microsoft 2023 Partner of the Year Awards winners and finalists

Furthering our AI Ambitions – Announcing Bing Chat Enterprise and Updates to Microsoft 365 Copilot

Find out how to get started with Bing Chat Enterprise

How Microsoft Sales Copilot will empower sellers and customer service agents

Find more out about the Microsoft AI Cloud Partner Program

Tags: AI, Azure, Bing Chat Enterprise, Cloud, healthcare, Microsoft 365 Copilot, Microsoft Inspire, Power Automate

Today, Microsoft is announcing new advances to Azure Quantum aimed at accelerating scientific discovery.

From the Iron Age revolutionizing agriculture to the Silicon Age revolutionizing how we communicate, new materials have always created inflection points for societal progress. Advances in this space will be transformative. For example, the transportation industry can develop more efficient and powerful batteries. Pharmaceutical researchers can develop new drugs. The chemistry industry will be able to discover safer and more sustainable compounds for the products we use daily, like electronics, paints and textiles. And most importantly, scientists will be able to solve society's most pressing challenges like reversing climate change and addressing food insecurity.

However, understanding nature is not an easy task. For accurate calculations of the most complex quantum states of molecules, the number of energy states of just 100 electrons can exceed the number of atoms in the visible universe.

Now, with a new generation of AI, the world's most advanced AI models are coming together with our universal user interface – natural language – and nature's foundational language – chemistry – ushering in a new era of scientific discovery. And that's just the first step as we prepare for an even bigger transformation ahead of us: quantum supercomputing.

Innovators can start experimenting with the world's best quantum hardware in Azure Quantum and get ready to solve more complex problems when quantum supercomputing becomes a reality. We are fast approaching the time when scientists and businesses will be able to solve previously intractable problems to unlock growth and human progress.

Today, we are introducing three advances in Azure Quantum towards this vision.

Azure Quantum Elements accelerates scientific discovery by integrating the latest breakthroughs in high-performance computing (HPC), artificial intelligence and quantum computing.

Reduce time to impact and costs by accelerating the R&D pipeline and bringing innovative products to market more quickly, with some customers seeing a six-month to one-week speed up from project kick-off to solution. Dramatically increase the search space for new materials, with the potential to scale from thousands of candidates to tens of millions. Speed up certain chemistry simulations by 500,000 times, which is like compressing a year into one minute. Get ready for scaled quantum computing by addressing quantum chemistry problems today with AI and HPC, while experimenting with existing quantum hardware and getting priority access to Microsoft's quantum supercomputer in the future.

Using this comprehensive system, researchers can make advances in chemistry and materials science with unprecedented scale, speed and accuracy.

Scale. Scientists can understand the complex reactions needed to produce a product, find new candidates, and optimize the entire process. For example, scientists can now use Azure Quantum Elements to explore 1.5 million potential configurations in complex reactions comprised of 50,000 elementary steps. Early adopters are already using this massive scale to find more sustainable replacements for use in many everyday products, or entirely new products for innovative scenarios.

Speed. Azure Quantum Elements accelerates simulation by incorporating Microsoft's specialized AI models for chemistry, which we trained on millions of chemistry and materials data points. We based these models on the same breakthrough technologies you see elsewhere with generative AI. While Copilot understands the language of humans, Azure Quantum Elements also understands the language of nature – chemistry.

Accuracy. Today, scientists are using unique AI models and HPC scale to run simulations with higher levels of accuracy not available before. Azure Quantum Elements also integrates classical and quantum computing to provide an on-ramp to even greater simulation accuracy. In the future, scientists believe a quantum supercomputer will enable predictive chemical design at 100X accuracy.

Industry innovators, including BASF, AkzoNobel, AspenTech, Johnson Matthey, SCGC, and 1910 Genetics have already adopted Azure Quantum Elements to transform their research and development, and today, others can join them. Azure Quantum Elements will be available in private preview in a few weeks, and you can sign up today to learn more.

Copilot in Azure Quantum helps scientists use natural language to reason through complex chemistry and materials science problems.

With Copilot in Azure Quantum, a scientist can accomplish complex tasks on top of a fabric of cloud supercomputing, advanced AI and quantum all integrated with the tools they use today. It can generate the underlying calculations and simulations, query and visualize data and help get guided answers to complicated concepts. Just as Copilot in other Microsoft products is transforming software development, productivity and search, our ambition is for Copilot in Azure Quantum to transform and accelerate scientific discovery — whether it's creating safer and more sustainable products, accelerating drug discovery or solving the most pressing challenges on the planet.

Copilot also helps people learn about quantum computing and write code for today's quantum computers. It provides a fully integrated, browser-based experience available to try for free with a built-in code editor, quantum simulator and seamless code compilation. We designed it to make the complex more manageable and help anyone explore quantum, chemistry and materials science while bringing these transformational fields closer together. Check it out today.

The ultimate unlock will come when organizations are able to accurately design new chemicals and materials with a quantum supercomputer. Our industry will follow a similar path as classical supercomputers did in the 20th century. From vacuum tubes, to transistors, to integrated circuits, advances in underlying technology will enable scale and impact.

As we make progress as an industry, quantum hardware will fall into one of the three categories of Quantum Computing Implementation Levels:

Level 1 – Foundational: Quantum systems that run on noisy physical qubits, which includes all of today's Noisy Intermediate Scale Quantum (NISQ) computers. Level 2 – Resilient: Quantum systems that operate on reliable logical qubits. Level 3 – Scale: Quantum supercomputers that can solve impactful problems that even the most powerful supercomputers cannot.

To reach Scale, a fundamental physics breakthrough is required. Microsoft has achieved this breakthrough and today presented the peer reviewed data in a journal by the American Physical Society.

This means that Microsoft has achieved the first milestone towards a quantum supercomputer. We can now create and control Majorana quasiparticles. With this achievement, we're well on our way to engineering a new hardware-protected qubit. With it, we can then engineer reliable logical qubits to reach the Resilient Level and then progress to reach Scale.

A quantum supercomputer will be able to solve problems that are intractable on a classical computer and scale to solve the most complex problems facing our world. To do this, it must be both performant and reliable. Customers need to understand how capable a quantum system is of solving real problems, from the machine to the network overhead. That's why measuring a supercomputer can't be about counting physical or logical qubits.

To understand a supercomputer's performance, our industry needs a new measurement. For that, we're offering the metric of reliable Quantum Operations Per Second (rQOPS), which measures how many reliable operations can be executed in one second. It considers the full system performance, as opposed to just qubit performance, so there is an assurance that an algorithm will run correctly.

Our industry is yet to transition from the NISQ era and therefore, today's quantum computers are all at Level 1 with a rQOPS of zero. The first quantum supercomputer will need at least 1 million rQOPS and will scale to more than 1 billion to solve impactful chemistry and materials science problems.

With the advent of any innovative technology, there are risks that need to be planned for and mitigated. Microsoft's AI principles guide us and these fundamental tenets apply to quantum as well. As we develop new services like Azure Quantum Elements and engineer our first quantum supercomputer, we will apply additional measures of rigor incorporating feedback throughout the process.

We are also preparing now for a quantum-safe future. Microsoft has a comprehensive plan to ensure all our services remain secure and is partnering with our customers and the industry to support this important transition.

The opportunity in front of us is immense. Scientists and businesses will revolutionize the building blocks of everyday products to usher in a new era of innovation and economic growth. Together, we can compress the next 250 years of chemistry and materials science into the next 25.

Please join Satya Nadella and me today at a virtual briefing Accelerating Scientific Discovery at 8 a.m. PT or catch it later on demand. We'll share more about the Azure Quantum Elements private preview, Copilot in Azure Quantum, and our roadmap to a quantum supercomputer.

Tags: Azure, Azure Quantum, Azure Quantum Elements, Satya Nadella, supercomputer

Space engages people of all ages by sparking curiosity of the unknown. With NASA's progress toward its first lunar missions in more than 50 years, a new generation of Americans can witness the awe-inspiring power and joy of a moon landing. It can also inspire people of all ages and backgrounds to pursue space-related science, technology, engineering and mathematics (STEM) careers, from astronauts to engineers and physicists to technicians.

In recent years, we have witnessed significant diversity milestones in space exploration. Among these are the first all-female spacewalk and NASA's diverse international crew of astronauts for the Artemis II lunar mission, where the first woman and first person of color will step foot on the moon. Although these achievements signal an important shift in the industry, women still represent only 20% of the space workforce. Black and Hispanic professionals remain underrepresented in STEM professions. Of the entire STEM workforce, a total of 24% are members of an underrepresented minority group. These statistics highlight the urgent need to continue expanding access to STEM education and the study of space — a core tenet of the Microsoft-NASA partnership that aims to address these inequities head on.

Critical to Microsoft and NASA's more than two-decades of partnership is investing in the next generation by promoting STEM education across all communities. As part of this effort Microsoft is honored to collaborate with NASA to host our first Space Education Day — a showcase of students and educators across Washington, D.C. — on Tuesday, June 20 at Microsoft's office in Arlington, Virginia.

Space Education Day provides local students an opportunity to connect with Microsoft and NASA members during a day of innovative learning and technology demonstrations. We are thrilled to welcome students from the greater Washington, D.C region, home to a diverse pre-kindergarten through 12th grade population. There will be live demonstrations with the help of students who learned to program in the Minecraft Education portal, an immersive learning platform inspired by NASA's Artemis Missions. Microsoft's Minecraft Education recently partnered with NASA and the Challenge Center to equip students with future-ready skills for the next era of space exploration, and now students will have the opportunity to exercise these skills in real-time.

"Our next generation of innovators need opportunities and resources that engage the Artemis Generation to reach for the moon and beyond," said Mike Kincaid, associate administrator for NASA's Office of STEM Engagement. "Investing in our students from all backgrounds today will build a diverse workforce that will ultimately achieve future breakthroughs in space."

Microsoft Space Camps – Minecraft Artemis: Students will explore the lunar surface and learn about NASA's Artemis program in a fun and immersive Minecraft environment. Azure Space Hackathon – College students will showcase their experience and technology using NASA satellite imagery and Microsoft's AI capabilities and the Planetary Computer to find offshore windfarms around the world. NASA MSI Space Accelerators – Partners will discuss the goals and objectives of the NASA Space Accelerator program to connect minority-serving institutions to mentorship and funding opportunities from NASA and Microsoft.

Space Education Day continues Microsoft and NASA's efforts to develop high-impact initiatives and educational resources that promote STEM education for all. A student hackathon hosted last May by Azure Space and NASA called Coding for the Cosmos convened students from two Historically Black Colleges and Universities (HBCUs) in Washington, D.C. to establish new technologies, including deploying code to the International Space Station.

Technology and the power of space can be used as a powerful tool to inspire, prepare and engage people of all ages and backgrounds, and Microsoft will continue to seek new opportunities to keep everyone reaching for the stars.

"Our collaboration with NASA allows us to provide new and exciting opportunities for students of all backgrounds to explore the space industry and related technologies," said Steve Kitay, Senior Director of Azure Space. "One of the key purposes of Azure Space is to bring space, cloud computing, and other new technologies to the next generation – particularly those that might not otherwise have the opportunity."

By continuing to integrate Microsoft technologies with NASA's missions, data and expertise, it is our goal to broaden the ecosystem of STEM and empower our future workforce to explore the endless boundaries of space.

Tags: AI, Azure Space, Minecraft: Education Edition, NASA, Space Education Day, STEM

AI is creating unparalleled opportunities for businesses of every size and across every industry. We are seeing our customers embrace AI services to drive innovation, increase productivity and solve critical problems for humanity, such as the development of breakthrough medical cures and new ways to meet the challenges of climate change. At the same time, there are legitimate concerns about the power of the technology and the potential for it to be used to cause harm rather than benefits. It's not surprising, in this context, that governments around the world are looking at how existing laws and regulations can be applied to AI and are considering what new legal frameworks may be needed. Ensuring the right guardrails for the responsible use of AI will not be limited to technology companies and governments: every organization that creates or uses AI systems will need to develop and implement its own governance systems. That's why today we are announcing three AI Customer Commitments to assist our customers on their responsible AI journey.

First, we will share what we are learning about developing and deploying AI responsibly and assist you in learning how to do the same. Microsoft has been on a responsible AI journey since 2017, harnessing the skills of nearly 350 engineers, lawyers and policy experts dedicated to implementing a robust governance process that guides the design, development and deployment of AI in safe, secure and transparent ways. More specifically we are:

Sharing expertise: We are committed to sharing this knowledge and expertise with you by publishing the key documents we developed during this process so that you can learn from our experiences. These include our Responsible AI Standard, AI Impact Assessment Template, AI Impact Assessment Guide, Transparency Notes, and detailed primers on the implementation of our responsible AI by design approach. Providing training curriculum: We will also share the work we are doing to build a practice and culture of responsible AI at Microsoft, including key parts of the curriculum that we use to train Microsoft employees. Creating dedicated resources: We will invest in dedicated resources and expertise in regions around the world to respond to your questions about deploying and using AI responsibly.

Second, we are creating an AI Assurance Program to help you ensure that the AI applications you deploy on our platforms meet the legal and regulatory requirements for responsible AI. This program will include the following elements:

Regulator engagement support: We have extensive experience helping customers in the public sector and highly regulated industries manage the spectrum of regulatory issues that arise when dealing with the use of information technology. For example, in the global financial services industry, we worked closely for a number of years with both customers and regulators to ensure that this industry could pursue digital transformation on the cloud while complying with its regulatory obligations. One learning from this experience has been the industry's requirement that financial institutions verify customer identities, establish risk profiles and monitor transactions to help detect suspicious activity, the "know your customer" requirements. We believe that this approach can apply to AI in what we are calling "KY3C," an approach that creates certain obligations to know one's cloud, one's customers and one's content. We want to work with you to apply KY3C as part of our AI Assurance Program.

Risk framework implementation: We will attest to how we are implementing the AI Risk Management Framework recently published by the U.S. National Institute of Standards and Technology (NIST) and will share our experience engaging with NIST's important ongoing work in this area. Customer councils: We will bring customers together in customer councils to hear their views on how we can deliver the most relevant and compliant AI technology and tools. Regulatory advocacy: Finally, we'll play an active role in engaging with governments to promote effective and interoperable AI regulation. The recently launched Microsoft blueprint for AI governance presents our proposals to governments and other stakeholders for appropriate regulatory frameworks for AI. We have made available a presentation of this blueprint by Microsoft Vice Chair and President Brad Smith and a white paper discussing it in detail.

Third, we will support you as you implement your own AI systems responsibly, and we will develop responsible AI programs for our partner ecosystem.

Dedicated resources: We will create a dedicated team of AI legal and regulatory experts in regions around the world as a resource for you to support your implementation of responsible AI governance systems in your businesses. Partner support: Many of our partners have already created comprehensive practices to help customers evaluate, test, adopt and commercialize AI solutions, including creating their own responsible AI systems. We are launching a program with selected partners to leverage this expertise to assist our mutual customers in deploying their own responsible AI systems. Today we can announce that PwC and EY are our launch partners for this exciting program.

Ultimately, we know that these commitments are only the start, and we will have to build on them as both the technology and regulatory conditions evolve. But we are also excited by this opportunity to partner more closely with our customers as we continue on the responsible AI journey together. Tags: AI, Responsible AI

This year we're celebrating the 30th anniversary of the Global LGBTQIA+ Employees and Allies at Microsoft (GLEAM) employee resource group that is a driving force for LGBTQIA+ inclusion and Pride at Microsoft around the world.

At a time when LGBTQIA+, transgender, non-binary, and gender-expansive communities need support, Microsoft invites everyone to keep and make more Pride by sharing stories, open-sourcing the entire Pride campaign design, contributing to LGBTQIA+ organizations, and creating a wide product release.

Our efforts to support and advocate for LGBTQIA+ people do not start or end at Pride. By supporting our employees through global advocacy, charitable giving and better workplace practices, we hope to make a difference for LGBTQIA+ communities and beyond, as our drive for positive global impact continues.

Microsoft is celebrating LGBTQIA+ communities through stories, products, events and inviting everyone to make Pride.

At a time when Pride is getting harder to organize and celebrate, we're empowering organizers around the world by open-sourcing the entire Pride campaign design and inviting everyone to remix, share and make more Pride. We're releasing campaign assets today on GitHub and adding ready-made templates to Clipchamp.

Last year, we open-sourced our Pride flag design representing 40 LGBTQIA+ identities. Our flag update released today reflects almost 50 identities, expands representation of Asexual, Aromantic and Trans identities, and is available in new designs and sizes.

Our Pride expression reflects on the open, inviting, grass-roots nature of Pride. Brush strokes, bright colors and a variety of flag combinations celebrate a community that's wide, global, and ever-growing.

Our Pride centers on simple truths about the LGBTQIA+ experience shared by employees around the world. Hugging your loved one is Pride. Openly serving your country is Pride. Mutual respect is Pride. And much more. By sharing their stories, LGBTQIA+ people at Microsoft hope to invite everyone to make more Pride.

Year-around, we recognize the many ways this diverse community continues to contribute and influence innovation at Microsoft and beyond. Join us in celebrating with powerful employee stories of strength, community, and leadership by visiting Inclusion Is Innovation.

Last year, we made a statement of LGBTQIA+ visibility by releasing the Xbox Pride controller. This beloved and viral design is available year-round on Xbox Design Lab.

This year, we're taking it a step further. Xbox Game Studios Publishing is announcing a long-term partnership with the Gay and Lesbian Alliance Against Defamation

(GLAAD) to increase LGBTQIA+ representation and storytelling in Xbox games. Through this partnership, we're hoping to bring more LGBTQIA+ stories, characters and lived experiences to Xbox in ways that will resonate with gamers around the world.

During June and beyond, Xbox is featuring a variety of game collections that showcase LGBTQIA+ creators, lead characters and games with inclusive gender and relationship options. These collections will exist year-round as part of our ongoing work to create more inclusive gaming ecosystems and elevate content that resonates with communities.

These are just some of the many celebrations on Xbox. Discover more on Xbox Wire.

Xbox is taking next steps for more LGBTQIA+ representation and storytelling.

Make Pride with us – tune in to our live event on June 16 at 10 a.m. PDT

This year, LGBTQIA+ communities at Microsoft invite everyone to join our live "Make Pride" event on YouTube to learn about the state of LGBTQIA+ rights from organizers, players, and advocates from LGBTQIA+ communities at Microsoft and beyond. Discover how we can bridge borders and bring about collective change for all.

We hope you can join us on June 16 from 10 a.m. to 12 p.m. PDT to watch the livestream at aka.ms/talkpride

Since the introduction of our first Pride products in 2018, they've been a visible celebration of the intersectionality and strength of the LGBTQIA+ communities.

Microsoft Unlocked – Discover all things Pride at Microsoft. Xbox – Explore nonprofit partnerships, community stories, in-game content, and more – all created by and with LGBTQIA+ communities. Xbox Gear – Wear your Pride with limited-edition T-shirts, totes and more. Xbox games – Celebrate Pride with LGBTQIA+ creators on Xbox. Xbox Design Lab Pride controller – Customize the Xbox Wireless Controller with Xbox Design Lab and show your Pride with the new bold design. Available year-round in all Xbox Design Lab markets. Microsoft Store – Explore collections of games and movies curated by the LGBTQIA+ communities at Microsoft. Windows – Bring your Pride to your Windows with new wallpapers celebrating Pride and many LGBTQIA+ communities. Microsoft 365 – Turn on your Pride in select Microsoft 365 apps* and celebrate with a special theme inspired by the flags of the LGBTQIA+ communities. Microsoft Teams – Show where you stand with over 50 virtual backgrounds representing different LGBTQIA+ flags. Outlook mobile – Embrace your Pride year-round in Outlook mobile on iOS and Android with permanent themes inspired by the Pride, Trans, Lesbian, Bisexual and Nonbinary flags. Microsoft Rewards – Join Microsoft Rewards to support OutRight International by simply searching on Microsoft Bing. Bing – Explore the stories of people who've contributed to the ongoing progress for LGBTQIA+ equality. Microsoft Life – Explore the stories of our employees from around the world. Inclusion is Innovation – Explore LGBTQIA+ stories year-round. Clipchamp – Unleash your creativity on Clipchamp with Pride templates, filters, stickers and more. Microsoft Create – Choose from dozens of easily customizable templates with Microsoft Create. Microsoft Designer – Create unique Pride designs and get suggested text to match, powered by AI. There are many ways to make Pride and honor the power and strength of the LGBTQIA+ communities.

Acting on the message – donating $200,000 to LGBTQIA+ nonprofits

We're continuing our long-standing support of LGBTQIA+ organizations. To recognize the launch of our 2023 Pride campaign and products, we're contributing a total of $200,000 to Outright International, Stonewall, SAATHII, Casa Rara and GLAAD to help in the fight for LGBTQIA+ equity and equality. That's in addition to the $1.4 million we've donated together with our employees since last year, and over $30 million we've donated in the last 30 years.

Microsoft introduced sexual orientation into our non-discrimination policies in 1989 – and we've been driving toward LGBTQIA+ inclusion ever since. In 1993, we began offering employee benefits for same-sex domestic partnerships, making us one of the first companies in the world to do so. We stood as a public supporter for marriage equality even before it became legal in the United States – and we continue to advocate for every type of family. Microsoft has attained a 100 score on the Human Rights Campaign Foundation's Corporate Equality Index for 17 years.

Since its founding in 1993, GLEAM has honored Pride by lifting up LGBTQIA+ voices and used the reach of our products to share the messages of inclusion and support.

* "Show your Pride" theme is available in the following apps – iOS: Outlook, OneNote, Office, Word, Excel and PowerPoint; in Android: Outlook; in Mac OS Word, Excel, PowerPoint, OneNote In Outlook, you can find the Pride themes in Settings > Appearance In all other apps, you can find Pride theme in Settings.

As innovation has progressed through radio, the internet, Wi-Fi, smartphones, and the Internet of Things, we have consistently faced security concerns with each technological milestone. Every new and disruptive technology comes with both opportunities and challenges.

With AI, we are heeding this lesson from the past and proactively addressing the security challenges that will inevitably arise.

Yet while the AI revolution feels like the biggest innovation in a generation, scaled quantum computing is set to disrupt many aspects of technology again — and we must prepare for it now.

Quantum computing at scale has the potential to help solve many of the world's most complex and pressing problems. Whether it's addressing food sustainability, developing better batteries, or mitigating climate change via carbon capture, scientists will have unprecedented computing power at their disposal. This transformational computing power capable of driving so much societal good could also be used by bad actors looking to cause disruption and harm. By advancing our security capabilities to meet this moment, people and organizations can reap the profound benefits of quantum computing without succumbing to these threats.

Microsoft embarked on the road to quantum more than 20 years ago and is in a unique position to contribute to a quantum-safe future. The investments we have made in this emerging field help us to understand new risks it may introduce and how to mitigate them early and effectively.

Today, most security systems in existing IT environments rely on public-key cryptography, which is used almost everywhere from messaging to transactions to securing data at rest. These cryptographic systems are based on mathematical problems that are difficult and time- consuming for classical computers but will be much easier and quicker for quantum computers to solve.

The strength of current cryptographic systems lies in the complexity of certain mathematical problems, one of which involves finding the factor of extraordinarily large numbers — a task that would take traditional computers millions of years to solve. This is the core principle behind the RSA algorithm that's been in use since the 1970s. Systems using RSA today range from hardware devices such as smart cards and routers, to software applications such as web browsers and email clients. RSA is also used throughout the supply chain of these systems, from the manufacturing of components to the distribution of software updates.

Yet, the emergence of quantum computers has the potential to dramatically upset this balance. Using Shor's algorithm, a quantum computer may be able to unravel these large-number factors in mere minutes, rendering RSA and similar asymmetric algorithms vulnerable. As we progress, algorithm agility, resiliency and flexibility will be needed to easily switch or combine cryptographic approaches — a process that will require significant financial investment, changes in existing infrastructure, and timely planning, execution and coordination across supply chains and ecosystems.

A quantum machine capable of running Shor's algorithm will likely need more than a million stable qubits — thousands of times more than today's quantum computers. These powerful scaled machines are on the way and responsible companies will ensure these quantum systems are not used by bad actors.

At Microsoft, our quantum machine will be delivered as a cloud service through Azure. Just as we do with other technologies, Microsoft will deploy technical and operational controls to ensure our quantum machine will not be used maliciously.

But not every quantum machine in the future will be protected in this way. Immediate risks, such as "Harvest Now, Decrypt Later" scenarios and the potential obsolescence of un-updatable IoT devices, already demand our attention. For these reasons, we must start preparing and acting now, because the transition to become quantum safe for most organizations will take time. That's why we recommend organizations get ready today, which we explain in more detail below. The risk posed by quantum computers is not imminent nor insurmountable, but the transition to become quantum-safe for most organizations will be a significant undertaking.

Just over two decades ago, the Y2K challenge wasn't insurmountable or unsolvable, but it took a huge, industry-wide effort to get ready for the change. Today cryptographic systems are spread all over the globe, and the distributed and interconnected services, products and platforms handling those systems means there is an immense threat surface that needs to be prepared and updated to become quantum resistant.

The security industry has been preparing for quantum computers and the associated risks to classical cryptography. Governments and the private sector are investing in research, development, and standardization of quantum-safe approaches such as post-quantum cryptography (PQC) algorithms and potential quantum technologies to strengthen security. As a first step toward PQC adoption, the U.S. National Institute for Standards and Technology (NIST) has been engaged in a years-long effort to solicit, evaluate and standardize quantum-resistant algorithms for broader adoption.

In Europe, the European Telecommunication Standards Institute (ETSI) is assessing quantum-safe cryptographic protocols and standards and their practical implementation. The International Organization for Standardization (ISO) is evaluating PQC algorithms and has established a technical committee to build collaboration on international standards for PQC.

Microsoft has been investing in PQC research, development, experimentation and collaborations since 2014, playing a role in the emergence of PQC and public standards globally. We are participating in SC27/WG2 international standards efforts and have been in close collaboration with NIST, supporting and contributing to their National Cybersecurity Center of Excellence project on Migration to Post-Quantum Cryptography, whose goal is to prepare organizations for the PQC transition.

Microsoft is a core member and supporter of the Open Quantum Safe (OQS) project, and we are leading the PQC working group for SAFECode, a global industry forum for business leaders and technical experts to advance industry standards and help organizations prepare for the PQC transition. We have also been focused on quantum technologies and their impact on security with dedicated research and development of tools.

As the ecosystem progresses, we continue to encourage industry and government to invest in the global adoption of harmonized cryptographic standards and additional quantum-safe measures to facilitate secure global trade in the future.

Given Microsoft's unique position and wide perspective developing both hardware and software — along with our experience from past efforts transitioning to new cryptographic algorithms — we know that the journey to achieve quantum safety will be a significant undertaking.

This will be an iterative and collaborative process, and we are committed to being a trusted partner across industry and government. Transparency and clarity will be key to success, and as we continue to make progress, we will share learnings and recommendations with the broader community.

One of the best ways for an organization to accelerate their quantum-safe readiness is to move to the hyperscale cloud, but not all our customers and partners are using the cloud. With this in mind, we are taking a comprehensive approach across our platforms and systems.

Today we are taking the necessary steps across our own portfolio and ecosystem to ensure our products and services remain secure against potential risks the technology continues to develop.

We have formed a group of experts from across the company to concentrate on this matter with constant input from regulators, industry partners, vendors and legal experts and research teams. We have also started efforts to create, test, and implement practical cryptographic solutions that can resist potential threats posed by quantum computers. We are deepening our knowledge of quantum-safe algorithms and mitigation options for various use cases, considering hybrid encryption schemes to accommodate adaptive updates in cryptography algorithms, creating a cryptographic inventory to identify vulnerable cryptography in our platforms and services, and developing a multi-phase roadmap to address gaps and prioritize crucial areas.

From the cloud to on-premises environments, we are assessing every piece of technology that connects to Microsoft. Our goal is to make this journey as simple and manageable as possible both for us and for our customers and partners.

The time to prepare is now — and Microsoft is here to help

It will take time to implement such sweeping changes, but the sooner you start, the safer you'll be. It is essential to raise awareness and deepen all of our understanding of the risks — and to start now.

If you're wondering where to begin, creating an inventory of critical data and cryptography technologies can reveal areas where cryptography is implemented incorrectly or in a way that's unsuitable for its intended purposes. It is crucial to identify internal standards and processes and assess all options to update those cryptography protocols and libraries to mitigate potential risks.

Based on those inventories and assessments, we recommend prioritizing your systems and services based on criteria such as criticality, dependencies and cost. From there, develop a transition roadmap.

We are already helping several customers and partners, notably those in risk-sensitive industries, in their quest to be quantum-safe by providing resources and transition strategies. Yet, the urgency for all organizations to embark on this journey cannot be overstated. We encourage customers and partners to act now, and we're here to support.

As quantum technology continues to advance and change the world, our commitment to the security of our products and customers has never been stronger. We are dedicated to minimizing the efforts required by our customers and partners to become quantum-safe, using our world-leading research and engineering teams to keep our products and services secure.

Related link: Read more about how we build security into everything we build and deliver at Microsoft.

Recent advances in artificial intelligence have sparked both wonder and anxiety as we contemplate its transformative potential. AI holds enormous promise to enrich our lives, but this anticipation comes intertwined with apprehensions about the challenges and risks that may emerge. To nurture a future where AI is leveraged to the benefit of people and society, it is crucial to bring together a wide array of voices and perspectives.

With this goal in mind, I am honored to present the "AI Anthology," a compilation of 20 inspiring essays authored by distinguished scholars and professionals from various disciplines. The anthology explores the diverse ways in which AI can be channeled to benefit humanity while shedding light on potential challenges. By bringing together these different viewpoints, our aim is to stimulate thought-provoking conversations and encourage collaborative efforts that will guide AI toward a future that harnesses its potential for human flourishing.

I first encountered GPT-4, a remarkable large-scale language model, in the fall of 2022 while serving as the chair of Microsoft's Aether Committee. The Aether leadership and engineering teams were granted early access to OpenAI's latest innovation, with a mission to investigate potential challenges and wider societal consequences of its use. Our inquiries were anchored in Microsoft's AI Principles, which were established by the committee in collaboration with Microsoft's leadership in 2017. We conducted a comprehensive analysis of GPT-4's capabilities, focusing on the possible challenges that applications employing this technology could pose in terms of safety, accuracy, privacy and fairness.

GPT-4 left me awestruck. I observed unexpected glimmers of intelligence beyond those seen in prior AI systems. When compared to its predecessor, GPT-3.5 — a model utilized by tens of millions as ChatGPT — I noticed a significant leap in capabilities. Its ability to interpret my intentions and provide sophisticated answers to numerous prompts felt like a "phase transition," evoking imagery of emergent phenomena that I had encountered in physics. I found that GPT-4 is a polymath, with a remarkable capacity to integrate traditionally disparate concepts and methodologies. It seamlessly weaves together ideas that transcend disciplinary boundaries.

The remarkable capabilities of GPT-4 raised questions about potential disruptions and adverse consequences, as well as opportunities to benefit people and society. While our broader team vigorously explored safety and fairness concerns, I delved into complex challenges within medicine, education and the sciences. It became increasingly evident that the model and its successors — which would likely exhibit further jumps in capabilities — hold tremendous potential to be transformative. This led me to contemplate the wider societal ramifications.

Questions came to mind surrounding artistic creation and attribution, malicious actors, jobs and the economy, and unknown futures that we cannot yet envision. How might people react to no longer being the unparalleled fount of intellectual and artistic thought and creation, as generative AI tools become commonplace? How would these advancements affect our self-identity and individual aspirations? What short- and long-term consequences might be felt in the job market? How might people be credited for their creative contributions that AI systems would be learning from? How might malicious actors exploit these emerging powers to inflict harm? What are important potential unintended consequences of the uses, including those we might not yet foresee?

At the same time, I imagined futures in which people and society could thrive in extraordinary ways by harnessing this technology, just as they have with other revolutionary advances. These transformative influences range from the first tools of cognition — our shared languages, enabling unprecedented cooperation and coordination — to the instruments of science and engineering, the printing press, the steam engine, electricity, and the internet, culminating in today's recent advances in AI.

Eager to investigate these opportunities in collaboration with others across a wide array of disciplines, we initiated the "AI Anthology" project, with OpenAI's support. We invited 20 experts to explore GPT-4's capabilities and contemplate the potential influences of future versions on humanity. Each participant was granted early confidential access to GPT-4, provided case studies in education, scientific exploration and medicine, drawn from my explorations, and asked to focus on two core questions:

How might this technology and its successors contribute to human flourishing? How might we as society best guide the technology to achieve maximal benefits for humanity?

Building upon the ideas presented in my Tanner Lecture at the University of Michigan in November 2022 (Arc of Intelligence: Humanity and its Tools of Reason and Imagination), these questions highlight the importance of long-term thinking and maintaining an optimistic perspective on AI's potential to enrich human lives. We could unlock immense potential benefits. But to realize this potential, we must create technical innovations and policies to protect against malicious uses and unintended consequences.

This anthology is a testament to the promise of envisioning and collaboration and to the importance of diverse perspectives in shaping the future of AI. The 20 essays offer a wealth of insights, hopes and concerns, illustrating the complexities and possibilities that arise with the rapid evolution of AI.

As you read these essays, I encourage you to remain open to new ideas, engage in thoughtful conversations, and lend your insights to the ongoing discourse on harnessing AI technology to benefit and empower humanity. The future of AI is not a predetermined path, but a journey we must navigate together with wisdom, foresight and a deep sense of responsibility. I hope that the ideas captured in these essays contribute to our collective understanding of the challenges and opportunities we face. They can help guide our efforts to create a future where AI systems complement human intellect and creativity to promote human flourishing.

Welcome to the "AI Anthology." May it inspire you, challenge you, and ignite meaningful conversations that lead us toward a future where humanity flourishes by harnessing AI in creative and valuable ways.

We will publish four new essays at the beginning of each week starting today. The complete "AI Anthology" will be available on June 26, 2023.

As Microsoft's Chief Scientific Officer, Eric Horvitz spearheads company-wide initiatives, navigating opportunities and challenges at the confluence of scientific frontiers, technology and society. He is known for his contributions to AI theory and practice, including research on principles and applications of AI amidst the complexities of the open world.

The views, opinions and proposals expressed in these essays are those of the authors and do not necessarily reflect the official policy or position of any other entity or organization, including Microsoft and OpenAI. The authors are solely responsible for the accuracy and originality of the information and arguments presented in their essays. Participation in the "AI Anthology" was voluntary and no incentives or compensation were provided to the authors.

You only need two simple letters to accurately convey the major shift in the technology space this year: A and I. Beyond those letters, however, is a complex, evolving and exciting way in which we work, communicate and collaborate. As you will see, artificial intelligence is a common thread as we embark on Microsoft Build, our annual flagship event for developers.

It's already been a landmark year for the industry, starting in January with the announcement of an extension of our partnership with OpenAI to accelerate AI breakthroughs and to ensure these benefits are broadly shared with the world. And in February, Microsoft announced an all-new, AI-powered Bing search engine and Edge browser to transform the largest software category in the world – search.

Since then, developments have accelerated at a rapid pace, with several key milestones along the way, including:

Greater availability of Azure OpenAI Service with added support for ChatGPT and OpenAI's groundbreaking GPT-4 model. Copilots across a wide range of users, including Dynamics 365 Copilot, Microsoft 365 Copilot and Copilot for Power Platform. Expansion of a new AI-powered Bing to the Windows 11 taskbar, mobile and Skype; Bing Image Creator to chat; and a full open preview of the platform, no waitlist required.

This is just the beginning of the new era of AI. That's why Microsoft Build is so important. During this event, we'll be showcasing how AI is redefining what and how developers build, as well as how AI is changing the future of work.

Before we get into the news, let's talk about two concepts we are discussing at length during Microsoft Build: copilots and plugins.

A copilot is an application that uses modern AI and large language models (LLMs) like GPT-4 to assist people with complex tasks. Microsoft first introduced the concept of a copilot nearly two years ago with GitHub Copilot, an AI pair programmer that assists developers with writing code, and we continue to release copilots across many of the company's core businesses.

We believe the copilot represents both a new paradigm in AI-powered software and a profound shift in the way that software is built – from imagining new product scenarios, to the user experience, the architecture, the services that it uses and how to think about safety and security.

Plugins are tools first introduced for ChatGPT, and more recently Bing, which augment the capabilities of AI systems, enabling them to interact with application programming interfaces (APIs) from other software and services to retrieve real-time information, incorporate company and other business data, perform new types of computations and safely take action on the user's behalf. Think of plugins as the connection between copilots and the rest of the digital world.

With that said, let's focus on the news and announcements we're unveiling during Microsoft Build.

Microsoft is announcing that we will adopt the same open plugin standard that OpenAI introduced for ChatGPT, enabling interoperability across ChatGPT and the breadth of Microsoft's copilot offerings.

Developers can now use one platform to build plugins that work across both consumer and business surfaces, including ChatGPT, Bing, Dynamics 365 Copilot and Microsoft 365 Copilot.

And if you want to develop and use your own plugins with your AI application built on Azure OpenAI Service, it will, by default, be interoperable with this same plugin standard. This means developers can build experiences that enable people to interact with their apps using the most natural user interface: the human language.

As part of this shared plugin platform, Bing is adding to its support for plugins. In addition to previously announced plugins for OpenTable and Wolfram Alpha, we will also have Expedia, Instacart, Kayak, Klarna, Redfin and Zillow, among many others in the Bing ecosystem.

In addition to the common plugin platform, Microsoft is announcing that Bing is coming to ChatGPT as the default search experience. ChatGPT will now have a world-class search engine built-in to provide more up-to-date answers with access from the web. Now, answers are grounded by search and web data and include citations so users can learn more, all directly from within chat. The new experience is rolling out to ChatGPT Plus subscribers starting today and will be available to free users soon by simply enabling a plugin.

We're also announcing that developers can now integrate their apps and services into Microsoft 365 Copilot with plugins.

Plugins for Microsoft 365 Copilot include ChatGPT and Bing plugins, as well as Teams message extensions and Power Platform connectors – enabling developers to leverage their existing investments. And developers will be able to easily build new plugins for Microsoft 365 Copilot with the Microsoft Teams Toolkit for Visual Studio Code and Visual Studio. Developers can also extend Microsoft 365 Copilot by bringing their data into the Microsoft Graph, contextualizing relevant and actionable information with the recently announced Semantic Index for Copilot.

More than 50 plugins from partners will be available for customers as part of the early access program, including Atlassian, Adobe, ServiceNow, Thomson Reuters, Moveworks and Mural, with thousands more available by the general availability of Microsoft 365 Copilot.

New Azure AI tooling to help developers build, operationalize deploy their own next-generation AI apps

It starts with our new Azure AI Studio. We're making it simple to integrate external data sources into Azure OpenAI Service. In addition, we're excited to introduce Azure Machine Learning prompt flow to make it easier for developers to construct prompts while taking advantage of popular open-source prompt orchestration solutions like Semantic Kernel.

In Azure OpenAI Service, which brings together advanced models including ChatGPT and GPT-4, with the enterprise capabilities of Azure, we're announcing updates to enable developers to deploy the most cutting-edge AI models using their own data; a Provisioned Throughput SKU that offers dedicated capacity; and plugins that simplify integrating other external data sources into a customer's use of Azure OpenAI Service. We now have more than 4,500 customers using Azure OpenAI Service.

At Microsoft, we've been committed to developing AI technology that has a beneficial impact and earns trust, while also sharing our own learnings and building new tools and innovations that help developers and businesses implement responsible AI practices in their own work and organizations. At Build, we're introducing several new updates, including Azure AI Content Safety, a new Azure AI service to help businesses create safer online environments and communities. As part of Microsoft's commitment to building responsible AI systems, Azure AI Content Safety will be integrated across Microsoft products, including Azure OpenAI Service and Azure Machine Learning.

We're also introducing new tools to Azure Machine Learning, including expanding Responsible AI dashboard support for text and image data, in preview, enabling users to evaluate large models built with unstructured data during the model building, training and/or evaluation stage. This helps users identify model errors, fairness issues and model explanations before models are deployed, for more performant and fair computer vision and natural language processing (NLP) models. And prompt flow, in preview soon, provides a streamlined experience for prompting, evaluating and tuning large language models. Users can quickly create prompt workflows that connect to various language models and data sources and assess the quality of their workflows with measurements such as groundedness to choose the best prompt for their use case. Prompt flow also integrates Azure AI Content Safety to help users detect and remove harmful content directly in their flow of work.

In addition, Microsoft announced new media provenance capabilities coming to Microsoft Designer and Bing Image Creator in the coming months that will enable users to verify whether an image or video was generated by AI. The technology uses cryptographic methods to mark and sign AI-generated content with metadata about its origin.

Today's world is awash with data, constantly streaming from the devices we use, the applications we build and the interactions we have. And now, as we enter a new era defined by AI, this data is becoming even more important. Powering organization-specific AI experiences requires a constant supply of clean data from a well-managed and highly integrated analytics system. But most organizations' analytics systems are a labyrinth of specialized and disconnected services.

Microsoft Fabric is a unified platform for analytics that includes data engineering, data integration, data warehousing, data science, real-time analytics, applied observability and business intelligence, all connected to a single data repository called OneLake.

It enables customers of all technical levels to experience capabilities in a single, unified experience. It is infused with Azure OpenAI Service at every layer to help customers unlock the full potential of their data, enabling developers to leverage the power of generative AI to find insights in their data.

With Copilot in Microsoft Fabric in every data experience, customers can use conversational language to create dataflows and data pipelines, generate code and entire functions, build machine learning models or visualize results. Customers can even create their own conversational language experiences that combine Azure OpenAI Service models and their data and publish them as plugins.

Our customers benefit from our partner collaborations, such as with NVIDIA, that enable organizations to design, develop, deploy and manage applications with the scale and security of Azure. NVIDIA will accelerate enterprise-ready generative AI with NVIDIA AI Enterprise Integration with Azure Machine Learning. Omniverse Cloud, only available on Azure, enables organizations to aggregate data into massive, high-performance models, connect their domain-specific software tools and enable multi-user live collaboration across factory locations. NVIDIA GPUs leveraging ONNX Runtime & Olive toolchain will support the implementation of accelerating AI models without needing a deeper knowledge of the hardware.

Microsoft Dev Box, an Azure service that gives developers access to ready-to-code, project-specific dev boxes that are preconfigured and centrally managed, is introducing several new capabilities to enhance the developer experience and boost productivity. While in preview, we've seen many customers experimenting with Dev Box, and we've migrated more than 9,000 developers internally to the service for day-to-day software development.

Now, we've added additional features and capabilities, including customization using configuration-as-code and new starter developer images in Azure Marketplace that provide dev teams with ready-to-use images that can be customized further for specific dev team needs. Additionally, developers can now manage custom environments from a specialized developer portal, Azure Deployment Environments. Dev Box general availability will begin in July.

Unveiling a new home for developers on Windows 11 with Dev Home

Dev Home will launch at Microsoft Build in preview as a new Windows experience developers can get from the Microsoft Store.

Dev Home makes it easy to connect to GitHub and configure cloud development environments like Microsoft Dev Box and GitHub Codespaces. Dev Home is open source and fully extensible, enabling developers to enhance their experience with a customizable dashboard and the tools they need to be successful.

Last fall at our Windows and Surface launch, Chief Product Officer Panos Panay talked about the power of AI to unlock new interaction models on the PC with Windows Studio Effects and DALL-E 2 in Microsoft Designer, and at CES he talked about how AI is going to reinvent the way people get things done on Windows.

Windows will be the first PC platform to centralize AI assistance with the introduction of Windows Copilot. Together, with Bing Chat and first- and third-party plugins, users can focus on bringing their ideas to life, completing complex projects and collaborating instead of spending energy finding, launching and working across multiple applications.

This builds on the integration we released into Windows 11 back in February that brought the new AI-powered Bing to the taskbar.

A preview of Windows Copilot will start to become available for Windows 11 in June.

As you can see, it's going to be a busy time at Microsoft Build. To give you a sense of what developers are going to experience at the event, we're expecting approximately 200,000 registered attendees, with 350 sessions and more than 125 hours of content over two days. In total, we'll announce more than 50 new products and features.

For more information, make sure to watch keynotes on demand from Microsoft Chairman and CEO Satya Nadella, Kevin Scott and Scott Guthrie on Day 1. On Day 2, watch the keynotes anchored by Rajesh Jha and Panos Panay. Additionally, you can explore all the news and announcements in the Book of News and read more stories and news about products from Microsoft Build here:

Microsoft outlines framework for building AI apps and copilots; expands AI plugin ecosystem

Bing at Microsoft Build 2023: Continuing the Transformation of Search

Bringing the power of AI to Windows 11 – unlocking a new era of productivity for customers and developers with Windows Copilot and Dev Home

Introducing Microsoft Fabric: Data analytics for the era of AI

Tags: AI, Azure AI Content Safety, Azure OpenAI Service, Bing, copilots, developers, Microsoft 365 Copilot, Microsoft Build, plugins, Windows 11

As governments today look at technology innovation, there is a need to address the rapidly evolving demands of their citizens while protecting the most sensitive data and delivering on promises of trust and security. Modernization of government services without compromising security, digitization of manual processes and improved user experiences for both citizens and government employees are all benefits of digital transformation, and the right technology platforms can create even better opportunities for social and economic growth.

The cloud can offer agility and flexibility, advanced cybersecurity features, and access to the latest innovations, including AI, to accelerate digital transformation and the delivery of essential public services. Many governments want to take advantage of the benefits of the public cloud while also managing their data in accordance with their local policies and regulatory requirements. They need solutions that ensure digital sovereignty, giving them control over their data in the cloud. In July of last year, we announced Microsoft Cloud for Sovereignty, a new solution that will enable governments to build and digitally transform workloads in the Microsoft cloud while helping to meet many of their specific compliance, security and policy requirements. Microsoft Cloud for Sovereignty creates software boundaries in the cloud to establish the extra protection that governments require, leveraging hardware-based confidentiality and encryption controls.

Since then, we have engaged with numerous government leaders, policymakers and public sector organizations in Europe and the rest of the world about why the cloud is the best solution for sovereignty needs. We have delivered two Microsoft Cloud for Sovereignty private preview releases that validated the power and scale of the cloud with adherence to government requirements and empowered government customers with the digital transformation they are striving for. It also helped identify three key obstacles impeding governments from meeting their digital transformation and innovation goals: Complying with legislative and regulatory requirements; protecting and securing sensitive data; and innovating without compromise.

Before taking advantage of the many benefits of the cloud, government policymakers want to have confidence in the security and privacy of their data and the ability to keep innovating while protecting that data. A key element of this is meeting their legislative or regulatory obligations. This is where Microsoft's transparency commitments come in.

Many governments require transparency regarding how Microsoft responds to third-party data requests. At the core of our business, we adhere to the following policies across our services and commit this to our customers:

Microsoft does not provide direct and unfettered access to our customers' data. We do not share encryption keys or the ability to break our encryption with anyone, including any government. Governments must follow the applicable legal process to request customer data. They must serve us with a warrant for content or a court order or subpoena for subscriber information or other non-content data. All requests must target specific accounts and identifiers. Microsoft's legal compliance team reviews all submissions to ensure they are valid and rejects those that are not. We use a variety of legal means to challenge data requests, provide data only when a request is valid and provide only the data specified in a legal order.

As all major cloud service providers operate globally, governments often wonder how legislation such as the U.S. CLOUD Act impacts cloud providers. This law does not change any of the legal and privacy protections. Microsoft adheres to the same principles and customer commitments for government demands for user data.

Microsoft's semi-annual law enforcement request report, part of our transparency measures, reveals that the vast majority of our customers are never impacted by government requests for data. For example, in the first half of 2022, Microsoft received 41 legal demands from law enforcement in the United States for commercial, educational, and public sector customers who purchased more than 50 seats. Of those demands, there were no disclosures of content data related to a non-U.S. customer whose data was stored outside of the United States.

With Microsoft Cloud for Sovereignty, we provide additional sovereign guardrails that help ensure data is protected in transit and in storage by customer-owned keys not accessible to Microsoft and protected in use when using Azure confidential computing, delivering unmatched software and hardware protections such that only customers with the proper keys can access the data when unencrypted.

With the rapid increases in volume, severity, and sophistication of cyberattacks, it is increasingly clear that the cloud is the best way to protect data. The cloud provides government customers with world-class protection and unmatched resources and scale to detect, respond and deter attacks early on.

Microsoft Cloud and Azure are trusted by millions of customers with their most sensitive and mission-critical workloads. We offer a wide variety of tools to monitor and respond to security threats, as well as specialized services to help governments keep their data safe. With more than 8,500 Microsoft security experts across 77 countries, we have helped provide a critical perspective on the security landscape and are continuously fine-tuning our security approaches as we learn from each cyberattack. With the addition of the Microsoft Cloud for Sovereignty, government customers can establish their own software boundary in the cloud while taking full advantage of Azure's best-in-class security, resilience and scale capabilities. Microsoft Cloud for Sovereignty allows customers to leverage key Azure capabilities including:

Microsoft's Azure Key Vault Managed Hardware Security Module allows customers to maintain complete control of the cryptographic key, while still benefiting from the redundancy, resilience, cybersecurity and managed experience of the cloud. Azure confidential computing is a unique service that protects data in use and allows the data to be processed only after the cloud environment is verified to be a trusted execution environment. In this way, confidential computing helps protect data from being accessed by cloud operators, malicious admins, and even privileged software such as the hypervisor. It helps keep data protected throughout its lifecycle — in addition to existing solutions of protecting data at rest and in transit, data is now protected while in use. In Azure, the root of trust is with independent hardware, so even Microsoft operators cannot access the memory encryption keys. This independent hardware root trust is what helps government customers to independently cryptographically verify the identity and "known good state" of the cloud operating environment they are relying on.

With Microsoft, governments can take advantage of our view of the evolving threat landscape. With industry-leading AI, we synthesize 65 trillion security signals a day across all types of devices, apps, platforms and endpoints, which is an eight times increase from the 8 trillion daily signals captured just two years ago. And we apply the learnings from that signal intelligence, as well as from our world-class threat intelligence, into all the products and services we offer. Furthermore, we now have more than 15,000 partners working with us across our security ecosystem, helping to bring better solutions and more choices to the market. Our global threat intelligence perspective enables early detection and response to emerging threats across multi-cloud, hybrid, on-premises and heterogeneous platforms.

Embracing digital innovation while ensuring digital sovereignty is a challenge for governments. Investing in a private, isolated on-premises datacenter might, at face value, seem to be the only way to achieve the level of control and security a government requires. But building on a private cloud requires maintaining on-premises datacenters, which are no longer viable due to their inability to scale and deliver the security and functionality that the cloud provides. Finally, we also hear from our customers that they are operating under increasingly restrained resources and shrinking budgets. The cloud offers governments the ability to operate on a lean budget, compared to on-premises. Migrating to the cloud can enable governments to take advantage of the agile benefits of the cloud and focus their spending on the latest innovations in the delivery of services.

As technologies advance, they are taken to the cloud first. Governments that adopt the cloud benefit from the latest innovations changing how governments operate, including AI, blockchain, digital identity and online services, while those that maintain or expand their private cloud investments may not benefit from the same growth and innovation.

Innovating while meeting digital sovereignty requirements is complex and can differ greatly between customers, industries and geographies. We believe a cloud solution designed to address sovereignty needs to have advanced capabilities built with the needs of government customers in mind and must be customizable and flexible to adhere to evolving local policies and regulatory requirements around the handling of data.

Now for the first time, governments are not forced to choose between digital innovation and data control; they can have both. With Microsoft Cloud for Sovereignty, we are investing further in guardrails such as Sovereign Landing Zones, local policy packs and increased transparency throughout Azure, creating a virtual sovereign cloud environment for customer workloads. This helps enable customers to take advantage of best practices and implement secure, consistent and compliant environments and adhere to evolving local regulations while taking full advantage of the cloud.

The public sector will always need to protect and secure sensitive data, comply with legislative and regulatory requirements and innovate without compromise. At Microsoft, we strive to meet these needs by providing customers with transparency and unsurpassed data controls, investing in cybersecurity and collaborating with governments worldwide to meet their policy and regulatory needs.

It is no longer necessary for governments to choose between innovation and digital sovereignty. Governments can now harness the full power of the cloud, including broad platform capabilities, while having greater control over their data and increased transparency to the operational and governance processes of the cloud — allowing governments to ensure compliance while driving their digital transformation.

In March, we introduced the world to Microsoft 365 Copilot – your copilot for work. We're excited to share the next step in our journey as we bring Copilot to more customers and introduce new capabilities. We're also releasing new data and insights from our 2023 Work Trend Index on how work is changing in the era of AI.

Microsoft 365 Copilot Early Access Program Since March, we've been testing Copilot with 20 enterprise customers, learning alongside companies like Chevron, Goodyear, General Motors and Dow. Their overwhelming feedback is that Copilot has the potential to revolutionize work. They point to how it is a game changer for meetings and is beginning to transform the way they create. And, they've identified areas where we can do more to help people adapt to this new way of working, like the need for more conversational, multi-turn interactions. As we bring Copilot to more customers, we'll continue to rely on this kind of feedback to refine Copilot and help guide users as they adapt to this new way of working.

"The potential of Microsoft 365 Copilot is undeniable, and it's energizing to explore the possibilities as we couple the ingenuity of our people with the functionality of the tool. Early access has given us visibility into how it can further streamline processes, speed insights, spark ideas, enhance productivity and evolve the way we work. We're proud to team with Microsoft as we continue to achieve new levels of innovation and advance the future of energy." – Guy Moore, Workforce Enablement Lead, Chevron

Today, we're excited to announce the Microsoft 365 Copilot Early Access Program, an invitation-only, paid preview that will roll out to an initial wave of 600 customers worldwide.

And, to help every customer get AI-ready, we'll also start rolling out Semantic Index for Copilot – a new capability in Microsoft 365 E3 and E5. Semantic Index for Copilot is a sophisticated map of your user and company data. For example, when you ask it about the "March Sales Report," it doesn't simply look for documents with those words in the file name or body. Instead, it understands that "sales reports are produced by Kelly on the finance team and created in Excel." And it uses that conceptual understanding to determine your intent and help you find what you need. The Semantic Index for Copilot is critical to getting relevant, actionable responses to prompts in Microsoft 365 Copilot. And, it enhances enterprise search results for E3 and E5 customers – whether they are using Copilot or not.

2023 Work Trend Index The platform shift to AI is well underway. Just as customers turned to Microsoft to help them make the shift to remote and flexible work, they're now turning to us to help them understand how this new era of AI will change work yet again.

Our 2023 Annual Work Trend Index surveyed 31,000 people in 31 countries and analyzed trillions of aggregated productivity signals in Microsoft 365, along with labor market trends on LinkedIn to understand what the new era of AI means for work. The data paints a clear picture: The pace and volume of work has increased exponentially, and employees are struggling under the weight of work – putting innovation at risk. Leaders and employees alike are eager for AI to lift the burden.

"This new generation of AI will remove the drudgery of work and unleash creativity," said Satya Nadella, Chairman and CEO, Microsoft. "There's an enormous opportunity for AI-powered tools to help alleviate digital debt, build AI aptitude, and empower employees."

The report unearthed three key insights business leaders need to know now, as they look to responsibly adopt AI:

Digital debt costing us innovation We're all carrying digital debt: The volume of data, emails and chats has outpaced our ability to process it all. Every minute spent managing this digital debt is a minute not spent on creative work. In a world where creativity is the new productivity, digital debt is more than an inconvenience – it's impacting business.

When 62% of employees report spending too much time searching for information, communicating and coordinating, they have less time for deep thinking, creating and social collaboration. Nearly two in three people say they don't have enough time and energy to do their job. And those same employees are 3.5 times more likely to struggle with innovation and strategic thinking.

There's a new AI-employee alliance One of the predominant narratives around AI is the notion employees are fearful it may ultimately put them out of a job. While 49% of our respondents had some concerns about job security, a staggering 70% would still choose to delegate as much work as possible to AI to lessen their workloads. Simply put: Employees know what's in it for them, the promise of relief outweighs the threat. They envision using AI not just for administrative tasks, but also for analytical and creative work.

This is one place where employees and leaders agree. Our data shows managers are two times more likely to say they're looking to empower people with AI, rather than replace them. In fact, reducing headcount was last on their list. Their top hopes for AI? Increasing productivity, automating tasks, increasing employee well-being and enabling employees to focus on impactful work.

Every employee needs AI aptitude AI will introduce an entirely new interaction model between humans and computers, and soon we won't be able to imagine work without it. A platform shift this big will require new skilling – from prompt engineering to reimagining workflows with AI. Eighty-two percent of leaders anticipate employees will need new skills in the AI era, and we're already seeing a 79% year-over-year increase in the number of LinkedIn job postings in the U.S. that reference "GPT" or "GAI" (generative artificial intelligence).

Employees are ready for this new aptitude — learning new skills is the No. 1 thing they'd do with more time and energy. Leaders feel the same: Learning new skills is also the No. 1 thing they wish their employees would do with more time and energy.

Introducing new Microsoft 365 Copilot capabilities When we introduced Copilot in March, we unveiled capabilities across the Microsoft 365 suite of apps that millions use every day to get work done. We're not stopping there – we're continuing to add new Copilot capabilities to bring AI to every part of the suite, enabling employees and organizations to unleash creativity, unlock productivity and uplevel skills.

Copilot in Whiteboard will make Microsoft Teams meetings and brainstorms more creative and effective. Using natural language, you can ask Copilot to generate ideas, organize ideas into themes, create designs that bring ideas to life and summarize whiteboard content. By integrating DALL-E, OpenAI's image generator, into PowerPoint, users will be able to ask Copilot to create custom images to support their content. Copilot in Outlook will offer coaching tips and suggestions on clarity, sentiment and tone to help users write more effective emails and communicate more confidently. Copilot in OneNote will use prompts to draft plans, generate ideas, create lists and organize information to help customers find what they need easily. Copilot in Loop helps your team stay in sync by quickly summarizing all the content on your Loop page to keep everyone aligned and able to collaborate effectively. Copilot in Viva Learning will use a natural language chat interface to help users create a personalized learning journey including designing upskilling paths, discovering relevant learning resources and scheduling time for assigned trainings.

As we bring Microsoft 365 Copilot to customers, we are guided by our AI principles and Responsible AI Standard and decades of research on AI, grounding and privacy-preserving machine learning. A multidisciplinary team of researchers, engineers and policy experts reviews our AI systems for potential harms and mitigations — refining training data, filtering to limit harmful content, query- and result-blocking sensitive topics, and applying Microsoft technologies like InterpretML and Fairlearn to help detect and correct data bias. We make it clear how the system makes decisions by noting limitations, linking to sources, and prompting users to review, fact-check and adjust content based on subject-matter expertise.

The platform shift to AI is underway, ushering in a new wave of productivity growth. And for both overwhelmed employees and leaders looking to bolster productivity, that promise can't come soon enough. AI represents a whole new way of working – one that will require new skills and habits. Organizations that embrace this new way of working from AI on "autopilot" to AI as copilot will create a brighter, more fulfilling future of work for everyone.

Read the 2023 Annual Work Trend Index on WorkLab and visit the Microsoft 365 blog to learn more about our product announcements.

Continued U.S. leadership in emerging technology requires a sustainable supply of advanced chips to power innovation from artificial intelligence to quantum computing. The CHIPS and Science Act, passed last year, aims to boost domestic research and manufacturing capacity for critical microelectronics. To support this ambitious effort, the U.S. Department of Defense (DoD) launched the Rapid Assured Microelectronics Prototypes (RAMP) using Advanced Commercial Capabilities program, an effort to accelerate the secure, sustainable development of microelectronics for defense technologies.

As part of this effort, three new state-of-the-art chips were developed using the RAMP secure design capability developed by Microsoft within the Azure Government cloud environment to ensure compliance with DoD supply chain requirements. This achievement is a key milestone that marks the first time such chips were designed in the cloud and transmitted via the cloud to chip foundries for manufacture. This process rapidly accelerates the time to market for critical microelectronic components and represents a significant milestone in secure cloud-based microelectronic design and manufacturing. It is also extendable to the commercial design environment in Azure, accelerating the goals of the CHIPS Act to enable a sustainable domestic supply chain for semiconductors.

DoD originally awarded RAMP in September 2020 through the Strategic & Spectrum Missions Advanced Resilient Trusted Systems, managed by National Security Technology Accelerator, with the follow-on prototype RAMP PHASE II awarded in August 2021.

These components are foundational to the security of new and emerging technologies. RAMP demonstrates just-in-time, scalable, on-demand compute and storage infrastructure, enabling significant reductions in simulation time.

Microsoft's RAMP platform shows how the application of cloud technologies can meet stringent security, protection and compliance requirements while at the same time significantly shortening chip time to market, bringing benefits in security, innovation and scale.

At a commercial level, the microelectronics industry uses Azure to accelerate and improve the design process for chip manufacturing. Customers routinely use Azure's infrastructure solutions to optimize design turnaround times, such as high-performance virtual machines and storage, as well as platform solutions that are custom built for silicon design workloads.

In this RAMP demonstration, BAE Systems and Raytheon Technologies demonstrated state-of-the-art, cloud-based design capabilities on the RAMP platform, in collaboration with foundry, IP and electronic design automation partners.

As Katie Sobolewski, BAE R3 product line director, noted, "RAMP helped us meet our design tape-out dates by deploying a tailored cluster of hardware to accelerate our design simulations in hours. Normally, the addition of new hardware on-premises can take months. Access to Azure's performant compute enabled three times faster runtimes, ultimately ensuring we met our aggressive schedule."

Built on top of proven rapid and secure cloud-based design capabilities, the RAMP solution implemented innovative technologies in support of the DoD's Microelectronics Quantified Assurance (MQA) framework. MQA is an evidence-backed risk assessment system that gathers data, from design to manufacturing, to assess the reliability and security of DoD designs.

"We're leading a technology breakthrough in microelectronics with Microsoft and our industry partners to enable an onshore semiconductor foundry with a state-of-the-art trusted design," said Van Andrews, vice president of Department 22 at Raytheon Intelligence & Space, a Raytheon Technologies business. "The application of MQA on this program uncovered several opportunities to assure the security of our design. The integration of MQA with Microsoft is a differentiating capability that strengthens the resiliency of microelectronics. In addition, easy onboarding and the simplified interface improve efficiency for users."

In addition to Microsoft's Azure cloud technologies, the RAMP demonstration used industry leading microelectronics commercial technologies to develop the end-to-end design solution. The RAMP partners include: Ansys, Inc., BAE Systems, Battelle Memorial Institute, Cadence Design Systems, Cliosoft, Cycuity, Flex-Logix, Inc., GlobalFoundries, Intel Corporation, Raytheon Technologies, Siemens EDA, Synopsys, Inc., and Zero ASIC Corporation. These partnerships spanned commercial and government ecosystems enabling the DoD to leverage the best microelectronics development capabilities available.

Microsoft continues to push ahead in advancing secure design and development of critical physical systems for our infrastructure, such as semiconductors and microelectronics. The successful handover of these BAE and Raytheon designs to manufacturing, as part of the RAMP program demonstration, is a major milestone for the industry, as well as the silicon and defense industrial base ecosystem. While the current RAMP program focus now shifts to secure manufacturing, we can now scale the demonstrated secure design capabilities to defense and commercial engineering design projects.

Microsoft and its RAMP commercial industry team partners will continue to bring its combined expertise and experience to transform critical missions, programs, and technologies for DoD.

Just three months ago, we unveiled the new AI-powered Microsoft Bing and Edge to reinvent the future of search with your copilot for the web. We aimed to tackle a universal problem with traditional search – that nearly half of all web searches go unanswered, resulting in billions of people's searches falling short of the mark. We launched the new Bing to bring you better search results, answers to your questions, the ability to create and compose, and with a new level of ease of use by being able to chat in natural language. Bing combines powerful large language models like OpenAI's GPT-4 with our immense search index for results that are current, cited and conversational – something you can't get anywhere else but on Bing. This is fundamentally changing the way people find information. In just 90 days, our customers have engaged in over a half a billion chats, using chat features to get summarized answers to help them with everything from finding the best place to travel for someone with pollen allergies, to organizing the last 10 years of worldwide volcanic activity into a table. We have also seen people create over 200 million images with Bing Image Creator. All up, Bing has grown to exceed 100 million daily active users and daily installs of the Bing mobile app have increased 4X since launch. As a result we are seeing growth of Bing share and it follows the eight straight quarters of growth in our Microsoft Edge browser share. We're excited about continuing to make Bing more accessible by its introduction to the Windows taskbar, reaching over half a billion customers every month. Entering the next generation of AI-powered search Today I'm thrilled to share we are moving to the next generation of AI-powered Bing and Edge to transform the largest category of software in the world – search – by greatly expanding the vision and capabilities we think of as Your Copilot for the Web. This next generation is defined by:

Opening up Bing to more people by moving from Limited Preview to Open Preview and eliminating the waitlist for trial. Moving from text-only search & chat to one that is incredibly more visual with rich image/video answers and coming shortly, new multimodal support. Moving from single use chat/search sessions to multi-session productivity experiences with chat history and persistent chats within Edge. Opening up platform capabilities so developers and third parties can build on top of Bing to help people take actions on their queries and complete tasks.

Bing now in Open Preview Thanks to tremendous customer adoption, engagement and feedback, we're ready to take the next step and are announcing the new Bing is now in Open Preview and no longer has a waitlist. This means that it will now be easier than ever for everyone to try the new Bing and Edge by simply signing into Bing with your Microsoft Account. Making search more visual We know from research that the human brain processes visual information about 60,000 times faster than text, making visual tools a critical way people search, create and gain understanding. Bing has always been known for its visual experiences including features like Knowledge Cards and visual search. And now we're delivering those same experiences in chat. We're introducing richer, more visual answers including charts and graphs and updated formatting of answers, helping you find the information you seek more easily. Click here to load media The above video shows visual improvements to Bing chat. We recently announced the integration of Bing Image Creator into the new Bing chat experience making Bing the only search experience with the ability to generate both written and visual content in one place, from within chat. And today, I'm excited to share that we are expanding Image Creator to all languages in Bing – that's more than 100 languages – so now you can create images in your native language. Click here to load media Video showing Bing Image Creator. We're also beginning our journey to a redesigned Microsoft Edge where one in four Bing chats originate. Edge continues to be your copilot for the web, the first to use AI, and the only browser with Bing built-in. As these changes begin to roll out, you'll begin to see a sleeker and enhanced user interface including a streamlined look, rounded corners, organized containers and semi-transparent visual elements. Lastly, we're expanding what's possible with multi-modal capabilities and are beginning the work to incorporate visual search in chat so you will be able to upload images and search the web for related content. Making search more productive Two of the most requested features we've heard are maintaining access to your chat history and being able to share and export. Starting shortly, you'll be able to pick up where you left off and return to previous chats in Bing chat with chat history. And when you want to dig into something deeper and open a Bing chat result, your chat will move to your Edge sidebar, so you can keep your chat on hand while you browse. Over time, we're exploring making your chats more personalized by bringing context from a previous chat into new conversations. Click here to load media Chat history video. Click here to load media Persistent chat video. Starting soon, we're also adding export and share functionalities into chat. For times when you want to easily share your conversation with others in social media or continue iterating on a newly discovered idea, you can export it directly – the format stays the same to make an easy transition to continue in collaborative tools like Microsoft Word. Chat in Microsoft Edge will also soon have improved summarization capabilities for long documents, including PDFs and longer-form websites, making it easier to consume dense online content. We are also introducing Edge actions. Available in the coming weeks, people will soon be able to lean on AI to complete even more tasks with fewer steps. For example, if you want to watch a particular movie, actions in Edge will find and show you options in chat in the sidebar and then play the movie you want from where it's available. Edge mobile will also soon include page context, so you can ask questions in Bing chat related to the mobile page you're viewing. The compose feature in sidebar can also now tailor drafts based on feedback you give like tone, length, phrasing and more. Click here to load media Edge actions video. Click here to load media Compose video. Moving from a product to a platform The new AI-powered Bing has already helped people more easily find or create what they are looking for, making chat a great tool for both understanding and taking action. The integration of Image Creator saves you time by completing the task of creating the image you need right within chat. We'll soon build third-party plug-ins into the Bing chat experience creating a platform for developers. For example, if you're researching the latest restaurant for dinner in Bing chat, it will leverage OpenTable to help you find and book a reservation. Or, with Wolfram|Alpha, you can create powerful visualizations and get answers to complex science, math and human-curated data-based questions directly from Bing chat. We are working with our partners at OpenAI to make it easier and as consistent as possible for developers to take advantage of this opportunity. We believe these types of skills are a game-changer in the reinvention of search and to advance opportunities for developers in search. We look forward to sharing more details at Microsoft Build later this month. Click here to load media Bing actions. Continuing to build in the open, responsibly As we've said from the beginning, responsible AI is at the center of every new experience we build with the new Bing and Edge. And getting the new Bing into preview so we can learn from real-world testing and feedback has been critical to our success and ability to expand the experience to more users. We believe innovating and learning in the open is part of a responsible approach. But we don't stop there. Together with our partners at OpenAI, we've continued to implement safeguards to defend against harmful content based on what we're learning and seeing in preview. Our teams continue to work to address issues such as misinformation and disinformation, content blocking, data safety and preventing the promotion of harmful or discriminatory content in line with our AI principles. Learn more here about our approach to responsible AI with the new Bing. If you haven't tried the new Bing and Edge yet, now is the time to experience firsthand how we're reinventing search with your copilot for the web. We'll also continue our thoughtful and measured approach of gathering feedback and making rapid adjustments in preview. We continue to use this feedback to make weekly changes to the preview by incorporating the most requested features and updates and adding new experiences along the way. Experience the future of search by visiting bing.com today or downloading the Bing mobile app in your favorite app store. Tags: AI, Bing, Bing Image Ceator, Microsoft Edge

This past quarter has put a spotlight on advancements in generative AI, and the possibilities of its application to deliver pragmatic business outcomes. For organizations around the world, there is incredible opportunity to enhance products and services while unlocking innovation and business value to help shape the future of industry. The pace and scale of technological innovation happening today is unprecedented and breathes optimism into a demanding economy. Those leveraging the Microsoft Cloud — the most trusted, comprehensive and integrated cloud — are best positioned to take advantage of these advancements. As leaders look to embrace AI, it becomes more critical than ever to prioritize having a data-driven business, fortified with digital and cloud capabilities. This approach will help organizations leverage generative AI as an accelerant to transformation. From Rogers improving connectivity for Canadians through automation and AI to the Ministry of Education in the United Arab Emirates developing an AI tutor for students, the possibilities are both limitless and part of today's realities.

Liebherr-Components is creating a best-in-class IoT solution with Microsoft Defender for IoT.

Modernizing manufacturing operations in the cloud to boost efficiency, lower costs and increase security

Mercedes-Benz is leveraging conversational AI capabilities within Azure OpenAI Service to deliver best-in-class customer service with Mercedes Virtual Assistant, while Austrian construction company Strabag is building a risk management solution that improves operational efficiency to save time and reduce financial losses. Coca-Cola Hellenic Bottling Company is continuing to scale its finance and supply chain operations on Azure while lowering enterprise applications costs. Australian-based Graintech estimates a 68% reduction in carbon emissions and reduced travel spending by using HoloLens and Teams to enable remote work and collaboration. FUJIFILM built its Cash Application service using Azure AI and Dynamics 365 to automate 20% of its manual invoicing processes. MAPEI is better engaging its customers with self-service tools while saving costs by consolidating its sales, customer service and marketing systems in the cloud. Belgian manufacturer Picanol Group is promoting tighter integrations between IT and operations by managing and securing its entire IT environment on the Microsoft Cloud. U.K.-based Kier Group is lowering infrastructure costs by consolidating its on-premises environment and migrating 90% of its server workloads to Azure. Krones is improving productivity and security for employees while protecting its globally distributed endpoints and over 6,400 patents by upgrading to Windows 11 Enterprise. Seeing an opportunity to embrace AI and IoT technology to mitigate supply chain disruptions, Liebherr-Components co-developed a turnkey security solution with Reycom AG using Defender for IoT.

IPLAND is using Azure Automated Machine Learning to optimize in-store product displays.

Transforming retail experiences for consumers with AI and data-driven insights

To accelerate product launches and enhance customer service, Unilever has adopted Azure as its primary cloud platform and become a cloud-only enterprise by partnering with Accenture and Microsoft in one of the largest cloud migrations in the consumer goods industry. Campari Group is leveraging Dynamics 365 Copilot capabilities to increase the impact of its marketing campaigns by creating more personalized content and deeper customer insights. To launch the largest autonomous store chain in Europe, Żabka is creating frictionless customer journeys using AI-powered real-time computer vision technology from AiFi and Microsoft Cloud for Retail for store analytics and insights. Retailers like Canadian Tire and IFCO are leveraging Power BI to better understand product performance and develop detailed forecasts for customers. Sakata is automating price updates to reduce cumbersome manual processes, resulting in times savings of more than 60%. In Ukraine, retail technology company IPLAND is applying automated machine learning to increase image recognition speed and data collection for consumer goods companies looking to optimize their in-store product displays. Northern Tool + Equipment is providing intelligent customer experiences that guarantee faster and more reliable delivery times by working with real-time data within Microsoft Supply Chain Center.

AustralianSuper is using Power Platform low-code capabilities and leveraging the expertise of Microsoft's Designated Support Engineers.

To democratize access to financial services in Mexico, Finvero is utilizing cloud and AI solutions to provide simpler, friendlier and more agile experiences for users. Leveraging support from the Microsoft for Startups program, Australia-based Thriday is freeing up time for small business owners by simplifying administrative tasks and improving financial management solutions using AI and machine learning. At its Hack Day event, AustralianSuper co-created a solution with Microsoft to reduce its manual effort for onboarding advisers by 90% in just two weeks. India's largest private-sector bank, HDFC Bank, is using the Microsoft Cloud to securely modernize its data landscape to scale information management and data analytics. In Israel, global solutions provider Sapiens has cut its time to market in half — reducing operational overhead by at least 40% — by automating its development life cycle and delivery processes with Azure. In Brazil, ClearSale is combining advanced AI and human intelligence to ensure e-commerce purchases aren't being made by scammers or hackers, and Cielo is making it easier to securely optimize access time for new suppliers — reducing the onboarding time needed from 60 days to two hours or less.

Ecolab is combining its digital platform capabilities with the Microsoft Cloud for Sustainability.

Advancing sustainability progress across transportation and energy industries with data and automation

To help organizations reduce water consumption and identify greenhouse gas emission savings, Ecolab has expanded its partnership with Microsoft, combining its digital platform capabilities with the Microsoft Cloud for Sustainability. Serving more than 1.4 million passengers in the U.K., public transportation provider FirstGroup is supporting its sustainability goals and making it easier to collaborate and solve problems by centralizing the visibility and management of its hybrid and multicloud environments with Azure Arc. In Germany, rail transportation leader Deutsche Bahn is building upon its commitment to a cleaner transportation infrastructure for the country by using Dynamics 365 to organize, analyze and visualize its data with the help of partner proMX. Greece-based company Public Power Corporation cut carbon emissions by over 99% while reducing its total cost of ownership by 30% by integrating its data sources on Azure. To enhance productivity for workers across the world, Norway-based Aker Solutions moved its global data center to the cloud and realized an estimated 40% reduction in CO2 emissions. To meet growing customer needs, German energy company EnBW automated its operations to make it possible to manage 10,000 renewable energy plants with only 20 employees.

ZS Associates is adopting Microsoft Azure and Microsoft 365 to streamline collaboration and enhance employee productivity.

Empowering professional services companies with cutting edge data and generative AI solutions

To achieve its mission of saving 1 million lives every year by 2030, Laerdal Medical is integrating is integrating Azure Speech Services into its training programs to create more immersive, real-life simulations between patients and providers. Amdocs is empowering service providers to improve end-user experiences across all channels and applications with its Customer Engagement Platform that combines best-in-class cloud, AI and telco technologies. ZS Associates is accelerating its new project launches by 75% through better access to collaboration and data tools using Azure and Microsoft 365. In Australia, KPMG is giving employees access to a proprietary version of ChatGPT within Teams so employees can safely explore and use the technology to drive innovation, boost efficiencies and improve the people experience. EY has developed a generative AI chatbot to modernize payroll employee care that can answer complex questions. Hungary-based digital solutions provider BPiON is deploying and maintaining applications almost 10 times faster — allowing the cloud-native company to expand its operational infrastructure to two new countries in less than a day.

I believe the only way to navigate uncertainty is through continued and accelerated innovation, and Microsoft itself is no exception. We have been delivering AI solutions for customers successfully over the last several years using cognitive services, machine learning and digital twins. Moving forward, we will infuse generative AI capability into our consumer and commercial offerings to deliver copilot capability for all services that our customers know and love across the Microsoft Cloud. Additionally, we will work with our partners, start-ups, digital natives and our customers to enable them to leverage these same capabilities in their own solutions. I am excited to see what our customers will create with generative AI in partnership with us. Together, we can apply the world's most advanced AI models to meet business imperatives responsibly, securely and with the confidence that can only be achieved with the Microsoft Cloud.

Tags: AI, AI GPT, Azure, Azure AI, Azure OpenAI Service, Microsoft 365, Microsoft AI, Microsoft and GPT, Microsoft Cloud, Microsoft Copilot, Microsoft Dynamics 365 Copilot, Microsoft Generative AI, Microsoft HoloLens, Microsoft Supply Chain Center, Microsoft Teams, Power BI

At its core, manufacturing is about innovation, as it builds economic capacity, bolsters workforces, and generates opportunity and growth. Today we're seeing the industry undergo a once-in-a-century transformation driven by changing consumer demands, labor challenges, supply chain disruptions, technological advancements and sustainability demands. We've come a long way since the industrial revolution, and the industry has evolved into a much more complex place. To explore these challenges and opportunities, we'll join global leaders and colleagues at the world's biggest trade show for industrial technology, Hannover Messe. As we enter this week of innovation, we're discussing four key areas that are reshaping the future of manufacturing: empowering a connected workforce, fueling intelligent engineering and factories with the industrial metaverse, building agile supply chains and accelerating sustainability. Powering these trends is artificial intelligence (AI), which offers more potential for our industry than anything we've ever experienced, and we are just scratching the surface of what's possible. Here we explore how Microsoft brings these four trends to life to advance the industry: Empowering the connected workforce To support intelligent factories, a connected workforce is key, and 63% of frontline manufacturing workers are excited about the job opportunities that technology creates. From the front office to the factory floor, Microsoft solutions can improve operations by streamlining daily work, driving efficiency, safety and productivity as well as helping foster an engaged workforce. We're building solutions to address the unique challenges manufacturers face — by helping streamline frontline operations, enhance communication and collaboration, improve the employee experience and strengthen security across shared devices. Today, we are sharing new capabilities and updates for the workforce, including:

The native integration of AI-empowered Power Virtual Agents (PVA) into Power Apps is now in public preview. AI-infused virtual agents will be readily available in Power Apps, allowing frontline manufacturing workers to easily find intelligent answers within the context of their manufacturing applications. Leverage Teams as your calling solution to connect with teammates, customers and suppliers. Later this year, factory floor employees will be able to make and receive calls through shared mobile Android phones from wherever their work takes them.

To learn more about how we're empowering today's manufacturing workforce, read the latest Microsoft 365 Blog. Click here to load media Building resilient and agile supply chains Supply chain disruption is not new; however, its complexity and the rate of change are outpacing organizations' ability to address issues. Manufacturers are under pressure to prevent and minimize disruptions, and as a result, almost 90 percent of supply chain professionals plan to invest in ways to make their supply chains more resilient. To support our customers, Microsoft offers solutions like Supply Chain Platform and Supply Chain Center, which enable organizations to reduce risk across complex supply chains and provide a ready-made "command center," allowing organizations, such as Kraft Heinz, to meet their unique needs without replacing current systems. Additionally, Microsoft Dynamics 365 Copilot gives businesses real-time predictive insights using generative AI across materials, inventory, distribution network and more, which means potential disruptions can be avoided before they happen. Today, Microsoft is working with customers to improve their supply chain resilience. Semiconductor company STMicroelectronics transformed their supply chain, advanced research and development, and scaled manufacturing leveraging Microsoft Azure HPC. They are now able to bring their products to market faster, and double their capacity by 2025, while becoming carbon neutral by 2027. Toyota Material Handling Europe is bringing warehouse automation to Dynamics 365 Supply Chain Management, with autonomous guided vehicles that can perform repetitive tasks quickly and accurately, leading to increased efficiency and productivity. This robotics technology can help manage inventory levels, resulting in better stock control and reduced carrying costs. Learn more about our supply chain solutions from the Dynamics365 team in this blog.

People and AI working together: The next generation of intelligent engineering and factories with the industrial metaverse One of the most significant changes in manufacturing has been the rise of intelligent factories. Imagine an industrial ecosystem connected to a common virtual space that allows everyone to visualize and interact with complex processes in real-time, with real-world data. The industrial metaverse, built with foundational technology like AI, Digital Twins, Mixed Reality, ML and Cloud-To-Edge is changing how companies run and manage their business by delivering immediate value through remote operations and skilling, AI process and asset optimization and robotics, automation, and autonomous operations. Similarly, the Industrial Metaverse is creating new opportunities to make product engineering more collaborative and immersive. This enables manufacturing companies to reduce and accelerate time-to-market in an increasingly competitive environment. Our commitment to immersive experiences that support manufacturers and frontline workers continues with today's announcement that we're bringing Windows 11 to HoloLens 2, enhancing security and offering the latest tools for developers. We're also launching new updates to Dynamics 365 Guides, including the ability to annotate anything within arm's reach, a custom security feature called "Restricted Mode" that gives organizations more control over their mixed reality apps and new updates to Teams calling within Guides to enhance the user experience. Siemens is using generative AI to help industrial companies drive innovation and efficiency across the design, engineering, manufacturing and operational lifecycle of products. It will bring a Teamcenter app to Microsoft Teams that leverages the language models in Azure OpenAI Service to enhance cross-functional collaboration. In addition, at Hannover Messe we are demonstrating how an Azure AI Service powered assistant can augment the creation, optimization and debugging of code in software for factory automation, and how we can use industrial AI to enable visual quality inspection on the shop floor. Rockwell is integrating robotics and AI technologies to empower companies to optimize their field service workers and asset management in factories. NVIDIA's Omniverse Cloud makes it easy for teams, from design and engineering to frontline workers, to digitize their workflows, from building digital twins of factories to running closed-loop simulations.

Sustainability, energy optimization and fuel cell innovation are more important than ever Sustainability is a top priority for manufacturers who are adopting eco-friendly practices and implementing initiatives, such as using renewable energy sources, adopting circular economy principles to reduce waste and optimizing production processes to minimize environmental impact. Through our technology like the Microsoft Cloud for Sustainability and our rich partner ecosystem, we are building a critical foundation for an energy efficient and sustainable future. This week, Seeq Corporation announced the Seeq Solution for Microsoft Sustainability Manager, which enables teams to integrate time series process data preparation, analysis and continuous improvement, reducing the environmental impact of process industry operations. Many of society's hardest sustainability problems are chemistry and materials science problems that will ultimately require a scaled quantum machine to solve. However, given the continuous increases in classical computing capabilities, more quantum chemistry problems can be solved today using state-of-the-art cloud services. Johnson Matthey, a global leader in sustainable technologies, has teamed-up with Microsoft's Azure Quantum chemists to develop new predictive modeling tools with the supercomputing capabilities of Azure HPC and refined workflows to accelerate chemical simulations, explore the potential of AI and get quantum ready. Many manufacturers are facing the urgent situation of rising energy costs on the factory floor and the production plant, which is why we're working with partners and customers to prioritize top use cases that will help manufacturers gain visibility into where energy loss is occurring and triage the issue through targeted assessments. These are some of the exciting innovations happening today to help us build a stronger, more resilient and more sustainable tomorrow. I look forward to seeing many of you at Hannover Messe next week. Tags: AI, Hannover Messe 2023, HoloLens 2, Microsoft Cloud for Sustainability, Microsoft Dynamics 365 Copilot

The odds are against today's defenders Today the odds remain stacked against cybersecurity professionals. Too often, they fight an asymmetric battle against prolific, relentless and sophisticated attackers. To protect their organizations, defenders must respond to threats that are often hidden among noise. Compounding this challenge is a global shortage of skilled security professionals, leading to an estimated 3.4 million openings in the field. The volume and velocity of attacks requires us to continually create new technologies that can tip the scales in favor of defenders. Security professionals are scarce, and we must empower them to disrupt attackers' traditional advantages and drive innovation for their organizations. In the last few months, the world has witnessed a wave of innovation as organizations apply advanced AI to new technologies and use cases. We are ready for a paradigm shift and taking a massive leap forward by combining Microsoft's leading security technologies with the latest advancements in AI. Today, at our inaugural Microsoft Secure event, I am delighted to welcome you to the new era of security — shaped by the power of OpenAI's GPT-4 generative AI — and thrilled to introduce to you Microsoft Security Copilot. Security Copilot — end-to-end defense at machine speed and scale Microsoft Security Copilot is the first security product to enable defenders to move at the speed and scale of AI. Security Copilot combines this advanced large language model (LLM) with a security-specific model from Microsoft. This security-specific model in turn incorporates a growing set of security-specific skills and is informed by Microsoft's unique global threat intelligence and more than 65 trillion daily signals. Security Copilot also delivers an enterprise-grade security and privacy-compliant experience as it runs on Azure's hyperscale infrastructure. When Security Copilot receives a prompt from a security professional, it uses the full power of the security-specific model to deploy skills and queries that maximize the value of the latest large language model capabilities. And this is unique to a security use-case. Our cyber-trained model adds a learning system to create and tune new skills. Security Copilot then can help catch what other approaches might miss and augment an analyst's work. In a typical incident, this boost translates into gains in the quality of detection, speed of response and ability to strengthen security posture. Security Copilot doesn't always get everything right. AI-generated content can contain mistakes. But Security Copilot is a closed-loop learning system, which means it's continually learning from users and giving them the opportunity to give explicit feedback with the feedback feature that is built directly into the tool. As we continue to learn from these interactions, we are adjusting its responses to create more coherent, relevant and useful answers. Security Copilot also integrates with the end-to-end Microsoft Security products, and over time it will expand to a growing ecosystem of third-party products. So, in short, Security Copilot is not only a large language model, but rather a system that learns, to enable organizations to truly defend at machine speed. We absolutely believe that security is a team sport, and security should be built with privacy at the core. We've built Security Copilot with security teams in mind— your data is always your data and stays within your control. It is not used to train the foundation AI models, and in fact, it is protected by the most comprehensive enterprise compliance and security controls. While remaining private, each user interaction can be easily shared with other team members to accelerate incident response, collaborate more effectively on complex problems and develop collective skills. Technology that elevates human strengths Human creativity and knowledge will always be imperative for defense. Security Copilot can augment security professionals with machine speed and scale, so human ingenuity is deployed where it matters most. In delivering this experience, we are guided by three principles:

Simplify the complex. In security, minutes count. With Security Copilot, defenders can respond to security incidents within minutes instead of hours or days. Security Copilot delivers critical step-by-step guidance and context through a natural language-based investigation experience that accelerates incident investigation and response. The ability to quickly summarize any process or event and tune reporting to suit a desired audience frees defenders to focus on the most pressing work. Catch what others miss. Attackers hide behind noise and weak signals. Defenders can now discover malicious behavior and threat signals that could otherwise go undetected. Security Copilot surfaces prioritized threats in real time and anticipates a threat actor's next move with continuous reasoning based on Microsoft's global threat intelligence. Security Copilot also comes with skills that represent the expertise of security analysts in areas such as threat hunting, incident response and vulnerability management. Address the talent gap. A security team's capacity will always be limited by the team's size and the natural limits of human attention. Security Copilot boosts your defenders' skills with its ability to answer security-related questions – from the basic to the complex. Security Copilot continually learns from user interactions, adapts to enterprise preferences, and advises defenders on the best course of action to achieve more secure outcomes. It also supports learning for new team members as it exposes them to new skills and approaches as they develop. This enables security teams to do more with less, and to operate with the capabilities of a larger, more mature organization.

Unrivaled security capabilities With Security Copilot, we are taking the agility advantage back to defenders by combining Microsoft leading security technologies with the latest advancements in AI. By working with Security Copilot, organizations get access to an unrivaled depth and breadth of security AI capabilities, including:

Ongoing access to the most advanced OpenAI models to support the most demanding security tasks and applications A security-specific model that benefits from continuous reinforcement, learning and user feedback to meet the unique needs of security professionals; Visibility and evergreen threat intelligence powered by your organization's security products and the 65 trillion threat signals Microsoft sees every day to ensure that security teams are operating with the latest knowledge of attackers, their tactics, techniques, and procedures; Integration with Microsoft's end-to-end security portfolio for a highly efficient experience that builds on the security signals; A growing list of unique skills and prompts that elevate the expertise of security teams and set the bar higher for what is possible even under limited resources.

YouTube Video Click here to load media Delivering security AI in a responsible way Without a doubt, AI will transform how organizations around the world interact with security technologies. To achieve their highest potential, security AI solutions must be delivered in a safe, secure and responsible way. With Security Copilot, we reinforce our commitment to impactful and responsible AI practices by innovating responsibly, empowering others, and fostering positive impact. The cornerstone of this work is our commitment to how Security Copilot handles your data:

Your data is your data. It's yours to own and control, and yours to choose how you want to leverage and monetize. Your data is not used to train or enrich foundation AI models used by others – no one beyond your organization is benefiting from AI trained on your data or business processes. Your data and AI models are protected at every step by the most comprehensive enterprise compliance and security controls in the industry.

The new era of security At Microsoft, we believe that security is ultimately about people. With Security Copilot, we are building a future where every defender is empowered with the technologies and expertise that enable them to reach their full potential. Technology will play an essential role on this journey, but successful security is, and will continue to be, a human endeavor. We're excited to be on this journey with you and we look forward to sharing more soon. Welcome to the new era of security. Editor's note, April 12, 2024: Microsoft Copilot for Security with new capabilities is now available worldwide. Watch the Microsoft Security Copilot demo > Microsoft Secure: Explore innovations transforming the future of security Stay up to date > Related links: Meeting the moment: advancing the future through responsible AI How AI impacts Future of Security Microsoft Security reaches another milestone — Comprehensive, customer-centric solutions drive results Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web Citations: Cyber Signals Volume 2

We launched the Microsoft Cloud Partner Program last October to meet the changing needs of customers and help our partners succeed. With its robust set of dedicated partner offerings, including new Solutions Partner designations that align to our six commercial solution areas across Azure, Modern Work, Security and Business Applications, we created a place where partners can continue to thrive.

Today, I want to detail some of the new investments we're making to continue to support more of our partners as they invest with Microsoft, grow their businesses and deliver customer success. These investments, like the Cloud Partner Program itself, are designed to not only help customers navigate new ways of working but also enable partners to differentiate themselves and deliver unique value to the market.

Empowering partners with AI Partners have long tapped into the power of machine learning and AI using Microsoft's developer tools and cloud technologies. As the AI landscape continues to evolve, we are committed to bringing the latest tools and resources — all guided by our AI principles, Responsible AI Standard and decades of research — to our partners so that they can continue to deliver innovative solutions for their clients.

Recent AI integrations across Microsoft's suite of tools present a real opportunity for partners to offer unique innovation, differentiated value and AI expertise to customers.

With Bing, better search, more complete answers, and the ability to generate content mean that users can better harness the world's knowledge. Microsoft Dynamics 365 Copilot provides interactive, AI-powered assistance across business functions, relieving the burden of mundane tasks. Microsoft 365 Copilot combines the power of large language models (LLMs) with your data in the Microsoft Graph and the Microsoft 365 apps to turn your words into the most powerful productivity tool on the planet. Today, ChatGPT in Azure Open AI Service enables partners to build predictive models, automate processes and improve decision-making for themselves and their customers. By applying for preview access to GPT-4 in Azure OpenAI Service, partners can start building with OpenAI's most advanced model yet. The AI and Machine Learning in Microsoft Azure Specialization empowers partners to showcase their expertise in this quickly evolving area of the Microsoft business.

We believe AI will fundamentally change every software category, unlocking a new wave of productivity growth. We are committed to empowering partners to harness the power of this innovation on behalf of customers around the world, while helping them navigate this new era of technology.

Introducing designations for ISV solutions To help customers identify which partner organizations are best suited to their needs, Microsoft created Solutions Partner designations within the Microsoft Cloud Partner Program. The first available Solutions Partner designations are anchored on the Microsoft Cloud in six solution areas aligned to how Microsoft goes to market. By attaining designations, partners demonstrate their organization's capabilities in delivering customer success in the solution areas.

Today, we are expanding Solutions Partner designations to include options that differentiate our partners that develop software: our independent software vendor (ISV) partners.

Designations for ISV solutions are aligned to how Microsoft goes to market and how customers are buying: by industry (e.g., finance, retail); by use cases across industries (e.g., security); and by specific imperatives of line-of-business leaders (e.g., marketing and sales). These designations distinguish an application's specific capabilities and help customers identify proven solutions for their business needs.

To learn more, please visit the Microsoft Cloud Partner Program FAQ and the ISV growth page on our partner website.

Announcing new Business Applications specializations Specializations continue to provide partners with ways to further differentiate their organization and demonstrate their expertise and experience in specific technical scenarios aligned to the solution areas for project-based services.

Last week, we introduced four Business Application specializations. These new offerings are aligned to the Business Applications solution area for Microsoft Dynamics 365 and make it easier for customers to find partners that possess the expertise they need. The new specializations include:

Finance: Designed for partners with demonstrated knowledge, substantive experience and documented success in Dynamics 365 Finance, and that innovate solutions across the financial spectrum. Sales: Available to partners with proven skills in customer experience (CX) transformation and supporting customers' growth. Service: Developed for partners with experience delivering personalized service, high-quality interactions and seamless, connected experiences at scale. Supply Chain: Open to partners with demonstrated knowledge, substantive experience and documented success in Dynamics 365 Supply Chain Management, and that innovate solutions across the supply chain spectrum.

With these additions, we now offer a total of 28 specializations, providing partners with flexibility and alignment on where they choose to invest for their business and serve their customer needs. Learn more about these specializations on the Dynamics 365 for Partners blog.

Introducing multiparty private offers The Microsoft commercial marketplace is the most partner-focused business platform in the industry and is dedicated to helping partners reach more customers, simplify sales and unlock growth.

To help partners unlock the power of the commercial marketplace, we will soon be offering multiparty private offers. Multiparty private offers empower partners to come together, create personalized offers with custom payouts and sell directly to Microsoft customers with simplified selling through the marketplace. And for customers with a cloud consumption commitment, every dollar of the sale will count toward their commitment when they purchase solutions that are Azure IP co-sell eligible. Multiparty private offers will enter private preview this spring and be made publicly available in the near future. In addition, we're continuing to expand the ways that partners can package and sell their solutions. In the coming months, we will give partners the ability to transact containers and VM software reservations through our commercial marketplace.

Coming soon: Designations for training and support services Set to launch later this year, Solutions Partner designations for training services and support services will enable customers to quickly identify partners that have met the requirements, including demonstrating their customer success.

The Solutions Partner designation for training services will recognize a partner's capabilities aligned to performance, quality of training and a minimum of one solution area. The Solutions Partner designation for support services will build on the solution area designations and highlight a partner's breadth of and excellence in support capabilities.

Badging for both designations will help partners differentiate their organizations with customers and in the Microsoft commercial marketplace.

Highlighting diverse-led and social impact businesses in the marketplace Inclusion fosters innovation, and many customers are looking to find partners that align to their business values. This is why we are enabling partners to submit relevant diversity and social impact business classifications in Partner Center, which will appear in their business profile in the marketplace. By building this information into our system, it's easy for customers to discover diverse-led businesses and their solution offerings in the commercial marketplace and through our co-sell channels.

On April 19, Microsoft will also hold a virtual Commercial Marketplace Impact Event for diverse and minority-owned partner businesses, as well as partners with social impact solutions (e.g., sustainability, accessibility, nonprofit) focused on educating those partners on opportunities to accelerate their growth through the Microsoft commercial marketplace.

More on the horizon Microsoft, partners are fundamental to our success. We're committed to investing in our ecosystem to enable partners to further differentiate their expertise, collaborate on deals through the commercial marketplace, and position themselves to meet an expanding range of customer and industry needs.

This commitment to our partners' success is unwavering. With our rich product and platform road map, Microsoft will continue to help our partners do even more to enable digital transformation and drive business growth and profitability.

As we look ahead, we are focused on ensuring partners have the skills, tools, and resources they need to compete — and win — in delivering innovative solutions that drive customer success.

Tags: AI, Microsoft 365 Copilot, Microsoft Cloud Partner Program, Microsoft Dynamics 365 Copilot, partners

Last month we introduced the new AI-powered Bing and Microsoft Edge, your copilot for the web – delivering better search, complete answers, a new chat experience and the ability to create content. Already, we have seen that chat is reinventing how people search with more than 100 million chats to date. We've seen people use chat in a variety of ways, from refining answers to complex questions to using it as a form of entertainment or for creative inspiration. Today we're taking the chat experience to the next level by making the new Bing more visual.

We're excited to announce we are bringing Bing Image Creator, new AI-powered visual Stories and updated Knowledge Cards to the new Bing and Edge preview. Powered by an advanced version of the DALL∙E model from our partners at OpenAI, Bing Image Creator allows you to create an image simply by using your own words to describe the picture you want to see. Now you can generate both written and visual content in one place, from within chat.

We know from research that the human brain processes visual information about 60,000 times faster than text, making visual tools a critical way people search, create and gain understanding. Based on Bing data, images are one of the most searched categories – second only to general web searches. Historically, search was limited to images that already existed on the web. Now, there are almost no limits to what you can search for and create.

For those in the Bing preview, Bing Image Creator will be fully integrated into the Bing chat experience, rolling out initially in Creative mode. By typing in a description of an image, providing additional context like location or activity, and choosing an art style, Image Creator will generate an image from your own imagination. It's like your creative copilot. Just type something like "draw an image" or "create an image" as a prompt in chat to get creating a visual for a newsletter to friends or as inspiration for redecorating your living room.

Bing Image Creator preview will also be available in Microsoft Edge, making it the first and only browser with an integrated AI-powered image generator. To use Bing Image Creator in Edge, simply click the Bing Image Creator icon in the sidebar to create your image or invoke from Bing chat in Edge.

At Microsoft, our teams are guided by our Responsible AI principles and the Responsible AI Standard to help them develop and deploy AI systems responsibly. To curb the potential misuse of Image creator, we are working together with our partner OpenAI, who developed DALL∙E, to deliver an experience that encourages responsible use of Image Creator. We have ensured OpenAI's safeguards, plus additional protections, have been incorporated into Image Creator. For example, we have put controls in place that aim to limit the generation of harmful or unsafe images. When our system detects that a potentially harmful image could be generated by a prompt, it blocks the prompt and warns the user. We also make it clear that Image Creator's images are generated by AI, and we include a modified Bing icon in the bottom left corner of each image to help indicate that the image was created using Image Creator. We continue to work closely with OpenAI to build, test and review mitigations for our integrations.

Since making the new Bing available in preview, we have been testing it with people to get real-world feedback to learn and improve the experience. People used it in some ways we expected and others we didn't. In this spirit of learning and continuing to build new capabilities responsibly, we're rolling out Bing Image Creator in a phased approach by flighting with a set of preview users before expanding more broadly. We will initially only include Image Creator in the Creative mode of Bing chat and our intention is to make it available in Balanced and Precise mode over time. We are also working on some ongoing optimizations for how Image Creator works in multi-turn chats. We continue to believe the best way to bring these technologies to market is to test them carefully, in the open, where everyone can provide feedback.

To support the growing demand for more visual search experiences, we are also making Stories and Knowledge Cards 2.0 available to all Bing users. Stories provide a more engaging way to search and interact with content, offering images and short videos. Also new to Bing users today, Knowledge Cards 2.0 is an AI-powered infographic-inspired experience that provides fun facts and key information at a glance. It's been updated to include interactive, dynamic content like charts, graphs, timelines, visual stories and more. With these updates and more coming, our goal is to deliver more immersive experiences in Bing and Edge that make finding answers and exploring the web more interesting, useful and fun.

Bing Image Creator integrated into Bing chat will begin to roll out to Bing preview users on both desktop and mobile starting today. For those not in the new Bing preview, the preview experience of Image Creator is now available at bing.com/create for Bing users around the world in English. We will add more language support over time.

Bing Image Creator is also available in Microsoft Edge from the Image Creator icon in sidebar for both desktop and mobile starting today for Edge users around the world in English. We will also soon integrate Image Creator into Edge from the new Bing button in chat mode in the preview version of Edge.

If you're not yet in the new Bing preview, you can sign up for the waitlist today. We're adding more people every day. Thanks for your continued feedback and we look forward to sharing more updates soon.

Since the beginning of modern medicine, the arc of innovation has delivered previously unimaginable breakthroughs and treatments that have improved health outcomes and lengthened lifespans. We are now in an era where digital transformation is redefining the way organizations approach patient engagement, care team collaboration and the provider experience. From better access to patient data through electronic health records (EHR) to improved access to healthcare through telemedicine, technology has given providers solutions that enable greater productivity and, most importantly, improve the quality of care available to patients.

But there is an opportunity to do more. The healthcare industry still faces challenges that technology can play a vital role in addressing. Next-generation AI has the potential to revolutionize healthcare by empowering clinicians to focus on personalized patient connections – strengthening the human interaction in medicine, reducing costs, and easing the administrative and cognitive burdens providers face. This is the reason that Microsoft and Nuance joined forces in 2022.

Today we are introducing a new solution, Dragon Ambient eXperience (DAX™) Express, which represents the next breakthrough for healthcare and a major milestone in our journey to automate clinical documentation at scale. DAX Express is an automated clinical documentation application integrated into the workflow that is the first to combine proven conversational and ambient AI with the advanced reasoning and natural language capabilities of OpenAI's GPT-4. Extending the proven Dragon Medical portfolio of solutions and building on the market-leading DAX ambient solution launched in 2020, DAX Express is the next milestone in Nuance's long-standing mission to reduce administrative burden and empower clinicians to spend more time taking care of patients and less time on paperwork.

AI solutions at work in healthcare today Physicians and nurses have been overwhelmed by the administrative demands that come with providing high-quality care. They must navigate complicated coding and billing requirements, manage the cognitive burden to accurately record and recall increasing amounts of patient data, and treat an aging and growing population. As a result, many organizations, including the U.S. Surgeon General and professional medical associations, are urging the development of comprehensive, secure solutions that can be seamlessly integrated into clinical workflows to reduce clinician burnout.

DAX Express tackles this head-on, with advanced automated clinical documentation seamlessly integrated in physicians' workflows. It's another proof point of how Microsoft and Nuance's solutions, backed by industry-defining AI and amplified by the power of the Microsoft Cloud for Healthcare, are reshaping care delivery with measurable and growing outcomes:

Relieving workforce burnout – Our solutions are proven to lead the industry in addressing this seemingly intractable problem – with physicians reporting up to 70% reduction in feelings of burnout and fatigue. Supporting specialty workflows – From surgeons to radiologists, our solutions analyze vast amounts of patient data, deliver workflow automation, facilitate reporting and communication, and provide AI insights that support more informed decision-making, planning and treatment—improving radiologist efficiency by 50% and reducing time-to-intervention by 74%. Improving adherence – By analyzing data to uncover findings, simplifying patient and physician communication, and providing comprehensive care plan tracking, our solutions deliver a 52% improvement in follow-up adherence. Increasing access to care – Our automated clinical documentation solutions give back time to clinicians who often choose to see more patients, adding five appointments per average clinic day – enabling clinicians to provide their best care to more people. Enhancing patient engagement – AI-powered chatbots are just one example of how AI is providing patients with quick and accessible information using built-in medical knowledge bases and triage protocols, which can trigger seamless handoff from bot interaction to a doctor, nurse or support agent. And, by delivering consistent and contextually relevant patient experiences, healthcare organizations are realizing 30% increases in patient self-service rates and 50% reductions in patient support costs.

Shared mission and complementary capabilities in healthcare Microsoft and Nuance came together with a shared vision to reshape healthcare: leveraging our unique capabilities to solve the industry's biggest challenges and equip the entire ecosystem to achieve more. At our core, we believe that technologies like AI, in partnership with clinicians, play a key role in accelerating progress in the industry, making physician and patient experiences more personal and engaging, and helping increase access to care. Our North Star is empowering clinicians to return their focus to patient care – using our proven solutions, combined with their judgment, to reduce cognitive burdens and support better outcomes.

Microsoft and Nuance have for years been ahead of the curve in innovating AI solutions, and organizations large and small have long trusted our responsible, secure applications and infrastructure. Microsoft's years of strategic investments in research, cloud and AI – including the acquisition of Nuance – have led us to where we are today. Through the Microsoft Cloud for Healthcare, Microsoft is providing responsible, integrated AI capabilities at scale that make it easier to improve the entire healthcare experience. Microsoft is also driving research, incubations and moonshots that drive real-world impact across healthcare and the life sciences. For example, Microsoft Research partners with leading organizations to advance and build infrastructure for emerging precision health modalities and empowers scientists with AI to speed up the discovery and development of breakthrough medicines. And we're empowering health organizations to tackle some of the toughest challenges in global health with our AI for Health philanthropic program.

Similarly, Nuance has decades of experience developing healthcare solutions used by hundreds of thousands of practitioners that are proven to consistently deliver value for physicians, nurses, radiologists and patients. Nuance has long been at the forefront of innovating conversational and ambient AI in healthcare – most notably with Dragon Medical One and most recently with DAX – and for years has leveraged expertise in large language models, natural language processing and clinical workflows to deliver refined, trusted AI solutions for clinical documentation around the globe. With this new innovation, the Nuance healthcare portfolio gives customers even more flexibility to automate and enhance their clinical documentation workflows with the accuracy and reliability of Dragon Medical One, the customized, full-service experience of DAX, and the immediacy and speed of DAX Express. Together, we are combining the power of Microsoft Azure, a deeply rich health data platform, and robust engineering and AI expertise to deliver outcomes-focused healthcare applications at scale to improve provider and patient experiences.

DAX Express represents the next step in delivering AI technology that provides an immediate, practical and highly accessible entry point for healthcare organizations to adopt at scale a new generation of AI-powered solutions, leveraging their existing investments in trusted Nuance solutions. For the more than 550,000 Dragon Medical users, DAX Express automatically and securely creates draft clinical notes in seconds, available immediately for clinical review and completion, after each patient visit in the exam room or via telehealth patient conversations. Clinicians will benefit from the seamless capabilities of Dragon Medical One, DAX Express, and DAX, which are tightly integrated into the electronic medical record, beginning from pre-visit through post-encounter, reducing cognitive burdens and helping increase the joy of medicine.

Responsible AI in healthcare DAX Express is developed in alignment with Microsoft's responsible AI standards, and consistent with our long-standing commitment to data security and privacy, helping to ensure AI systems and solutions that are trustworthy and safe. Microsoft and Nuance believe that while AI in healthcare has the potential to make a lasting, positive impact on the industry and the patients it serves, it is imperative we ensure the technology is used responsibly and transparently. As we have for years, we will continue to work closely with healthcare providers and the broader industry to help ensure AI is used in a way that is ethical and transparent.

Microsoft and Nuance are uniquely positioned to amplify the healthcare industry's ability to deliver meaningful outcomes with the power of AI. We are proud to be at the cutting edge of healthcare innovation, and we look forward to continuing to help the industry solve healthcare's biggest challenges.

Summary The new Bing, your AI-powered copilot for the web, is now in preview, and you might be surprised at how much it can help us be better at our craft of communications. These are some starter prompts to ask Bing, which have helped me and my team in our work over the past few weeks. If you like the answers you find with Bing, remember to follow-up with the source links for more information. The new Bing is in preview, and we're continuing to take feedback and apply those learnings to improve the experience over time.

We're in the thick of it, for sure. As communications professionals, we're figuring out what this new age of AI means for us, for our craft, while at the same time trying to explain AI to the world.

In speaking at an event this week in New York on this topic, one of the things I reflected on was the need to bring people along on this change curve. You might start off on the curve like "nope" – as soon as you start seeing stories about these AI advancements, your response is that this is not good for our profession. But then as you dig into it, you feel maybe it's not a "nope" and move up the curve to "there's hope." There are all sorts of things I could do! After all, one of our secret powers as communicators has always been language. And you have this moment and realize, you can accelerate this secret power with AI.

Here's how I've been doing just that with these prompts using the new AI-powered Bing. The results were crazy impressive. Try for yourself by signing up for the preview at bing.com/new and remember to give feedback directly to Bing so we can continue to improve the experience.

Useful information to get started Choose a conversation style: "More Creative," "More Balanced" or "More Precise" The new Bing won't hesitate to offer some follow-up questions to help hone in on the answer you're looking for. For example, when you're using the new Bing to do idea generation, it's helpful to follow-up by asking it to "give me a few more." Related to that, don't leave Bing hanging! It may have an ancillary topic related to the topic you're asking about. Bing will always show you where it's getting its information, so if you like some of the answers you're getting and want more details, don't forget to check out the sources that Bing is pulling from. Reminder! Avoid putting confidential information in the chat.

This first example comes from The Wall Street Journal reporter Joanna Stern. To prepare for an interview with our Chairman and CEO Satya Nadella, Joanna used the new Bing to help with preparing interview questions. Here's what she asked Bing:

"In the voice of Joanna Stern, generate a list of 10 interview questions for Satya Nadella about AI and the new Bing" "Answer question 5 in the voice of "

Since then, I've been using similar prompts to prepare for media interviews. For example, in preparing for this On With Kara Swisher podcast episode, here's a prompt that I used:

"Help me prepare for a podcast interview with Kara Swisher. The topic we've agreed to discuss is the rise of Artificial Intelligence tools and services. Based on her most recent writing and podcasts, give me 10 questions I should anticipate."

Remember, you have enough time to make sure Bing is prepared – so it can help to first ask if it's familiar with Kara Swisher and her interview style. Then add more details – who is she talking to? Then fire the prompt about the 10 questions.

It's been a crazy few weeks of news here at Microsoft and I've started using Bing to help me get quick snapshots of media coverage to see how news is landing in the moment. Here are a few examples I pulled recently:

"What's the latest news about today?" "What's the overall tone of these stories?" "How does this compare with news about in the same time period?"

This is not a substitute for deeper measurement or analysis, but it's a surprisingly good first snap when you need something now.

Bing chat is also a great brainstorm idea generator. We've experimented using it to get inspiration for our social media posts, especially with "More Creative" mode. Here are some examples that my team has experimented with:

"Generate a few tweets that include information and uses a tone" (such as "lighthearted tone") "Write a LinkedIn post for this blog, " "Generate a few tweet ideas for about " "Take the following information and turn it into a character tweet / LinkedIn post: ," then "Can you write it again but make it more engaging?"

These initial prompts are usually followed by "give me a few more versions."

There are days when I spend a lot of time writing (and rewriting) the body of a blog post, finally get it to a good place and only then start to wonder, what is my headline? By this point I've used up every ounce of creativity and begrudgingly write something somewhat coherent and feel slightly bad about myself. But these days, I feel less bad. While I still spend a lot of time writing (I really do love writing!) now with the new Bing, I've been using this prompt to help me generate headlines (a type of writing that I love less). Here's what I did:

"Suggest a few variations of headlines for the following," Copy your text and paste it to the Bing chat

FAQs is another type of writing I've been keen to accelerate with AI. Here's how I've done that:

I hope these prompt examples help you kickstart your own questions and interactions with this incredible new tool.

Humans are hard-wired to dream, to create, to innovate. Each of us seeks to do work that gives us purpose — to write a great novel, to make a discovery, to build strong communities, to care for the sick. The urge to connect to the core of our work lives in all of us. But today, we spend too much time consumed by the drudgery of work on tasks that zap our time, creativity and energy. To reconnect to the soul of our work, we don't just need a better way of doing the same things. We need a whole new way to work. Today, we are bringing the power of next-generation AI to work. Introducing Microsoft 365 Copilot — your copilot for work. It combines the power of large language models (LLMs) with your data in the Microsoft Graph and the Microsoft 365 apps to turn your words into the most powerful productivity tool on the planet. "Today marks the next major step in the evolution of how we interact with computing, which will fundamentally change the way we work and unlock a new wave of productivity growth," said Satya Nadella, Chairman and CEO, Microsoft. "With our new copilot for work, we're giving people more agency and making technology more accessible through the most universal interface — natural language." Copilot is integrated into Microsoft 365 in two ways. It works alongside you, embedded in the Microsoft 365 apps you use every day — Word, Excel, PowerPoint, Outlook, Teams and more — to unleash creativity, unlock productivity and uplevel skills. Today we're also announcing an entirely new experience: Business Chat. Business Chat works across the LLM, the Microsoft 365 apps, and your data — your calendar, emails, chats, documents, meetings and contacts — to do things you've never been able to do before. You can give it natural language prompts like "Tell my team how we updated the product strategy," and it will generate a status update based on the morning's meetings, emails and chat threads. With Copilot, you're always in control. You decide what to keep, modify or discard. Now, you can be more creative in Word, more analytical in Excel, more expressive in PowerPoint, more productive in Outlook and more collaborative in Teams. Click here to load media Microsoft 365 Copilot transforms work in three ways: Unleash creativity. With Copilot in Word, you can jump-start the creative process so you never start with a blank slate again. Copilot gives you a first draft to edit and iterate on — saving hours in writing, sourcing, and editing time. Sometimes Copilot will be right, other times usefully wrong — but it will always put you further ahead. You're always in control as the author, driving your unique ideas forward, prompting Copilot to shorten, rewrite or give feedback. Copilot in PowerPoint helps you create beautiful presentations with a simple prompt, adding relevant content from a document you made last week or last year. And with Copilot in Excel, you can analyze trends and create professional-looking data visualizations in seconds. Unlock productivity. We all want to focus on the 20% of our work that really matters, but 80% of our time is consumed with busywork that bogs us down. Copilot lightens the load. From summarizing long email threads to quickly drafting suggested replies, Copilot in Outlook helps you clear your inbox in minutes, not hours. And every meeting is a productive meeting with Copilot in Teams. It can summarize key discussion points — including who said what and where people are aligned and where they disagree — and suggest action items, all in real time during a meeting. And with Copilot in Power Platform, anyone can automate repetitive tasks, create chatbots and go from idea to working app in minutes. GitHub data shows that Copilot promises to unlock productivity for everyone. Among developers who use GitHub Copilot, 88% say they are more productive, 74% say that they can focus on more satisfying work, and 77% say it helps them spend less time searching for information or examples. But Copilot doesn't just supercharge individual productivity. It creates a new knowledge model for every organization — harnessing the massive reservoir of data and insights that lies largely inaccessible and untapped today. Business Chat works across all your business data and apps to surface the information and insights you need from a sea of data — so knowledge flows freely across the organization, saving you valuable time searching for answers. You will be able to access Business Chat from Microsoft 365.com, from Bing when you're signed in with your work account, or from Teams. Uplevel skills. Copilot makes you better at what you're good at and lets you quickly master what you've yet to learn. The average person uses only a handful of commands — such as "animate a slide" or "insert a table" — from the thousands available across Microsoft 365. Now, all that rich functionality is unlocked using just natural language. And this is only the beginning. Copilot will fundamentally change how people work with AI and how AI works with people. As with any new pattern of work, there's a learning curve — but those who embrace this new way of working will quickly gain an edge.

The Copilot System: Enterprise-ready AI Microsoft is uniquely positioned to deliver enterprise-ready AI with the Copilot System. Copilot is more than OpenAI's ChatGPT embedded into Microsoft 365. It's a sophisticated processing and orchestration engine working behind the scenes to combine the power of LLMs, including GPT-4, with the Microsoft 365 apps and your business data in the Microsoft Graph — now accessible to everyone through natural language. Grounded in your business data. AI-powered LLMs are trained on a large but limited corpus of data. The key to unlocking productivity in business lies in connecting LLMs to your business data — in a secure, compliant, privacy-preserving way. Microsoft 365 Copilot has real-time access to both your content and context in the Microsoft Graph. This means it generates answers anchored in your business content — your documents, emails, calendar, chats, meetings, contacts and other business data — and combines them with your working context — the meeting you're in now, the email exchanges you've had on a topic, the chat conversations you had last week — to deliver accurate, relevant, contextual responses. Built on Microsoft's comprehensive approach to security, compliance and privacy. Copilot is integrated into Microsoft 365 and automatically inherits all your company's valuable security, compliance, and privacy policies and processes. Two-factor authentication, compliance boundaries, privacy protections, and more make Copilot the AI solution you can trust. Architected to protect tenant, group and individual data. We know data leakage is a concern for customers. Copilot LLMs are not trained on your tenant data or your prompts. Within your tenant, our time-tested permissioning model ensures that data won't leak across user groups. And on an individual level, Copilot presents only data you can access using the same technology that we've been using for years to secure customer data. Integrated into the apps millions use every day. Microsoft 365 Copilot is integrated in the productivity apps millions of people use and rely on every day for work and life — Word, Excel, PowerPoint, Outlook, Teams and more. An intuitive and consistent user experience ensures it looks, feels and behaves the same way in Teams as it does in Outlook, with a shared design language for prompts, refinements and commands. Designed to learn new skills. Microsoft 365 Copilot's foundational skills are a game changer for productivity: It can already create, summarize, analyze, collaborate and automate using your specific business content and context. But it doesn't stop there. Copilot knows how to command apps (e.g., "animate this slide") and work across apps, translating a Word document into a PowerPoint presentation. And Copilot is designed to learn new skills. For example, with Viva Sales, Copilot can learn how to connect to CRM systems of record to pull customer data — like interaction and order histories — into communications. As Copilot learns about new domains and processes, it will be able to perform even more sophisticated tasks and queries. Committed to building responsibly At Microsoft, we are guided by our AI principles and Responsible AI Standard and decades of research on AI, grounding and privacy-preserving machine learning. A multidisciplinary team of researchers, engineers and policy experts reviews our AI systems for potential harms and mitigations — refining training data, filtering to limit harmful content, query- and result-blocking sensitive topics, and applying Microsoft technologies like InterpretML and Fairlearn to help detect and correct data bias. We make it clear how the system makes decisions by noting limitations, linking to sources, and prompting users to review, fact-check and adjust content based on subject-matter expertise. Moving boldly as we learn In the months ahead, we're bringing Copilot to all our productivity apps—Word, Excel, PowerPoint, Outlook, Teams, Viva, Power Platform, and more. We'll share more on pricing and licensing soon. Earlier this month we announced Dynamics 365 Copilot as the world's first AI Copilot in both CRM and ERP to bring the next-generation AI to every line of business. Everyone deserves to find purpose and meaning in their work — and Microsoft 365 Copilot can help. To serve the unmet needs of our customers, we must move quickly and responsibly, learning as we go. We're testing Copilot with a small group of customers to get feedback and improve our models as we scale, and we will expand to more soon. Learn more on the Microsoft 365 blog and visit WorkLab to get expert insights on how AI will create a brighter future of work for everyone. And for all the blogs, videos and assets related to today's announcements, please visit our microsite. Tags: AI, Microsoft 365, Microsoft 365 Copilot

Today, we're announcing the next generation of AI product updates across our business applications portfolio, including the launch of the new Microsoft Dynamics 365 Copilot – providing interactive, AI-powered assistance across business functions. According to our recent survey on business trends, nearly 9 out of 10 workers hope to use AI to reduce repetitive tasks in their jobs. With Dynamics 365 Copilot, organizations empower their workers with AI tools built for sales, service, marketing, operations and supply chain roles. These AI capabilities allow everyone to spend more time on the best parts of their jobs and less time on mundane tasks. Customer relationship management (CRM) and enterprise resource planning (ERP) systems have long been mission-critical customer and business data sources; however, they frequently require burdensome tasks like manual data entry, content generation and notetaking. Dynamics 365 Copilot takes advantage of recent advancements in generative AI to automate these tedious tasks and unlock the full creativity of the workforce. Dynamics 365 Copilot puts CRM and ERP to work for business users to accelerate their pace of innovation and improve business outcomes in every line of business:

Copilot in Microsoft Dynamics 365 Sales and Viva Sales helps sellers dramatically reduce the time they spend on clerical tasks. AI helps write email responses to customers and can even create an email summary of a Teams meeting in Outlook. The meeting summary pulls in details from the seller's CRM such as product and pricing information, as well as insights from the recorded Teams call. With sellers spending as much as 66% of their day checking and responding to emails, this presents a significant business upside to give the seller more time with their customers. Copilot in Dynamics 365 Customer Service empowers agents to deliver exceptional customer care. Dynamics 365 Copilot drafts contextual answers to queries in both chat and email, in addition to providing an interactive chat experience over knowledge bases and case history so this AI-powered expertise is always available to answer questions. We're also making it easier for customer service departments to build virtual agents in minutes with conversation boosters in Power Virtual Agents, which harnesses the power of Azure OpenAI Service and Bing to provide answers from company websites and internal knowledge bases that you choose. Copilot in Dynamics 365 Customer Insights and Dynamics 365 Marketing empowers marketers to simplify their workflow in data exploration, audience segmentation and content creation.

With Copilot in Dynamics 365 Customer Insights, marketers can curate highly personalized and targeted customer segments by having a dialogue with their customer data platform using natural language. Marketers can receive suggestions about additional segments that may not have been previously considered. This new capability can handle complex calculations and match customers that fit a select profile. Using Copilot in Dynamics 365 Marketing, marketers can describe their customer segment in their own words to create a target segment with the query assist feature. Marketers can also use Dynamics 365 Copilot to effortlessly get inspiration for fresh email campaign content based on a simple request. Copilot makes suggestions based on key topics entered by the marketer, the organization's existing marketing emails, as well as from a range of internet sources to increase the relevance of generated ideas.

Copilot in Dynamics 365 Business Central streamlines the creation of product listings for online commerce. Product attributes such as color, material and size can be used to create compelling product descriptions for online storefronts in seconds. The descriptions can be further tailored by choosing tone of voice, format and length. Business Central customers using Shopify can seamlessly publish the products with descriptions to their Shopify store in just a few clicks. Lastly, Copilot in Microsoft Supply Chain Center, which Microsoft Dynamics 365 Supply Chain Management customers can access, will proactively flag external issues such as weather, financials and geography that may impact key supply chain processes. Predictive insights then surface impacted orders across materials, inventory, carrier, distribution network and more. Supply chain planners can then automatically draft an email generated by Dynamics 365 Copilot to alert impacted partners and mitigate potential disruptions before they happen.

The next era of business applications is being transformed by generative AI. Users will increasingly expect their CRM and ERP applications to include AI-powered expertise. Dynamics 365 Copilot brings the latest AI breakthroughs to every line of business, improving customer experience, employee experience and operational efficiency. Essential to our approach as we bring these latest advancements to customers is our commitment to responsible AI by design – our framework for the safe deployment of AI technologies. Today's announcement builds on recent AI momentum across Microsoft 365, Dynamics 365, and the Power Platform. This includes: the next generation of AI capabilities in Microsoft Teams, the collaboration platform for work with more than 280 million monthly active users; Viva Sales, which helps sellers by bringing a sales copilot to their flow of work in Microsoft 365; and Power Apps, enabling citizen developers to write code using natural language. You can read more about today's AI announcements from Emily He, CVP, Business Applications and Platform here. Join us on March 16 as we share the next step in our journey to reinvent productivity with AI. You can RSVP for the event on the Microsoft LinkedIn page.

YouTube Video Click here to load media Tags: AI, CRM, Microsoft Dynamics 365 Copilot, Microsoft Viva

Chief financial officer (CFO) and finance teams are typically known for their fiscal duties – accounting, expenses and financial planning – that help ensure an organization is compliant and runs smoothly for long-term financial health. Over the past few years, the role of finance teams has drastically evolved to include driving innovation. During the pandemic, CFO guidance was critical to keep organizations running – by closely monitoring revenue and expenses and at times, making tough business decisions. Now, these teams are being asked to expand their role to become futurists and play a leading role in transformation. To learn more about this evolution, Microsoft conducted a survey with U.S. finance leaders to get their perspective on the future of finance.

In Microsoft's new Economic Guardians of the Future report, we examined how financial teams and leaders are working today, challenges they're facing, and the role of technology in their day-to-day lives. What we found is that CFOs and their teams are facing a new, urgent directive: striking the right balance between strategic innovation while protecting the long-term financial health of their organizations – all during an uncertain economic climate.

Striking the right balance: Economic guardian and innovation driver In our research, we uncovered that most finance leaders feel they are being asked to deliver on conflicting objectives – to accelerate the pace of innovation while also ensuring brand trust and mitigating risks. They must both reduce costs and increase growth to deliver short-term profit and long-term vision and corporate purpose. Despite their reputation as risk-averse adversaries to innovation, 79% of finance leaders believe that to meet future needs they must play a significant role in owning business innovation and empowering transformation across the organization. Now more than ever CFOs and their teams are focused on transformation and the future.

We're already seeing this evolution in customers we partner closely with. London Stock Exchange Group's (LSEG) CFO Anna Manz's role goes beyond performance management and driving an efficient function into delivering the big strategic decisions. Most recently this included LSEG's collaboration with Microsoft to empower financial markets with next generation data, analytics and workspace solutions. It is challenging LSEG to think about how they can work with customers and develop products differently, and it's crucial that a cross-functional team including finance understand the scale of the opportunity and the capability to execute. Combining LSEG's unparalleled depth and breadth of financial data and analytics and deep knowledge of financial services, paired with Microsoft's technology and embedded Microsoft office workflow, creates a tremendous opportunity to bring critical transformation in the financial services industry.

Drive employee satisfaction through AI and automation While finance teams are being asked to take on more than ever, they are not seeing incremental increases in people resourcing. Striking the balance between automation and an organization's most important asset — its people — is key for the future of finance.​ Doing more with less isn't about overburdening an already taxed workforce, it's about augmenting human capabilities with automation, enabling employees to move business forward.​ Eighty-two percent of finance leaders found AI and automation very important to helping evolve their team's role to support their organization's strategic goals. Implementing AI and automation can free up valuable time for finance teams to focus on work that drives business strategy and modernization.

As finance professionals seek more ways to work smarter, not harder, low code automation tools remove IT barriers. Empowered with automation and AI, people are more connected, collaborative and ready to tackle more strategic work.

Agility and innovation are crucial when facing the unexpected Business agility is essential, and finance teams are doubling down their efforts to keep up with shifting markets. Organizational health has become increasingly dependent on connected data, efficient processes and action-oriented insights that facilitate agile, data-driven decision making.

Microsoft's Economic Guardians of the Future report found that 88% of financial leaders feel overwhelmed by their organization's data. To address this need, Microsoft is announcing the first of many new capabilities in Dynamics 365 Finance: Business performance analytics which empowers finance professionals by equipping them with faster reporting and analytics. This new capability streamlines financial reporting by centralizing data from multiple business workstreams into an easy-to-use interface. Powered by Dataverse, this capability enables users to create and share personalized reports, and get secure, near real-time access to critical financial information. With improved visibility, finance teams can make informed decisions and provide timely financial insights to business leaders.

But this product advancement couldn't be done without our customers' partnership. Robert Walters, a global recruitment and talent management company, is a prime example of how innovative technology and agility helps CFOs drive strong cross-functional partnerships and digital transformation. Robert Walters partnered with Microsoft to replace their entire finance platform, all during the pandemic. The results? An on-time, in-budget success, which led to streamlining the delivery of information to teams, enabled real-time data visibility, and in one specific instance an annual cost savings of 150,000 pounds. By utilizing innovative technologies and fostering a strategic partnership between the CFO and chief information officer, the company effectively unified data and information, driving faster, data-driven decision-making. With this hyper-connect business approach their recruiters have the information they need to drive better outcomes for recruits and clients.

Empower your workforce by prioritizing job fulfillment, collaboration and faster innovation It comes as no surprise that employees want to spend less time on repetitive and transactional tasks, and more time working as a team to drive better business decisions.

At Microsoft, we've been on this journey for years, transforming our finance operations using our cloud-based products including Azure, Excel, Power Platform, Dynamics 365 and Teams — something we call Modern Finance. But this goes beyond technology. We are reimagining what is possible and that required a cultural shift to reevaluate our processes, alleviate pain points, foster a culture of learning and innovation, and empower our people to do their best work. One highlight is that we have many citizen developers on our finance team who are leveraging low-code technology to transform business processes. These initiatives have saved us over 977 hours of manual labor annually, with over $25 million in savings and a huge boost in productivity.

When we combine people, processes, and tools — this is where the real magic happens. Don't underestimate the importance of culture, how to build a team and a culture that allows learning, sets the pace of industry-wide transformations and encourages innovation.

Modern Finance is here to stay We are living through a period of historic economic, societal, and technological change, but for all the uncertainty we continue to see in the world one thing is clear: Business leaders are turning to finance and innovative technology to help them do more with less, to navigate the unexpected and emerge much stronger. Financial transformation is a journey, not a destination.

Tags: AI, Azure, Dynamics 365, Excel, Microsoft Teams, Power Platform

Two weeks ago, we introduced the world to the all new AI-powered Bing and Microsoft Edge — your copilot for the web. Since then, based on strong and positive product feedback and engagement, we've welcomed more than one million people in 169 countries off the waitlist into the preview. We continue to expand the preview to more people every day. Our preview community is actively using the breadth of new features across Search, Answers, Chat and Creation with total engagement up significantly. Feedback on the new capabilities is positive, with 71% of testers giving the new Bing a "thumbs up" on the new search and answers capabilities. We're even more excited about the breadth of feedback we are receiving on where and how we can improve and we are acting on it with regular updates.

We're seeing some interesting use cases and queries in preview testing. I recently learned about a father showing his son the new Bing, and together they discovered and created in a way not easily done with today's search engines. They started off by creating sci-fi stories using simple prompts in chat, eventually leading to the development of a video game idea where Bing not only helped create a plot, but also generated the code to input directly into Scratch – a visual programming tool. In just a few queries, they captured the wonder and potential of the new Bing and Edge. We are hearing many similar stories on how the new Bing is helping people discover and create in ways previously not possible.

In this spirit of learning and continuing to build new capabilities, we're excited to share today the preview release of the new Bing and Edge mobile apps. We're beginning to roll out the incredible capabilities of the new Bing and Edge on your smartphone along with some exciting new features, such as voice input. In addition, we are creating a new chat experience, beginning with Skype, to enhance your social communications with your friends and family.

The new Bing and Edge goes mobile; Now introducing voice access

Because we know 64% of searches occur on mobile phones, we are releasing all new Bing and Edge mobile apps to serve as your copilot for the web even when you are away from your desktop.

Imagine an unexpected layover in a new city. As you plan a quick afternoon stop in Tokyo, you ask Bing to help find a place to store your luggage. It then provides tips for navigating the metro system on your way to the famed Shinjuku station. With a few hours to explore, Bing creates a short itinerary, helping you get the most out of your quick visit, and even translates along the way.

Available on iOS and Android today, the Bing mobile app offers a fresh look and experience. Tapping the Bing icon at the bottom will invoke a chat session, where you can engage in all the same ways you can from the desktop. Ask simple or complex questions and receive answers and citations. Choose how you want your answers displayed – bullet points, text or simplified responses. Explore the Bing chat experience to refine your query or compose an email, poem or list.

With the introduction of the new Bing mobile app, we're adding one of the preview community's most requested features – voice. Available on mobile and on desktop, voice search provides more versatility in how you can deliver prompts and receive answers from Bing.

In addition, those who have access to the preview will be able to utilize the new Bing experience from the homepage of the Microsoft Edge mobile app.

To better assist you when you are collaborating with friends and family, we are introducing AI-powered Bing for Skype. More than 36 million people use Skype daily to connect through phone calls and chats across borders and around the world and the new Bing is going to enable some helpful and fun new scenarios and capabilities.

Imagine having a copilot for your friends and family as you stay connected and plan your next get together. Simply add Bing to the group, as you would any Skype contact, and now you can ask Bing to answer questions and provide information for the entire group. For example, if your family is chatting about the next family reunion, you can simply ask Bing for suggestions on travel destinations, expected weather forecasts and interesting events around your time of travel, and everyone in the chat will get access to the results. When you are catching up with friends, you can ask Bing to simply fetch information from the web, for example, the latest news or last night's award shows to add to your conversation.

You can choose how you want your answers to be displayed – bullet points, text or a simplified response. Bing can accommodate your preferences. Fluent in more than 100 languages, and capable of translating between them, Bing can offer unique value to this global communications tool.

Available worldwide in preview today, the new Bing in Skype can provide helpful, real-time answers to all your questions. As we learn and fine-tune this amazing new capability, we envision bringing it to other communications apps, like Teams, in the future.

If you're currently on the Bing preview experience, these features are available to you later today. In the first few days of testing these mobile experiences, you may occasionally find connectivity issues in low-bandwidth situations. We're aware of the issue and are working on a fix.

If you're among those awaiting access, we appreciate both your patience and your excitement. We're working as fast as possible to onboard more people every day. If you're interested in trying it for yourself, please sign up for the Bing preview today. We hope you enjoy the new capabilities and please keep the feedback coming so we can continue to improve the product for you.

To empower people to unlock the joy of discovery, feel the wonder of creation and better harness the world's knowledge, today we're improving how the world benefits from the web by reinventing the tools billions of people use every day, the search engine and the browser.

Today, we're launching an all new, AI-powered Bing search engine and Edge browser, available in preview now at Bing.com, to deliver better search, more complete answers, a new chat experience and the ability to generate content. We think of these tools as an AI copilot for the web.

"AI will fundamentally change every software category, starting with the largest category of all – search," said Satya Nadella, Chairman and CEO, Microsoft. "Today, we're launching Bing and Edge powered by AI copilot and chat, to help people get more from search and the web."

There are 10 billion search queries a day, but we estimate half of them go unanswered. That's because people are using search to do things it wasn't originally designed to do. It's great for finding a website, but for more complex questions or tasks too often it falls short.

We have brought together search, browsing and chat into one unified experience you can invoke from anywhere on the web, delivering:

Better search. The new Bing gives you an improved version of the familiar search experience, providing more relevant results for simple things like sports scores, stock prices and weather, along with a new sidebar that shows more comprehensive answers if you want them. Complete answers. Bing reviews results from across the web to find and summarize the answer you're looking for. For example, you can get detailed instructions for how to substitute eggs for another ingredient in a cake you are baking right in that moment, without scrolling through multiple results. A new chat experience. For more complex searches – such as for planning a detailed trip itinerary or researching what TV to buy – the new Bing offers new, interactive chat. The chat experience empowers you to refine your search until you get the complete answer you are looking for by asking for more details, clarity and ideas – with links available so you can immediately act on your decisions. A creative spark. There are times when you need more than an answer – you need inspiration. The new Bing can generate the content to help you. It can help you write an email, create a 5-day itinerary for a dream vacation to Hawaii, with links to book your travel and accommodations, prep for a job interview or create a quiz for trivia night. The new Bing also cites all its sources, so you're able to see links to the web content it references. New Microsoft Edge experience. We've updated the Edge browser with new AI capabilities and a new look, and we've added two new functionalities: Chat and compose. With the Edge Sidebar, you can ask for a summary of a lengthy financial report to get the key takeaways – and then use the chat function to ask for a comparison to a competing company's financials and automatically put it in a table. You can also ask Edge to help you compose content, such as a LinkedIn post, by giving it a few prompts to get you started. After that, you can ask it to help you update the tone, format and length of the post. Edge can understand the web page you're on and adapts accordingly.

With the new Bing, exploring the web isn't just easier, it's also more fun. Ask it things like:

My anniversary is coming up in September, help me plan a trip somewhere fun in Europe, leaving from London.

Will the Ikea Klippan loveseat fit into my 2019 Honda Odyssey?

I like electronic music and want to go to my first festival this year. Do you have any recommendations or tips for me?

The new Bing experience is a culmination of four technical breakthroughs:

Next-generation OpenAI model. We're excited to announce the new Bing is running on a new, next-generation OpenAI large language model that is more powerful than ChatGPT and customized specifically for search. It takes key learnings and advancements from ChatGPT and GPT-3.5 – and it is even faster, more accurate and more capable. Microsoft Prometheus model. We have developed a proprietary way of working with the OpenAI model that allows us to best leverage its power. We call this collection of capabilities and techniques the Prometheus model. This combination gives you more relevant, timely and targeted results, with improved safety. Applying AI to core search algorithm. We've also applied the AI model to our core Bing search ranking engine, which led to the largest jump in relevance in two decades. With this AI model, even basic search queries are more accurate and more relevant. New user experience. We're reimagining how you interact with search, browser and chat by pulling them into a unified experience. This will unlock a completely new way to interact with the web.

These groundbreaking new search experiences are possible because Microsoft has committed to building Azure into an AI supercomputer for the world, and OpenAI has used this infrastructure to train the breakthrough models that are now being optimized for Bing.

Together with OpenAI, we've also been intentional in implementing safeguards to defend against harmful content. Our teams are working to address issues such as misinformation and disinformation, content blocking, data safety and preventing the promotion of harmful or discriminatory content in line with our AI principles.

The work we are doing with OpenAI builds on our company's yearslong effort to ensure that our AI systems are responsible by design. We will continue to apply the full strength of our responsible AI ecosystem – including researchers, engineers and policy experts – to develop new approaches to mitigate risk.

The new Bing is available today in a limited preview on desktop, and everyone can visit Bing.com today to try sample queries and sign up for the waitlist. We're going to scale the preview to millions in the coming weeks. A mobile experience will also be in preview soon.

We're excited to put the new Bing and Edge into the real world to get the critical feedback required to improve our models as we scale.

Amy Hood, Microsoft executive vice president and chief financial officer, will host a conference call with investors at 2:30 p.m. PT.

Brad Smith, Microsoft vice chair and president: Meeting the moment: advancing the future through responsible AI

Last year, I had the opportunity to speak with customers across a wide range of sectors about how they are approaching the current economic climate. Whether they are focusing on increasing efficiency or driving cost savings, we are all essentially trying to do more with less and prioritize investments that will drive the most important and meaningful outcomes. Unfortunately, these resource constraints are coming at a time when data sources and data volumes continue to rise at staggering rates. According to a study published by the IDC, the amount of new data created, captured, replicated and consumed is expected to double in size by 2026. [1] This surge in data means IT departments are trying to adapt from terabyte-scale data management to now petabyte-scale — and doing so with even fewer resources. Beyond this immense growth in data, organizations are also constantly adding new cloud solutions, applications and experiences to support their hybrid workforce and changing business needs. As the number of solutions grows, employees need connected experiences that enable them to bring their work from one tool to the next seamlessly. Unfortunately, the outcome is often the opposite as new solutions fail to integrate with the existing technology ecosystem, leading to siloed experiences and greater inefficiency. Without security and governance solutions built to work across multi-cloud and app environments, this rise in apps and data also expands attack surfaces and opens gaps for adversaries to exploit and infiltrate. These attacks result in devastating business disruptions and financial damages with the average breach costing $4.35 million.[2] Developing a cybersecurity strategy that can account for all potential attack surfaces is more complex than ever with 86% of surveyed decision-makers believing their current cybersecurity strategy does not keep up with their multi-cloud environments. [3] All of these potential issues, from data replication to siloed experiences to growing attack surfaces, stem from disconnected data and applications across multi-cloud and multi-system environments. That's why, across our Microsoft Cloud offerings, we've worked with our thriving Independent Software Vendor (ISV) ecosystem to create a library of over 1,400 connectors that allow you to seamlessly connect your third-party services to Microsoft Cloud solutions like Microsoft Power Platform, Microsoft Power Query, Microsoft 365, Microsoft Graph and Microsoft Sentinel. The Microsoft Cloud is now not only the most trusted and comprehensive cloud, but it's also one of the most connected and open clouds on the market with a connector library that continues to grow every day. These pre-existing integrations can help you:

Connect to and unlock value from all your data across your on-premises, multi-cloud, and third-party sources. Combine Microsoft and third-party apps to create deeply integrated app experiences that support your unique business scenarios. Safeguard your entire organization and eliminate gaps by ensuring your security, governance, and compliance solutions work across platforms and cloud environments.

No matter where your data currently resides or how you choose to manage it going forward, you can gain peace of mind knowing your Microsoft Cloud investments can connect to all your data and turn it into impact. That was one of the reasons Walmart chose to standardize on Microsoft Power BI for their reporting and analytics needs. "One of the key criteria in selecting our solution was knowing that Power BI works so well with a vast number of partners — not necessarily just Microsoft products," says Micah Dail, Director, Finance Data & Analytics at Walmart. "At Walmart, we use a myriad of different vendors in our technology stack, so having a tool like Power BI that can bring everything together was critical." Elisabeth-TweeSteden Hospital (ETZ), a leading clinical and education hospital and trauma center in the Netherlands, used pre-built Teams connectors to create powerful, integrated app experiences that help their clinicians get more done. With the Microsoft Teams Electronic Health Records (EHR) connector for Epic, ETZ clinicians can create and even join Microsoft Teams-supported appointments directly from the Epic EHR system, use interpreter services and other accessibility features, gather metadata about the appointment, view consumption reports, and much more. "We used Teams to help build a fully integrated solution so that clinicians can document virtual patient consultations without having to start up other systems," says Ceasar van Eeden, Application Analyst at Elisabeth-TweeSteden Hospital. By using connectors to combine Microsoft and third-party tools, ETZ created an intelligent solution that best suited their unique business needs. For Pettie Maas-van Son, a reproductive fertility physician at Elisabeth-TweeSteden Hospital, Teams is a valuable tool to deliver crucial, timely care for parents who are expecting. Photo courtesy of Elisabeth-TweeSteden Hospital. Coca-Cola UNITED, one of the largest and fastest growing bottlers and distributors of Coca-Cola products, saw multiple opportunities to automate and streamline processes using a low-code platform. To get the most out of this investment, they required a solution that could connect to and integrate with their existing SAP system which is hosted on Microsoft Azure. Beyond the simplicity of the platform, the extensive library of pre-built integrations with SAP and Azure services made the Microsoft Power Platform the ideal choice. "Once I learned about the data connectors and how they would work with our SQL database and our enterprise resource planning (ERP) system, everything else just fell into place," says Jared Simmons, Systems Analyst at Coca-Cola UNITED, "We also value the extensibility of the platform, enabling us to build and integrate custom connectors using Azure Logic Apps." These custom connectors allowed Coca-Cola UNITED to create unique solutions integrated directly in their SAP system. This huge library of connectors also helps ensure Microsoft's industry-leading security, governance, and compliance solutions can easily work across platforms and cloud environments, leaving no gaps in protection. Take the global exam provider, Pearson VUE, for example. Pearson VUE needs to keep exam content completely confidential to maintain trust with its customers and protect test-takers. As the cybersecurity landscape gets increasingly challenging, they have adopted a host of Microsoft security solutions to help them stay protected. "We use Microsoft Sentinel to see everything that's going on in our estate — whether from Microsoft or non-Microsoft security solutions — and be as proactive as possible," says Vladan Pulec, Enterprise Architect at Pearson VUE. "The easy interoperability among Microsoft Sentinel and so many other solutions make it easier for us to standardize procedures and achieve greater cybersecurity."

YouTube Video Click here to load media If you are a Microsoft Cloud customer and want to learn more about the connectors currently available to you, check out the following documentation:

Power Platform connectors Power Query connectors Microsoft Sentinel connectors Microsoft Graph connectors Azure Event Grid connectors Integrate your service with Microsoft products and services (for ISVs)

[1] IDC, Worldwide IDC Global DataSphere Forecast, 2022–2026: Enterprise Organizations Driving Most of the Data Growth (Doc #US49018922), May 2022. [2] Ponemon Institute, Cost of a Breach Report, July 2022, sponsored by IBM Security. [3] Microsoft Cloud Security Priorities and Practices Research, March 2022, prepared by Emerald Research Group which surveyed 250 senior security decision-makers. Tags: Apps, connectors, Microsoft Power Platform

Photo credit: Yuichiro Chino/Getty Images In a rapidly shifting economy, organizations are relying on cloud technologies and intelligent platforms to drive cost efficiencies, while continuing to innovate across their businesses. To provide resiliency and scalability of operations, leaders are collaborating with their customers and partners to design new solutions for the emerging trends and challenges of today's world. This co-innovation approach results in shared success and new opportunities, from accelerating the pace of digital transformation and reducing the cost of innovation to elevating employee and customer experiences. Microsoft has long adopted a data-driven, co-innovation approach to help organizations anticipate industry needs and lead through uncertainty. To build upon our industry strategy and help transform the future of financial markets, we are excited to partner with the London Stock Exchange Group to accelerate its own digital transformation and bring intuitive next-generation productivity, data and analytics and modelling solutions to its global customers. Alstom is digitizing railway solutions to make mobility more sustainable, collaborating with Microsoft to further develop cloud solutions in a highly regulated market. To prepare students for the workforce and teach them to create enterprise-grade applications with low-code, the Project Management Institute and Microsoft are introducing the Power Platform University Hub. Lockheed Martin is working with Microsoft to power the next generation of defense technologies for the U.S. Department of Defense, advancing AI, machine learning and simulation capabilities. As part of our continued commitment to democratizing AI as a technology platform, Azure OpenAI Service is now generally available and being used by companies like Al Jazeera, KPMG and Moveworks to improve experiences for end-users and streamline operational efficiencies. And just this morning, we announced the next phase of our partnership with OpenAI to ensure the benefits of AI are shared responsibly and broadly with the world. Continued and accelerated innovation is the only path forward. From ideas to action: Unleashing the potential of co-innovation Sweden-based Lynk & Co and Microsoft are teaming up to boost the in-car experience by building a custom app that lets users join Microsoft Teams meetings on the go. Australian startup Forcite Helmets has created a smart helmet to give motorcyclists a safer, more dynamic riding experience by hosting its trip-routing and hazard-warning system on Azure and expanding internationally with support from the Microsoft for Startups program. The National Australia Bank is improving the employee experience with tech support by working closely with Microsoft engineers and architects to develop a secure chatbot with Azure Cognitive Services, saving $1.2 million within 7 months of deployment. Through a collaboration with Microsoft, New Mexico's Special Supplemental Nutrition Program for Women, Infants, and Children is simplifying access to services for the communities it serves with Dynamics 365 and Power BI, reaching more than 30,000 people since going live. In pursuit of a new data strategy and data platform, Amadeus is creating a data mesh together with Microsoft cloud architects and cloud-scale analytics experts. Working toward a shared vision of driving Africa's growth, MTN Group is developing joint go-to-market plans with Microsoft to accelerate digitization, digital education, sustainability and financial inclusion. The Microsoft Cloud is empowering organizations of all sizes to innovate for impact. The power of data-driven operations: Finding new ways to do more with less Grupo Bimbo is already seeing significant improvements in productivity and operational efficiency with Power BI and is using Azure Synapse Analytics to handle large volumes of data with ease. To deliver treatments faster, Bayer developed a data engineering and data science platform, using AI and machine learning to evaluate data from clinical trials more efficiently. Hong Kong-based AIA has achieved a cost savings benefit of more than 20% from migrating to Azure and is now adding AI, virtualization and other advanced capabilities. After making efforts to streamline financial management and month end processes, the New York City Department of Environmental Protection has reduced the number of consultants needed to support its in-house IT team by 50%. The University of Toronto is seeing its payroll run almost two times faster and financial month end run about three times faster since moving all of its SAP workloads to Azure. South Africa-based Exxaro Resources is driving productivity gains and new efficiencies with Azure IoT, tracking everything from truck activity on field sites to driver fatigue. Animal Supply Company is automating routine invoicing tasks, delivering more than $500,000 in annual cost savings and freeing up 50% of its invoicing experts' time with the help of Azure AI. U.K.-based Waverton Investment Management has upgraded its digital infrastructure with Microsoft, already reducing its carbon footprint by 60% and planning to be fully on Azure by the end of 2023. The Arnott's Group is deploying Microsoft Sustainability Manager to achieve its sustainability targets ahead of schedule, improving visibility into emissions activities and the ability to report progress in near real time. Organizations are realizing gains with Microsoft's data and AI capabilities. Industry modernization: Enhancing supply chains and customer experiences Canada-based GoodLeaf Farms is automating its farm operations and producing more healthful and abundant crops by streamlining its data with Azure and Power Platform. Belgium-based company HB Antwerp is developing a blockchain ledger on Azure so that conscious-minded buyers can know where their diamonds come from. U.S. food production company Conagra Brands is digitizing its supply chain on Azure, increasing operational output of goods by 30% and production capacity by 20% at one of its facilities. Singapore-based Integrated Health Information Systems is fostering stronger interactions by empowering patients and care teams with its Health Discovery Plus platform that securely runs on Azure. Global tire manufacturer Bridgestone is improving customer services by centralizing demand forecasting data on its analytics platform that is also crucial to environmental, social and corporate governance (ESG) reporting.

YouTube Video Click here to load media The future of collaboration: Empowering employees in their remote work experiences Organizations like the GC GROUP, U.S. Bank, Piraeus Bank and Sasuchan are integrating Viva Connections with Microsoft Teams to make it easier for employees to learn and connect. Brazil-based Globo is improving collaboration and communication with Microsoft Teams and training employees to use Power Platform tools. Nissin Foods Group is adopting Microsoft 365 and Surface devices to make working from anywhere comfortable, highly secure and easy. In Indonesia, more than 9,600 Bank BTPN employees have the flexibility to work productively from anywhere, including other branches, while minimizing unwanted cyber incidents. Telecommunications and digital services provider Spark is onboarding new employees significantly quicker through self-service capabilities on Microsoft 365 and Dynamics 365. Hungary-based Groupama Biztosító is simplifying communication and collaboration for its 1,500 company agents with Teams Phone, achieving enterprise-level security and 70% cost savings on telephony. Employers are making remote work secure and enjoyable with Microsoft Teams, Surface, and Viva. The industrial metaverse: Optimizing everyday processes with mixed reality Hokkaido Electric Power Co., Inc. is improving its remote training and field monitoring assistance at power stations by integrating HoloLens 2 with Microsoft Teams. To support critical business continuity, Canada-based J.D. Irving, Limited is connecting engineers and technicians with Dynamics 365 Remote Assist to immediately evaluate issues with sawmill machinery. Boeing is evolving the way it is building airplanes, shifting focus from paper-based 2D blueprints to 3D work instructions and augmented reality using HoloLens 2. Toyota Motor North America is using HoloLens 2 and Dynamics 365 Guides as a training tool for its employees, cutting defects in half during a pilot project. Norway-based energy company Equinor is embracing the industrial metaverse to design offshore windfarms and simulate their effectiveness before implementing them in the physical world. BMW China Training Academy is providing futuristic in-store experiences and making it easier for salespeople to learn new car features intuitively and quickly. Industry leaders are creating immersive experiences with metaverse technologies. As our current environment evolves, leaders will continue finding ways to maximize the value of their digital investments to stay ahead of competitors and keep pace with industry expectations by seeking a comprehensive, trusted cloud partner to meet their unique business needs. Empowered by the Microsoft Cloud, organizations are leveraging data and AI capabilities to accelerate the pace of innovation and operate more efficiently to achieve their goals. We are incredibly proud to continue on this path of co-innovation to serve customers big and small as they advance business, societal and sustainability outcomes around the world. Tags: AI, Azure, co-innovation, Dynamics 365, HoloLens, hybrid work, metaverse, Microsoft 365, mixed reality, Power Platform, Security, supply chain, Surface, Teams, Viva

Today, we are announcing the third phase of our long-term partnership with OpenAI through a multiyear, multibillion dollar investment to accelerate AI breakthroughs to ensure these benefits are broadly shared with the world.

This agreement follows our previous investments in 2019 and 2021. It extends our ongoing collaboration across AI supercomputing and research and enables each of us to independently commercialize the resulting advanced AI technologies.

Supercomputing at scale – Microsoft will increase our investments in the development and deployment of specialized supercomputing systems to accelerate OpenAI's groundbreaking independent AI research. We will also continue to build out Azure's leading AI infrastructure to help customers build and deploy their AI applications on a global scale. New AI-powered experiences – Microsoft will deploy OpenAI's models across our consumer and enterprise products and introduce new categories of digital experiences built on OpenAI's technology. This includes Microsoft's Azure OpenAI Service, which empowers developers to build cutting-edge AI applications through direct access to OpenAI models backed by Azure's trusted, enterprise-grade capabilities and AI-optimized infrastructure and tools. Exclusive cloud provider – As OpenAI's exclusive cloud provider, Azure will power all OpenAI workloads across research, products and API services.

"We formed our partnership with OpenAI around a shared ambition to responsibly advance cutting-edge AI research and democratize AI as a new technology platform," said Satya Nadella, Chairman and CEO, Microsoft. "In this next phase of our partnership, developers and organizations across industries will have access to the best AI infrastructure, models, and toolchain with Azure to build and run their applications."

"The past three years of our partnership have been great," said Sam Altman, CEO of OpenAI. "Microsoft shares our values and we are excited to continue our independent research and work toward creating advanced AI that benefits everyone."

Since 2016, Microsoft has committed to building Azure into an AI supercomputer for the world, serving as the foundation of our vision to democratize AI as a platform. Through our initial investment and collaboration, Microsoft and OpenAI pushed the frontier of cloud supercomputing technology, announcing our first top-5 supercomputer in 2020, and subsequently constructing multiple AI supercomputing systems at massive scale. OpenAI has used this infrastructure to train its breakthrough models, which are now deployed in Azure to power category-defining AI products like GitHub Copilot, DALL·E 2 and ChatGPT.

These innovations have captured imaginations and introduced large-scale AI as a powerful, general-purpose technology platform that we believe will create transformative impact at the magnitude of the personal computer, the internet, mobile devices and the cloud.

Underpinning all of our efforts is Microsoft and OpenAI's shared commitment to building AI systems and products that are trustworthy and safe. OpenAI's leading research on AI Alignment and Microsoft's Responsible AI Standard not only establish a leading and advancing framework for the safe deployment of our own AI technologies, but will also help guide the industry toward more responsible outcomes.

Satya Nadella shared the below communication today with Microsoft employees.

We're living through times of significant change, and as I meet with customers and partners, a few things are clear. First, as we saw customers accelerate their digital spend during the pandemic, we're now seeing them optimize their digital spend to do more with less. We're also seeing organizations in every industry and geography exercise caution as some parts of the world are in a recession and other parts are anticipating one. At the same time, the next major wave of computing is being born with advances in AI, as we're turning the world's most advanced models into a new computing platform.

This is the context in which we as a company must strive to deliver results on an ongoing basis, while investing in our long-term opportunity. I'm confident that Microsoft will emerge from this stronger and more competitive, but it requires us to take actions grounded in three priorities.

First, we will align our cost structure with our revenue and where we see customer demand. Today, we are making changes that will result in the reduction of our overall workforce by 10,000 jobs through the end of FY23 Q3. This represents less than 5 percent of our total employee base, with some notifications happening today. It's important to note that while we are eliminating roles in some areas, we will continue to hire in key strategic areas. We know this is a challenging time for each person impacted. The senior leadership team and I are committed that as we go through this process, we will do so in the most thoughtful and transparent way possible.

Second, we will continue to invest in strategic areas for our future, meaning we are allocating both our capital and talent to areas of secular growth and long-term competitiveness for the company, while divesting in other areas. These are the kinds of hard choices we have made throughout our 47-year history to remain a consequential company in this industry that is unforgiving to anyone who doesn't adapt to platform shifts. As such, we are taking a $1.2 billion charge in Q2 related to severance costs, changes to our hardware portfolio, and the cost of lease consolidation as we create higher density across our workspaces.

And third, we will treat our people with dignity and respect, and act transparently. These decisions are difficult, but necessary. They are especially difficult because they impact people and people's lives – our colleagues and friends. We are committed to ensuring all those whose roles are eliminated have our full support during these transitions. U.S.-benefit-eligible employees will receive a variety of benefits, including above-market severance pay, continuing healthcare coverage for six months, continued vesting of stock awards for six months, career transition services, and 60 days' notice prior to termination, regardless of whether such notice is legally required. Benefits for employees outside the U.S. will align with the employment laws in each country.

When I think about this moment in time, the start of 2023, it's showtime – for our industry and for Microsoft. As a company, our success must be aligned to the world's success. That means every one of us and every team across the company must raise the bar and perform better than the competition to deliver meaningful innovation that customers, communities, and countries can truly benefit from. If we deliver on this, we will emerge stronger and thrive long into the future; it's as simple as that.

I want to extend my deepest thanks and gratitude to everyone who has contributed to Microsoft up to this point and to all of you who will continue to contribute as we chart our path ahead. Thank you for the focus, dedication, and resilience you demonstrate for Microsoft and our customers and partners each day.

Today, Microsoft is announcing the acquisition of Fungible Inc., a provider of composable infrastructure aimed at accelerating networking and storage performance in datacenters with high-efficiency, low-power data processing units (DPUs).

Fungible's technologies help enable high-performance, scalable, disaggregated, scaled-out datacenter infrastructure with reliability and security.

The Fungible team will join Microsoft's datacenter infrastructure engineering teams and will focus on delivering multiple DPU solutions, network innovation and hardware systems advancements.

Today's announcement further signals Microsoft's commitment to long-term differentiated investments in our datacenter infrastructure, which enhances our broad range of technologies and offerings including offloading, improving latency, increasing datacenter server density, optimizing energy efficiency and reducing costs.

Microsoft Chairman and Chief Executive Officer Satya Nadella with Manny Medina, cofounder and CEO of Outreach, one of M12's portfolio companies.

As we begin a new year, I am delighted to share some exciting changes we've made at M12 to better deliver value to our portfolio companies and to Microsoft. While I experimented with ChatGPT while writing this blog post, and have been impressed by its results, I decided on a traditional, human approach.

2022 was quite the year overall, with some exceptional volatility in the startup and venture capital ecosystem. We whipsawed from valuing speed of execution and growth above efficiency, to prizing diligence with guidance toward efficiency and profitability as valuations returned to pre-pandemic levels.

In the six years since M12 was founded, we're proud to have invested in over 100 companies, including 15 unicorns and 6 IPOs. Millions of people rely on the products and services from M12 portfolio companies to protect them from identity fraud, ensure healthcare facilities have accurate patient data, engage in learning to help them do their jobs better and make their tasks easier though AI experiences. We are at our best when we partner early with teams and will continue to focus on investing in Series A and B startups.

However, over the last 12 months, we realized startups and Microsoft needed a new M12, and so we developed a more focused investment strategy tightly aligned to Microsoft. This has helped us create exceptional value through connections, customers and unique benefits for our portfolio. You might say we've leaned into the "M" of M12, where we tap more into Microsoft and the power of its network.

Our closer connection to Microsoft focuses our work and positions our portfolio companies to accelerate their growth, two things that are proving to be even more critical as we navigate the volatile macroeconomy. This approach is part of our new mission, and we've added additional experienced investors and operators to our team to make it a reality.

As we continue our work to build and nurture innovative early-stage startups, our goal is for our companies to have meaningful partnerships with us that empower them to win.

We're thesis-driven and aligned to Microsoft stakeholders to support portfolio growth

Our core thesis areas are AI, cloud infrastructure, cybersecurity, developer tools and vertical SaaS (areas like retail and healthcare). Great companies in these spaces such as Innovaccer, a healthcare data platform, and At-Bay, a cyber insurance platform, often find success partnering with Microsoft on product integrations, growing their customer base, building out ecosystems and getting traction in our sales motions.

We also selectively make frontier investments in companies that innovate in autonomous systems and Web3, metaverse and gaming. These are spaces we think have the potential to influence the future direction of the industry, and we work to ensure they too can realize the power of having a deeper connection to Microsoft.

For example, Web3 is an emergent category we are paying close attention to and are partnering with Microsoft to support companies that are innovating here.

Space and Time, one of our latest investments in this area, is a decentralized database company. Their plans include an integration with Microsoft Azure to help customers access, manage and run analytics on blockchain-native data, a key driver of their growth goals. In this new era of ubiquitous computing, the comprehensive Microsoft Azure cloud platform and industry-leading identity and security capabilities provide a trusted set of services to develop and run Web3 applications, enabling Space and Time to build new and advanced use cases in Web3.

In addition to our work with Space and Time, we've seen our thesis-driven approach help create more value across the M12 portfolio.

Our new approach to stakeholder engagement and thesis-driven investment saw its biggest moment in recent months with our announcement of the M12 GitHub Fund. GitHub, which is part of the Microsoft family of companies, is all about empowering developers. With DevOps being one of our core thesis areas, our partnership created a fund managed by M12 to make investments in promising open source startups developing on GitHub's platform. CodeSee, an open source platform that enables developer teams to visually understand their codebases, and Novu, open source notification infrastructure for developers, are our first portfolio companies to benefit from this effort. We see the establishment of this fund as a powerful proof point that Microsoft, M12 and our portfolio are all better together.

Leaning into the power of our platform to deliver value

To deliver on the goal of bringing differentiated value to startups, we've leveraged our redesigned investment approach to better support our portfolio. Our investing and operating team members are now aligned to our specific thesis areas, which allows us to develop deeper expertise and partnerships in these domains.

Before every investment, we challenge ourselves to identify the unique value we can add. To meet that commitment, today more than half our team is dedicated to helping our portfolio companies realize their growth objectives. This support may take the form of a startup showcase with Microsoft's CEO and most senior leaders; introducing our portfolio companies to new customers; a senior Microsoft executive formally advising a portfolio company; facilitating technology collaborations or engaging in co-selling or marketing activities. No matter the case, we're always working to find creative solutions to deliver value for our portfolio companies.

Aqua Security has a strong history of working with Microsoft to include deep product collaborations, and co-selling and joint marketing activities to accelerate Azure deployments. In one example of this relationship, Aqua enables software licensing and procurement directly through the Microsoft Azure Marketplace, allowing customers to utilize existing purchasing methods in place for Azure services. This solution simplifies the process of building out a secure infrastructure for cloud applications, and customers deploying containers on Azure can utilize the solution to quickly complete their purchasing cycle.

Aqua and Microsoft have jointly conducted a series of regional marketing programs to educate customers on cloud native technologies and provided the opportunity to interact with local technical resources from both companies. As Aqua's enterprise customer base and scale of their cloud deployments increase, Microsoft's prime position as a cloud leader is a distinctive benefit to their success.

This type of collaboration and many others like it make us confident we can be a driver of innovation and an enabler of growth. By investing in areas where we have a unique ability to support our portfolio companies, we see a bright future for our work ahead and the technology that our fund will enable. We're excited by what our approach can do for entrepreneurs, and we can't wait to see what's in store for 2023 and beyond.

Feel free to reach out to us or learn more about our team at m12.vc. We're always looking to connect with interesting companies and partners to help accelerate the future of technology.

Vehicles are becoming increasingly smarter, more connected, and software defined.

It's hard to believe a mere 200 years ago, the horse and carriage were the predominant mode of transportation — and had been for hundreds of years. And then, with the invention of the internal combustion engine, moving people and goods changed immensely in a short period of time. Mobility became more accessible, affordable and created opportunities far beyond the scope of imagination.

It was that once-in-a-century change that brought us to where we are today.

The mobility industry is at a critical inflection point. Major innovation is underway: Vehicles are becoming smarter, more connected, and software-defined — and in some cases, autonomous. Electrification is taking hold. A focus on sustainability is rising to the forefront. Innovators in our industry are reimagining every aspect of moving people and goods around the world, and Microsoft is proud to partner with those who are reinventing mobility.

And yet, this metamorphosis is challenged by global macroeconomic challenges, supply chain disruptions, energy shortages and lagging worker availability. It's never been more important to do more with less.

As we kick off CES 2023 in Las Vegas, we're bound to see emphasis on both sides of this paradigm shift — innovations that improve mobility while at the same time increasing efficiencies, providing opportunities to reduce our environmental footprint, and connecting people in new ways. As a key partner to the industry, Microsoft has a unique viewpoint into its evolution — we don't produce vehicles, monetize our customers' data, or compete with their brands. Instead, we help empower automakers and mobility service providers with intelligent cloud and edge technologies to help them build their own unique digital platforms to push the boundaries of what's possible.

Microsoft accelerates the transformation of automotive, mobility and transportation companies into data- and software-driven mobility services providers. Azure cloud-based analytics services unlock unique insights collected across customer interactions, data sources, workforce and operations performance, and our extensive cloud services and partner ecosystem enables new revenue streams from differentiated mobility services.

This year's CES — which kicks off tomorrow — features a dedicated transportation hall, giving us an opportunity to dive in and explore the future of mobility. Here are key themes to watch as organizations bring their latest and greatest to the most influential tech event in the world.

Across industries, a resilient supply chain is top of mind.

While resiliency is a key ingredient of long-term organizational viability, it's taken on a new meaning in the current climate. Just about every industry has felt the cascading impacts of supply chain disruptions, and mitigating those disruptions is crucial to restoring expectations.

In the trucking industry, timing is everything. Ensuring that trucks get to customers on time requires identifying problems early and being able to respond nimbly. But a lack of visibility across the supply chain makes it difficult to know when there are issues that could impact production and, consequently, delivery. To help streamline the processes and keep an end-to-end eye on its entire supply chain, Daimler Truck North America is using the Microsoft Supply Chain Platform to improve visibility, reduce downtime and increase efficiencies.

And when it comes to vehicles, visibility into the supply chain is essential to production. With the new MO360 Data Platform, Mercedes-Benz is connecting its about 30 passenger car plants worldwide to the Microsoft Cloud, enhancing transparency and predictability across its digital production and supply chain. The MO360 Data Platform is the evolution of Mercedes-Benz' digital production ecosystem MO360 and allows teams to identify potential supply chain bottlenecks faster and enable a dynamic prioritization of production resources.

Empowering frontline workers is critical to agility. ZF has deployed Power Platform to enable employees to collaborate with professional developers.

2. Using AI and connected data to create agile business processes.

AI and data are helping fundamentally reimagine critical business practices and play a key role in streamlining cumbersome, outdated processes. When utilized responsibly, AI and machine learning can have a positive impact on daily life — and many in the industry are taking advantage of opportunities to improve experiences for both employees and customers, while impacting sustainability as well.

Travel and tourism is approaching — and even surpassing — pre-pandemic levels, and American Airlines is preparing for this influx by using technology in new ways to make its business more efficient and the travel experience more seamless for its customers. By building its platform on Microsoft Azure and applying AI, machine learning and data analytics to its systems, it's been able to reduce time waiting on the runway, saving thousands of gallons of jet fuel per year, and give connecting customers extra time to make their next flights.

With 180+ production locations around the world, empowering frontline workers isn't a nice to have — it's a must have for ZF. A global technology company that supplies materials for passenger cars, commercial vehicles and industrial technology, the company has empowered employees to use low-code solutions on Microsoft Power Platform to collaborate with its professional developers. In one example, ZF employees helped improve lean manufacturing audits by developing a solution that eradicated use of spreadsheets and migrated data into robust Power BI reports — and created significant time and resources savings.

Lynk & Co has Microsoft Teams built into the infotainment system of its 01 model.

Differentiation is critical when options abound. Brands that invest in knowing their customers and providing specialized experiences throughout the customer lifecycle will excel, and we're going to see a new emphasis on personalization in the automotive space. No longer are buyers "stuck" with the static experience delivered upon vehicle purchase. In-car personalization through connected and software-defined vehicles unlocks the ability to provide ongoing updates to the vehicle experience.

Just this week, General Motors announced it's partnering with Microsoft to deploy new software-defined vehicle services. As GM's Ultifi platform nears deployment in 2023, GM will use Microsoft Azure cloud and AI services to help simplify its software development and bring convenient and secure digital mobility services to millions of customers faster.

And in productivity, Lynk & Co has developed a unique, built-in meetings app within the infotainment system of its 01 model vehicle, working alongside Microsoft. The app enables users to join Teams meetings on the go, bringing a new dimension of the in-car experience and adding versatility to mobility.

Innovation in the software-defined and autonomous vehicle space is happening at a very fast pace, and we mustn't overlook the importance of coming together to reduce complexity and encourage collaboration. Together, we can go farther, faster.

We believe an open-source environment is critical to long-term success, and the work that the Eclipse Foundation is driving is key to delivering this. Microsoft intends to improve developer efficiencies by providing a layer of abstraction that simplifies management of the underlying in-vehicle architecture and lower-level capabilities, and we've contributed two projects that focus on modern Application Programming Model and Digital Twins to the foundation's work. These two interrelated projects aim to significantly reduce the complexity involved in developing in-vehicle applications and enable a large developer community.

We continue to support the development of assisted and autonomous driving and are excited with the partnership with many players in the industry including Wavye, Cruise and others.

This week, Supernal, Hyundai Motor Group's Advanced Air Mobility company, announced a collaboration with us to advance autonomy, digital operations and cloud integration technologies for the aviation industry by running on the Microsoft Cloud platform. With Microsoft Azure, Supernal can leverage the flexibility and cloud computing power needed to run secure simulations at scale and accelerate its time to commercialization.

The industrial metaverse empowers OEMs to streamline training and improve service regardless of geographic location.

5. Designing new experiences in the metaverse that inspire, connect and empower.

"Metaverse" is perhaps the buzzword of the industry, with much excitement building about what's possible in this new dimension. We're seeing several applications that are particularly effective in the industry, on both the industrial and consumer-facing sides.

In particular, the industrial metaverse offers opportunities for improved training scenarios, with experts and those connecting with them able to be geographically dispersed.

With HoloLens 2 and Dynamics 365 Guides, automakers like Toyota Motor North America, Nissan Motor Corporation and others, train employees without having experts physically in the same location, without stacks of paper manuals, and in a way that makes it possible for employees to learn while doing. This approach can reduce the time required to learn new processes and skills, and the content can be created without special programming or IT skills.

And we're seeing our customers embrace streamlining and improving consumer-facing experiences as well. Recently, FIAT announced a fully virtual showroom for its new FIAT 500 hosted in the metaverse and running on Microsoft Azure, where customers in Italy can interact with product specialists to discover the vehicle, simulate a test drive on the famous La Pista 500, and make a new vehicle purchase — from the comfort of their couches. Developed with Microsoft and Touchcast, you can experience this virtual showroom now.

These are just a few of the trends we expect to continue into the coming year. While none of us know exactly what's to come, one thing's for sure — the industry is evolving faster than ever before — and we are excited to help empower a safe, sustainable and productive future of mobility.

I'm delighted to share the news that Melanie Nakagawa will join Microsoft in January as our new Chief Sustainability Officer. Reporting directly to me as a Corporate Vice President, Melanie will partner with teams across Microsoft and take on the leadership role for our company-wide environmental sustainability work.

Melanie brings to Microsoft almost two decades of environmental sustainability experience at the nexus of policy, business and technology, which will be vital as we continue our sustainability journey. She most recently served as Special Assistant to the President and Senior Director for Climate and Energy on the National Security Council at the White House, one of several roles she has held in the U.S. government. At the White House, Melanie played a leadership role on international and domestic climate initiatives, as well as energy issues that included the international energy response to the war in Ukraine.

This built on Melanie's prior work, including as the director of climate strategy for a climate tech-focused private equity firm working with growth stage companies in North America, Europe and Asia. She also brings experience in the nonprofit and academic sectors on environmental and energy policy and regulatory issues.

Melanie joins Microsoft at a critical time. January will mark the third anniversary of our ambitious climate goals to be carbon negative by 2030 and remove our historical carbon emissions by 2050. While I'm pleased with our progress, we must accelerate our momentum and broaden even further our climate-related work.

This urgency reflects the current state of climate issues around the world. As I found while meeting with global leaders last month at the United Nations COP27 climate conference in Egypt, the world confronts a complex and sobering challenge. As the United Nations Environment Programme reported in October in its annual Emissions Gap report, current national climate plans fall short of what will be needed to meet the world's climate targets.

And as U.N. Secretary General António Guterres said at COP27, the "deadly impacts of climate change are here and now." This means the world must not only push harder toward the goal of a Net Zero economy by the middle of the century, but move quickly and aggressively, especially in the Global South, to help vulnerable populations adapt to a world with a changed climate.

Given the enormity of these challenges, the pursuit of progress will require extraordinary innovation in the years and decades ahead.

And while every month seems to bring new foreboding studies, I also found cause for optimism in Egypt. For example, globally almost 4,000 companies have now dedicated themselves to the pursuit of climate pledges. And as the world focuses on the implementation of climate pledges – a major theme at COP27 – businesses have an increasingly important role to play. This was on bold display at COP27 as the United States' Special Presidential Envoy for Climate, John Kerry, announced that the First Movers Coalition, launched just last year with the World Economic Forum, had grown to 65 companies dedicated to working and moving together faster.

Especially for a company like Microsoft, with our focus on helping the world's organizations innovate through technology, our climate-related role could not be clearer. Cloud-based digital services, the better use of data, and rapid advances in AI will create new opportunities for us to help every organization achieve more progress in addressing the world's climate and energy needs.

This connects directly with the three-fold sustainability mission that we launched as a company in September at the U.N. General Assembly meetings and that Melanie will now lead.

First, we will continue to drive toward achieving by 2030 our commitments to become carbon negative, water positive and zero waste as a company while contributing to the biodiversity of the planet. The team that Melanie leads includes environmental scientists of international stature, and they will help keep Microsoft's work grounded in the best available science. And more than ever, the Environmental Sustainability operations team will partner with Microsoft's Finance team and business and sustainability experts across the company to achieve the company's internal and operational goals.

All this will build on recent and important steps across Microsoft. These include the construction of a new Thermal Energy Center for our Redmond campus, the pursuit of a Net Zero water certification for our Silicon Valley campus, and our most recent steps toward Zero Waste operations through the opening of our 4th and 5th Circular Centers in Singapore and Chicago. These complement our global renewable energy investments for our datacenters and investments that have made Microsoft the largest carbon removal purchaser in the world.

Second, we will accelerate innovation and deliver technology to help our customers and partners achieve their sustainability goals. Like Lucas Joppa, our first Chief Environmental Officer, Melanie will work with me to bring together leaders across Microsoft to work together and learn from each other. The good news is that sustainability has become an important team sport across Microsoft, with senior leaders in place for product development, marketing and sales, including Microsoft's Elisabeth Brinton and Darryl Willis, who work hand in hand with our customers and partners to transform their businesses with our sustainability and energy solutions. Each quarter we're strengthening the Microsoft Cloud for Sustainability, adding capabilities and investing in the next-generation, cloud-based sustainability data ecosystem the world needs. All this connects closely with the broader array of innovations in the Microsoft Cloud and our own climate and renewable energy innovations across one of the world's largest arrays of datacenter operations.

Third, we will partner with governments, nonprofits, and businesses to spur the broader societal enabling factors critical to global sustainability progress. This includes existing and new initiatives that Melanie and her team will lead to help:

Broaden the use of climate-related data and more powerful AI, including by the United Nations and across the Global South; Advance new and innovative climate, energy, and sustainability laws, policies and regulations; Support reliable, interoperable, and globally aligned measurement accounting and reporting systems for carbon emissions; Build new markets for climate and sustainability solutions, including through our Climate Innovation Fund and carbon removal purchases; and Help develop and support the skills and talent needed for both specialized sustainability roles, as well as for existing jobs that will evolve to require sustainability fluency.

These three sustainability missions are grounded in three tenets that will guide our future sustainability work as a company.

First, we believe there is a virtuous cycle connecting these three missions. Progress in each mission helps strengthen our ability to pursue the next. In this sense, all three are interrelated and dependent on each other.

Second, we believe that cross-sector efforts will be indispensable for sustainability progress. As with almost all big problems in the world, we need a three-legged stool: business, nonprofits and governments. We believe that businesses have a unique role to play in innovation, especially when it comes to climate, energy and digital technology and product innovation. Nonprofits are often the best at incubating new societal solutions, often by using innovations that come from the business sector. And governments can bring solutions to scale in a way that no one else can, both through their public budgets and the power to legislate and regulate. Even in a divided world, the planet's sustainability challenges require that we all come together.

Finally, across Microsoft (and the world, for that matter), environmental sustainability is becoming infused in almost everything we do, and our success requires navigating a matrix rather than managing a system of command and control. Melanie and the Environmental Sustainability team she will lead acts as a fulcrum across Microsoft, helping to bring everyone together and speaking publicly for the company. This is like the role that our corporate teams play in a variety of other areas, including accessibility, digital safety, privacy, human rights and responsible AI. As we've learned, the difference between success and failure always turns on our ability to work effectively as a team across a large company and with an even larger digital ecosystem.

It's typically my role and privilege to help empower, work with and support talented leaders across all these areas and more. None of the issues are easy. And the environmental sustainability challenges may even be harder than most.

I'm excited that Melanie Nakagawa will help lead so many talented people across our company as we address the planet's sustainability needs. And I look forward to supporting her!

In October, our Microsoft Ignite event drove incredible dialogue around how organizations can navigate a challenging market. Digital perseverance as encapsulated so well by Judson Althoff, our executive vice president, and chief commercial officer, resonates deeply with us across the Azure business and is sharpening how we think of strategic innovation in the face of headwinds.

There's that word: Innovation. It continues to mean so many things to so many and, honestly, innovation becomes harder to distill amid constrained resources and near-term pressure. As we close 2022, reflecting on our own transformation, we remain inspired by a holistic cloud strategy; one which balances what's coming around the corner with what lies ahead. Now more than ever, continuing to innovate in the right places is essential to maintaining a productivity edge internally, and strengthening differentiation externally. Your app estate is on the front lines of it all.

Last week, we shared insights from a range of cloud customers who plan to concentrate on migration and modernization plans for business resilience. Now I want to hone in on what kinds of modern experiences are in high demand to help organizations prioritize. Today, we're releasing our first App Innovation Report, which identifies where digital investments can root innovation in human pain points. It all comes back to what problems need solving and looking at return on investment from another critical angle: the people who technology serves. As different as we are and as complex as our digital world has become, we psychologically seek a lot of the same things. We each value flow, integration, simplicity, efficiency, protection, freedom and fulfillment. By tapping into these common threads, businesses can deliver experiences people enjoy and keep coming back to.

This fall we studied over 1,500 people working on the front lines of business challenges in healthcare, manufacturing and retail, and while we spoke to them as employees, we considered their perspectives as patients and shoppers too; the business-to-business-to-consumer (B2B2C) duality to our application expectations is pretty fascinating. Through interviews and surveys in five innovation hubs — the United States, Norway, Brazil, Japan and India — we applied social science to the state of the digital experience. It felt good to take a step back and ask our customers' customers one simple question: What do you want and need from apps today? The answers surprised us — not because they were grandiose or futuristic but because they were foundational.

Top 16 forces of app innovation: "Promise scores" above reflect a combination of people's rational preference and emotional passion. Scores are indexed against an average of 100, so scores above 100 represent the most promising innovation areas.

People feel stuck, frustrated and lost because of disjointed apps

People around the world in a vast range of roles expressed the most passion about disruption to their workflow. On the receiving side of apps of all kinds, there's palpable frustration from having to manually patch together disparate tasks and tools. We learned people are struggling to secure data automatically, to streamline repetitive input, and to have well-informed interactions with others. While we heard desire for immersive technologies bubbling to the forefront, it wasn't as prominent as pragmatic everyday realities and, from an Azure lens these areas signal opportunities to help advance AI automation, API management and app intelligence. Whether you're migrating workloads, modernizing or building something new for flexibility and scale, the cloud can architecturally address so many of the trends we found.

In the industries we researched, people share their companies' concerns about data breaches on a personal, day-to-day level. In healthcare, they spoke about pressures surrounding HIPAA requirements when handling patient data. A sense of nervousness about security was also expressed in manufacturing, a highly targeted industry for ransomware, and retail workers worry about their accountability in handling customers' financial information. Security is so incredibly important to us in Azure, so hearing from the usage and functionality side of this space emboldens us to partner on app development that can continue to advance confidence up and down organizations.

The cloud investments we make and the customization we code affect so many layers of a company and, in the applications space, innovation boils down to experiences people can count on and even enjoy using — it's one of the most fulfilling aspects of the business we're in. Well-architected workloads don't only enable cost optimization, they provide a foundation of innovation where business leaders, technology and developer teams can adapt quickly to deliver secure, intelligent and intuitive apps people keep coming back to.

The need for more meaningful conversations stands out in retail

In light of the season, one industry-specific breakdown is especially timely. We see experience needs rank differently in retail compared to other cross-industry averages, with repetitive input surpassing automatic security, and desire for meaningful conversations breaking through in a way we don't see elsewhere. Because shoppers today do so much research before visiting stores, retail workers want digital solutions that can help them provide information about product features, pricing, discounts and availability without breaking their connection to the customer. A sales engagement that feels jarring or closed off today could become warmer and interactive tomorrow.

Retail-specific needs: "Promise scores" from retail workers above are also indexed against an average of 100. Scores as high as 141 and 133 indicate exceptionally strong rational preference and emotional passion.

Business leaders agree with worker concerns but need help prioritizing

Rounding out the research, we put our report in front of business leaders — those who choose Azure services and customers of other cloud providers. We also wanted to understand how chief technology, digital and operations officers are thinking about app design, and the themes voiced by workers were equally important to them. So, if workers have unmet needs like this and leadership is aligned, what's next? Prioritization.

How we each define innovation is as unique as the cloud journeys we're on but giving app experiences more shape helps us see strategic pockets in new ways and grounds cloud investments in the people who run your business, in the people you're selling to. What kinds of inefficiencies are draining resources most immediately? Which growth territories will you target to compete? Organizations can't tackle every app innovation project at once, but together, we can carve out data-driven spots that present the most promise.

Spot the opportunities: People's rational preference and emotional passion for different app innovation areas are mapped above in the context of satisfaction with existing solutions in the market and relevance in their lives. Clockwise from the top-right quadrant, the urgency of each innovation area is broken out for planning.

One of the many things that drew me to Azure's business is how many people it touches. When we look at Azure customers like the NBA, H&M or Starbucks, we see how very tangible outcomes are in the palm of our own hands as consumers and suddenly, what the cloud makes possible begins to click no matter who you're talking to. When I watch real-time data on my Boston Celtics, when I'm able to shop for loved ones in a single click, and I can enjoy an iconic red cup without waiting in line, I am experiencing cloud-first app investments that pay off in spades. From the back end to the front end of app design, we're reminded that business impact of everything digital is highly human.

On behalf of the Azure team, we hope the App Innovation Report inspires your work going into the new year. And no matter how or what you celebrate, I wish you and your loved ones a safe and happy season.

Source: Future of App Innovation study conducted by Ipsos between July and September 2022 among 1,500 workers in healthcare, manufacturing and retail in the U.S., Brazil, India, Japan and Norway.

Microsoft looks to generate $5 billion in revenue from LSEG and the financial services sector through 10-year strategic partnership. Today, we announced a 10-year strategic partnership with the London Stock Exchange Group (LSEG), a world-leading financial markets infrastructure and data provider. Following LSEG's acquisition of Refinitiv, a market leader in data services, LSEG has differentiated itself in the market with an end-to-end proposition across trading, execution, data and analytics solutions. Together we look forward to empowering the future of financial markets by delivering next generation data, analytics and workspace solutions that transform how firms connect, research, analyze, collaborate and transact across the entire financial markets value chain.

YouTube Video Click here to load media Organizations across the capital markets value chain are facing an increasingly complex operating environment with macroeconomic headwinds, stricter regulatory controls and traditional revenue sources becoming more challenging. These conditions are putting greater pressure on firms to reinvent business models and do more with less. However, their legacy platforms, siloed information, limits on scale and data overload hinder their ability to deliver the best client experience, insights and tools. This requires a digital transformation approach underpinned by modern cloud and AI technology. LSEG has already started to address these issues for their customers, and through this strategic partnership, we will accelerate that transformation. Additionally, capital markets run on data and by investing in the co-creation of new solutions, we will capitalize on the incremental opportunity we have together and support clients in their transformation. "This strategic partnership is a significant milestone on LSEG's journey towards becoming the leading global financial markets infrastructure and data business and will transform the experience for our customers," said David Schwimmer, CEO of London Stock Exchange Group. Democratizing financial markets data Foundational to the partnership will be the digital transformation of LSEG's technology infrastructure and data and analytics platforms onto the Microsoft Cloud. This will include Refinitiv platforms that power over 40,000 financial institutions in 190 countries with data, analytics and insights across millions of active time series databases, daily evaluations, exchange trades and derivatives, equity quotes and significant research on public and private companies. With this foundation, we will co-create an open, centralized, financial data platform enabling seamless data democratization, collaboration and new monetization opportunities across the financial services ecosystem. This will unlock new offerings for customers to generate more sophisticated and timely insights. Delivering next-generation workspace experiences The financial markets community spends a large part of their day working across multiple terminals and platforms, disparate data sets and siloed analytical tools with limited collaboration capabilities. To enhance productivity and time to value, we will work together to co-create an open all-in-one data, analytics, workflow and collaboration solution that will reimagine client experiences for the first time. This will be realized through the next generation of LSEG Workspace on Microsoft Teams platform that will support in-application rich experiences for understanding trends and analyzing risk and building scenarios while meeting strict security, privacy and compliance requirements. Additionally, with enhanced Excel integration, customers will be able to create financial models, run data analytics and visualizations using LSEG content delivered in Excel and work seamlessly between LSEG Workspace and Microsoft 365. The initial focus will be on delivering interoperability between LSEG Workspace and Microsoft Teams, Excel and PowerPoint with other Microsoft applications and a new version of LSEG's Workspace, accessed entirely within the Microsoft 365 suite, to be added in the future. Creating intelligent analytic solutions Today, firms can experience duplicated costs and complexity in harnessing the full power of analytics to unlock models that garner intelligent insights for business decision making. To address this need, we will work together to co-create next generation analytics and modeling solutions which are cloud-based and will enable powerful model construction, validation, diagnostics and deployment using Microsoft Azure AI, Synapse, Power BI, Excel and Teams with LSEG's advanced analytics and modeling capabilities. It will empower investment bankers, traders, wealth and asset managers, as well as risk, compliance, strategy and advisory managers to run hyperscale analytics models against data efficiently and seamlessly. Built on top of Azure Synapse, Azure Machine Learning and Microsoft Purview, the new cloud-based analytics and modeling solutions will enable users to expose, share and collaborate across proprietary and third-party data securely and confidently, meeting stringent data privacy, security and compliance requirements, eliminating the need to move or copy the data to another location (which can be expensive, time-consuming and introduce security and compliance concerns). Additionally, quantitative analysts, data scientists and engineers will be able to build custom models to drive their analysis and decision making seamlessly. Both can help organizations spend less time, money and effort building and maintaining their own infrastructure. "Bringing together our leading data sets, analytics and global customer base with Microsoft's comprehensive and trusted cloud services and global reach creates attractive revenue growth opportunities for both companies" said David Schwimmer, CEO of London Stock Exchange Group. Additionally, we will explore the development of digital market infrastructure based on cloud technology, with a goal to transform how market participants interact with capital markets across a broad range of asset classes. This partnership represents a significant milestone for the future of financial markets and builds on Microsoft's investments across capital markets and more broadly across the financial services industry. Microsoft estimates this partnership, and broader market opportunity, could generate an additional $5 billion in revenue for the company over the next 10 years, including the $2.8 billion minimum spend commitments from LSEG for cloud services and support. Microsoft will also purchase an approximately 4% equity stake in LSEG through the acquisition of shares from the Blackstone/Thomson Reuters Consortium. "We are delighted to welcome Microsoft as a shareholder. We believe our partnership with Microsoft will transform the way our customers discover, analyze and trade securities around the world, and create substantial value over time. We look forward to delivering on that potential," said David Schwimmer, CEO of London Stock Exchange Group. To learn more about our partnership, please see the press release. For broadcast-quality footage, please contact [EMAIL] To learn more about our work in this space, please visit our capital markets site. Tags: AI, Microsoft 365, Microsoft Azure, Microsoft Cloud, partnership, Synapse

Today, Microsoft announced it has acquired Lumenisity® Limited, a leader in next-generation hollow core fiber (HCF) solutions. Lumenisity's innovative and industry-leading HCF product can enable fast, reliable and secure networking for global, enterprise and large-scale organizations. The acquisition will expand Microsoft's ability to further optimize its global cloud infrastructure and serve Microsoft's Cloud Platform and Services customers with strict latency and security requirements. The technology can provide benefits across a broad range of industries including healthcare, financial services, manufacturing, retail and government.

Organizations within these sectors could see significant benefit from HCF solutions as they rely on networks and datacenters that require high-speed transactions, enhanced security, increased bandwidth and high-capacity communications. For the public sector, HCF could provide enhanced security and intrusion detection for federal and local governments across the globe. In healthcare, because HCF can accommodate the size and volume of large data sets, it could help accelerate medical image retrieval, facilitating providers' ability to ingest, persist and share medical imaging data in the cloud. And with the rise of the digital economy, HCF could help international financial institutions seeking fast, secure transactions across a broad geographic region.

Lumenisity's next generation of HCF uses a proprietary design where light propagates in an air core, which has significant advantages over traditional cable built with a solid core of glass, including:

Increased overall speed and lower latency as light travels through HCF 47% faster than standard silica glass.[1] Enhanced security and intrusion detection due to Lumenisity's innovative inner structure. Lower costs, increased bandwidth and enhanced network quality due to elimination of fiber nonlinearities and broader spectrum. Potential for ultra-low signal loss enabling deployment over longer distances without repeaters.

Lumenisity was formed in 2017 as a spinoff from the world-renowned Optoelectronics Research Centre (ORC) at the University of Southampton to commercialize breakthroughs in the development of hollow core optical fiber. In 2021 and 2022, the company won the Best Fibre Component Product for their NANF® CoreSmart® HCF cable in the European Conference on Optical Communication (ECOC) Exhibition Industry Awards. As part of the Lumenisity acquisition, Microsoft plans to utilize the organization's technology and team of industry-leading experts to accelerate innovations in networking and infrastructure.

[1] Speed of Light in Transparent Materials, Matthew J. Parry-Hill and Michael W. Davidson – National High Magnetic Field Laboratory, 1800 East Paul Dirac Dr., The Florida State University, Tallahassee, Florida, 32310, Sept. 10, 2018

Tags: cloud infrastructure, dark fiber, datacenters, Government, healthcare, network connectivity, networking, optical fiber, telecommunications

The recent announcement by the U.S. Department of Defense (DoD) selecting Microsoft among the cloud service providers to support its Joint Warfighting Cloud Capability (JWCC) contract is a significant milestone in the department's goal to establish an enterprise-level tactical cloud. With the announcement, Microsoft will help deliver mission-critical 21st century technology to our nation's service members and strengthen U.S. national security. The selection is another clear demonstration of the trust the DoD places in Microsoft and our technologies.

Our work on JWCC will build on the success of our industry-leading cloud capabilities to support national security missions that we have developed and deployed across the department and service branches. We continue to develop new technologies that utilize the latest commercial innovations, enable interoperability and are designed to bring data analysis and insight to the tactical edge. Microsoft's investment is also aligned with the department's 2021 OCONUS cloud strategy, including building Azure Government Secret and Azure Government Top Secret classified clouds to provide the DoD with secure cloud capabilities at all classification levels. We've designed ruggedized edge devices to bring advanced compute capabilities to the most remote regions of the world. We expanded our Azure Space and 5G capabilities to ensure ubiquitous connectivity so that service members can process, analyze and operationalize data whenever and whenever they need it. This continuous innovation puts us ahead of the curve and in a strong position to meet all the key milestones for JWCC, along with key DoD priorities such as their AI and cloud strategies and Joint All-Domain Command and Control (JADC2) initiatives, in the coming months and years.

The solutions and services we've developed are built on a foundation of cybersecurity, deployed across all devices, clouds, apps and platforms. We're supporting the DoD's deployment of its Zero Trust strategy to further secure their network and systems, building on our existing efforts with programs like Navy's Flank Speed to protect nearly half a million identities and devices. Microsoft brings together massive signal depth and diversity of over 43 trillion signals per day synthesized daily using sophisticated data analytics and AI algorithms, and a global team of 8,500-plus security experts to deliver unparalleled protection.

Collaboration across industry and academia is also key to accelerating innovation and advancing the DoD's mission. That's why we've set up deep partnerships to advance research and development in emerging areas. We recently announced a strategic partnership with Lockheed Martin to innovate with our classified cloud, enhance secure 5G connectivity at the edge, and improve gaming, exercising, modeling and simulation (GEMS) capabilities to bring military planning and coordination through immersive environments. Earlier this year, we announced a collaboration with the Naval Postgraduate School to advance cloud-enhanced intelligent edge networking and computing, modernize remote training and education, and develop solutions to operationalize emerging technologies in the field for the Navy and Marine Corps. And we're helping defense partners, like Raytheon Technologies and Oshkosh, move to the cloud to help the broader industry align to the DoD's priorities.

We believe the multi-cloud approach for JWCC is the right one for the DoD's enterprise infrastructure. Multi-cloud is already an established best practice in the commercial industry because it enables organizations to maximize flexibility, enhance resiliency and access the best technologies across providers. With comprehensive infrastructure and data management solutions like Azure Arc, Microsoft Purview and Defender for Cloud, Microsoft will be a key partner to the DoD as they navigate the multi-cloud environment and ensure seamless interoperability of systems and services.

As we have demonstrated for more than 40 years, our focus on who we are serving will never waver. We remain committed to the department and to those serving around the world who are helping keep us all safe.

At Microsoft, we take our commitment to creating a safe, inclusive work environment for all employees very seriously. In keeping with this commitment, today we are announcing new steps to prevent discrimination and harassment, following a 2021 proposal that was approved by Microsoft's shareholders.

Based on this proposal, Microsoft's Board of Directors engaged the law firm of ArentFox Schiff LLP to conduct an independent review of the company's sexual harassment and gender discrimination policies and practices. ArentFox has completed its work and has provided the Board with a report detailing its findings and recommendations. In addition, the Microsoft management team has prepared an implementation plan that addresses all the recommendations in the ArentFox report. In many cases, the recommendations build on and extend work already in progress. The Board has thoroughly reviewed the ArentFox report, and it has approved the specific actions in the implementation plan.

Today the company is publishing a full transparency report from ArentFox as well as a summary of management's implementation plan. The transparency report can be viewed here. This 50-page report contains all the findings and recommendations made by ArentFox, with only limited confidential and privileged information omitted (as suggested by the terms of the shareholder resolution last year). Management's implementation plan can be viewed here.

"Cultivating a culture where everyone is empowered to do meaningful work and can thrive is our greatest responsibility at Microsoft," said Satya Nadella, Chairman and CEO, Microsoft. "The Board of Directors has taken this investigation seriously and the resulting comprehensive report outlines important areas where we can continue to improve, as well as progress we've made. The Board of Directors and our senior leaders are fully committed to this implementation plan as we continually work to close the gap between our espoused culture and the lived experience of our employees."

We believe that today's plan will further strengthen Microsoft's culture and build upon a broad array of existing policies, trainings, and complaint and investigative procedures. While we have taken meaningful steps in recent years to make improvements in all these areas, we recognize that this work requires a journey and not a single destination, with a commitment to continuous self-improvement.

We also remain committed to broad transparency. We therefore will continue to regularly update our employees and stakeholders on our progress and look forward to sharing the results of this work in the future.

Comprehensive Policy Review: The company will implement a broad review of key policies critical to culture and compliance to improve employee ease of access, discoverability and awareness, as well as revisit appropriate policy owners in an effort to improve employee education and experience. HR and Legal will implement this recommendation by June 30, 2023.

Targeted Policy Revisions: The company will revise the Anti-Harassment/Anti-Discrimination policy to incorporate recommended language designed to provide employees with greater clarity on workplace behavioral expectations and investigation process. Similarly, the Conflicts of Interest policy will be revised to provide greater clarity on the definition of familial and romantic relationships and set forth an internal process for disclosure of certain consensual relationships. HR and Legal will implement this recommendation by December 31, 2022.

Broaden Awareness of Legal Rights: The company will amplify existing internal communications about employees' rights to seek external relief for concerns raised under the anti-harassment and discrimination policies. HR and Legal will implement this recommendation by December 31, 2022.

Reinforcing Policies with Expanded Training: The company will enhance and expand existing training of new senior leaders and executives. HR and Legal will implement this recommendation by March 31, 2023.

Reconsideration Process: Currently, the company reviews requests for reconsideration of an investigation when initiated by an investigation party. Going forward, we will formalize and transparently post on an internal webpage available to all employees, the reconsideration process for investigation parties who identify any new information or information that was not considered during the investigation. The Workplace Investigations Team will implement this recommendation by December 31, 2022.

Investigation Survey: In March 2022, the company launched a survey to request feedback from employees who raise and respond to concerns to learn about their investigation experience. We plan to continue surveying investigation parties and will consider additional questions that will help us understand the employee experience. Utilizing survey data, we will consider potential connections to existing listening systems to improve insights and draw connections. Through our Employee Relations Steering Committee, we will create a regular review of aggregated feedback to inform action planning and continued improvements to our investigations process. The Workplace Investigations Team and the Global Employee Relations Team will implement this recommendation by March 31, 2023.

Investigative Process: In 2021, the company developed new investigation outcomes intended to address behaviors that are inconsistent with Microsoft's values of respect, integrity and accountability but do not violate the Anti-Harassment and Anti-Discrimination policy. To create greater clarity with our employees on possible investigation outcomes, we will reconsider the terminology for substantiated policy violations and behavioral concerns. We will also provide additional training for Workplace Investigators and Global Employee Relations specialists on communicating those outcomes. The Workplace Investigations Team and the Global Employee Relations Team will implement this recommendation by December 31, 2022.

Post-Investigation Action and Employee Experience: In 2020, the company developed disciplinary guidelines for more principled and consistent decision-making. We will develop an auditing process to ensure that discipline is implemented and/or that HR follow up occurs by March 31, 2023. To reinforce Microsoft's policy prohibitions against retaliation, we want to ensure that complainants in investigations do not fear negative consequences based on having raised a concern or participated in an investigation. We will take additional measures to make our employees aware of our policies. We will also consider ways in which we can strengthen our internal HR processes and communications to address perceptions of negative career impact. The Global Employee Relations Team will implement an auditing process by March 31, 2023, and additional measures with HR and Legal, also by March 31, 2023.

Leadership Accountability: We will implement changes to emphasize that senior leaders will continue to be held accountable for substantiated policy violations and behavioral concerns. These changes will include more regular reporting and expanded use of data and analytics by the Workplace Investigation Team and Global Employee Relations Team to earlier identify challenges and trends for use by the corporate Senior Leadership Team in discussions on talent, culture and acceptable leadership behavior. We will also develop a new cross-function process for discipline decision-making in investigations involving senior leaders and implement by December 31, 2022. The Workplace Investigations Team and the Global Employee Relations Team will implement the reporting aspect of this recommendation by March 31, 2023.

Training: We will review and enhance our training tools and reminder practices to facilitate the completion of required training by all employees. We will audit to ensure accountability. HR will implement this recommendation by December 31, 2022.

Transparency: As part of our continued efforts to increase transparency regarding investigations, we will continue to report on investigations to the Board. We will also publish anonymized disciplinary action data on an internal investigations webpage, which will include an informational video on our disciplinary guidelines and ranges of outcomes. Further, we will provide greater transparency to complainants by sharing more specificity about disciplinary actions in circumstances where discipline is otherwise not visible. Finally, we will begin annual public reporting on Microsoft's implementation of our sexual harassment and gender discrimination policies, including the total number of reported sexual harassment concerns, percentage of those substantiated, and types of corrective actions taken (which span coaching and training, formal warnings, compensation impacts, demotion, and termination of employment). The Global Employee Relations Team will implement greater transparency with complainants by December 31, 2022. The Global Employee Relations Team will publish anonymized disciplinary action data and an informational video by June 30, 2023. The Workplace Investigations Team will begin annual public reporting by December 1, 2022.

Gender Diversity in Leadership Roles: The company will continue efforts to increase the percentage of women in leadership roles. We will maintain accountability through external and internal aspirational goals and our continued commitment to highlight, discuss, develop and promote our internal and external female talent. The Senior Leadership Team will also continue with regular discussions on talent with a robust focus on growing and developing future senior leaders. We will also continue to focus on our talent sourcing and bench and our development programs to ensure we are continuing to create opportunities for women. This work is already in progress and a focus on diversity will be an ongoing company priority. While there is not an end date for diversity-related aspirations like this, HR will report to the Senior Leadership Team on progress at least quarterly and at the end of the fiscal year.

Data & Systems Improvements: Investigations data and systems currently reside in separate tools. We will improve the coordination of the data systems and analytics by developing a plan for integrated tools for reporting and insights across the Legal Compliance and HR functions. The increased coordination will enable greater insights with speed and informed decision-making regarding investigations and culture. The Workplace Investigation Team and Global Employee Relations Team will implement enhanced coordinated reporting to the Microsoft Board of Directors by December 1, 2022, and improved coordination of data systems and analytics by June 30, 2023.

The leader of the Workplace Investigations Team (WIT) and the leader of the Global Employee Relations Team (GER) will be accountable for implementing the plan, with support from other key senior stakeholders in the Human Resources and Legal functions. Microsoft commits to complete this action plan by the close of fiscal year 2023, with some recommendations completed by the earlier dates noted above. The management implementation team will provide progress updates to the Chief People Officer and the company's Vice Chair and President on a quarterly basis and will provide a completion report to the Board of Directors at the conclusion of the fiscal year.

As a result of this report and actions, we are confident that the implementation of these recommendations will not only address the specific concerns raised by ArentFox's review, but will also allow us to build a stronger, more inclusive and equitable work environment for our employees moving forward. We look forward to continued growth and progress in these areas in order to maintain a culture where all of our employees can thrive.

Today, I am honored to kick off our inaugural Marketplace Summit. For the first time, we are welcoming thousands of independent software vendors (ISVs) and Software as a service (SaaS) providers as we broadcast live from Microsoft Studios to share how we are committed to our shared success by helping ISVs maximize the marketplace opportunity.

The mass migration to the cloud since the pandemic is forcing an inflection point where customers are looking to optimize and streamline their investments — this creates an opportunity for SaaS providers and ISVs. Because of this growing demand for SaaS services, we are firmly committed to provide the right resources, tools and support to take advantage of these opportunities. Today, we are thrilled to announce the global public preview of the ISV Success Program. We are excited to offer this personalized support at the solution level and support every ISV to innovate on the Microsoft Cloud and rapidly go-to-market through the commercial marketplace.

Creating scalable growth models for ISVs Marketplace is at the core of the ISV Success program as we aim to help every ISV not only build on the Microsoft Cloud but go-to-market with Microsoft and sell through the commercial marketplace.

In an uncertain economic environment cloud budgets are durable and still on an upward trajectory. Cloud marketplaces are outpacing cloud growth and are becoming the centralized go-to-market channel of the future. According to Tackle's 2022 annual State of Cloud Marketplaces report, cloud marketplaces continue to generate more revenue year over year and Bessemer Venture Partners noted that in 2021, marketplace transactions grew an estimated 70% to $4 billion, which is 3x faster growth than the public cloud at large.

With the cloud embedded into every business, there is massive opportunity for any company selling SaaS or cloud-based solutions, but it's becoming harder and harder for ISVs to differentiate and acquire new customers. Those who embrace the marketplace and are strategically executing a marketplace-first, go-to-market approach are seeing the most cost-effective growth. At Microsoft the marketplace continues to become central to how we connect customers and partners at scale and ISVs of all sizes and maturity find success when they build a cloud-centric, go-to-market strategy.

"Wiz was born in the cloud and our ecosystem is built around the marketplace motion," says Trish Cagliostro, Head of Worldwide Channels and Alliances at Wiz. "When we work with customers in the marketplace, they usually have cloud consumption commitments through their Microsoft contracts. They can use this to consolidate IT spend and reduce total cost of ownership, which is powerful and gives Wiz the opportunity to have elevated conversations with customers."

To support every ISV in harnessing the growing cloud opportunity, Microsoft is investing in programs and benefits across the Microsoft commercial marketplace. We are making concrete investments in programs that can benefit all ISVs, regardless of their size, scope or history. Over the past year, the marketplace has matured into a thriving commerce environment. We've seen:

288% growth in SaaS-billed sales 319% increase in customers with consumption commitments buying via marketplace 200% increase in private offers sold by Cloud Solution Providers

This momentum continues as customers look to centralize their cloud portfolio through B2B marketplaces to increase efficiency, buy with confidence, and spend smarter. Some of the recent marketplace innovations include:

Enterprise deal making With 95% of the Fortune 500 using Azure, Microsoft has direct access to enterprise customers and through the marketplace, we can unlock access to these customers. We've seen a 164% increase in enterprise sales through the marketplace, and this will continue to grow with our commitment to partner success. Furthermore, Microsoft is unique in that we offer every dollar of cloud spend to count toward a customer's cloud consumption commitment when they buy an eligible solution. This, paired with our enhancements to private offers and multi-year SaaS functionality, is cutting through procurement red tape and closing 7- and 8- digit deals regularly through the marketplace.

Empowering the ecosystem Microsoft has always been a partner-focused organization. As customer needs continue to evolve, Microsoft commercial marketplace is empowering our ecosystem of over 400,000 partners around the world to work together at a faster pace, on a larger scale and on their terms. Earlier this year, we announced the general availability of margin sharing. ISV partners can create private offers and extend a margin to scale through partners in the Cloud Solution Provider program. With margin sharing, SaaS providers and ISVs can extend their sales forces and reach new markets.

To further enhance the ability for all partners to work together, we also announced that multi-party partner private offers will soon be entering private preview. This additional functionality will empower any set of partners to come together, create personalized offers with customized payouts, sell directly to Microsoft customers through marketplace and have the sale count toward the customers' cloud consumption commitment. This will pioneer new opportunities for every partner and we look forward to sharing more details in the coming months.

The value of partnership Only through co-innovating and jointly going to market can we truly meet customers where they are — and that synergy is the goal of the commercial marketplace. According to IDC, partners that build their own software and services are the most profitable high-growth partner business model. They estimate for every $1 of Microsoft revenue, $10.11 is made for partners that build software — 25% more than the estimates for services-led partners.

From the beginning, Microsoft has been a partner-first company. And as Microsoft's portfolio has grown, we've increased our focus to ensure that our partners can access all of the innovation we create through our research and development investments, as well as to deliver their own services and solutions on top of that innovation.

Today marks an important milestone as we commit to supporting every ISV in their growth with Microsoft with the ISV Success Program and the variety of other tools and resources we outlined to support ISV success. For more details, Anthony Joseph, vice president for Marketplace and ISV Journey, covers the opportunity with the ISV Success Program on the Azure blog channel. We are excited to work side-by-side with the ISV and SaaS community to realize all the potential of building on the Microsoft Cloud and selling through the commercial marketplace.

Although supply chain disruption is not new, its complexity and the rate of change are outpacing organizations' ability to address issues at a global scale. Many solutions today are narrowly focused on supply chain execution and management and are not ready to support this new reality. Enterprises are dealing with petabytes of data, spread across legacy systems, Enterprise Resource Planning programs (ERPs), and custom solutions, resulting in a fragmented view of their supply chain. An organization's supply chain agility and resilience will be determined by how they can connect their data and orchestrate actions across all relevant systems to quickly respond to changing market conditions. To be truly agile, supply chain software needs to increase visibility across data sources, predict and mitigate disruption, streamline collaboration, and fulfill orders – sustainably and securely. Today we are announcing the Microsoft Supply Chain Platform, which helps organizations maximize their existing supply chain investments with an open approach, bringing the best of Microsoft AI, collaboration, low code, security, and Software as a Service (SaaS) applications to a composable platform. As part of the new Microsoft Supply Chain Platform, we are also announcing the preview of Microsoft Supply Chain Center. The Supply Chain Center empowers customers to get started quickly with a ready-made "command center" that natively works with existing supply chain data and applications. The Microsoft Supply Chain Platform: An open, collaborative and composable foundation for data and supply chain orchestration Across Microsoft, we deliver a breadth of capabilities for organizations such as Grupo Bimbo, Mercedes Benz and Tillamook to build their own custom supply chain solutions. With today's announcement, we are making it easier for customers to leverage the full Microsoft Cloud to enhance their supply chain. The Microsoft Supply Chain Platform provides building blocks across Microsoft Azure, Dynamics 365, Microsoft Teams and the Power Platform for customers to adopt capabilities that address their specific supply chain needs. With Dataverse, customers can use hundreds of data connectors to gain visibility and act across their supply chain. The Power Platform also enables custom workflows, data reporting, and applications that connect to existing Supply Chain systems. Partnering to empower customers in supply chain transformation The Microsoft partner ecosystem will continue to play a critical role in enabling customer supply chain resiliency and agility. With the Microsoft Supply Chain Platform, partners can bring their industry and domain expertise to create integrated solutions leveraging Dynamics 365 Supply Chain Management, Microsoft Azure, Microsoft Teams and the Power Platform. A rich supply chain partner ecosystem includes advisors and implementers like Accenture, Avanade, EY, KPMG, PwC and TCS. In addition, to help customers find the best solution for their supply chain needs, the Microsoft Supply Chain Platform connects to Blue Yonder, Cosmo Tech, Experlogix, Flintfox, InVia Robotics, K3, O9 Solutions, SAS, Sonata Software, To-Increase and many more. Accelerating business agility and efficiency with the Microsoft Supply Chain Center At the core of the Supply Chain Platform is the Microsoft Supply Chain Center, now available in preview. The Supply Chain Center provides a "command center" experience for practitioners to harmonize data from across existing supply chain systems, like Dynamics 365 and other ERP providers including SAP and Oracle, along with stand-alone supply chain systems. Data Manager in Supply Chain Center enables data ingestion and orchestration to provide visibility across the supply chain and drive action back into systems of execution. During preview, our launch partners C.H. Robinson, FedEx, FourKites and Overhaul, will offer native experiences within the Supply Chain Center. Existing Dynamics 365 Supply Chain Management customers will automatically gain access to the Supply Chain Center as part of their current agreements. The Supply Chain Center includes pre-built modules that address supply chain disruptions across supply and order fulfillment, as outlined below:

The supply and demand insights module uses advanced Azure AI models to predict upstream supply constraints and shortages through supply intelligence. Organizations can perform simulations using data from their supply chain network to predict being out of stock, over-stocking or missed-order lines. Combined with smart news insights, which provides relevant news alerts in the Supply Chain Center on external events, supply chain practitioners can make decisions and plan with real-world event information and historical insights for product demands. The order management module in Supply Chain Center intelligently orchestrates and automates fulfillment with a rules-based system using real-time omnichannel inventory data, AI and machine learning. Organizations can quickly meet future order volumes and fulfillment complexities by extending their capabilities with pre-built connectors to specialized technology partners for order intake, delivery and third-party logistics services. Existing Dynamics 365 Intelligent Order Management customers will automatically get access to Supply Chain Center and the order management module at launch. With secure, built-in Teams integration, customers can mitigate supply constraints by collaborating with external suppliers in real time to secure new supply sources, troubleshoot transportation issues and communicate up and downstream impacts based on changes. By having partner modules built into the Supply Chain Center, customers can easily unlock specific solutions like freight visibility from Overhaul. Since everything runs off Dataverse, the data is consistent no matter what module you are using. This eliminates pasting information back and forth and reconciling which reports have the most up-to-date information.

We're excited to announce Daimler Trucks North America, iFit and Kraft-Heinz as some of our Supply Chain Center preview customers:

"We have hundreds of thousands of parts that we are sourcing from suppliers globally. Our customers, dealers and partners expect excellence in all we do, which also means looking closely at all aspects of our supply chain. The Microsoft Supply Chain Platform and Supply Chain Center will enable us to make processes more efficient end-to-end." – Lutz Beck, CIO, Daimler Truck North America "We are optimizing transportation costs by leveraging AI-powered capabilities within the Microsoft Supply Chain Platform and Supply Chain Center to recommend inventory allocation. The actionable insights from the Supply Chain Platform trigger freight planning and execution to support order fulfillment for our customers." – Robert Critchley, VP Transportation and Warehousing, iFit

"Our collaboration with Microsoft is a critical piece of our supply chain transformation. The platform gives our team the ability to be more agile – assessing risk and opportunities faster than ever before. Simply put, it provides the end-to-end visibility we need to keep our products on people's tables across North America, and there's nothing more important." – Mitch Arends, Senior Vice President, Head of Operations, Kraft Heinz North America

YouTube Video Click here to load media Building sustainable supply chains with Dynamics 365 and Microsoft Cloud for Sustainability Sustainable supply chains are one of the biggest opportunities for organizations to reduce their carbon footprint. However, it is one of the most challenging areas to address because decision makers lack the data and insights to optimize the selection of suppliers, routes, fulfillment and products. Using capabilities from Dynamics 365 Supply Chain Management and Order Management in the Supply Chain Center, customers can make sustainable decisions on product sourcing, package-free returns, transportation logistics, manufacturing and warehouse automation and more. Last month at Microsoft Ignite, we introduced new capabilities in Sustainability Manager, a Microsoft Cloud for Sustainability solution, which enables organizations to reduce indirect value chain emissions (also known as "Scope 3" emissions), which account for a disproportionate share of most organizations' carbon footprints. The solution already includes prebuilt calculation methodologies for more than half of the 15 categories of Scope 3. Reimagine your supply chain with Microsoft We believe supply chain agility is critical to persevere in today's economic climate. This means understanding and taking real-time actions based on supply chain data, securely and with sustainable business practices. We're excited to deliver on this first step in our commitment to reimagine the supply chain with our customers and partners. Learn more To learn more about these new solutions, join us at Supply Chain Reimagined, a free digital event on Nov. 16, 2022. You'll learn how to enhance your supply chain to delight your customers, empower your workforce, and boost operational efficiency to reduce your costs. You'll also hear real-world best practices from Microsoft and industry leaders. Plus, ask the experts your questions during the live Q&A chat — register now. Tags: Microsoft Supply Chain Platform, supply chain

A critical priority for the U.S. Department of Defense (DoD) is accelerating the speed at which information is collected, analyzed and delivered to where it is needed to support mission goals. Satellites are increasingly playing an important role in this ability to harness and move data around the world, but limitations in the current government architecture can still present challenges to getting information where it's needed, when it's needed. To solve this, the DoD's Defense Innovation Unit (DIU) has partnered with the U.S. Space Force in developing a Hybrid Space Architecture (HSA), designed to rapidly scale available space resources and bring together the best capabilities across government and commercial industry to establish ubiquitous, resilient connectivity and insights for mission leaders.

Today, DIU announced that Microsoft has been selected to bring our advanced, trusted cloud and innovative Azure Space capabilities, alongside a space partner ecosystem, to serve as a foundation to realize their Hybrid Space Architecture vision.

The HSA is essentially DoD's effort to build an internet in space and will support the Department's goal of establishing information advantage for national security. By collaborating with commercial partners, HSA vastly expands its range of satellite and space systems across diverse orbits, ground stations and communication paths to provide "secure, assured and low-latency data communications anywhere on and off Earth."

The HSA will be built in part upon Microsoft Azure and will leverage key capabilities from our suite of Azure Space solutions, including Azure Orbital Cloud Access and Azure Orbital Ground Station. This will be supported by Microsoft's private global wide area network, which provides resilient global data transport from ground stations to the appropriate cloud destinations. Combined, these capabilities bring together key functions of the HSA including remote sensing, multi-path communications and cloud computing.

Beyond accelerating the flow of data, a key innovation of the hybrid space architecture is accelerating the speed to insights of that data. Space is one of the largest untapped data lakes (central repositories of data) in the world and mission owners will benefit from making sense of massive volumes of data to better inform decision making, beyond what they have the power to do today. With Azure, the DIU will be able bring the best practices in commercial innovation to the HSA to utilize AI and machine learning to harness data at speed and scale for operational advantage and increased efficiency — at all security classification levels.

Underpinning this effort is our focus on cybersecurity, building on Microsoft's commitment to advance cybersecurity in space. With a broad view of the threat landscape — informed by 43 trillion threat signals analyzed daily, combined with cutting-edge AI and the human intelligence of our more than 8,500 experts, including threat hunters, forensics investigators, malware engineers and researchers — we see firsthand what organizations are facing and we're committed to protecting our nation from rapidly evolving cyberthreats. Microsoft deploys these industry leading cybersecurity solutions across all devices, clouds, apps and platforms that are continuously updated with the latest threat intelligence insights to provide advanced, timely defenses. Our goal is to ensure that not only Microsoft, but also our partners, can adhere to the same high security standards, and can meet and exceed the U.S. government's security requirements while keeping pace with the evolving cyberthreat landscape.

Another key enabler in our approach to HSA is tapping into our extensive Azure Space partner ecosystem. We have developed partnerships with leading space industry companies to support the goals of HSA. Developing relationships with these partners helps the industry and government further establish a standardized approach to connectivity and ensure system interoperability across the platform. We are bringing this wide-ranging connected ecosystem to bear to support the HSA vision.

The Hybrid Space Architecture program is a demonstration of what's possible when government and industry work closely to tackle the biggest challenges in the universe — crafting a transformational platform to support space-based national security missions. By bringing the latest Azure Space capabilities to this project, establishing consistent standards across industry, and collaborating with numerous industry partners, we're demonstrating the art of possible to support advanced and secure space-based data connectivity and analytics.

Learn more about how Azure Space is combining the possibilities of space with the power of the cloud.

Today I am sharing Microsoft's 2022 Global Diversity & Inclusion Report, our fourth annual report and our ninth year of releasing our global workforce demographic data. This year's data shows that globally we are a more diverse Microsoft overall today than we have ever been, with the highest year-over-year representation progress of the past five-year period for many employee communities. We're motivated by our ongoing progress, and this year's growth feels particularly meaningful in the context of significant challenges around the world. As one of the most transparent companies of our size when it comes to the diversity and inclusion data we share, we are continually evaluating where we are now and where we aspire to be. This year, in addition to the extensive data we usually share, we're adding new data on U.S. populations that include multiracial employees and those with military experience, as well as data on workforce exits by women and men globally and race and ethnicity in the U.S., and more detail on representation of women worldwide by geographic regions. We are also sharing additional pay analysis data to further highlight the opportunity for us to continue to make progress on representation at all levels of the company. Key highlights:

For the first time, women now make up more than 30% of the Microsoft's core* workforce worldwide at 30.7%, up 1.0 percentage point since last year. Since 2018, representation of women has grown at least 1.0 percentage point every year. At Microsoft, we are committed to the principle of pay equity. Pay equity accounts for factors that legitimately influence total pay, including things like job title, level and tenure. As of September 2022:

Inside the U.S., all racial and ethnic minorities who are rewards eligible combined earn $1.008 total pay for every $1.000 earned by U.S. rewards-eligible white employees with the same job title and level and considering tenure. Inside the U.S., women who are rewards eligible earn $1.007 total pay for every $1.000 earned by rewards-eligible employees who are men and have the same job title and level and considering tenure. Outside the U.S., women who are rewards eligible earn $1.002 total pay for every $1.000 earned by men who are rewards eligible with the same job title and level and considering tenure in the combined geographies we report on.

Employees from racial and ethnic minority communities now make up 53.2% of Microsoft's core U.S. workforce, up 1.9 percentage points from last year. Representation across Asian, Black and African American, Hispanic and Latinx, and multiracial employee populations in our core U.S. workforce has increased for each group since last year:

Asian representation has grown by 0.3 percentage points to 35.8%, and has grown by 3.9 percentage points since 2018. Black and African American representation grew 0.9 percentage points to 6.6%, the highest year-over-year increase in the past five years. Hispanic and Latinx representation grew 0.6 percentage points to 7.6%, the highest year-over-year increase in the past five years. Multiracial representation is 2.6%, up 0.1 percentage point from last year.

We're energized by our strides toward fulfilling our commitment made in 2020 to double the number of Black and African American and Hispanic and Latinx people managers, senior individual contributors and senior leaders in the U.S. by 2025 as part of our Racial Equity Initiative.

For Black and African American people managers (below Director level), we're 116.0% of the way to our 2025 commitment. For Black and African American Directors, Partners and Executives — including people managers and individual contributors — we're 92.0% of the way to our commitment. For Hispanic and Latinx people managers (below Director level), we're 46.5% of the way to our commitment. For Hispanic and Latinx Directors, Partners and Executives — including people managers and individual contributors — we're 57.6% of the way to our commitment.

7.8% of employees in the U.S. self-identified as having a disability. This is an increase of 0.7 percentage points from last year.

This year, we added new reporting to continue to evolve how we reflect the many dimensions of identity within our workforce and to deepen transparency on our progress. For the first time, we're sharing:

New dimensions of self-identification: We've added details on the population of U.S. employees who identify as multiracial. Increasing employees' options to be more specific about their racial and ethnic identities helps us gather more actionable data, and we've heard from employees that having these options can help people feel more seen and included in the workplace. We've also expanded options for Asian employees in the U.S. to identify their backgrounds in further detail. The Asian community is the single largest racial and ethnic minority group within our company, encompassing more than 20 sub-identities. We aim to expand the detail we share in future reports once we have representative participation in this additional layer of self-identification. Representation of U.S. employees with military status: Microsoft employs thousands of veterans and reserve service members around the world, and we offer employees the choice to identify as a person with military experience in 38 countries including the U.S. This data helps us to create more visibility for members of the Microsoft military community and better understand and support the diversity of our workforce. This year's report shows that 4.7% of U.S. employees in our core Microsoft business self-identified as having served the U.S. Armed Forces or as having Protected Veteran status, a 0.4 percentage point increase from 2021. We aim to expand the detail we share in future reports once we have representative participation globally. Workforce exits data: This year, for the first time, we're reporting data on workforce exits of employees who have left Microsoft voluntarily or involuntarily. This reporting shows exits representation has declined 0.2 percentage points year over year for women globally. In the U.S., for Black and African American employees, exits representation has declined 0.3 percentage points year over year. Exits representation in the U.S. rose for Asian (5.1 percentage points), Hispanic and Latinx (0.4 percentage points), Native American and Alaska Native (0.2 percentage points), and multiracial (0.3 percentage points) employees since last year. Exits representation for Native Hawaiian and Pacific Islander employees was unchanged year over year. Additional pay data: We have reported on pay equity since 2016 in support of our commitment to pay employees equitably for substantially similar work. While pay equity is a critical factor, it is only one factor in how we think about progress. To further highlight the opportunity to continue to make progress on representation at all levels of the company, we have chosen this moment to be even more transparent and also voluntarily disclose the unadjusted differences in median total pay for women inside and outside of the U.S., and for racial and ethnic minorities, Asian, Black and African American, and Hispanic and Latinx employees in the U.S. The median is the middle value of a data set. Therefore, median pay for a group of employees represents the value where half of the employees in that group are paid higher than that point and half of the employees in that group are paid lower. This analysis allows us to surface differences in median pay when we don't adjust for things like job title, level, and tenure. As we continue to increase representation for women and racial and ethnic minorities at more senior levels, and continue to ensure pay equity for all, the gap between the medians will reduce. View additional pay data on page 30 of the report.

This year's report also explores how we innovate for inclusion, setting the standard on accountability and transparency, advocating for employees and communities globally, and continuing to broaden career pathways into our industry.

YouTube Video Click here to load media The work of diversity and inclusion at Microsoft isn't about a pledge or a performance, but about being deeply connected to the powerful impact on our lived experience when the people we spend our workdays with value inclusion and embrace difference. This is one of the reasons we are encouraged by what our employee sentiment measurement tells us. This year, for instance, when asked whether they feel included in their teams, employees responded to this companywide survey question with an average score of 86 globally. As we maintain our momentum on increasing representation and strengthening our culture of inclusion, we'll continue to listen, we'll continue to learn and just as importantly, we'll continue to act. *Core Microsoft business represents 83.9% of the worldwide broader Microsoft workforce and does not include our minimally integrated companies. Tags: allyship, Diversity and Inclusion Report

Photo credit: Weiquan Lin/Getty Images It was great to connect with so many of you in person and virtually at Ignite, Microsoft's annual gathering of developers and IT professionals. While attendees acknowledged the business impact of current macroeconomic challenges, they remain committed to maximizing the value of their digital investments by generating economies of scale with the Microsoft Cloud. Doing more with less is more important than ever as organizations are navigating uncertainty and shaping the future. Cloud-powered technologies, such as artificial intelligence (AI), the Internet of Things (IoT), and machine learning, offer organizations unparalleled agility and efficiency, accelerate innovation, and make security comprehensive while driving growth and advancing sustainability commitments. At Ignite, we showed how organizations help people use digital capabilities to enhance their personal and professional lives: The German carmaker Mercedes-Benz is simulating and refining its vehicle production processes infinitely in the Microsoft Cloud to make its car production 20% more efficient by 2025. To provide mutual customers with more choice, Cisco and Microsoft are providing the option to run Microsoft Teams natively on Cisco Room and Desk devices. 3M is offering its Post-It® App on the Teams App Store, allowing users to digitize handwritten notes. U.K.-based Haleon is enabling people who are blind or have low vision to hear the labels of more than 1,500 consumer health products in the U.S. and U.K. with Microsoft Seeing AI. With a reimagined global app running on Azure, the NBA is providing fans with new personalization features like wall-to-wall content from every NBA game and unprecedented behind-the-scenes access to players and teams. And just last week, Switzerland-based global bank UBS announced it would move over 50% of its applications to Azure as its primary cloud platform to further drive the modernization of its global technology estate. Mercedes-Benz and Microsoft: New MO360 Data Platform makes car production more efficient, resilient and sustainable. From left, Judson Althoff, Executive Vice President and Chief Commercial Officer of Microsoft, Jan Brecht, Chief Information Officer of Mercedes-Benz Group AG, Jörg Burzer, member of the Board of Management of Mercedes-Benz Group AG, responsible for Production & Supply Chain Management. Industry leaders unlock economies of scale with public cloud commitments The iconic motor racing brand McLaren Automotive is committed to exceptional customer service and is using Azure for its production and development systems, unlocking cost savings of up to 70% by reserving resources for dedicated use on a one- or three-year basis. Global manufacturer Mars runs approximately 10,000 cloud flows and 4,000 apps and is using Power Platform to automate processes, including enterprise-wide approvals. Telecommunications company O2 Czech Republic is reducing the total cost of ownership by 30% for each workload it moves from its on-premises infrastructure to Azure. Taiwan-based manufacturer Inventec is improving production efficiency with Azure IoT, machine learning and 5G services, saving 50% on deployment time for its AI and auto optical inspection application and 25% on maintenance efforts. With Power BI, Denmark-based GN has raised the quality of its data by approximately 50% in only three quarters. With its cloud migration journey still underway, Greece-based energy company HELLENIC PETROLEUM Group has already seen a 30% reduction in IT infrastructure costs after migrating 50% of its workloads to Azure. Bahrain Public Transport Company is delivering increased punctuality and reliability to its customers while reducing costs by 30% with Azure. Manufacturing and transportation leaders drive efficiencies with the Microsoft Cloud. Organizations reinvent their operations for a greener future Coca-Cola Hellenic is embracing the industrial metaverse with HoloLens and Microsoft Mesh to visualize its bottling facilities and gain visibility across production lines. The technology also allows the company to reduce its energy consumption based on insights into the operating status of machines and its carbon dioxide emissions. Equinor is designing products and simulating their effectiveness before it implements them in the physical world. With Microsoft Energy Data Services and Azure, the Norway-based energy company is breaking down data silos to streamline operations for a low-carbon future. Germany-based energy company E.ON is accelerating the rollout of sustainable district heating and cooling grids by training its models 50% faster with Azure Data Factory and Azure Machine Learning. Telstra is looking to help enterprise, government and small business customers in Australia accelerate their sustainability progress and business growth through data insights gained with Microsoft Cloud for Sustainability. Canada-based Ontario Power Generation is choosing Azure to modernize its applications and IT infrastructure and eventually reduce its carbon footprint by accelerating its cloud migration and transforming business operations. The company has also joined the CEO Pledge initiative to commit its technology to the Government of Canada's Computers for Schools Plus program.

YouTube Video Click here to load media AI is the bridge to re-engage people across physical and digital worlds U.K.-based Barclays is deploying Microsoft Teams to bring its employees worldwide closer together. Unity is working with Microsoft to empower digital creators, 3D artists and developers to build and run real-time 3D experiences on Azure, making it easier to publish to Xbox consoles and PCs so they can reach gaming and non-gaming communities. Nissan is choosing HoloLens 2 and Dynamics 365 Guides to help its operators learn and reduce the workload on instructors. Luxembourg-based SES is leveraging Viva Insights to cut down 100,000 meeting hours a year with meeting-free Wednesdays already making 70% of SES employees feel more productive. ESPN employees are taking advantage of a virtual community for career development with games and real-time analytics built with SharePoint spaces and Microsoft Power Platform low-code capabilities. Australia-based Bank of Queensland is using Microsoft Cloud for Financial Services and Microsoft Surface devices to provide its customers and bankers with more personalized and intuitive experiences. Barclays is enabling employees around the globe to connect without friction with Microsoft Teams. Digital capabilities enable social inclusion and support education India-based SEEDS is applying an AI model to generate heat wave risk information for vulnerable communities. Non-profit NWEA is creating ways for improved accessible math assessments for students with vision disabilities. The Welsh Government is holding multilingual meetings with the Language Interpretation feature in Microsoft Teams to enable citizen participation. With Azure Cognitive Services, Twitter is making live audio conversations accessible to users who are deaf or hard of hearing, and Swedish national public television broadcaster SVT is improving public news accessibility across 21 regions. Blind Veterans UK is bringing together veterans of different ages for peer support with Microsoft Teams, which is accessible through their mobile devices and landlines. To foster digital equity and facilitate the search for jobs, education and access to healthcare, the Seattle Housing Authority is equipping residents with Surface Laptop Go devices. The Seattle Housing Authority is providing convenient access to digital tools. Winning the good fight requires relentless security commitments The Queensland Department of Education in Australia has secured 271,000 devices and servers in just six weeks with Microsoft Defender for Endpoint. Agribusiness and food company Land O'Lakes is turning to Microsoft Defender for Cloud and Microsoft Sentinel as the recipe for multicloud protection. With Microsoft Purview, the Prince William County School District in the U.S. is ensuring confidential information is properly handled and harmful content kept away from students. The State of Michigan Office of Children's Ombudsman is enhancing data integrity and automating processes to investigate complaints regarding Michigan's child welfare system more effectively. Brazil-based Grupo Fleury is deploying Microsoft 365 to bring advanced security and compliance controls to the company's internal communications. FUJIFILM Group is introducing Microsoft 365 as the global communications foundation to deliver security without compromising usability. Microsoft Intune enables a unified experience for the Group's more than 10,000 employees. Queensland Department of Education serves more than 580,000 students. Despite economic, societal challenges, organizations across industries continue to adapt to evolving market dynamics to meet customer needs. Their solid digital foundation has gone a long way in helping them innovate quickly and deliver critical business outcomes. As their business and technology advisor, Microsoft is incredibly proud to see how many of our customers have prospered and even thrived in the current environment, and we are looking forward to co-innovating with them to lay the groundwork for their future success. Tags: AI, automation, Azure, Dynamics 365, HoloLens, manufacturing, metaverse, Microsoft 365, Microsoft Defender, Microsoft Intune, Microsoft Mesh, nonprofit, Power Platform, Security, Surface, sustainability

Last week, we announced our new creator tools in Microsoft 365, Microsoft Bing and Edge, designed to help ignite people's imaginations and express themselves. Today, Image Creator from Microsoft Bing begins rolling out in a limited preview to select markets and will be coming soon to Microsoft Edge. To learn more about how Image Creator works, check out this post from the Bing team.

As I mentioned in last week's announcement, we are in the early stages of bringing DALL.E 2 by OpenAI to these experiences. We are in limited preview with the Microsoft Designer app and today we are beginning to roll out Image Creator in Bing to a small set of geographies. We are intentionally taking our time to ensure we gather feedback, learn and apply those learnings before rolling things out more broadly.

The rollout of DALL∙E 2 across Microsoft products and services reflects how the company's investment in AI research is helping to thoughtfully infuse AI into everything it builds, produces and delivers to help everyone boost productivity and innovation. And we recognize there's still important work to do. At Microsoft, our teams are focused on the development of tools and techniques that guide people toward the appropriate and responsible use of AI tools, such as DALL∙E 2 in Image Creator. And to curb misuse, we are implementing precautions and moderation policies.

For example, OpenAI has made their content filters more robust over time so that they are more effective at blocking images that violate their content policy – which is designed to block generations that include violent, adult or political content, among other categories – while still allowing creative expression. In addition, we will not show any of the text strings used as input within the images created and we may distort faces, all as part of our commitment to responsible AI. We are working closely with OpenAI to build, test and review mitigations for our integrations.

As we begin this journey together, we look forward to working with our customers, researchers and the global community to continuously improve and we welcome feedback, ideas or concerns. We're excited about the potential this technology holds to help everyone better express themselves and their ideas.

The past few years have been a period of dramatic change, with disruptions to our society, the global economy, political landscape, technology and the way we work. As organizations adapt to new challenges, they are turning to their technology providers to help them succeed.

Against this backdrop, at this year's Microsoft Ignite we are focused on how Microsoft Cloud is helping customers do more with less. In today's uncertain economic environment, companies are looking to change the way they do work to get better results, with less friction and more choice.

Microsoft Ignite is a showcase of the best of what's next for Microsoft and its partners, technical decision-makers and information technology implementers. The products and updates unveiled during Microsoft Ignite are designed to help customers, partners and developers get the full value of Microsoft's technologies.

At every layer of the tech stack – whether it's infrastructure, data, hybrid work, developer tools or security – we believe the Microsoft Cloud offers best-of-category products and best-of-suite solutions to help organizations be more secure, and to help their workers complete their tasks more easily and effectively.

We know that things become most useful to businesses when they become part of ordinary workflows, which is why we've made huge investments in our overall AI technology, in Microsoft 365, in our developer tools, in the Microsoft Cloud and in our security offerings. These are all things that are important to our customers, and so we're aligning our resources to better help them.

Now, let's dive into some of the more than 100 services and updates being announced at Microsoft Ignite.

When we introduced the Microsoft Intelligent Data Platform earlier this year, we set out to unify Microsoft's database, analytics and data governance offerings so our customers didn't have to. We removed a major pain point around stitching together fragmented data workloads, enabling customers to gain quicker business insights and spend time on what matters most. That platform is now expanding, with 12 partners joining the ecosystem.

Our enterprise-grade, highly scalable, serverless distributed database Azure Cosmos DB already helps mission-critical applications of any size or scale deliver speed and availability to users anywhere in the world, increasing geographic availability, scalability and distribution.

Now, we're adding distributed database support for PostgreSQL, a popular open-source database engine. This means PostgreSQL developers can take advantage of the speed, scale and performance of Azure Cosmos DB, and open-source developers can access both structured and unstructured data in one familiar database service.

With our hybrid and multicloud offerings, we're enabling customers to take advantage of the capabilities of Azure wherever they are, whether that's new functionality to deploy Azure Kubernetes Service on-premises or at the edge with Azure Arc. We're also continuing to invest in the infrastructure that carries it all, with Azure public multi-access edge compute. It's helping operators like AT&T deliver 5G in U.S. cities.

We're also announcing Visual Studio images for Microsoft Dev Box, a service in preview that provides developers with cloud-based, ready-to-code workstations. With Visual Studio available as soon as a developer creates a dev box, developers can save time and focus on coding what matters.

Azure OpenAI Service, part of the Azure AI platform, will offer invite-only access to DALL∙E 2, a model that allows users to generate custom images using text or images. The addition of the DALL∙E 2 model expands the breadth-of-use cases Azure OpenAI Service customers can employ with the security, compliance and responsible AI guardrails that Azure provides. Select customers will be able to generate content, images and code to bring visual ideas to life – all from inputting simple text.

With Power Automate, Microsoft is simplifying workflows using the power of AI in everyday natural language. Users can create a cloud flow that corresponds to the request made using natural language. Here, AI helps to turn low-code into no-code, making it easier for more people to build different types of automation faster.

The introduction of Microsoft Syntex integrates offerings from across the Microsoft Cloud, from Microsoft 365 and Azure to Power Platform and Microsoft Purview. Syntex automatically reads, tags and indexes high volumes of content and connects it where it's needed in context – in search, in applications and as reusable knowledge. It puts people at the center, with content integrated into collaboration and workflows like eSignature.

Microsoft Intelligent Document Processing, powered by AI Builder and Power Automate, enables end-to-end document processing with no code required. Processing documents can be a cumbersome task and AI Builder automates this process, giving workers time back to focus on more high-priority work.

With more sellers on the road and in digital meetings, we're making conversation intelligence free for Dynamics 365 Sales Enterprise and the newly released Viva Sales. Conversation intelligence gives sellers the power of insights while helping them be more productive by using analytics and data science to help sellers cut down time spent on administrative work and spend more time with their customers.

At Microsoft Ignite we're introducing Microsoft Places, a new workplace app that optimizes the use of physical space. According to our recent Work Trend Index, more than 80% of employees go into the office for each other. Places provides insights into when colleagues are coming into the office or what meetings are best suited for in person.

You'll also see intelligent recap, which is like having a virtual assistant at every meeting. It will use AI to assign tasks from meetings, automatically generate chapters from the meeting and share personalized insights from the recording, enabling users to discover information that matters most to them. Intelligent recap, along with other new features, will be delivered as part of the new Teams Premium add-on, purchased separately.

With Microsoft Edge we are continuing to invest in distinct experiences that make the workday easier. Microsoft Edge Workspaces is a shared set of browser tabs where groups can view the same websites and latest working files in one place. For example, let's say a new team member is being brought on to an existing project. Instead of overwhelming them with links and files over email, the team can share one link to their Microsoft Edge Workspaces, which has all their relevant files and links open as browser tabs. As the team works together, tabs are updated in real time so everyone stays on the same page.

Comprehensive security means helping protect everyone wherever they choose to work

The new Defender for DevOps tackles one of the biggest problems faced by the tech industry – new and legacy code not written with security in mind, which can be rapidly exploited by attackers. Organizations can secure applications as they are being built, giving developers the assurance that when they get code from external sources or are working with large application teams, they know they can get ahead of security vulnerabilities and deliver a more secure solution.

We are also offering Power Platform Managed Environments, which enables organizations to introduce low code broadly to spur innovation with the knowledge that IT admins maintain governance and security capabilities over low-code implementations, allowing teams to create at scale without security concerns.

Microsoft is helping organizations build the identity system of the future, a connective network that enables people, organizations, apps and even smart devices to make real-time access decisions.

With Microsoft Entra we are introducing identity and access management solutions designed to securely work across multiple cloud platforms. Microsoft Entra Identity Governance helps organizations ensure the right people have the right access to the right resources at the right time. Within Entra Identity Governance, Workload Identities controls and secures identities for digital workloads, such as apps and services, and controls user access to cloud resources, while certificate-based authentication enables customers to adopt easily deployable phishing-resistant authentication.

To learn more about announcements being made at Microsoft Ignite, be sure to watch the sessions from Satya Nadella, Scott Guthrie, Panos Panay, Judson Althoff, Alysa Taylor and Vasu Jakkal. In addition, more than 100 product announcements can be found within the Book of News, and you can also explore the product specific blogs below for a more granular view of the news and learn more about how this technology is being used today.

How Microsoft Azure helps drive agility and optimization for your business

From Hot Wheels to handling content: How brands are using Microsoft AI to be more productive and imaginative

Re-energize your workforce at home, in the office, and everywhere in between

5 top cybersecurity capabilities announced at Microsoft Ignite to help you secure more with less

As companies and employees settle into new work norms post-pandemic, we're living through a once-in-a-lifetime shift in work patterns. Whether work happens from home, at the office, in a warehouse or on the go, work is no longer a place, but an experience that transcends place and time.

Over the past two-and-a-half years, new patterns of work have emerged that put people, communications, and workflows at the center. In that time, Microsoft Teams has grown to 270 million monthly active users, becoming the most ubiquitous platform for workplace collaboration and business processes. Teams' growth is a sign of how work has changed, and where it's going.

People crave deeper, richer ways of collaborating and co-creating from wherever they are. We believe that in the not-so-distant future, the metaverse will play a key role in providing new ways of connecting. In fact, our Work Trend Index data shows that 50% of Gen Z and millennials envision doing some of their work in the metaverse in the next two years.

As a platform company, our approach to workplace productivity and collaboration is to ensure the software experiences we deliver can benefit users on all their favorite devices. In that spirit, today we are announcing several ways we are partnering with Meta to provide customers with more choice and security as they venture into the metaverse.

First, we are bringing Mesh for Microsoft Teams to Meta Quest devices. Mesh for Teams builds on years of research and Microsoft Cloud innovation, from Azure Digital Twins to Dynamics 365 Remote Assist and Teams video meetings. It is designed to help people gather virtually in the place where work happens on any device, including smartphones, laptops and mixed-reality headsets. Mesh for Teams with Meta Quest Pro and Meta Quest 2 devices will enable people to connect and collaborate as though they are together in person.

In addition, Microsoft 365 apps will be available on Meta Quest devices, enabling people to interact with content from their favorite productivity apps including Word, Excel, PowerPoint, Outlook, and SharePoint within VR. In the future, Windows 365 will also be available on Meta Quest devices, with the ability to stream a Windows Cloud PC including your personalized apps, content and settings.

Microsoft Intune and Azure Active Directory will support Meta Quest Pro and Meta Quest 2, enabling IT administrators to provision these devices for users with a Quest for Business subscription from Meta, with confidence so that the security and management options they expect from PCs and mobile devices will carry over to VR.

Finally, Microsoft and Meta are exploring ways to bring Xbox Cloud Gaming to Meta Quest Store, allowing gamers to stream hundreds of high-quality Xbox games to phones, tablets, PCs, select smart TVs, and the Meta Quest platform.

This partnership complements our commitment to HoloLens, and we are excited to continue evolving our mixed-reality devices roadmap, leading with our cloud-first approach, and with Windows and Microsoft 365 at the fore. All of this advances our efforts to build the foundations of a responsible metaverse, with security, compliance and enterprise-grade experiences as foundational to how we will continue to unlock new possibilities for the future of work.

To learn more about our partnership and additional announcements, please watch the Meta Connect keynote.

This morning in London, I sat down with Satya Nadella and Ryan Roslansky, LinkedIn's CEO, to discuss how leaders can empower employees — and drive business impact — in the face of economic uncertainty. No matter how your people work, from remote to in-person to somewhere in between, understanding your workforce is more important than ever.

The world has changed, and anyone who thinks we're going back to the way we worked in 2019 is sorely mistaken. It's become clear to us that work is no longer a place, but an experience that binds employees and leaders across home offices, company headquarters, time zones and more. Employees around the world have embraced flexibility and hybrid work schedules, reveling in the freedom to work where, when and how they want. At the same time, many leaders still crave the familiarity of the traditional office where brainstorming happened in person and chance encounters in the hallways led to unexpected collaboration. In short, there's a disconnect between employers and employees. And in an unpredictable economy, that disconnect is only growing wider.

The solution? A new approach to employee engagement, one that embraces the digitally connected, distributed workforce, realigns employees to business priorities and re-onboards them to the company culture and mission. In our latest Work Trend Index Pulse Report, out today, we offer three urgent pivots leaders must make to bridge the divide — and help employees not only do their best, but be their best.

"Thriving employees are what will give organizations a competitive advantage in today's dynamic economic environment," said Satya Nadella, Microsoft Chairman and CEO. "Today, we're announcing new innovations across our employee experience platform Microsoft Viva to help leaders end productivity paranoia, rebuild social capital, and re-recruit and re-energize their employees."

Many leaders used to rely on the visibility of the office to assess organizational productivity, hearing the click of keyboards and seeing the long hours at desks. In the shift to hybrid work, 85% of leaders say they have a hard time knowing for sure that their people are being productive. Yet whether they were working remotely, in person, or a mix of both, 87% of employees say they are productive at work. That fear has led some companies to institute digital monitoring, tracking trivial metrics to assuage their angst.

It can be stressful to trust that your people are working when you can't see them. And all employees need metrics to gauge their on-the-job performance. But rather than relying on productivity theater, leaders must focus on impact, or what employees do that matters. Giving employees clarity on what work drives impact is even more essential now, as nearly half of workers say they're burned out. This should alarm every leader, as burnout not only stifles employee well-being, but also chokes off innovation and decimates retention.

To start, listen to your people. It sounds simple, but more than half of companies (57%) rarely, if ever, hear about their employees' experiences at work. Actionable employee insights are critical to keeping a pulse on your organization — and hearing feedback can help you catch issues before they reach a breaking point that costs you talent, time and money.

At Microsoft, we're dialing up new ways to help leaders drive clarity and alignment, eliminate time-wasting busywork, and determine what is getting in the way of your people making a real impact. Today we are introducing Viva Pulse, a new app that will enable managers and team leads to seek regular and confidential feedback on their team's experience. Viva Pulse uses smart templates and research-backed questions to help managers pinpoint what's working well and where to focus, and also provides suggested learning and actions to address team needs. This is complementary and additive to robust company-wide engagement tools like Glint, which will be coming to Viva next year.

And earlier this year, we launched Viva Goals, a new goal-setting framework that helps organizations align employee work to business outcomes. New integrations in Viva Goals help bring goals into the flow of work, including an enhanced Microsoft Teams app to check in and collaborate on Objectives and Key Results (OKRs), an integration with Azure DevOps to automatically update work items, a connection to Power BI datasets to track Key Performance Indicators (KPIs) and Key Results, and integrations with Microsoft Planner and Project for automatic project management updates.

The number of organizations that have rolled back their return-to-office mandates is proof that people need a better reason to come into the office than policy alone. The data shows that the reason they will come in actually benefits organizations: People, we found, come in for each other. Eighty-four percent of employees say they'd be motivated to go to the office to socialize with coworkers, while 85% are motivated by rebuilding team bonds. The data underscores we have a deep human need to connect with the people we associate with, and that spending time together can help us build stronger relationships — which are key to doing our best work.

Firms desperately need to rebuild that social capital. Half of employees say their relationships outside their immediate teams have frayed, and 43% say they feel disconnected from their company. In a hybrid world, leaders need to think beyond the office, too: No matter where your employees work, consistent communication and connection is critical to help them feel like they're part of a community.

In addition to optimizing office space for time together, a digital employee experience ensures employees can stay connected and engaged when working apart. We're introducing Viva Amplify, a new app that empowers employee communications teams and leaders to elevate their messages and reach employees where they are with consistency and impact. The app centralizes communications campaigns, offers writing guidance to improve message resonance, enables publishing across multiple channels and distribution groups in Microsoft 365 and provides metrics for improvement.

We also recently launched Viva Engage, an app that fosters digital community building through conversations and self-expression tools. And within Viva Engage, we're excited to announce Leadership Corner, a space where employees can interact directly with leadership, share ideas and perspectives, host "ask me anything" (AMA) events and more. Leaders can use intuitive dashboards to track metrics and employee sentiment in real time.

To help colleagues easily discover connections and collaborate across the organization, we're introducing People in Viva, a new capability that uses AI to unlock insights across interests, knowledge and team goals. This experience will be available through Microsoft 365 profile cards and as a new app.

Seventy-six percent of employees say that if they could benefit more from learning and development, they'd stay at their company longer. The desire for training on the job has exploded over the last few years: Employees consider opportunities to learn and grow as the No. 1 driver of great work culture, a jump from 2019 when it ranked ninth. It's undoubtedly more cost-effective to upskill workers than hire brand new people. These findings make it clear that if you don't invest in the talent you already have, they will leave. In a tight labor market, re-recruiting your people — and investing in their careers for the long run — could be the difference between a successful and a struggling organization.

With Viva, your workforce can master those skills right in the flow of work. The Viva Learning app makes it easy for employees to discover, share and track the latest learning content in Microsoft Teams, so a company's entire workforce can stay up to date on required and recommended training. Enhanced integrations between Viva Learning and LinkedIn Learning will make it even easier for people to access content from LinkedIn Learning Hub right in the flow of work in Teams. Learners will see all of their LinkedIn Learning Hub content synced, including custom content, curated learning paths and the courses they have already completed, all reflected directly within Viva. And admins will be able to set the integration up directly within their settings on LinkedIn Learning Hub – no APIs needed.

And to help put collective knowledge to work for all employees, we're introducing Answers in Viva, a new chat-like experience that uses AI to match employee questions to answers and experts across your organization.

Work can be hard, but how you work shouldn't be. Today we're introducing several enhancements across the Viva platform to help employees get back their time — and have more impact. To streamline the Viva experience and help employees start their day, the new Viva Connections home experience brings all Microsoft Viva apps into one place, and updates to the Viva briefing email will provide more personalized recommendations to help employees catch up on work, meetings and learning.

As we look to the future, we're also thinking about how to tailor the employee experience to meet the unique needs of specific roles. Viva Sales, the first role-based employee experience app in the platform, will be generally available Oct. 3. Viva Sales brings together a seller's CRM with Microsoft 365 and Teams to provide a more streamlined and AI-powered selling experience — right in the tools they're using every day. We're also announcing a partnership with Seismic to personalize and scale customer engagements through AI-generated content recommendations.

The world of work has fundamentally changed, and to set ourselves up for long-term success, our approach to leadership must change with it. I won't sugarcoat it: It may be uncomfortable. Change always is. But by looking to the data and supporting employees — through clarity, connection and learning opportunities — your people and organizations will flourish.

Eighty percent of businesses expect to be hybrid – four tips for data security and compliance.

Sustainability is the #1 ESG priority for Small and Medium Businesses – how to reduce your carbon footprint.

Early adopters of technology grow faster – practical steps to accelerate your digital journey.

Small and medium-sized businesses (SMBs) are the cornerstone of every community. With an estimated 400 million SMBs operating worldwide, they account for 99% of global organizations. SMBs are the main source of job creation, making up between 60%-85% of employment. [1] [2] And according to the International Labor Organization, by some estimates, SMBs contribute up to 70% of GDP.[3]

With that kind of scale and impact, SMBs need the ability to innovate to be successful and thrive in a digitally enabled economy.

During the pandemic, we saw that businesses able to accelerate their digital transformation fared better than those that did not. Many reported growth through their focus on tech intensity.

Under such unprecedented circumstances, Microsoft was curious to learn how SMBs view the current realities of their businesses, to understand their goals and priorities, and to assess whether technology is seen as a catalyst to future success.

Analysys Mason was commissioned to conduct research with over 3,000 SMBs from 10 different areas of the world, representing the global footprint of SMBs.

Today, we are releasing our findings with the Microsoft SMB Voice and Attitudes to Technology Study 2022.

The study confirms that nearly 70% of businesses consider growth as their primary driver, and technology "early adopters" saw faster and higher growth. Businesses identified as early adopters are twice as likely to have achieved their past business objectives plus higher revenue growth over the past year and are also four times more likely to be highly confident in their future business success. Today, businesses are focusing on sustaining growth despite geopolitical uncertainty, high inflation and energy prices. For 51% of businesses, the ongoing impact of COVID-19 remains a concern.

More than two-thirds of SMBs plan to increase their IT budgets in the next year, prioritizing investments in the following ways:

Eighty percent of businesses expect to have a hybrid workforce for at least the next two years and require the technology to support their business goals in a reliable and secure way.

While the majority of SMBs recognize the importance of technology in reaching business goals, 16% see it as essential. However, many SMBs need partners to map the right technology to their business strategy. Almost half choose technology partners, with 33% favoring Managed Service Providers or Cloud Solution Providers whom they expect to proactively recommend technology solutions to accelerate companies' business goals.

Millennials hold 41% of technology decision-making roles. They tend to work for newer companies and 13% are more likely to see technology as important or essential to business success. Nearly one-third of millennials identify as technophiles in their personal lives, carrying some of their consumer traits into how they research and purchase technology, placing high value on peer ratings and reviews when making purchasing decisions. In 2016, Gallup flagged that millennials favor purpose over paycheck in their How Millennials Work and Live report. Millennials as business owners apply those values to their business and technology priorities.

Environmental, Social, and Governance (ESG) goals – no longer just for the enterprise

Two-thirds of SMBs in the study report having ESG assessment and monitoring frameworks in place. Millennials are leading the way, being the most likely to have an annual report dedicated to their ESG progress. In terms of focus areas, 37% cited lowering their environmental impact, 30% wanted to improve their diversity and inclusion, while 30% were focused on job creation. The study points to a link between an ESG focus and attainment of business objectives – with both these traits being most prevalent among early adopters.

Ninety-nine percent of the world's businesses are actively seeking solutions to address current challenges and generate new opportunities. This community impacts economies worldwide and the livelihoods of billions. They do this not to just be resilient and survive, but to thrive and bring innovation to their industries and value to their customers. Microsoft continues to listen to and learn from our SMB customers, aligning to what is their top of mind.

Together, the possibilities are endless, and opportunity abounds for all.

Download the full Microsoft SMB Voice and Attitudes to Technology Study 2022.

For more resources to help with your digital journey, visit the Microsoft Small Business Resource Center

[1] Small & medium-sized enterprises | National Action Plans on Business and Human Rights (globalnaps.org)

[3] The power of small: Unlocking the potential of SMEs – InfoStories (ilo.org)