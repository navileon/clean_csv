Build and Deploy Secure AI Applications with AIShield and Amazon SageMaker

Adversarial machine learning (AML) attacks, also known as "artificial intelligence attacks" (AI attacks), involve deliberate attempts to manipulate or compromise machine learning models or even make it reveal sensitive information. AML attacks impact one or more of the three main tenets of security–confidentiality, availability, and integrity. A recent Gartner survey indicates that two in five organizations have experienced artificial intelligence (AI) security breaches, underscoring the urgency to address this issue. In this post, we will explore how AIShield's seamless integration within the Amazon SageMaker environment alleviates AI security concerns by mitigating risks before and after deployment, enabling customers to develop and deploy AI applications with confidence. Bosch Global Software Technologies (BGSW) is an AWS Select Tier Services Partner and AWS Marketplace Seller that is using machine learning solutions to improve its manufacturing, healthcare, and Internet of Things (IoT) solutions. Risks and Consequences of AI Attacks Adversarial machine learning attacks lead to various forms of organizational harm and loss—for example, financial, reputational, safety, intellectual property, people's information, and proprietary data. In healthcare, banking, automotive, telecom, public sector, and other industries, AI adoption suffers due to security risks from emerging attacks which relate to safety, nonconformance to AI principles, regulatory violations, and software security. AI attacks typically exploit vulnerabilities within data payloads. The traditional defense mechanisms like layer 3/4 network firewalls and layer 7 application firewalls often fall short in providing comprehensive protection against such threats. Specifically, the concept of zero trust architectures, while valuable in theory, can be a complex task to implement efficiently due to the persistent threat of adversarial attacks. AI Security Challenges The challenge of securing AI is complicated due to the multifaceted technology landscape, shortage of skilled staff, and unavailability of essential components of AI security in open source. Developing an enterprise solution from scratch can be costly, and responsible AI adoption and uncertainty about regulatory compliance provides further challenges. The cumulative impact includes reduced revenue due to delayed time to market and increased costs, making the return on investment (ROI) in AI less attractive for organizations. Organizations seek holistic AI security solutions that enhance their security posture, improve developer experience, seamlessly integrate into existing systems, ensure compliance, and offer resilience throughout the AI/ML development lifecycle. In the DevOps context, this means:

Developers require a straightforward solution that can scan AI/ML models, identify vulnerabilities, and automatically remediate them during the development phase. Deployers and operators, including security teams, need tools such as endpoint detection and response (EDR) specific to AI workloads. They need to rely on solutions capable of detecting and responding to emerging AI attacks to prevent incidents and reduce the mean time to detect (MTTD) and mean time to resolve (MTTR). Managers need visibility into the security posture of the AI/ML models they deploy to ensure better governance, compliance, and model risk management at an organizational level.

Enhancing Security of AI Workloads with AIShield AIShield is a comprehensive AI security product for organizations running AI workloads on Amazon SageMaker, helping them protect against AML attacks. By leveraging the capabilities of both AIShield and Amazon SageMaker, organizations can maximize the value of their AI/ML efforts. Let's explore how AIShield can be used by organizations to overcome AI security challenges in the AI/ML lifecycle.

Figure 1 – Intervention points of AI security in the ML development workflow. There are two main intervention points for AIShield in the ML development workflow:

Model evaluation: After model validation and prior to the deployment, AI models undergo evaluation. AIShield offers an API-based AI security vulnerability assessment, which can be used in the development workflow by developers to assess the security posture of their AI application.

Figure 2 – Outcomes of API-based AI security vulnerability assessment for AI/ML developers.

Model operationalization: The threat-informed defense model can be deployed within the model deployment environment for real-time detection of any attack vector. The defense model is trained to bifurcate the effective attack vectors from the original data.

Figure 3 – Defense model deployment architecture – parallel deployment. AIShield dashboards provide visualization to the managers on the overall security posture, analytics on the outcomes, and AI security licenses usages which can help lead to better model governance practices.

Figure 4 – AI security posture dashboard for organization leaders. AIShield in SageMaker Studio ML Development Workflow AIShield offers seamless integration through AWS Marketplace and provides easy access to its cloud-based solutions. AIShield seamlessly integrates with various AWS machine learning environments, including Amazon SageMaker Studio Notebooks and Amazon SageMaker Notebook Instances. SageMaker Studio Notebooks are highlighted in this post for their native features, including Amazon SageMaker Experiments and Amazon SageMaker Model Registry. These features offer improved organization, tracking, and management of models, providing valuable capabilities for users leveraging SageMaker Studio Notebooks. Consider a scenario where a developer is utilizing SageMaker Studio to build a machine learning model for a specific use case, such as digital recognition. In this context, let's see how AIShield can be integrated into this pipeline quickly.

Figure 5 – Reference architecture on AWS. This integration process is divided into two parts—developer flow and deployment flow—ensuring a comprehensive end-to-end integration of AIShield API with SageMaker Studio Notebook. Part 1: Developer Flow The developer flow focuses on the activities conducted by developers during the integration process (see Figure 2). 1.1 – Set Up SageMaker Studio When integrating AIShield into a machine learning pipeline like SageMaker Studio, the initial step involves installing the AIShield PyPI package. This installation facilitates rapid experimentation during model development. On the other hand, AIShield APIs provide scalable solutions that are better suited for MLOps pipelines, enabling seamless integration and deployment of AIShield functionalities.

!pip install aishield # install the aishield pypi package import aishield as ais # import the aishield packageHTML

1.2 – Model Development and Iterative Refinement As with any model development pipeline, the developer goes through the process of defining, training, and refining the model until it reaches the desired performance level. Following this, the model is typically stored in an Amazon Simple Storage Service (Amazon S3) bucket. 1.3 – API Shield API Integration After subscribing to AIShield, the AIShield API can be initialized by providing the necessary parameters (URL and organization ID) received during the subscription process. This initialization step allows developers to access and utilize the functionalities provided by the AIShield API.

""" Description: AIShield API URL and subscription keys """ baseurl = " " url = baseurl + "/api/ais/v1.5" org_id = " " # fill in Org_Id provided in welcome email

""" Description: Initialize the AIShield API """ client = ais.AIShieldApi(api_url=url, org_id=org_id)HTML

To leverage AIShield's model vulnerability analysis, the necessary data, labels, and model file are provided to AIShield, securely stored using pre-signed URLs utilizing Amazon S3 storage. AIShield adopts a low data and no model approach, typically utilizing a representative sample of 2-5% of the dataset to generate test reports and assess model performance. Furthermore, the model-under-test can be hosted as an API, with traffic routed through AWS PrivateLink to ensure a secure and private connection. For a quick demonstration, the model can be submitted to AIShield for analysis, involving the registration of the model within the AIShield system.

""" Description: Perform model registration and upload the input artifacts like sample Data, Label and Model """ status, job_details = client.register_model(task_type=task_type, analysis_type=analysis_type) model_id = job_details.model_id print('Model id: {} \nInput artifacts will be uploaded as:\n data_upload_uri: {}\n label_upload_uri: {}' '\n model_upload_uri: {}'.format(model_id, job_details.data_upload_uri, job_details.label_upload_uri, job_details.model_upload_uri))

upload_status = client.upload_input_artifacts(job_details=job_details, data_path=data_path, label_path=label_path, model_path=model_path) print('Upload status: {}'.format(', '.join(upload_status)))HTML

AIShield supports diverse task types, including image and tabular classification, time series forecasting, and offers analysis for various attack types like extraction and evasion. Users have the flexibility to choose specific tasks and desired attack analysis options within AIShield, ensuring customization and adaptability to meet their specific requirements.

""" Description: Define the task and analysis type """ task_type = ais.get_type("task", "image_classification") analysis_type = ais.get_type("analysis", "extraction")HTML

After initiating the vulnerability analysis in AIShield, once the analysis is completed the output artifacts such as reports, defense models, and attack samples can be uploaded to an S3 bucket.

""" Description: Run vulnerability analysis """ my_status, job_details = client.vuln_analysis(model_id=model_id, vuln_config=vuln_config) my_job_id = job_details.job_id print('status: {} \nJob_id: {} '.format(my_status, my_job_id)) print('Click on the URL to view Vulnerability Dashboard (GUI): {}'.format(job_details.job_monitor_uri)) """ Description: Fetch Job status using Job ID """ my_status = client.job_status (job_id = my_job_id) print('job status ', my_status) """ Description: Download the Vulnerability Report """ if my_status == "success": """ 1. Get aishield output artifacts & optionally upload to s3 bucket 2. Parse the reports & log the values to experimentation """ parse_reports(job_id=my_job_id, postfix_str=num_attack_queries, run_name=run_name)HTML

1.4 – SageMaker Experiments To further streamline the workflow and keep track of the AIShield artifacts, the S3 locations of these artifacts can be pushed to SageMaker Experiments. This integration ensures the AIShield output artifacts are organized, tracked, and readily available for analysis and further experimentation within the SageMaker environment.

# upload the defense model to S3 bucket def_model_converted_path = convert_to_tf_protobuf(os.path.join(OUTPUT_PATH, 'Defense_artifact')) prefix = "defense_model_{}".format(postfix_str) def_mdl_loc = sess.upload_data(path=def_model_converted_path, bucket=bucket_name, key_prefix=prefix) print("defense report saved at:\n", def_mdl_loc)

# log saved artifacts path with load_run(experiment_name=experiment_name, run_name=run_name) as run: run.log_artifact(name="AIShield_Vulnerability_Report", value=vul_report_loc, is_output=True) run.log_artifact(name="AIShield_Defense_Report", value=def_report_loc, is_output=True) run.log_artifact(name="AIShield_Defense_Model", value=def_mdl_loc, is_output=True)HTML

The process of analyzing model vulnerability and generating corresponding defenses using AIShield can be orchestrated for multiple runs, allowing for flexibility in adjusting parameters such as the number of attack queries and the attack type. This enables comprehensive testing and evaluation of the model's robustness under various scenarios, enhancing its overall security.

To improve tracking and analytics, the metrics and artifacts generated from multiple runs can be logged to SageMaker Experiments and can be analyzed through charts. This enables comprehensive record-keeping and facilitates in-depth analysis of the results, enhancing the overall monitoring and evaluation process. This information aids in making informed decisions for selecting the specific run and, consequently, the defense model to deploy in the subsequent process. As the number of attack queries increase, the efficacy of vulnerability detection and defense model accuracy increase (see Figure 6).

Figure 6 – For the three runs: (1) Number of attack queries; (2) Original model accuracy;(3) Vulnerability analysis scores; (4) Defense model accuracy. Following that, the model card along with its relevant configurations can be saved and pushed to an S3 bucket. This ensures the model card is properly stored and easily accessible for future reference and documentation purposes.

# Create model card my_card = ModelCard( name=model_card_name, status=ModelCardStatusEnum.DRAFT, model_overview=model_overview, training_details=training_details, intended_uses=intended_uses, sagemaker_session=sess, ) my_card.create()HTML

Part 2: Deployment Flow The deployment flow focuses on the activities involved in deploying the integrated solution (the defense model and original model are shown in Figure 3), making it available for use by end-users. This typically includes the following steps. 2.1 – Model Packaging Once developers are satisfied with their main application model's and defense model's performance as in the developer workflow, both can be downloaded from respective S3 buckets for deployment.

# Define the s3 location for original & defense model to be deployed (both models are created in the developer flow) orig_model_loc = 's3:// ' defense_model_loc = 's3:// ' orig_model_key = urlparse(orig_model_loc).path.strip("/") defense_model_key = urlparse(defense_model_loc).path.strip("/")

# Download both the models from s3 bucket s3.download_file(bucket, orig_model_key, "orig_model.tar.gz") s3.download_file(bucket, defense_model_key, "def_model.tar.gz")HTML

2.2 – Deployment and Monitoring Both the original and defense models can be deployed to the target environment, leveraging SageMaker deployment capabilities. Deploy the main application model as usual.

# Create a Sagemaker Tensorflow object for the original model orig_model = TensorFlowModel(model_data=orig_model_loc, role=role, framework_version="2.3.1")

# Deploy the original model predictor = orig_model.deploy(initial_instance_count=1, instance_type="ml.c4.large") print('predictor endpoint: ', predictor.endpoint)HTML

Next, deploy the defense model the same way as the original model.

# Create a Sagemaker Tensorflow object for the defense model defense_model = TensorFlowModel(model_data=defense_model_loc, role=role, framework_version="2.3.1")

# Deploy the defense model defense_predictor = defense_model.deploy(initial_instance_count=1, instance_type="ml.c4.large") print('Defense predictor endpoint: ', defense_predictor.endpoint)HTML

Figure 7 – Defense model deployment architecture. 2.3 – End-User Interaction The API abstraction can be done for parallel deployment of the defense model alongside the original model. The decision block can take remedial actions, such as blocking the user or randomizing the response from the main application model when the defense model identifies a malicious payload. Telemetry data can be sent to a Security Information and Event Management (SIEM) system like Splunk for further analysis and monitoring.

Figure 8 – Splunk SIEM reporting dashboard with AIShield integration. Use Case: Secure a Breast Cancer Screening AI Algorithm A Germany-based healthcare startup is using patented machine learning algorithms for accurate detection of breast cancer. To protect this core intellectual property from model extraction, evasion, and poisoning, the company needed a comprehensive security solution. AIShield's AI security product was leveraged by the company to conduct a risk vulnerability assessment of their AI model and improve its security posture. Key benefits of AIShield include:

Secures the core IP of ML algorithms for breast cancer detection. Protects against any manipulation with model evasion attacks and ensure patient safety. Helps adhere to medical cybersecurity guidelines and AI-based Software as a Medical Device (SaMD) cybersecurity requirements.

Use Case: Reduce Operational Losses from Credit Card Transaction Fraud A prominent UK bank, handling over 30% of the nation's £300 billion debit and credit card transactions, experienced nearly £70 million in external fraud losses in 2021 despite having a fraud detection model. This represented about 70% of total losses, marking it as the highest operational risk. The bank's CTO—along with AI/ML, risk and cybersecurity teams—aim to strengthen the current fraud detection model to decrease these losses and increase profit margins. The AIShield team and the bank's cybersecurity and AI teams conducted a joint proof-of-concept using the AIShield SaaS API. A fraud detection ML model was hosted on-premises and API access via AWS PrivateLink was provided to consume the product API. AIShield demonstrated automatic triggering of a vulnerability assessment as part of the model development workflow, with a reference implementation of SageMaker. In under six hours, the AIShield product generated a vulnerability assessment report, including explanations and defensive measures. These artifacts were delivered securely through signed URL mechanisms in S3. With its transparent and objective assessment of residual risk, the solution has the potential to significantly improve the bank's ability to reduce residual operational risk by up to 15%. The risk management team is now able to make quick and secure releases of fraud detection models in just eight hours, 11x faster and with a 30x productivity boost compared to the previous process. Solution Benefits AIShield offers a free trial for users to explore its capabilities and supports on-premises deployment with sales support. Technically, AIShield reduces vulnerability detection and remediation time for AI models from months to hours and provides reference implementations on GitHub. The benefits to businesses include an augmented return on investment (ROI), decrease in critical vulnerabilities, and estimated cost savings ranging from 40-60% of the security expenditure in AI projects. These advantages ensure a rapid realization of value through brand protection and regulatory compliance.

Vulnerability analysis for a wide variety of attacks across models, frameworks, and data Frequent updates to attack library Low data/no model approach

One-click application through PyPi package Detailed reference implementations Metrics pushed for every experiment

AWS PrivateLink enabled Updates to the model card/governance dashboards Detailed documentation and how-to videos

Model security by design allowing for scaled AI security initiatives Defense is lightweight and can be deployed on target – cloud/edge

Ease to integrate APIs into MLOps pipelines Containerized defense model available in multiple formats (ONNX, .h5) Pre-configured connectors to SIEM solutions like Splunk and Sentinel

Published apps in Splunk app base and Sentinel marketplace Pre-built workbooks and playbooks available Telemetry is OCSF-compliant Easy and smooth billing

Conclusion This post demonstrated the importance of AI security for organizations and how the AIShield security product can be integrated into Amazon SageMaker ML pipelines. The solution empowers organizations to leverage the combined capabilities of AIShield and SageMaker ML workflows, maximizing the value derived from their AI/ML initiatives with confidence and security assurance. AIShield has received notable accolades for its technology, including the CES Innovation Award 2023, IoT World Congress Award 2023: Best Cybersecurity Solution, and recognition from Gartner in its Market Guide for AI Trust, Risk and Security Management. Learn more about AIShield in AWS Marketplace. The sample code; software libraries; command line tools; proofs of concept; templates; or other related technology (including any of the foregoing that are provided by our personnel) is provided to you as AWS Content under the AWS Customer Agreement, or the relevant written agreement between you and AWS (whichever applies). You should not use this AWS Content in your production accounts, or on production or other critical data. You are responsible for testing, securing, and optimizing the AWS Content, such as sample code, as appropriate for production grade use based on your specific quality control practices and standards. Deploying AWS Content may incur AWS charges for creating or using AWS chargeable resources, such as running Amazon EC2 instances or using Amazon S3 storage. .

Bosch – AWS Partner Spotlight Bosch Global Software Technologies (BGSW) is an AWS Partner that's using machine learning solutions to improve its manufacturing, healthcare, and IoT solutions. Contact Bosch | Partner Overview | AWS Marketplace

TAGS: AIShield, AWS Partner Guest Post, AWS Partner References, AWS Partner Success Stories, Bosch

Multi-account infrastructure provisioning with AWS Control Tower and AWS Proton

Introduction The majority of the enterprise customers tend to establish centralize control and well-architected organization-wide policies when it comes to distribution of cloud resources in multiple teams. These teams are primarily divided into three categories: IT operations, Enterprise Security, and Application (App)-development. While delivery of business value from application standpoint falls under the purview of the App-development teams, the IT operations teams' control the cloud resource provisioning and security teams ensures the delivery and coordination between these teams happens at scale. Within AWS, AWS Control Tower offers the easiest way to set up and govern a secure, multi-account environment. It establishes a landing zone based on best-practices blueprints, and it enables governance using guardrails you can choose from a pre-packaged list. The landing zone is a well-architected, multi-account baseline that follows AWS best practices. First, it's good to know that AWS Control Tower shares a lot of terminology with the AWS Organizations service, including the terms organization and organizational unit (OU). While Organization refers to an entity that you create to consolidate your AWS accounts for administration as a single unit, Organization unit acts as a container for Accounts within a root and provides necessary hierarchy. Often there is a need for securing infrastructure in a consistent and compliant fashion, which requires a decoupling of the infrastructure management from the business application delivery that results in undifferentiated heavy lifting by teams. The pattern discussed in this post addresses this challenge by using a combination of AWS Control Tower to set up a well-architected, multi-account environment and AWS Proton to simplify multi-account continuous integration and continuous delivery (CI/CD) Deployments for application deployment and management. Solution overview Multi-account deployment using AWS Proton AWS Proton service is a two-pronged automation framework. As a platform team administrator, you create environment infrastructure as code template that defines shared infrastructure used by multiple applications and service templates that define deployment tooling for serverless and/or container-based applications. As an application developer, AWS Proton enables you to select desired service from the available service templates to automate your application deployments. For platform teams to improve visibility and efficiency at scale, AWS Proton offers a capability called Environment Account Connections. Environment account connections help platform teams to establish secure bi-directional connections between a single management account and multiple development team accounts, also referred to as environment accounts and is described in the following figure.

Walkthrough The solution described in this post assumes that you are using AWS Control Tower to create OUs and accounts. To know more on how to create an OU in the AWS Control Tower from AWS Management Console, refer to the AWS Documentation here. Prerequisites We used an AWS Cloud9 instance to run this tutorial and if you want to create a Cloud9 instance in your account, refer to the AWS Documentation here. If you are not using AWS Cloud9 you need to install the latest version of the AWS CLI. Step 1: Create service control policy for OUs For this solution, we created two OUs to demonstrate a typical customer use case. The first one is the Management OU for the AWS Proton Management Account, where platform teams can maintain the AWS Proton environment templates and service templates. The second one is the Development OU, which is used to create the AWS Proton Environment Account, where developer teams host their business applications. Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your OUs. Here, we have created a simple SCP AWS-Proton-Blocker and associated that with our Development OU. This SCP prevents any developer environment account from using AWS Proton to provision any resources while ensuring platform teams have full control on Infrastructure provisioning.

{ "Version": "2012-10-17", "Statement": [ { "Sid": "Statement1", "Effect": "Deny", "Action": [ "proton:*" ], "Resource": [ "*" ] } ] } Apache Configuration

Note: This policy can be scoped i.e., by providing fine-grained access control relevant for a particular team as per the needs of your organization. To learn more about how to create SCP, refer to AWS documentation here. To learn more about how to attach SCP to OU, refer to AWS documentation here. Step 2: Testing the SCP

Log in to the AWS Account created under Development OU from the login link provided to you by AWS at the time of creating AWS Account. You'll find log in details in the welcome email. Alternatively, you can also find the account login URL on the AWS Service Catalog dashboard under Provisioned Product. Search for AWS Proton from the AWS Management Console. Create an Environment template in AWS Proton as mentioned here. The Environment template creation fails due to the explicit deny at the OU level with the following error.

User: [EMAIL] is not authorized to perform proton:GetTemplateSyncConfig on resource arn:aws:proton:us-east-1:[aws-account-number]:environment-template/example-environment with an explicit deny in a service control policy Apache Configuration

By using the SCP at the OU level, we ensure that the Platform Engineering team can enforce the organizational guardrails between the service teams and departments within an organization. Step 3: Create environment template in the AWS Proton management account To run AWS Command Line Interface (AWS CLI) commands in the provisioned AWS accounts, we created AWS Cloud9 workspaces and attached an AWS Identity and Access Management (AWS IAM) role with AdministratorAccess policy to the AWS Cloud9 instances in accounts created under Management and Development OUs. Please follow these instructions to attach the AWS IAM role to an Amazon Elastic Compute Cloud (Amazon EC2) instances. For this post, we forked this GitHub repository and created a AWS CodeStar connection. Please find the steps to Set up an AWS CodeStar connection in the AWS Documentation. AWS Proton uses a source connection to trigger template updates or new application deploys whenever a new change is introduced. For the scope of this post, the connection is managed through the AWS CodeStar connections using GitHub as a provider. To read more on Service Sync configuration, please refer to this documentation.

Create a new environment template (multi-svc-env) based on the templates located under multi-svc-env/v1 and published it to a major version. This structure holds the AWS CloudFormation template that we'll use to provision the environment's infrastructure.

aws proton create-environment-template \ --region ${AWS_REGION} \ --name "multi-svc-env" \ --display-name "Multi Service Environment" \ --description "Environment with VPC and public subnets" Apache Configuration

Next, create a template sync configuration to register new environment template versions automatically.

REPOSITORY_ARN=$(aws proton list-repositories | \ jq -r '.repositories[] | select( .name | endswith("aws-proton-workshop-code")) | .arn'); REPOSITORY_NAME=$(echo $REPOSITORY_ARN | cut -d':' -f7); REPOSITORY_PROVIDER=$(echo $REPOSITORY_ARN | cut -d':' -f6 | tr a-z A-Z); aws proton create-template-sync-config \ --region ${AWS_REGION} \ --repository-name $REPOSITORY_NAME \ --repository-provider ${REPOSITORY_PROVIDER#"REPOSITORY/"} \ --branch main \ --subdirectory "aws-managed/multi-svc-env" \ --template-name "multi-svc-env" \ --template-type "ENVIRONMENT" Apache Configuration

To publish it, we need to run the following command:

aws proton update-environment-template-version \ --region ${AWS_REGION} \ --template-name "multi-svc-env" \ --major-version "1" \ --minor-version "0" \ --status "PUBLISHED" Apache Configuration

Step 4: Create environment account connection from management account in AWS Proton AWS Proton alleviates complicated cross-account policies by using a secure environment account connection feature. With environment account connections, platform engineers can give AWS Proton permissions to provision infrastructure in other accounts. To create and provision an environment from AWS Proton management account, log into the AWS Account created by AWS Control Tower under the Management OU. You'll find log in details in the welcome email accordingly.

Create a scoped AWS IAM role in the AWS Proton management account

aws cloudformation deploy \ --template-file proton-account-connection-roles.yaml \ --stack-name AWSProtonWorkshop-AccountConnectionRoles \ --parameter-overrides "EnvironmentAccountId=${SECONDARY_ENV_ACCOUNT_ID}" \ --capabilities "CAPABILITY_IAM" "CAPABILITY_NAMED_IAM" Bash

Now log in to the AWS Account and navigate to the AWS Proton's Console created under the Development OU, or copy and paste the output of AWSProtonWorkshop-AccountConnectionRoles stack in your browser to login to the Development AWS Account. Choose Environment Account Connections from the navigation pane. Then select Request to Connect in Sent requests to connect to a management account. (For the Management account ID use the Proton Management Account).

Switch back or log in again to the management account and you should have a new environment account connection request now in the AWS Proton Console. Go ahead and accept it!

Step 5: Create environment in Developer Team account After defining the environment template, service template in the AWS Proton Management account and establishing the Environment Account Connections to the development account, we're ready to provision the environment in the development account using AWS Proton. First, we define the specification, which is a YAML-formatted string that provides inputs defined in the environment template bundle schema file. A sample can be found here.

SPEC=$(cat <<-EOF proton: EnvironmentSpec spec: vpc_cidr: 172.16.0.0/16 dns_hostname: [name-of-your-app].dev.local EOF ); Apache Configuration

[[name-of-your-app] is a place-holder, replace that with the name of your app] The create-environment command creates the environment with the infrastructure needed for the App-Development teams to deploy micro-services. The template used below, deploys an Amazon Virtual Private Cloud (VPC) to secure networking boundary, an Amazon ECS cluster with optional configuration inputs like Amazon EC2 capacity, Container Insights, and Amazon ECS Executive logging for the application deployment.

MAJOR_VERSION=$(aws proton list-environment-template-versions \ --template-name "multi-svc-env" --region=${AWS_REGION} | \ jq -r ' .templateVersions[] | select( .status=="PUBLISHED") | .majorVersion' | tail -1 \ ); \ ACCOUNT_CONNECTION_ID=$(aws proton list-environment-account-connections \ --requested-by "MANAGEMENT_ACCOUNT" --statuses "CONNECTED" --region=${AWS_REGION} | \ jq -r ' .environmentAccountConnections[] | select( .environmentName | startswith("multi-svc")) | .id' \ ); \ aws proton create-environment \ --region ${AWS_REGION} \ --name "multi-svc-${AWS_ACCOUNT_ID}" \ --template-name "multi-svc-env" \ --template-major-version "$MAJOR_VERSION" \ --environment-account-connection-id "$ACCOUNT_CONNECTION_ID" \ --spec "$SPEC" Apache Configuration

In the Developer account (i.e., environment account), you can view and access the provisioned infrastructure resources. You can now use this environment to deploy application as AWS Proton Service. Please review the documentation to learn more about the service deployment in an AWS Proton environment.

Cleaning up You continue to incur cost until deleting the infrastructure that you created for this post. Use the below instructions to delete AWS resources in the blog. Delete the environment:

aws proton delete-environment --name multi-svc-${AWS_ACCOUNT_ID} # Wait for above to complete before proceeding next steps. It takes ~3 minutes. Apache Configuration

Once you have deleted your AWS Proton resources, please follow AWS Documentation here to delete your AWS Control Tower resources. Conclusion In this post, we showed you how customers improved the security posture of multi-account infrastructure deployments by using AWS Control Tower to provision and organize AWS accounts that adhere to best-practices. We used AWS Proton to amplify platform engineering impacts and improve developer productivity. This established workflow allowed Platform teams to have a unified view of all the AWS Proton managed resources from management account using the AWS Proton dashboard. To get started with AWS Proton, head over to our sample repository for examples and our documentation for a deeper dive into its full functionality.

SSG vs SSR in Next.js Web Applications: Choosing the Right Rendering Approach

Next.js, a popular React framework, has changed the way developers build modern web applications. It offers powerful features, such as Server-Side Rendering (SSR) and Static Site Generation (SSG), which optimize your application's performance and user experience. In this blog post, we'll explore the key differences between SSG and SSR, their advantages, when to choose one over the other, and how to deploy either approach using AWS Amplify. Amplify is a complete solution that lets frontend web and mobile developers easily build, ship, and host full-stack applications on AWS. What is Static Site Generation (SSG)? Static Site Generation, as the name suggests, generates static HTML files during the build time. Every time you build an app, a bunch of pages are created. These files are then served to users, which means the server doesn't need to do any extra work when a user visits the website. This approach is ideal for websites with content that doesn't change frequently, like blogs or documentation sites. Advantages of SSG

Speed: Pre-rendered HTML files are served directly, resulting in faster load times. Scalability: Static files can be easily served by Content Delivery Networks (CDNs), improving your application's ability to handle more global traffic around the world.

What is Server-Side Rendering (SSR)? Server-Side Rendering involves generating the HTML for each page on the server when a user requests it. With SSR, there is a server that pre-renders the page – it's like a template that you plug variables into and the server handles all of the rendering. This happens at request time, so when the user requests the page, they get the server side rendered page. This rendering happens all on the server side and never runs in the browser. So, unlike SSG where the page is already rendered in the server waiting to be served to the client, SSR renders the page on the server upon receiving a request. SSR is ideal for websites with dynamic or personalized content that changes frequently, like e-commerce websites or social media platforms. Advantages of SSR

Consistent user experience: Users see the latest content as it's generated on-the-fly, ensuring they always have up-to-date information. Personalization: SSR allows you to serve unique content based on user preferences or other dynamic data.

Is your content updated frequently? If your content doesn't change often, SSG is the better choice for improved performance and scalability. For dynamic content, SSR ensures users see the most up-to-date information. Does your website provide an interactive user experience? SSR websites provide an interactive user experience, whereas SSG websites are largely static sites with little to no dynamic content, unless your site is combined with CSR or SSR. Do you want to incur the rendering cost at build-time or run-time? Choose SSG if you want to incur the rendering cost at build-time and SSR for run-time. Do you have an SEO requirement? The primary difference between SSR and SSG for SEO is only the server response time.

Consider your Business Requirements While SSG provides excellent SEO benefits, SSR offers better support for dynamic content and personalization that may be crucial for search rankings.

User Experience: Consider whether your users need real-time data or personalized content. If so, SSR is the better option. Otherwise, SSG offers a faster experience with lower server requirements. Latest Data: SSR pages will always display the latest data because they are generated on the server with every request from the user. With every request, you can always be sure that you have the most recent/up-to-date information.

SSG vs SSR in Next.js: Choosing the Right Rendering Approach In this article, we will discuss the two rendering approaches in Next.js: SSG and SSR. We will also provide an example codebase that will help you determine the best rendering approach for your application. Next.js supports the ability to generate an "n" number of dynamic generated routes. In our example codebase, we fetch 100 of the top Hacker News posts in the last few days and render those IDs onto the home page. When clicked, each of those pages reveal the details of the post. Each of those pages will also have the route like "pages/[hackerNewsArticleId]". You can see how that template page is available on the pages folder with the [slug] designation. Now, you may be asking how do we define this list of paths to be statically generated? We will use two NextJS functions: getStaticProps and getStaticPaths. getStaticProps will pre-render the page at build time, fetch from the Hacker News API the list of posts information, and pass that information into the props of our component as well as getStaticPaths. Our second function, getStaticPaths, is the key to dynamic routing. It will receive the list of IDs from getStaticProps and create a list of path objects like this: export async function getStaticPaths() { return { paths: [{ params: { id: '1' } }, { params: { id: '2' } }], fallback: false, // can also be true or 'blocking' } }JavaScript Next.JS will now ingest this list of paths and generate a new page for each of those params. We have also now allowed the primary component ingest the Hacker News IDs to render for routing to the individual page. If you go to our dynamically rendered component page, you will see that we again call the getStaticProps method to call the Hacker News details endpoint to fetch the post's title, votes, etc. Notice that this is a snapshot of the blog post at the time of compilation. SSG Approach We will go over deploying a SSG Next.js app from this Github repository to Amplify. To start off, click below to deploy this project in your own environment. Here is more information about deploying and hosting with Amplify.

On the Deploy App page, go ahead and select Create new role, leaving all of the defaults for your Amplify backend role.

Once you've created the role, refresh the existing roles back on the Deploy App page and select the role you just created. Select Save and Deploy. Deploying your app will take 2-3 minutes. Click Continue when the box appears. Then, you can monitor as your Amplify app goes into the provision, build, and deploy phases.

Once it has finished deploying, you'll be able to visit the website hosted under the Domain.

When you select your domain, you'll be navigated to your Amplify app which will display your SSG Next.js app.

SSR Approach We will look how we can deploy a NextJS web application from GitHub directly to Amplify. Surprise! We actually already deployed a SSR page when we deployed to Amplify. The amazing thing about Next.JS is that you can have some pages be generated at build time, while other pages can still render dynamic data by leveraging SSR. To use Server-side Rendering for a particular page, we export an async function called getServerSideProps. This function will be called by the server on every request. Now, on your hosted link, navigate to the /ssr route (ie. Now, you should see a page like this:

Common Gotchas Next.js requires a different paradigm shift away from the very popular frontend frameworks leveraging the libraries of Create React App (CRA) and React Router. CRA is a starter kit that provides you with a boilerplate project for creating a React application. CRA uses client-side rendering (CSR), which means that your application is rendered in the browser, and typically it is best practice to break up your network requests based on user's interactions. In comparison to SSG, you now have to load all the data all at once. By including these Common Gotcha section, we hope you can learn from our mistakes and save some time and frustration.

Instead of passing along all of the build and data fetching time to the customer, you might be stuck hours waiting for your site to build because you have a large scale static website. Every time, you double the number of static pages, you are doubling the build time duration.

If you have an e-commerce website with 10,000 items, each taking about 1 second to fetch/build, you are looking at nearly 3 hours of building and deployment. In this example, you could go the incremental static regeneration pathway, where you can create and update pages after you've built your primary website bundle. This allows you to make changes without rebuilding the entire website.

In modern web development, it is often recommended that users make API calls only enough to fill the user's viewport and potentially prefetch data for subsequent user actions. However, with SSG, you will need to call all the APIs necessary to populate the entire webpage and all potential user's actions.

If you have a frequent update to a page on your website, for example, a Blog page that has multiple sections; SSG might cause you to rebuild the entire site very frequently for changes that are contained in a single page. The pages are generated before the requests are made, as a result, SSG pages don't have direct access to requests like HTTP headers and query parameters at build time. If you want to access these, you have to write custom middleware in addition to SSG.

During the generation process, there are external factors that can ultimately influence page loading speed like fetching data from external APIs. If the APIs are slow to respond, page generation loading times will be slower. Also, because SSR generates each page per request, this made it slower compared to SSG. To enable caching with SSR pages and improve the loading times (speed), you will need to add an additional HTTP header , Cache-Control, when calling "getServerSideProps". See the docs here. Learn more about caching.

SSR requires server to process and generate the front-end on each request, which is less scalable and slower; provided the website have complex pages and a lot of end clients to process. SSR dynamic HTML cannot be cached by static CDNs (Content Delivery Network, e.g. CloudFront), which would lead to longer trips to and from the server, and a slower time between requesting a page and the first byte loaded by a server (TTFB: Time to First Byte).

Although Google and Bing announced they leverage the same Chromium based crawlers with the latest rendering engine, Javascript still complicates the search engine's ability to parse your page and potentially lower your SEO rank. Search engines must download your Javascript files, and potentially not wait for the entire bundle to load before moving onto to other parts of your site. At the end of the day, there is a lot of uncertainty at guessing how Google or Bing ranks your page, and it potentially makes sense to make the crawler's life easier.

Conclusion Both SSG and SSR offer unique advantages for different types of Next.js applications. By understanding your project's content frequency, SEO requirements, user experience, and development complexity, you can make an informed decision about whether to use SSG or SSR in your Next.js project. Ultimately, the choice depends on your specific use case and the trade-offs you're willing to make between performance, scalability, and dynamic content. To get started, try your hand at building a Next.js 13 web app with user authentication using Amplify Hosting.

Alexa Perlov Alexa Perlov is a Solutions Architect at Amazon Web Services, based in New York City. She supports Media and Entertainment strategic accounts in utilizing AWS to achieve their business goals. She leads technical trainings and develops prototypes, highlighting a wide range of cloud domains.

Michael Tran Michael Tran is a Solutions Architect with Prototyping Acceleration team at Amazon Web Services. He provides technical guidance and helps customers innovate by showing the art of the possible on AWS. He specializes in building prototypes in the AI/ML space. You can contact him @Mike_Trann on Twitter.

Visualize hierarchical data using the hierarchy layout for pivot tables in Amazon QuickSight

All your trusted BI capabilities of Amazon QuickSight now come with powerful new AI features that integrate chat agents, deep research, and automation in one seamless experience with Amazon Quick Suite! Learn more »

Are you finding it cumbersome to scroll horizontally in pivot tables to see all your row fields? Are you dealing with data that has a hierarchical structure, like cost center or product hierarchies? If you are, then there's exciting news for you! Amazon QuickSight now offers a new layout option called hierarchy layout for pivot tables. This feature allows you to organize your data in a hierarchical outline format, displaying each level indented under the level above it. In this post, we delve into the advantages of using the hierarchy layout and explore its various configuration options. Hierarchy layout for pivot tables A new layout option known as hierarchy layout has been introduced in pivot tables. Currently, each field in the Rows field well adds a new column to the pivot table, which can be cumbersome for customers with numerous row fields because they have to scroll horizontally to view all fields. The hierarchy layout addresses this challenge by presenting all row fields in a single column in a hierarchical format. This format uses indentations to differentiate items from different fields, resulting in a more readable table. Additionally, this layout is a more condensed display, allowing for more room to display numeric data. In the words of one of our customers, Best Western, "With the recent addition of the hierarchy layout to the pivot tables visuals, the Best Western Hotels team can modify one of our most used dashboards. This dashboard allows the hoteliers to select the content of the first four columns and to omit up to three columns. This feature will allow the hoteliers to view more information without having to scroll right and left. The benefit to everyone is that more information is communicated more quickly."

Choosing between the hierarchy and tabular layout in pivot tables depends on the specific dashboarding and reporting needs. The hierarchy layout is ideal for displaying vast amounts of data while conserving space, making it an excellent choice for presenting hierarchical data in a clear and easy-to-read format. One of the main benefits of the hierarchy layout is that you don't need to display individual row field headings. On the other hand, if you prefer a more traditional table format, the tabular layout is an excellent option. This layout presents each row field in its own column, making it perfect for presenting detailed and granular data, such as survey responses with multiple answer options. Field configurations for Hierarchy layout Hierarchy layout offers different field configurations. For example, the following screenshot shows the hierarchy layout with values in rows.

The following screenshot shows the hierarchy layout with values in columns.

In the following sections, we look at the different features and functionality of hierarchy layouts, along with the process of configuring them. Convert to hierarchy layout All pivot tables created after launch will start with hierarchy layout as the new default experience without affecting existing pivot table visuals. You can toggle to hierarchy layout by navigating to Pivot options on the Format visual menu and selecting Hierarchy.

Sorting Similar to tabular layout, you can sort row, column, and value fields using the refreshed menu options. You can view singular field menu options for column and value fields as compared to combined row field menu options for all row fields together, allowing you to choose your field to sort by and order in which you would like to sort. The following screenshot illustrates sorting using the combined row header menu.

Expand or collapse To reveal detailed information in pivot tables, you have the option to expand row and column fields either through the menu or by choosing the plus sign. For more information, refer to Expanding and collapsing pivot table clusters. Although the plus and minus signs are ideal for expanding or collapsing individual nodes, the context menu offers a range of navigation options, including expanding or collapsing all nodes and expanding or collapsing up to a specific field level. As shown in the following screenshot, you can expand or collapse using the row header menu (choose the row header to display the menu).

You can also expand or collapse a single node using the plus and minus signs or use the menu option to collapse the node or expand all the nested nodes.

As shown in the following screenshot, you can also expand or collapse individual columns.

Row header styling The header styling options between the tabular and hierarchy layout remain largely unchanged except for the row header, which has new options like the ability to hide or rename the row label to something meaningful and apply the same row header styling to row labels.

Subtotals Styling We have enhanced the subtotal options in pivot tables with a new control called Apply styling to. This allows authors to choose their preferred styling for subtotals, including value only (new option), entire row (header and value), and header only, providing greater flexibility. It's worth noting that this option is also available for tabular layouts, ensuring a seamless transition between different layouts.

Export When exporting to Excel, there will be differences in the output between the hierarchy layout and tabular layout in pivot tables. The exported Excel file will reflect the specific layout chosen, providing accurate representation of the data based on the selected layout. The following screenshot shows an example output in hierarchy layout.

Other updates The pivot table update also includes redesigned context menus, and it now displays renamed columns in the sorting menu. Redesigned context menus As part of the update, we have redesigned the context menu options for pivot tables, replacing the super menu with a more condensed layout. This change ensures a unified experience for both authors and readers. Although both authors and readers can access field-level configurations through the context menu, authors have the additional capability to access further configurations directly from the field wells. The context menu for row, column, and value fields has been revamped to provide a consistent experience across both layouts. The only distinction is the inclusion of a combined menu option specifically for rows in the hierarchy layout. The following screenshots compare the old and new experience.

Display renamed column names in the sorting menu Previously, when renaming a column using the format pane, only the header display name was updated, causing confusion because the sort menu still showed the original column name. However, this issue has been addressed. Renamed columns now reflect the updated name in both the column headers and sort menus. For instance, if we rename Gross Margin to GM%, the new name will be consistently displayed in both the column header and sorting menu, as demonstrated in the following screenshot.

Conclusion This post explored the hierarchy layout feature available in pivot tables, highlighting its various functions such as interaction, styling, and export capabilities. The hierarchy layout can help condense multiple row levels into a more simplified and easily readable format, offering a practical solution for working with complex data. Start using hierarchy layout to enable hierarchical navigation and provide a drill-down experience on pivot tables today! If you have any questions or feedback, please leave a comment. For additional discussions and help getting answers to your questions, check out the QuickSight Community.

About the author Bhupinder Chadha is a senior product manager for Amazon QuickSight focused on visualization and front end experiences. He is passionate about BI, data visualization and low-code/no-code experiences. Prior to QuickSight he was the lead product manager for Inforiver, responsible for building a enterprise BI product from ground up. Bhupinder started his career in presales, followed by a small gig in consulting and then PM for xViz, an add on visualization product. Jose Banos is a Sr. WW Go-To-Market QuickSight Specialist focused on creating QuickSight awareness worldwide. He has over 17 years of experience in fields like Telco, cyber-security and data analytics. Jose started his career as presales, moving into business development and customer success working with customers in 4 continents and living in 3 different countries. He leads go-to-market plans across marketing, sales, and customer success and he enjoy helping customers to increase their revenue and reduce cost using QuickSight and other AWS services.

Realize Faster Time to Value with IBM's Modern Data Accelerators on AWS

Use cases for data have changed and expanded from the data warehouse days, as digital transformation has unleashed new and existing uses for data services. Many organizations still are managing expensive single use case data environments, however, and IBM's Modern Data Accelerators on AWS can help build modern implementation of a data fabric architecture, enabling customers to realize faster time to value. Customers tend to maintain the following data environments:

Data warehouses for batch business intelligence and reporting. Data lakes for data science predictive model testing and development. Digital databases for digital interactions.

These single-use environments tend to have significant duplication in data sourcing from both internal and external sources, leading to poor data quality, high maintenance costs, and inflexible data environments. In this post, we will discuss how IBM's Modern Data Accelerators can help build modern implementation of a data fabric architecture, which standardizes data integration across the enterprise. IBM Consulting is an AWS Premier Tier Services Partner and is recognized as a Global Systems Integrator (GSI) for many competencies including Data and Analytics Consulting, which positions IBM to help customers who use AWS to harness the power of innovation and drive their business transformation. Challenges with Legacy Data Environments Generally, legacy data environments are inter-connected with event-driven inbound and outbound digital channels, using the predictive models which are built in the data science sandboxes. However, real-time visualization requires a higher level of integration of an organization's data use cases. This challenge can be solved by building data environments into multi use-case data platforms that provide:

Common data provisioning data lakes: Cost effective, cloud-based stores to source real-time and batch data from internal and external sources. Integrated conform layer: Enterprise data conformed for specific domains such as customer, product, and transaction. Consumption layer: For infomarts, data science sandboxes, digital data stores, and operational data use cases.

A multi-use case data platform with common data provisioning that's instantiated on AWS generally provides a lower cost and more flexible data environment with high data quality. Modern Data Accelerators Offering by IBM Consulting IBM Consulting has built an integrated set of assets called Modern Data Accelerators that boost time-to-value and reduce delivery time as well as cost. Modern Data Accelerators allow enterprises to build out the multi-use case data platforms, and have been developed as an end-to-end set of assets that can quickly instantiate data capabilities for a data cloud implementation on AWS. These assets have been deployed in multiple geographies, and across industries.

Figure 1 – Modern Data Accelerators. It's important to note these assets are not just software. They are not extract, transform, load (ETL) tools, databases, big data stores, or other data management software. This is a combination of all of these which enable the usage of the data management services faster. There are essentially two options to accelerate data management services:

Perform traditional analysis, design, and coding. Install and configure assets, and then extend them for specific requirements.

Solution Overview The Modern Data Accelerators are known data management processes that have been codified to drive business value faster. These assets drive the lifecycle of a data environment from development to maintenance and ingestion, through analytics. Modern Data Architecture Components The following components provide a brief overview of IBM's Modern Data Accelerators.

Workload analysis and modernization automation: Creates persona-based, detailed inventories of data ecosystems and their dependencies. Provides automated translation of legacy data- processing code, and automated testing, which is essential for creating actionable migration roadmaps. Real-time and batch intelligent integration engine: A Kafka-based, real-time and batch engine that creates data pipelines for managing the ingestion, organization, and publication of data. These engines include custom connections, unified batch and stream capabilities, and ingestion APIs. The data onboarding process learns and adapts using machine learning models. Digital integration for intelligent workflows: This is a data provisioning asset for intelligent workflows that's designed for enterprise and unstructured data. Data mesh console: Provides an operational interface for data mesh implementations, which helps manage the lifecycle of data products and their dependencies. The data mesh console is designed to integrate and leverage data catalogs and data marketplaces, while also providing data product observability metrics. AI-driven cognitive classifier: The artificial intelligence-driven cognitive classifier automates the classification and organization of data against enterprise canonical models and provides real-time insight into data quality. Automated data tagging provides classification for the application of security policies across the data platform. Once trained on an organization's data, the tool can be integrated in the intelligent integration engine to classify, auto-map data, and send notifications in both real-time and batch mode. Lightweight master data management (MDM): Provides multi-domain entity matching capabilities that identifies duplicates or suspected duplicate entities across large datasets, using both probabilistic and deterministic matching logic. It's built leveraging both graph database and Elasticsearch capabilities.

Real-time and batch monitoring: Data quality across all components in the data fabric environment is maintained using customizable, open-source Grafana dashboards and portlets. Using modern technologies such as Airflow, the solution is able to provide a high degree of automated data pipeline and structure observability. Data marketplace: This is the central provisioning point for data consumption in the modern data platform on AWS. Data is persisted in original and curated forms with purpose-fit storage, allowing for publication and subscription of data. Data science marketplace: Contains a set of cloud-ready data science models that can be accessed and stored in the model marketplace, hosted in the data platform.

As stated earlier, each of these assets help accelerate time to value of data cloud solutions on AWS. See Figure 5 to understand how all of these components come together to build the Modern Data Accelerators.

Figure 3 – Various components of Modern Data Accelerators. Customer Success Story The following case study is a good representation of how the Modern Data Accelerators have helped accelerate customers' time to value. IBM Consulting was engaged to assist a large healthcare organization to transform its use of its analytic data in more productive and proactive ways while reducing the overall cost. The organization had a large legacy relational database that was primarily used for reporting purposes. The first step was to analyze the existing database workload with IBM's Workload Analysis tool to determine the types of data and personas using that data. With the objective to change the way business users consume data, IBM Consulting ingested data from hundreds of sources using IBM's real-time and batch intelligent integration engine into a new Amazon Simple Storage Service (Amazon S3)-driven data lake, where IBM envisioned 14 first-of-a-kind predictive models to define "member health profile" in 12 weeks. IBM used the data science marketplace to store the predictive models that were easy to reuse in subsequent models. The early success provided the momentum to right-size the end-state data platform, curate 23 new datasets into production in three months, and leverage real-time and batch monitoring capabilities to bring the new, single AWS Cloud platform into production within six months. Conclusion IBM's Modern Data Accelerators on AWS is a proven solution to reduce delivery time and cost for customers. It mitigates the risk of maintaining legacy data platforms while improving time to value. The Modern Data Accelerators help achieve the following objectives in a data fabric, data mesh, or modern data stack implementation:

Manage: Singular and integrated means of management of data at rest, in motion, and integration of the data. Govern: Map all enterprise assets to a single canonical model and data catalog. Secure: Information classification serves as the single means of defining security policies and entitlements.

To learn more about the solution, check out IBM's data platforms page. You can also refer this AWS blog post to explore modernizing data platforms, accelerating innovation, and unlocking business value with data mesh on AWS. .

IBM – AWS Partner Spotlight IBM Consulting is an AWS Premier Tier Services Partner and MSP that offers comprehensive service capabilities addressing both business and technology challenges that clients face today. Contact IBM | Partner Overview | Case Studies

TAGS: AWS Competency Partners, AWS MSP Partner Program, AWS Partner Guest Post, AWS Partner References, AWS Partner Success Stories, AWS Premier Tier Services Partners, AWS Public Sector Partners, AWS Service Delivery Partners, AWS Solution Provider Partners, AWS Well-Architected Partners, IBM, Managed Service Provider

Japan Manned Space Systems uses AWS Snowcone to automate and optimize data delivery from space to Earth

Pictured: AWS Snowcone SSD onboard the International Space Station (ISS). Image credit: NASA. Approximately 400 kilometers above the Earth's surface, Japan Manned Space Systems Corporation (JAMSS) used an AWS Snowcone SSD onboard the International Space Station (ISS) to quickly and autonomously transmit large volumes of data from space back on the ground into Amazon Web Services (AWS). The AWS Snowcone, which arrived on the ISS in July 2022 as part of the first Axiom Space Mission (Ax-1), is available to power on-orbit data processing experiments for researchers, students, and scientific organizations. Since its founding in 1990, Tokyo-based JAMSS has been entrusted with the operation of the Japanese Experiment Module (JEM), known as "Kibo," onboard ISS. JAMSS technologies can help ISS crew members plan, execute, and control space experiments in the Kibo module, as well as training for astronauts and flight controllers. Leveraging this unique and extensive experience, JAMSS seeks to become a global leader in crewed space expedition solutions for next-generation commercial space missions. Kibo means "hope" in Japanese, representing the infinite possibilities of science and technology to become a unifying hope for humanity. Similarly, JAMSS strives to bridge Earth and outer space in order to contribute to humanity and society, integrating the latest technologies to build new space solutions. As part of this initiative, JAMSS developed the JAMSS Automated File Dump System (JAFDS) solution that leverages the edge computing capabilities of the Snowcone to autonomously downlink, validate, and retransmit large files from the ISS to the ground. On April 21 2023 (UTC), JAMSS, in collaboration with AWS and Axiom Space, successfully demonstrated software that will help scientists transmit large amounts of data from space and help scientists on Earth analyze the information more quickly. Using AWS edge compute for space station data transmission solutions The ISS JEM, Kibo, houses research facilities in which astronauts conduct various experiments focused on space medicine, biology, Earth observations, material production, biotechnology, and communications research. During peak times, over 1 terabyte (TB) per week of data is downlinked from Kibo to Japan via the US National Aeronautics and Space Administration (NASA) Tracking and Data Relay Satellite (TDRS). TDRS is a network of geosynchronous (GEO) satellites that provide communication relay links for many missions such as the Hubble Space Telescope and the ISS. One of JAMSS's missions is to provide communication solutions for next-generation, low Earth orbit (LEO) commercial space station missions. In these future applications, space station operators may need to procure and distribute bandwidth to multiple users, or enable multiple satellite communication providers to service a single space station and contract bandwidth to individual end users. In such cases, space stations may be servicing discrete data transmissions for multiple end users, and each user will need to manage and optimize their data transmission within an allocated schedule and bandwidth. By leveraging cloud flexibility and edge computing with AWS, JAMSS can allocate communication per user for a determined time period according to a predefined delivery schedule and bandwidth. It also can specify discrete data delivery destinations per user when there are multiple communications. Solving the challenges of space data transmission without manual intervention Space communication is rarely constant. For example, during data downlink from space, packet loss may occur depending on communication conditions and network design, resulting in data acquisition delays. Furthermore, data retransmission may take multiple contact windows, which results in wasted communications opportunities and additional delays in data acquisition. In some cases, data downlink connection loss scenarios requires JAMSS to deploy operation and control personnel in on-ground mission operations centers to manually perform loss confirmation and retransmission requests and to manage these systems around the clock. To help customers avoid data acquisition delays and added cost associated with data downlink loss, JAMSS innovated the JAFDS solution. JAFDS consists of two software components: an onboard software component that is deployed to an Amazon Elastic Compute Cloud (Amazon EC2) instance in the AWS Snowcone in space, and a ground software component that runs on a virtual machine instance on the ground. Files desired to be transmitted are defined on the onboard software with a downlink schedule and selected bandwidth. When the scheduled communication window begins, the onboard software starts sending files to the ground software. The ground software automatically identifies data loss events, scheduling and retransmitting only lost file sections. Hash value matching is also confirmed to validate the integrity of the files. This reduces the number of connection windows needed to validate and reschedule failed transmissions, leading to faster data delivery and improved efficiency through autonomous operation. Seamlessly reproduce on-orbit data structures on the ground Large-capacity experimental data and video files are stored on an orbital file server and then downlinked in batches. JAFDS automatically compresses, encrypts, transmits, and re-creates the files from the on-orbit Amazon Elastic Block Store (Amazon EBS) volume to the ground software location while maintaining directory structure and file names. The ground software can be flexibly deployed on either on-premises virtual machines to support customized mission control centers, or on Amazon EC2 instances as part of a cloud-based mission operations center deployment on AWS. This allows access to files and experimental data as if they were on the ISS, and allows on-ground researchers and users to continue analysis activities and distribute data seamlessly. Furthermore, in future commercial space stations where multiple customer workloads may be hosted in the same station, JAFDS can preserve the isolation and integrity of file storage systems that may be accessed by many different end users.

Figure 1. JAFDS consists of two software components: an onboard software component that is deployed to an Amazon EC2 instance in the AWS Snowcone in space, and a ground software component that runs on a virtual machine instance on the ground. Ground operators define the files to be transmitted with a downlink schedule and desired bandwidth. Experimental data from the Snowcone is transmitted by the JAFDS onboard software according to the defined schedule through the TDRS network to the ground. The JAFDS ground software automatically identifies data loss events, scheduling and retransmitting only lost file sections. This reduces the number of connection windows, leading to faster data delivery and improved efficiency through autonomous operation. Technical demonstration on the AWS Snowcone aboard the ISS In the orbital demonstration carried out on April 21, 2023 (UTC), the JAFDS software was deployed and executed on an Amazon EC2 instance in the AWS Snowcone installed on the ISS. JAMSS conducted a test in which 1 gigabyte (GB) files were created for three different users and transmitted sequentially to ground. The technical demonstration successfully reproduced all the files to a virtual machine on the ground over a defined communication bandwidth (5 Mbps) and delivery schedule. The JAFDS software also successfully demonstrated autonomous downlink operation by automatically detecting file losses and then retrieving and retransmitting the previously lost data. The JAFDS solution is positioned to serve the data transmission needs of future commercial LEO use cases by leveraging AWS Snowcone's space-capable edge compute capabilities. JAMSS plans to further develop and demonstrate edge processing solutions on AWS for space applications to accelerate further innovation in the space data ecosystem. Learn more about AWS for aerospace and satellite Organizations of all sizes across all industries are transforming and delivering on their aerospace and satellite missions every day using AWS. Learn more about the cloud for aerospace and satellite solutions so you can start your own AWS Cloud journey today. Get inspired. Watch our AWS in Space story. Read more about AWS for aerospace and satellite:

Amazon and AWS to reimagine space station operations and logistics for Orbital Reef How Satellogic and AWS are harnessing the power of space and cloud AWS selects 13 startups for the 2023 AWS Space Accelerator What we learned at Amazon re:MARS 2022 for the public sector Managing the world's natural resources with earth observation AWS joins the Digital IF Interoperability (DIFI) Consortium How Natural Resources Canada migrated petabytes of geospatial data to the cloud

Subscribe to the AWS Public Sector Blog newsletter to get the latest in AWS tools, solutions, and innovations from the public sector delivered to your inbox, or contact us. Please take a few minutes to share insights regarding your experience with the AWS Public Sector Blog in this survey, and we'll use feedback from the survey to create more content aligned with the preferences of our readers.

TAGS: aerospace and satellite, Asia Pacific, AWS Snowcone, Axiom Space, Japan, NASA, public sector, satellite, satellites

Emma Higashikawa Emma Higashikawa is a solutions architect on the Aerospace and Satellite (A&S) team at Amazon Web Services (AWS). Emma helps space industry customers achieve their missions and solve technical challenges by working together to design and implement end-to-end solutions leveraging AWS.

How to validate authentication with self-signed certificates in Amazon CloudWatch Synthetics

In today's digital landscape, ensuring optimal application performance is crucial, and Amazon CloudWatch Synthetics enables proactive testing of web applications and APIs. If you are utilizing self-signed certificates and seeking to enhance your monitoring capabilities, this blog post will guide you step-by-step on how to modify the source code of your canary to support self-signed certificates for your endpoints, and the best part is that this method won't require you to change your authentication process. We will demonstrate how to utilize the Amazon CloudWatch Synthetics canary with the heartbeat blueprint in order to monitor an application that relies on a self-signed certificate, which are certificates issued and signed by a non-recognized and trusted third-party Certificate Authority (CA). Furthermore, we recommend you check out our previous blog posts, which covers other methods to monitor APIs. The blog post titled Multi-step API monitoring using Amazon CloudWatch Synthetics provides detailed steps for creating an Amazon CloudWatch Synthetics script to execute multi-step API verification, ensuring the functionality and stability of your APIs. Additionally, our second blog post How to validate authentication using Amazon CloudWatch Synthetics – Part 2 offers step-by-step guidance on updating a synthetic blueprint to monitor applications that requires both the server and the client to present their digital certificates for a mutual authentication (mTLS). These resources will complement your understanding and enable you to build a comprehensive monitoring strategy. Solution Overview In this solution, we will demonstrate the steps to create a Heartbeat monitor canary based on an existing blueprint and insert a self-signed certificate within the provided blueprint, allowing the expansion of capabilities provided by the blueprints. Creating the canary CloudWatch Synthetics lets you utilize blueprint scripts that are ready to be consumed. However, we must utilize the editor in the console to add the extra code snippets in order to authenticate with a certificate. To simulate how CloudWatch Synthetics handles authentication, we will use the self-signed.badssl.com website. You can also use your own HTTP endpoint to simulate the same output. The first call will return a failure response, as the certificate has not yet been added. However, the error will be fixed in the next steps. Steps to create a Canary based on Heartbeat blueprint script: 1. Open the Synthetics menu of the CloudWatch console. 2. Choose Create Canary. 3. Choose Heartbeat monitoring from the blueprints list. 4. Under Name, enter a name for your canary – for example, https-selfsigned-test. 5. Enter the URL under the Application or endpoint URL. 6. Please leave all other settings as default, the IAM role with required permissions will be created for you and also a bucket with name cw-syn-results-accountID-region. In case the bucket already exists in the account, you can search for the name cw-syn-results or create your own, more details at Creating a canary. 7. Scroll to the bottom and select Create canary. When the canary is created, it will be displayed in the Canaries list, as shown in Figure 1. For information about utilizing the heartbeat monitor canary blueprint, see in the Heartbeat monitoring Amazon CloudWatch User Guide.

Figure 1: Canaries page of the CloudWatch console Checking reports The canary reports show every step and result of the calls. In this case, the canary returned the ERR_CERT_AUTHORITY_INVALID error as shown in Figure 1. This error is expected, as the endpoint uses a certificate that is not issued by a trusted CA and the canary cannot verify the authenticity of the self-signed certificate against a trusted authority.

Figure 2: http-steps-test report failing with the error ERR_CERT_AUTHORITY_INVALID Downloading the certificate details To address the authority issue with self-signed certificates, we can download the necessary certificate from the badssl website. This downloaded certificate will serve the purpose of establishing trust, and it will be used by the canary to authenticate API requests made to the self-signed.badssl.com endpoint. The following steps demonstrate how to achieve this using AWS CloudShell using CLI commands, however, these certificates can also be downloaded manually from the badssl website by using a browser. Note: We do not recommend self-signed certificates unless you are certain you trust the source. Downloading the public certificate from badssl.com For this procedure, we will use AWS CloudShell, a browser-based shell that makes it easy to securely manage, explore, and interact with your AWS resources. We recommend CloudShell to run the scripts below. However, you can use your own command line for the same output. Follow the steps below to download the .pem file from self-signed.badssl.com 1. Open the CloudShell console. 2. Wait for the environment to be created. 3. Copy and paste the script below in the bash console created by the AWS CloudShell. # Install openssl if you don't have that installed # Ubuntu users -> sudo apt-get install openssl sudo yum install openssl -y

# Download the self-signed.badssl.com certificate openssl s_client -showcerts -connect self-signed.badssl.com:443 -servername self-signed.badssl.com 1 ? "\\n" : ""), $0} END {print ""}' > self-signed.pem

# Get the certificate content to be pasted on the canary code cat self-signed.pemBash 4. Choose Paste.

Figure 3: CloudShell popup to paste multi-line text 5. The script above will generate a text with the certificate, so copy the content beginning at —–BEGIN CERTIFICATE—– and ending at —–END CERTIFICATE—– which will be used in the next steps. Editing the canary script to add the certificate: To import the certificate, you need to access the created canary and add a variable with the previously copied certificate output. To edit the code, follow these steps: 1. Open the Synthetics menu of the CloudWatch console. 2. Choose the canary created above – for example, https-selfsigned-test. 3. Choose Actions and then choose Edit. 4. Using the Script Editor box, import the 'https' package at the top of the script. This packages will let the script adjust protocol settings to feed in the certificate. const https = require('https');JavaScript 5. Insert this block of code below prior to where you attempt to use the 'page.goto()' function to access your endpoint. For the blueprint, this is within the loadBlueprint async function between the 'const sanitizedUrl' line and the 'const response = await page.goto...' line: await page.setRequestInterception(true);

page.on("request", (interceptedRequest) => { const options = { method: interceptedRequest.method(), headers: interceptedRequest.headers(), body: interceptedRequest.postData(), ca: cert, cert: cert, }; const request = https .request(interceptedRequest.url(), options, function (response) { response.on("data", function (data) { interceptedRequest.respond({ status: response.statusCode, contentType: response.headers["content-type"], headers: response.headers, body: data, }); }); }) .on("error", function (err) { console.error("Unable to call %s", options.uri, err); return interceptedRequest.abort("connectionrefused"); }); request.end(); }); JavaScript Note: This code will attempt to intercept the request being made from your canary to your endpoint. From there, it should modify the request options to include your uploaded certificate as the trusted certificate authority. 6. Add the certificate details by replacing the from the code you just pasted above. const cert = ' 'JavaScript 7. Choose Save. Although hard coding certificates is not considered a best practice, we opted for that approach in this blog post to streamline the steps. Additionally, as the certificate is public, the risk is lower. To follow best practices and reduce risks associated when deploying in your environment, you can use AWS Secrets Manager to store the secrets in a secure way as explained on How to validate authentication using Amazon CloudWatch Synthetics – Part 2. After modifying the code, save and wait for the canary to run again. Next, canary run should be PASSED with the steps tab showing the request status as PASSED.

Figure 4: https-selfsigned-test showing the status as Passed For users who are already familiar with how to use Synthetics, an advanced option is available that enables the bundling of a canary script with the certificate into a zip file. This approach is explained in the documentation called Writing a Canary Script for Node.JS or Python, providing comprehensive guidance for seamless implementation. Clean Up It is important to note that deleting a canary is irreversible and all associated data and settings will be lost. Make sure you have made all necessary backups or extracted all relevant data before proceeding with the deletion. To delete a CloudWatch Synthetics canary, follow these steps: 1. Open the Synthetics menu of the CloudWatch console. 2. Choose the canary created above – for example, https-selfsigned-test. 3. Choose Actions and then choose Stop. 4. Wait for the canary to change the state to Stopped. 5. Choose Actions again and then choose Delete. 6. Choose the resources you want to delete, for example the Role and Role policy used by the canary. 7. Choose Delete to delete the resources created by the canary. 8. Enter the phrase Delete into the field to confirm that you want to delete the resources created during this workshop. 9. Chose Confirm. The console will delete the selected resources and display Canary successfully deleted.

Figure 5: Synthetics page showing that the canary was deleted Conclusion In this post, we explored how to utilize self-signed certificates with CloudWatch Synthetics, expanding the capabilities of blueprints to connect to APIs, even if the certificate is not signed by a recognized and trusted third-party CA. Now, armed with these insights, you can confidently implement self-signed certificates in your own projects. To learn more about how to use this feature and all other capabilities, read the CloudWatch Synthetics documentation. Also, the AWS Command Line Interface (CLI) documentation for Synthetics can be found here. About the authors

Mario Jorge Salheb Leitao Mario Leitao is a Senior Cloud Architect with the AWS Managed Services (AMS) team based in Sydney. He provides technical guidance, designs and builds cost-effective and efficient solutions for customers ensuring their success on AWS. Prior to working as a Cloud Architect, he worked as a Network Development Engineer in Dublin, Ireland.

Matheus Canela In his role as Solutions Architect at Amazon Web Services, Matheus advises digital-native companies in the transformation of their technology platforms, helping all levels of engineers to achieve their goals by following best practices. Before he joined AWS, Matheus was a Senior DevOps Consultant at CMD Solutions, which is a Premier Consulting Partner based in Sydney. Matheus has also worked as a developer and a security specialist. Because helping the community is in his DNA, Matheus organizes .NET meetups and helps the IT.BR community, supporting qualified engineers from Brazil to migrate to Australia.

Edgar Yu Edgar is a Software Development Engineer on the CloudWatch Synthetics team at AWS. His current focus revolves around building and improving software to empower customers to gather more performance data and achieve better monitoring over their web applications. Outside of AWS, he enjoys food.

Matheus Canela O. Faraco In his role as Solutions Architect at Amazon Web Services, Matheus advises digital native companies in the transformation of their technology platforms, helping all levels of engineers to achieve their goals by following the best practices. Before he joined AWS, Matheus was a Senior DevOps Consultant at CMD Solutions, which is a Premier Consulting Partner based in Sydney. Matheus has also worked as a developer and a security specialist. Because helping the community is in his DNA, Matheus organizes .NET meetups and helps the IT.BR community, supporting qualified engineers from Brazil to migrate to Australia.

Use metric name filters in Amazon CloudWatch Metric Streams for finer grained control

Amazon CloudWatch Metric Streams now supports filtering by metric name. This blog will show you how to gain finer control over the metrics you want to stream using Include and Exclude metric name filters in Metric Streams. Amazon CloudWatch Metric Streams simplifies the way you collect and analyze metric data by providing continuous, near-real-time streams of Amazon CloudWatch metrics to a destination of your choice. You can use metric streams to send metrics to AWS Partner Solutions, including Datadog, Dynatrace, New Relic, Splunk, and Sumo Logic. Alternatively, you can send metrics to your data lake built on AWS, such as to Amazon Simple Storage Service (Amazon S3). You can create a continuous, scalable stream that includes the most up-to-date CloudWatch metrics to power dashboards, alarms, and other tools that rely on accurate and timely metric data. Traditionally, customers and AWS Partners relied on polling CloudWatch APIs to retrieve metrics that power all sorts of monitoring, alerting, and cost management tools. Since the introduction of Metric Streams in 2021, customers had an alternative way to create low-latency scalable streams of metrics. At launch, Metric Streams included the ability to filter metrics at a namespace level; for example, to include or exclude all metrics from a specific namespace. However, some customers wanted the ability to filter at a more granular level. That's where Metric Name Filtering in Metric Streams comes into play, addressing the need for more precise filtering capabilities. Customers want the flexibility to be fast and specific based on their use case and needs when streaming their metrics. To address the customer need of being more specific on what metrics to stream, CloudWatch Metrics Streams added support for filtering by metric name in May 2023 (Amazon CloudWatch Metric Streams adds support for filtering by metric name). This additional filtering capability provides specific metric name selection in addition to the existing option to filter metrics by namespace. You can now select what metrics to include or exclude for each streamed namespace. Now Metric Streams gives customers both things: The low latency and reduced operational overhead already offered by Metric Streams, plus the additional specific control to choose what specific metric names to stream. As Metric Streams pricing is based on the quantity of Metric Stream updates being egressed, metric name filtering is also a great mechanism for finer cost control. For more information, see Amazon CloudWatch pricing. Metric Streams Filtering by Metric Name: Include Vs Exclude The filtering by metric name feature of Metric Streams allows you to choose a list of metrics to include or exclude from the stream for each metric namespace. The "Include" and "Exclude" are mutually exclusive, you can't have both in a single stream. However, both types of filters provide valuable additional control over your streamed metrics depending on your use case. The "Include" filters have the benefit of allowing you to exactly choose what metrics to stream. However, it requires you to maintain an updated list of metric names you need, and you will not automatically receive any new metric added by AWS services unless you manually update your list of included metric names. In contrast, the "Exclude" filters have the benefit of maintaining a likely shorter list of metrics you don't want and also the fact that you will automatically receive any new metrics added to your streamed namespaces with no further action. Additionally, Metric Streams allows you to create metric name filters on metrics that may not exist yet in your AWS account. For example, you may know your application will produce metrics from the "AWS/EC2" namespace, but that application might not have been deployed yet in that account. If that is the case, those metrics will not exist yet in your CloudWatch account. The drop-down selectors in the Metric Streams console will only show metrics that currently exist in CloudWatch. However, the CloudWatch Metric Streams console allows you to free type whatever combination of namespace and metric names you might need for your filters, allowing you to create filters on metrics that may not exist. Likewise, you can also create metric name filters on non-existing metrics via AWS CloudFormation. The same applies to custom metrics and namespaces. This functionality is further explained in the following walkthrough. Walkthrough using the AWS Management Console In this section, we will demonstrate how you can gain finer control over metrics you want to stream using include and exclude metric name filters using console. Steps to use "Include" metric name filter while creating Metric Stream: 1. Open the Amazon CloudWatch console. 2. In the navigation pane, choose Metrics Streams. Then choose Create metric stream. 3. Under Select namespaces you wish to stream section, choose Selected namespaces and then select the namespaces to include to include only a few metric namespaces in the metric stream. 4. Under the Select metrics for the metric stream, choose Include metrics by metric name. Specify each metric to include by selecting its namespace and then its metric name. As mentioned above, the console will only show metrics that currently exist in CloudWatch. For that reason, some or all of the metrics shown in the below screenshot may not show in your console, depending on what resources you have deployed in your account at the moment. However, you can still add the filters shown in this example by free typing the namespace and metric names in their respective drop-down menus. After you select one metric, you can repeat this step to include another metric from the same namespace or a different one. If you don't select any metric name for one of your included namespaces, then all metrics from that namespace will be included in the metric stream.

5. Under Configuration section, select Quick S3 setup option. 6. Keep the rest of the configuration as default and then choose Create metric stream button. Your stream with the desired filters will be created in a few seconds. Steps to use "Exclude" metric name filters while creating a Metric Stream Now you are going to create a completely new stream, but this time using "Exclude" metric name filters. These steps are independent from the previous section about "Include" filters. 1. Open the Amazon CloudWatch console. 2. In the navigation pane, choose Metrics Streams. Then choose Create metric stream. 3. To demonstrate the "Exclude" use case, we will exclude multiple metrics across different namespaces. For that, select "All namespaces" under the Select namespaces you wish to stream section. 4. In the "Select metrics for the metric stream" section, choose the option to "Exclude metrics by metric name". Specify each metric to exclude by selecting its namespace and then its metric name. Similar to the "Include" example before, some or all of the metrics shown in the screenshot below for this particular example may not show in your console, depending on the existence of those metrics in your CloudWatch account. However, you can still add the filters shown in this example by free typing the namespace and metric names in their respective drop-down menus. After you select one metric, you can repeat this step to exclude another metric from the same namespace or a different one. You can check Figure 2 for further reference.

Figure 2: Stream creation using exclude metric name filters from a specific namespace

5. Under Configuration section select Quick S3 setup option. 6. Keep the rest of the configuration as default and then choose Create metric stream button. Your stream with the desired filters will be created in a few seconds. Now we will explore another use case for "Exclude" filters where we exclude a subset of namespaces entirely and then specify certain metric names to exclude from the remaining namespaces. The steps to configure this remain the same as explained above. The only additional configuration you need to add is to expand the optional menu "Exclude metric namespaces" and select the namespaces to be excluded entirely as shown in Figure 3. In Figure 3 below, we demonstrate the exclusion of two namespaces entirely (AWS/Usage and AWS/Billing), resulting in no metrics being streamed from these namespaces. Additionally, we can exclude specific metrics from the other namespace by selecting "Exclude metric by metric name" in the optional menu "Select metrics for the metric stream".

Figure 3: Example showing how to exclude namespaces fully and specific metrics from the remaining namespaces

Viewing metrics stream details and metric data being streamed to Amazon S3 destination You can open the newly created metric streams by selecting the specific metric stream in the CloudWatch Metric Streams console, and then choose View details button as shown below:

You can check the implemented metric name filters by scrolling down on the same page under "Selected metrics" section, as shown in the screenshot below.

Figure 5: Implemented metric name filters in the metric stream detail page

CloudWatch Metric Streams will create the following resources on your behalf when you create the stream using the console:

An Amazon S3 bucket: Destination for Kinesis Firehose delivery stream Amazon S3 write role: Role to allow a firehose write an Amazon S3 bucket Firehose write role: Role to allow a metric stream put metrics into a firehose Kinesis Firehose: Delivery stream to which metrics would be pushed

You can view the associated resources created in the metric stream detail page, as shown in the screenshot below.

Figure 6: Resources created by Metric Streams in the metric stream detail page

Finally, on the same page, you can explore the Amazon S3 bucket in order to view the metric data being streamed using the link under "Destination details". It may take few minutes for metric data to be streamed to Amazon S3 bucket.

Figure 7: Amazon S3 bucket destination link in the metric streams detail page

You will be redirected to the Amazon S3 bucket console page where you can see the metric data being stored as shown below.

Figure 8: Metric Data stored in the Amazon S3 bucket

Walkthrough using AWS CloudFormation You can also configure the include or exclude filters while creating Metric Streams using AWS CloudFormation. Steps to create CloudFormation stacks The following step shows you how to create a metric stream with either "Exclude" or "Include" metric name filters programmatically, using AWS CloudFormation. 1. Download the below CloudFormation templates and save them to your local machine. Choose one depending on your use case: Include or Exclude filters. a. MetricStreams_ExcludeFilter_CFN.yaml – Use this CloudFormation template to create a metric stream with "Exclude" metric name filters. b. MetricStreams_IncludeFilter_CFN.yaml – Use this CloudFormation template to create a metric stream with "Include" metric name filters. 2. Navigate to the AWS CloudFormation service in the AWS Management Console. From the Stacks list, select Create Stack > With new resources (standard) to get started with stack creation.

3. Select Template is ready and select Upload a template file. Choose Choose file and upload the template "MetricStreams_ExcludeFilter_CFN.yaml" you downloaded in step one. Once successfully uploaded, choose Next.

4. Name the CloudFormation stack as "exclude-metric-name-filter-demo" and choose Next.

5. Use default settings for the Configure Stack Options and choose Next. 6. Review the stack options and select the "I acknowledge that AWS CloudFormation might create IAM resources" check box and then choose Submit. Your CloudFormation stack status should now be showing CREATE_IN_PROGRESS indicating that the resources defined within the stack are being provisioned. 7. When the CloudFormation stack has finished creating resources (this will take few minutes), the status will change to CREATE_COMPLETE.

You can follow the above same steps in order to create another stack by using "MetricStreams_IncludeFilter_CFN.yaml" template downloaded in step one in order to create metric stream with include metric name filter resources. You can name this second stack as "include-metric-name-filter-demo" as shown below.

8. Let's verify the creation of the desired resources. Open the Amazon CloudWatch console. In the navigation pane, choose Streams under the Metrics tab. You can then open the newly created metric stream by selecting the metric stream and then choose "View details" button as shown below.

Figure 14: Viewing details of a newly created metric stream using AWS CloudFormation

Figure 15: Viewing details of a newly created metric stream

Cleaning up the resources created In order to clean up the resources created by Metric Streams using the AWS Management console walkthrough steps, please follow the steps below. 1. Open the Amazon CloudWatch console. 2. In the navigation pane, choose Metrics Streams. Then choose the metric stream that you want delete and choose "Delete" button as shown below.

Figure 17: Deleting metric stream resource created using AWS Management Console

Note: The resources mentioned in the above screenshots are not deleted when you delete the metric stream using console. You need to explicitly delete them in their associated consoles. In order to clean up the resources created by metric stream using the AWS CloudFormation walkthrough, please follow the steps below. There are couple of prerequisites that you need to complete before deleting the resources created by AWS CloudFormation stacks. Prerequisite 1 Navigate to the Amazon CloudWatch console and select the metric stream created using AWS CloudFormation. Choose "Stop" to stop streaming any new metric updates to Amazon S3 bucket. Prerequisite 2 Once you have stopped streaming metrics to Amazon S3 bucket you need to empty it. Please follow the below steps to locate your Amazon S3 bucket and then empty it:

Navigate to the AWS CloudFormation console. From the Stacks list, select stack that you want to delete and go to "Outputs" section. Copy the Amazon S3 bucket name.

Navigate to the Amazon S3 console, locate the Amazon S3 bucket name that you copied in the previous step and then choose Empty. Once the bucket is emptied move on to the next steps.

Steps to delete AWS CloudFormation stacks 1. In the AWS CloudFormation console, select the stack named "exclude-metric-name-filter-demo". 2. In the top right corner, choose Delete.

3. In the pop-up window, select Delete. 4. Wait for the stack to reach the DELETE_COMPLETE status. You need to periodically select Refresh to see the latest stack status. Similarly, by following the steps above, you can also delete the second stack created with name "include-metric-name-filter-demo". Conclusion CloudWatch Metric Streams now offers enhanced filtering capabilities through metric name filtering, providing finer grained control over the metrics you stream. Whether you choose to stream all your metrics, or include or exclude some metrics by namespace or metric name, Metric Streams helps provide low-latency streaming of metrics to your chosen destination, while reducing operational overhead. Start leveraging the capabilities of metric name filtering in CloudWatch Metric Streams today and unlock the full potential of your telemetry data and observability workflows. About the authors

Félix Mezo Gómez Félix Mezo is a Product Manager in the Amazon CloudWatch team at Amazon Web Services.

Jay Joshi Jay is a Cloud Support Engineer at Amazon Web Services, specialized in Amazon CloudWatch and Amazon Route 53. He is also passionate about assisting customers in enhancing their systems using AWS Monitoring and Observability services.

Graham Coleman Graham Coleman is a Senior Product Manager at Amazon Web Services. He has over 20 years' experience, and is passionate about Monitoring and Observability.

Félix Mezo Gómez Félix Mezo Gómez is a Product Manager for CloudWatch at Amazon Web Services. He is passionate about all things observability, helping customers get the most out of their metrics, logs and traces.

Graham Coleman Graham Coleman is a Senior Product Manager at Amazon Web Services. He has over 20 years experience, and is passionate about Monitoring and Observability.

Jay Joshi Jay Joshi is a Monitoring and Observability Specialist at Amazon Web Services (AWS), where he helps customers build resilient applications through innovative observability solutions and best practices. When not solving complex observability challenges, he enjoys watching anime and spending time with his family. LinkedIn: /jay-p-joshi

Did you know that women hold only 8% of cloud roles across European companies? And that the number of female tech founders in Europe remains under 15%? It's hard to believe that this is the situation in 2023. Fortunately, initiatives exist to drive positive change towards gender equity in tech. Amazon Web Services (AWS) re/Start is a re-skilling program for people who are new to technology. It supports individuals to gain knowledge, learn new skills, and ultimately launch a career in tech. The program is deliberate about increasing diversity in the tech sector. As part of AWS' commitment to training 29 million people globally by 2025, AWS re/Start is creating thousands of new builders every day. But the global skills shortage and the lack of diversity in tech will not be solved by one vendor, nor one hiring organization alone. It will take a movement across multiple countries, organizations and initiatives to make a significant change. AWS re/Start already works with a network of collaborating organizations on a global scale. So, when the opportunity arose to bring together a consortium of organizations on a similar mission, we seized the opportunity. Enter the Femme Forward project Femme Forward is a European Union (EU)-funded project aimed at empowering women with the skills and confidence to launch tech careers or form new start-ups. It is delivered by a consortium of experienced social value training providers and companies committed to changing the gender balance in tech, and in increasing the number of female founders. Co-funded by the EU, the project will run from January 2023 until December 2024. By providing inspiring role models, practical skills, and a robust support network, Femme Forward aspires to ignite a new wave of female tech professionals and entrepreneurs. With mentoring opportunities available in seven European languages, this program strives to make tech accessible and to positively impact over 500 women. This is the first big step in reshaping the gender balance in tech and fostering a thriving community of empowered women in the digital age. Simplon, an AWS re/Start collaborating organization, will officially lead the project, managing the various design and skills training delivery organizations including Engim, Factoria F5, Tech & Teach, Top-IX, Big Blue Data Academy, Cyprus Computer Society, BeCode, Simplon Romania, Simplon.co, Le TechSpace, and Hochschule Hannover University. AWS re/Start will provide the majority of the content, exam vouchers, and learning platforms for the 'Tech Careers' stream. Tech & Teach will provide the content for the 'Tech Founders' stream, focusing on content such as securing investment, writing a business plan, and pitching. All delivery organization members will then deliver the content in their home countries of Italy, Belgium, Germany, Romania, France, Greece, Spain, and Cyprus. As part of their commitment to addressing DE&I issues, AWS customers, Randstad and Fujitsu, have also offered their services free of charge. Randstad used its vast knowledge and experience to successfully create an online knowledge repository of useful resources for building CVs and interviewing to supplement the Tech Careers stream. Fujitsu will deliver soft-skill sessions, including communication, critical thinking, and emotional intelligence techniques, to help participants to become confident, thoughtful, and articulate professionals. I agree with Farah Verbruggen, vice president of EMEA, Randstad, who also values this push for women into tech. She says that "Randstad was keen to actively engage in the Femme Forward project. Working in the staffing industry, we see the lack of diversity and the overall tech skills shortage as a significant challenge for our customers across Europe." Get involved If you would like to be a part of the program as a mentor, or you're a female founder who could deliver a session in our inspirational workshops, please register via the Femme Forward website. If you're an aspiring tech founder, or looking for a career change into tech, we welcome you to join as many sessions as your schedule allows. Please review the upcoming events and register your attendance. Later in 2024, our final consortium member, Digital Europe, will share the knowledge and insights gained from the program, using its network and expertise to disseminate the information. We'll share more updates as the project progresses.

Consumer packaged goods (CPG) companies are facing unique challenges in today's hypercompetitive marketplace. Whether those challenges are slow market growth, increased competition, shifting consumer behaviors, commodities shortages, or inflationary pressures, they must push ahead to stay afloat. Together, Amazon Web Services (AWS) and AWS Partners enable CPG companies such as Heineken, Dole Foods, and Philips to accelerate growth, innovation, and digitally transform the CPG industry. AWS for CPG: How AWS Partners are helping CPGs accelerate innovation To accelerate revenue growth, AWS and AWS Partners enable CPGs to reinvent product development, manufacturing, supply chain, marketing, unified commerce, and digital transformation with unmatched compute, artificial intelligence (AI), machine learning (ML), and analytics capabilities. In our latest ebook, "Accelerate innovation leveraging AI/ML in Consumer Packaged Goods with AWS Partners" we explore:

How AWS and its partners help CPG companies draw key business insights from petabytes of data using unmatched compute capabilities, AI and ML to predict opportunities and risks in time to act. How AWS and its partners empower CPG companies to drive digital transformation across the organization to reduce costs while innovating key enterprise applications How to find best-in-class AWS Partner solutions for CPG. How AWS Solutions Consulting provides vetted solutions to common business and technical problems—deployed with help from AWS Partners. How companies like Danone, Britvic, and Stanley Black & Decker are using AWS Partner solutions, services, and guidance to accelerate innovation and digital transformation in the CPG industry.

The AWS Partner Network (APN) The AWS Partner Network (APN) is an integral part of our commitment to provide customers of all sizes, and in any industry around the world, a distinguishable network of partners across 150+ countries leveraging reliable and secure cloud services. AWS Partners are uniquely qualified to help customers accelerate their journey to the cloud, to innovate, and to take full advantage of the breadth and depth of AWS. Partners choose to work with AWS because we are dedicated to providing all our partners with opportunities to grow their business and help them deliver on our shared obsession to meet customer needs. Why work with an AWS Partner? Whether it's time to modernize your business, expand your solutions, or drive your business into a new, innovative territory, an AWS Partner can be a game-changer. Our robust AWS Partner community features over 100,000 partners from more than 150 countries. This vibrant, diverse network of partners can help you deliver greater value for your business, increase agility, and lower costs. On top of this, AWS Partners offer a powerful set of qualifications that are hard to find anywhere else, such as: Innovation – AWS Partners help you keep pace with cutting edge technologies, providing innovative, scalable and cost-effective solutions and capabilities for your organization. Realize faster time to value with products and professional services from thousands of partners in AWS Marketplace. Expertise – AWS Partners are strategic experts and experienced builders that help address your specific business needs. Unlock greater business value with AWS Specialization Partners that have deep technical knowledge and proven customer success. Global Reach – Wherever you want your business to go, there is an AWS partner ready to help. Choose from a global community of trusted AWS Partners across software, hardware, and services. Between increased competition, pandemic-induced commodity shortages, and shifting consumer behaviors, CPG companies have seen it all. But that doesn't mean these challenges need to derail your track to success. No matter where you are in your journey, AWS and AWS Partners are here to help move you forward, giving you access to innovative, global companies and solutions that accelerate innovation and revenue growth. Ready to accelerate innovation in CPG? Download our ebook "Accelerate innovation leveraging AI/ML in Consumer Packaged Goods with AWS Partners" to learn more. Check out more AWS Partners or contact an AWS Representative to know how we can help accelerate your business.

TAGS: AI/ML, aws partner, CPG, Manufacturing, marketing, Product Development, Supply Chain, Unified Commerce

Kevin E. McCurdy Kevin E. McCurdy is global CPG segment lead at APN for AWS, responsible for identifying and engaging relationships with strategic ISV and SI Partners. Previously, he served as VP of demand signal management at E2open; was cofounder and VP of strategic accounts for Orchestro, which was later acquired by E2open; and was also cofounder and VP of business development and services at Mercari Technologies. McCurdy has over 25 years of experience in supply chain management, category management, and demand signal management, working with global CPG companies and retailers, including Coca-Cola, General Mills, Kellogg's, PepsiCo, Unilever, and Kraft-Heinz. He holds a BSc in business logistics and international business from Penn State.

Kate Wiley Kate Wiley is the Head of Retail Industry Marketing at AWS. Prior to AWS, Kate held several marketing roles for retailers such as Dick's Sporting Goods, Reebok, Drybar, and Jenny Craig. As the Head of Retail Industry Marketing, Kate is responsible for supporting and educating retailers on how to use the cloud to build closer consumer relationships with their brand, optimize operations, and accelerate their digital transformation with AWS.

How SeatGeek uses AWS Serverless to control authorization, authentication, and rate-limiting in a multi-tenant SaaS application

SeatGeek is a ticketing platform for web and mobile users, offering ticket purchase and reselling for sports games, concerts, and theatrical productions. In 2022, SeatGeek had an average of 47 million daily tickets available, and their mobile app was downloaded 33+ million times. Historically, SeatGeek used multiple identity and access tools internally. Applications were individually managing authorization, leading to increased overhead and a need for more standardization. SeatGeek sought to simplify the API provided to customers and partners by abstracting and standardizing the authorization layer. They were also looking to introduce centralized API rate-limiting to prevent noisy neighbor problems in their multi-tenant SaaS application. In this blog, we will take you through SeatGeek's journey and explore the solution architecture they've implemented. As of the publication of this post, many B2B customers have adopted this solution to query terabytes of business data. Building multi-tenant SaaS environments Multi-tenant SaaS environments allow highly performant and cost-efficient applications by sharing underlying resources across tenants. While this is a benefit, it is important to implement cross-tenant isolation practices to adhere to security, compliance, and performance objectives. With that, each tenant should only be able to access their authorized resources. Another consideration is the noisy neighbor problem that occurs when one of the tenants monopolizes excessive shared capacity, causing performance issues for other tenants. Authentication, authorization, and rate-limiting are critical components of a secure and resilient multi-tenant environment. Without these mechanisms in place, there is a risk of unauthorized access, resource-hogging, and denial-of-service attacks, which can compromise the security and stability of the system. Validating access early in the workflow can help eliminate the need for individual applications to implement similar heavy-lifting validation techniques. SeatGeek had several criteria for addressing these concerns:

They wanted to use their existing Auth0 instance. SeatGeek did not want to introduce any additional infrastructure management overhead; plus, they preferred to use serverless services to "stitch" managed components together (with minimal effort) to implement their business requirements. They wanted this solution to scale as seamlessly as possible with demand and adoption increases; concurrently, SeatGeek did not want to pay for idle or over-provisioned resources.

Exploring the solution The SeatGeek team used a combination of Amazon Web Services (AWS) serverless services to address the aforementioned criteria and achieve the desired business outcome. Amazon API Gateway was used to serve APIs at the entry point to SeatGeek's cloud environment. API Gateway allowed SeatGeek to use a custom AWS Lambda authorizer for integration with Auth0 and defining throttling configurations for their tenants. Since all the services used in the solution are fully serverless, they do not require infrastructure management, are scaled up and down automatically on-demand, and provide pay-as-you-go pricing. SeatGeek created a set of tiered usage plans in API Gateway (bronze, silver, and gold) to introduce rate-limiting. Each usage plan had a pre-defined request-per-second rate limit configuration. A unique API key was created by API Gateway for each tenant. Amazon DynamoDB was used to store the association of existing tenant IDs (managed by Auth0) to API keys (managed by API Gateway). This allowed us to keep API key management transparent to SeatGeek's tenants. Each new tenant goes through an onboarding workflow. This is an automated process managed with Terraform. During new tenant onboarding, SeatGeek creates a new tenant ID in Auth0, a new API key in API Gateway, and stores association between them in DynamoDB. Each API key is also associated with one of the usage plans. Once onboarding completes, the new tenant can start invoking SeatGeek APIs (Figure 1).

Tenant authenticates with Auth0 using machine-to-machine authorization. Auth0 returns a JSON web token representing tenant authentication success. The token includes claims required for downstream authorization, such as tenant ID, expiration date, scopes, and signature. Tenant sends a request to the SeatGeak API. The request includes the token obtained in Step 1 and application-specific parameters, for example, retrieving the last 12 months of booking data. API Gateway extracts the token and passes it to Lambda authorizer. Lambda authorizer retrieves the token validation keys from Auth0. The keys are cached in the authorizer, so this happens only once for each authorizer launch environment. This allows token validation locally without calling Auth0 each time, reducing latency and preventing an excessive number of requests to Auth0. Lambda authorizer performs token validation, checking tokens' structure, expiration date, signature, audience, and subject. In case validation succeeds, Lambda authorizer extracts the tenant ID from the token. Lambda authorizer uses tenant ID extracted in Step 5 to retrieve the associated API key from DynamoDB and return it back to API Gateway. The API Gateway uses API key to check if the client making this particular request is above the rate-limit threshold, based on the usage plan associated with API key. If the rate limit is exceeded, HTTP 429 ("Too Many Requests") is returned to the client. Otherwise, the request will be forwarded to the backend for further processing. Optionally, the backend can perform additional application-specific token validations.

Architecture benefits The architecture implemented by SeatGeek provides several benefits:

Centralized authorization: Using Auth0 with API Gateway and Lambda authorizer allows for standardization the API authentication and removes the burden of individual applications having to implement authorization. Multiple levels of caching: Each Lambda authorizer launch environment caches token validation keys in memory to validate tokens locally. This reduces token validation time and helps to avoid excessive traffic to Auth0. In addition, API Gateway can be configured with up to 5 minutes of caching for Lambda authorizer response, so the same token will not be revalidated in that timespan. This reduces overall cost and load on Lambda authorizer and DynamoDB. Noisy neighbor prevention: Usage plans and rate limits prevent any particular tenant from monopolizing the shared resources and causing a negative performance impact for other tenants. Simple management and reduced total cost of ownership: Using AWS serverless services removed the infrastructure maintenance overhead and allowed SeatGeek to deliver business value faster. It also ensured they didn't pay for over-provisioned capacity, and their environment could scale up and down automatically and on demand.

Conclusion In this blog, we explored how SeatGeek used AWS serverless services, such as API Gateway, Lambda, and DynamoDB, to integrate with external identity provider Auth0, and implemented per-tenant rate limits with multi-tiered usage plans. Using AWS serverless services allowed SeatGeek to avoid undifferentiated heavy-lifting of infrastructure management and accelerate efforts to build a solution addressing business requirements.

Umesh Kalaspurkar Umesh is a Sr Solutions Architect at AWS, and brings more than 20 years of experience in design and delivery of Digital Innovation and Transformation projects, across enterprises and startups. He is motivated by helping customers identify and overcome challenges. Outside of work, Umesh enjoys being a father, skiing, and traveling.

Anderson Parra Ander Parra is Staff Software Engineer at SeatGeek and has over 10 years of industry experience in software engineering. He has worked with many different types of systems with diverse constraints and challenges, mainly based on JVM (Java and Scala). He is also experienced in high-scale distributed systems and functional programming. Outside work, Ander enjoys being a father, coffee, and traveling.

Anton Aleksandrov Anton is a Principal Solutions Architect for AWS Serverless and Event-Driven architectures. Having over two decades of hands-on engineering and architecture experience, Anton works with major ISV and SaaS customers to design highly scalable, innovative, and secure cloud solutions.

João Mikos João Mikos has over 20 years of experience working with development and infrastructure in distributed systems for companies sprawling four continents across various business verticals. Previously, he led infrastructure teams for highly regulated fintech companies and a Solutions Architecture team at AWS. Now a Director of Engineering at SeatGeek, he leads the Developer Acceleration team, responsible for enhancing the experience of customer and partner engineers developing with SeatGeek and the teams developing within SeatGeek.

Samit Kumbhani Samit Kumbhani is a Sr. Solutions Architect at AWS based out of New York City area. He has 18+ years of experience in building applications and focuses on Analytics, Business Intelligence, and Databases. He enjoys working with customers to understand their challenges and solve them by creating innovative solutions using AWS services. Outside of work, Samit loves playing cricket, traveling, and spending time with his family and friends.

How to use AWS Verified Access logs to write and troubleshoot access policies

On June 19, 2023, AWS Verified Access introduced improved logging functionality; Verified Access now logs more extensive user context information received from the trust providers. This improved logging feature simplifies administration and troubleshooting of application access policies while adhering to zero-trust principles. In this blog post, we will show you how to manage the Verified Access logging configuration and how to use Verified Access logs to write and troubleshoot access policies faster. We provide an example showing the user context information that was logged before and after the improved logging functionality and how you can use that information to transform a high-level policy into a fine-grained policy. Overview of AWS Verified Access AWS Verified Access helps enterprises to provide secure access to their corporate applications without using a virtual private network (VPN). Using Verified Access, you can configure fine-grained access policies to help limit application access only to users who meet the specified security requirements (for example, user identity and device security status). These policies are written in Cedar, a new policy language developed and open-sourced by AWS. Verified Access validates each request based on access policies that you set. You can use user context—such as user, group, and device risk score—from your existing third-party identity and device security services to define access policies. In addition, Verified Access provides you an option to log every access attempt to help you respond quickly to security incidents and audit requests. These logs also contain user context sent from your identity and device security services and can help you to match the expected outcomes with the actual outcomes of your policies. To capture these logs, you need to enable logging from the Verified Access console.

Figure 1: Overview of AWS Verified Access architecture showing Verified Access connected to an application

After a Verified Access administrator attaches a trust provider to a Verified Access instance, they can write policies using the user context information from the trust provider. This user context information is custom to an organization, and you need to gather it from different sources when writing or troubleshooting policies that require more extensive user context. Now, with the improved logging functionality, the Verified Access logs record more extensive user context information from the trust providers. This eliminates the need to gather information from different sources. With the detailed context available in the logs, you have more information to help validate and troubleshoot your policies. Let's walk through an example of how this detailed context can help you improve your Verified Access policies. For this example, we set up a Verified Access instance using AWS IAM Identity Center (successor to AWS Single Sign-on) and CrowdStrike as trust providers. To learn more about how to set up a Verified Access instance, see Getting started with Verified Access. To learn how to integrate Verified Access with CrowdStrike, see Integrating AWS Verified Access with device trust providers. Then we wrote the following simple policy, where users are allowed only if their email matches the corporate domain.

Before improved logging, Verified Access logged basic information only, as shown in the following example log.

"identity": { "authorizations": [ { "decision": "Allow", "policy": { "name": "inline" } } ], "idp": { "name": "user", "uid": "vatp-09bc4cbce2EXAMPLE" }, "user": { "email_addr": [EMAIL] "name": "Test User Display", "uid": [EMAIL] "uuid": "00u6wj48lbxTAEXAMPLE" } }Plain text

Modify an existing Verified Access instance To improve the preceding policy and make it more granular, you can include checks for various user and device details. For example, you can check if the user belongs to a particular group, has a verified email, should be logging in from a device with an OS that has an assessment score greater than 50, and has an overall device score greater than 15. Modify the Verified Access instance logging configuration You can modify the instance logging configuration of an existing Verified Access instance by using either the AWS Management Console or AWS Command Line Interface (AWS CLI).

Open the Verified Access console and select Verified Access instances. Select the instance that you want to modify, and then, on the Verified Access instance logging configuration tab, select Modify Verified Access instance logging configuration.

Under Update log version, select ocsf-1.0.0-rc.2, turn on Include trust context, and select where the logs should be delivered.

After you've completed the preceding steps, Verified Access will start logging more extensive user context information from the trust providers for every request that Verified Access receives. This context information can have sensitive information. To learn more about how to protect this sensitive information, see Protect Sensitive Data with Amazon CloudWatch Logs. The following example log shows information received from the IAM Identity Center identity provider (IdP) and the device provider CrowdStrike.

"data": { "context": { "crowdstrike": { "assessment": { "overall": 21, "os": 53, "sensor_config": 4, "version": "3.6.1" }, "cid": "7545bXXXXXXXXXXXXXXX93cf01a19b", "exp": 1692046783, "iat": 1690837183, "jwk_url": " ", "platform": "Windows 11", "serial_number": "ec2dXXXXb-XXXX-XXXX-XXXX-XXXXXX059f05", "sub": "99c185e69XXXXXXXXXX4c34XXXXXX65a", "typ": "crowdstrike-zta+jwt" }, "sso": { "user": { "user_id": "24a80468-XXXX-XXXX-XXXX-6db32c9f68fc", "user_name": "XXXX", "email": { "address": [EMAIL] "verified": false } }, "groups": { "04c8d4d8-e0a1-XXXX-383543e07f11": { "group_name": "XXXX" } } }, "http_request": { "hostname": "sales.example.com", "http_method": "GET", "x_forwarded_for": "52.XX.XX.XXXX", "port": 80, "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0", "client_ip": "52.XX.XX.XXXX" } } }Plain text

The following example log shows the user context information received from the OpenID Connect (OIDC) trust provider Okta. You can see the difference in the information provided by the two different trust providers: IAM Identity Center and Okta.

"data": { "context": { "http_request": { "hostname": "sales.example.com", "http_method": "GET", "x_forwarded_for": "99.X.XX.XXX", "port": 80, "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15", "client_ip": "99.X.XX.XXX" }, "okta": { "sub": "00uXXXXXXXJNbWyRI5d7", "name": "XXXXXX", "locale": "en_US", "preferred_username": [EMAIL] "given_name": "XXXX", "family_name": "XXXX", "zoneinfo": "America/Los_Angeles", "groups": [ "Everyone", "Sales", "Finance", "HR" ], "exp": 1690835175, "iss": " " } } }Plain text

The following is a sample policy written using the information received from the trust providers.

permit(principal,action,resource) when { context.sso.groups has " " && context.sso.user.email.address like [EMAIL] && context.sso.user.email.verified == true && context has "crowdstrike" && context.crowdstrike.assessment.os > 50 && context.crowdstrike.assessment.overall > 15 };Plain text

This policy only grants access to users who belong to a particular group, have a verified email address, and have a corporate email domain. Also, users can only access the application from a device with an OS that has an assessment score greater than 50, and has an overall device score greater than 15. Conclusion In this post, you learned how to manage Verified Access logging configuration from the Verified Access console and how to use improved logging information to write AWS Verified Access policies. To get started with Verified Access, see the Amazon VPC console.

If you have feedback about this post, submit comments in the Comments section below. If you have questions about this post, contact AWS Support. Want more AWS Security news? Follow us on Twitter.

Ankush Goyal Ankush is an Enterprise Support Lead in AWS Enterprise Support who helps Enterprise Support customers streamline their cloud operations on AWS. He enjoys working with customers to help them design, implement, and support cloud infrastructure. He is a results-driven IT professional with over 18 years of experience.

Anbu Kumar Krishnamurthy Anbu is a Technical Account Manager who specializes in helping clients integrate their business processes with the AWS Cloud to achieve operational excellence and efficient resource utilization. Anbu helps customers design and implement solutions, troubleshoot issues, and optimize their AWS environments. He works with customers to architect solutions aimed at achieving their desired business outcomes.

TAGS: Amazon Verified Access, Amazon Web Services, AWS, Security Blog, Zero Trust

Introduction Reality capture (RC) is the process of digitizing real-world objects or environments. As this process can be complex, this post provides context to the current state of associated technologies. RC has vast implications in many industries, including Automotive, Aerospace, Oil and Gas, Engineering, Construction, and Real Estate. There is a fundamental shift taking place across these industries related to accurately capturing and digitizing the real world and objects within it. This technology has been improved over the last several years. What was previously a very expensive and time-consuming process that led to potentially inaccurate results, is now faster, less expensive, and more accurate. With the initial value of RC now being validated in enterprise, an emphasis on enablement is critical to usher in the next wave of adoption. In order to uncover this technology's full potential, it is necessary to decrease barriers to entry and implementation. This post will cover several aspects of digitization, including methods, an overview of digital twins, and a case study to show a real-world example with an AWS customer. You will also hear from the Reality Capture Network (RCN), an online education center that focuses on digitization. Matthew Byrd, Founder of RCN, will dive into trends he sees and use cases of RC around the world. Benefits and Challenges The benefits of RC have been validated from enterprise companies around the world. Common themes among these use-cases include surveying conditions, operational improvements, safety analysis, security analysis, and core business process advancements. The benefits can be substantial and so can the challenges. To realize these benefits, a company must understand the associated key Spatial Computing topics and challenges. These associated challenges include the 3D capture process (Digitization Methods), 3D model manipulation, and final asset implementation. These challenges will be discussed throughout this post. Digitization Methods In this post I will discuss three methods for digitizing the real world: LiDAR based scanning, Photogrammetry, and Neural Radiance Fields (NeRFs). LiDAR Scanning LiDAR based solutions send out light rays from a scanning device and calculate the time it takes to return. This is referred to as a Time of Flight (ToF) calculation. This method creates a point cloud of the real world and objects that the rays come into contact with. Point clouds can vary greatly in density depending on the settings selected and device limitations. A denser point cloud will result in a proportionally higher resolution end product. These point clouds can then be converted into 3D meshes that can be consumed by industry standard 3D software. Solutions like Autodesk's ReCap can help with this conversion.

LiDAR scanners can be expensive and in some cases need a technician to operate. There have been great advancements in decreasing these barriers, and now there are also simpler scanners that are very easy to use. Handheld LiDAR scanners and newer high end smart phone models with LiDAR built in, can allow users to capture environments and objects quickly and easily. Photogrammetry Photogrammetry is a fundamentally different process from LiDAR based solutions. This process takes a set of images of an object or area as an input, and uses an algorithm to build a 3D model recreation from those images. To keep processing times reasonable, typically these image sets range roughly from 40 to 250 photographs, but can use more or less. These input images can also be extracted from video, which makes the capture process very fast, under two minutes in some cases. This makes it easy for anyone to quickly capture an environment or object. The more images included, the longer it takes for processing and recreation. Depending on the quality of the input images, this can result in a 20-30% higher resolution asset.

Left: Screenshot of the photogrammetry process. Each white triangle above the building is one input photograph camera location. Right: Example of one of the input photographs. Thomas Jefferson Memorial source data from cyark.org, license CC BY-NC-SA.

Common uses of photogrammetry include cultural heritage (art, museum relics), media and entertainment (human face and body scanning for CGI character creation), gaming (hyper-realistic game asset creation), and site surveying (capturing buildings and areas of land, commonly with use of drones). Neural Radiance Field (NeRF) NeRF technology is the most infant of the three techniques. NeRF uses a process that is similar to photogrammetry in that it uses photographs or extracted video frames for 3D reconstruction. A NeRF is a unique asset type which can only be viewed in specialized software. The result can be difficult to convert into a clean 3D mesh to then be used with other standard 3D modeling software and tools. NeRF technology, including Nvidia's Instant NeRF is improving rapidly, however early results can have substantial artifacts in their raw output. These artifacts can make the resulting 3D asset difficult to work with and less desirable for certain use cases. However, it is important to note that artifacts are also typical of raw LiDAR and photogrammetry-based scans. With LiDAR and photogrammetry having been around longer, the tools available to refine, convert and integrate these models into other software are more mature as well. For this reason, NeRF based assets have not been used heavily within enterprise use cases yet. This is sure to change as the technology and tools become more mature.

Neural Radiance Field (NeRF) Scan of a street in New York City. Using Nvidia's Instant-ngp. Notice the blurry artifacts in the resulting output.

LiDAR, photogrammetry, and NeRF models can all be very accurate and high-resolution solutions, all aimed at different use cases. With LiDAR and photogrammetry having the ability to result in millimeter accuracy, and NeRF in its infancy, these are different tools for different jobs. As this post is focused on enterprise use cases, I will not expand further on NeRF technology and will instead dive deeper into LiDAR and photogrammetry. 3D Model Optimization 3D model resolution is important to acknowledge for the following reasons. Digital Twins are only as valuable as the information that can be gleaned from them. If the information is not high enough resolution, it may not be adequate to make necessary business decisions. If the resolution is too high, the application can become slow and difficult to use, or completely unusable. Thus, it is important to note that higher resolution does not always mean a more valuable resulting asset or digital twin. In order to understand how high of a resolution scan you want, you need to understand the use case for which you are scanning, and how end users will be interacting with the 3D model. High resolution scans can take substantial processing power and time to create, and the resulting scan can end up being a very large files size on the scale of gigabytes to terabytes of data. These can be expensive to process and store, and can be difficult to work with and manipulate downstream. One of the largest challenges by industries in 3D content creation is delivering high-resolution content that is also lightweight. The term "lightweight" in computer graphics is commonly used to describe a 3D asset that does not take a lot of computational processing power to visualize. Alternatively, "heavy" 3D models are complex and take a substantial amount of processing power to visualize and manipulate.

Image of two 3D model spheres rendered with lighting and shadows

Image of the same 3D model spheres. The sphere on the right is 16x heavier than the sphere on the left with hardly a noticeable difference when rendered

Having lightweight 3D assets is beneficial for several reasons. They are easier to manipulate and work with, more efficient to render and visualize, cost less to store in data storage, and are proportionally faster to send and receive between applications. Using 3D assets that have been optimized allows 3D experiences to run more smoothly and allows the creators to add additional complexity to the experience elsewhere. These lighter weight assets free up compute resources for other functionality or processing. It can be difficult to determine the right digitization method for a specific use-case. Important questions to ask include: Are there reflective or shiny surfaces? Is the lighting of the environment changing frequently? Does the solution need to be portable? How large of an area do you need scanned? Will the render happen on the cloud/server or on a mobile device? Is the target object or area indoors or outdoors? How important is the texture resolution? How quickly do you need to scan it? What is your budget? How do you want to use the resulting 3D asset? Is there already an existing dataset? These questions are necessary to pick the right tool for the job and validate that the resulting assets will meet business objectives downstream and bring value. One powerful way to leverage these resulting assets is within a Digital Twin. Digital Twins A Digital Twin is an interactive application that combines 3D models of existing physical facilities and an overlay of additional data to provide insights and enable efficient decision making. Common Digital Twin environments include factories, industrial facilities, and manufacturing plants. Digital Twins have proven to help with building operations, increase production output, and improve equipment performance. Regardless of which method is used to digitize an object or environment, the result is typically a simplified standard 3D model format. A standard model format is critical because it is typically necessary to bring that model into other 3D software. For any of the previously mentioned industries, Automotive, Aerospace, Real Estate, each have their own industry specific 3D software. These standard model formats can also be brought into other non-industry specific tools such as web applications, Realtime engines, or Digital Twin environments. The purpose of a Digital Twin is to give users a comprehensive understanding of the asset(s) in subject. The overlaying of additional information on top of the 3D model is critical for exactly that reason. Digital Twins allow organizations to bring together critical pieces of information to a single environment in order to gain a comprehensive view, obtain actionable insights and reach business objectives. If Information is not aggregated from multiple sources, for example a single static 3D model asset, it is not a digital twin. A Digital Twin is a living asset. A Digital Twin solution, AWS IoT TwinMaker allows developers to easily build Digital Twins of real-world environments. The "Scene" is a key feature of AWS IoT TwinMaker. The scene is a 3D canvas that allows you to import one or multiple 3D models. These models are organized into a hierarchy panel on the left side that allows you to control visibility of objects and overlay live data points. Grafana is a key integration to AWS IoT TwinMaker. Grafana is a third-party interactive data visualization application that integrates nicely with AWS IoT TwinMaker. This integration allows you to quickly create charts, graphs, and dashboard layouts leveraging your existing AWS IoT data. It also allows developers to bring in their full AWS IoT TwinMaker 3D scene and pull data from the data streams within those scenes. This allows for a robust and flexible Digital Twin dashboard solution.

AWS IoT TwinMaker 3D viewer with associated live data stream in a Grafana dashboard

Another way to create a similar robust Digital Twin dashboard that can be distributed on the web is by way of AWS IoT Application Kit, or leveraging AWS Partners like Prevu3D. IoT Application Kit is an open-source UI component library which allows developers to create web applications that integrate with AWS IoT TwinMaker quickly and securely. IoT Application Kit gives developers the flexibility of the web while also allowing the security, scalability, and reliability of AWS IoT TwinMaker and AWS IoT infrastructure under the hood. KCI – Case Study KCI is a global professional services firm that focuses on land development, preservation and infrastructure. KCI needs accurate surveys of existing and potential work sites to facilitate their projects. With a focus on infrastructural work, these sites can range in scale. KCI needed to create a pipeline to obtain high resolution 3D models of specific geographic areas. The areas in question ranged from 1 acre to 20 acres in size. At this scale, typical LiDAR based scanning methods from ground level do not scale well. Ground level LiDAR scans have limits on area sizes. Depending on desired scan resolution, to cover an area larger than an acre, multiple scans must be captured and then combined. This splicing of multiple LiDAR scans together is called the "registration process". This registration process can be difficult to automate, especially at a large scale, and can become heavy and difficult to use. Another common issue with LiDAR scanning from the ground level over large areas is the perspective of the scans. Depending on the use-case of the resulting scan, with obstacles and interference (trees, cars, hills, and buildings) the resulting assets can be incomplete and unusable. If images are taken from the sky, by plane, satellite or drone, it can lead to a more complete recreation of the subject area.

A scalable pipeline was built that allows KCI to process images using photogrammetry and receive back a resulting 3D model of the subject area. Benefits of this pipeline include not having to use local computer resources, as engineers need their local machines for other tasks. It also gives the ability to scale up and down resources on demand to never over or under utilize infrastructure, and the data is backed up on the cloud. This pipeline is built to be flexible. An exciting feature added to this pipeline because of this flexibility, was the integration of Amazon Rekognition. This gave KCI the ability to use machine learning to remove any unwanted aspects of the images sent into the photogrammetry process. For instance, KCI did not want cars in the resulting 3D model. The team was able to use Rekognition to find all cars within the input image set and remove them as a pre-processing step. This allowed a clean resulting 3D scan model without any cars.

Left: Original input image. Right: Illustration of Amazon Rekognition finding all cars to cull from the image.

These resulting 3D model assets can be used throughout the lifecycle of the project for different use cases. These use cases include topographic analysis, feasibility studies, excavation volume takeoffs, project planning, coordination, and visualization. This can lead to improved process efficiency and decreased potential for costly mistakes. With projects at this scale, this impact can be substantial. KCI recorded a reduction of processing time from 2 days down to roughly an hour. When it comes to digitization best practices and understanding industry sentiment, experts in the field can help provide guidance and context. Reality Capture Network Reality Capture Network (RCN) is a podcast, online education center, and annual conference that focuses on sharing knowledge and spreading awareness on new technologies and innovation in the built environment. Matthew Byrd, Founder and Host of RCN shares his thoughts and lessons learned below: "The demand for the digitization of existing assets is experiencing exponential growth. This trend encompasses various industries, from global oil and gas companies seeking to create digital twins of their facilities for simulations, training, capital improvement projects, airports searching for more efficient asset management, security planning, and facility maintenance programs. The use-cases of the technology are vast, and I believe [the world] is just beginning to witness the onset of a significant upward growth curve. It is an opportune time to explore these technologies." Matthew expands, "There are several reasons behind the growing adoption of Reality Capture. Firstly, [the world] is witnessing advancements in both hardware and software capabilities. Simultaneously, the cost of the technology is decreasing, making it better, faster, and more affordable. Additionally, an increasing number of companies are discovering innovative ways to leverage this data, offering advanced analytic tools, drafting and modeling capabilities, and cloud-hosted online viewers. These developments enable firms across diverse industries to utilize the data in ways that were previously impossible." Conclusion As digitization technologies evolve and improve, the benefits continue to grow, and the impact of these technologies has been proven. As the tools become easier to use and can handle more data, they will continue to become more valuable to their associated industries. Decreasing the barriers for users to consume and interact with these assets will greatly increase adoption across all associated industries. Understanding the different key technologies in the space and how they can be leveraged is important to creating an effective Digital Twin or 3D asset strategy. Picking the right tool for the job to obtain actionable data is as important as knowing how to leverage that data downstream. If the end product does not provide actionable insights, then it was likely not a good investment. There is a growing number of companies exploring within this digitization space. With hardware and software solutions improving quickly and companies investing more deeply, adoption will follow. AWS is working hard to enable these workloads and help usher the next wave of Spatial Computing. —– To learn more about digitization and spatial computing at AWS, check out these other blog posts: · Building a Digital Twin with Photogrammetry and AWS IoT TwinMaker · Photogrammetry reconstruction with 3Dflow on AWS · LiDAR 3D Point Cloud Labeling with Velodyne LiDAR Sensor · Epic Games launches RealityScan app on AWS · HxDR: Transforming geospatial data in the cloud with AWS and Hexagon

Adam Chernick Adam is a Worldwide Sr. Solutions Architect dedicated to Amazon GameLift Streams. He has focused his career on real-time 3D, generative AI and adjacent emerging technology.

Sharing data on Amazon FSx for OpenZFS across Linux and Windows clients

Many organizations need a high-performance shared file system that they can access simultaneously from Linux and Windows, despite different permission models across the platforms. For example, a media and entertainment enterprise may render workloads mutually on Linux and Windows clients. These customers may use mechanisms like "User Mapping" to make sure that their Windows clients can mount file systems shared via NFS and avoid file access conflicts. Amazon FSx for OpenZFS is a fully managed AWS service providing scalable OpenZFS file systems for high-performance applications. It features data management capabilities like snapshots and encryption, supports over 1 million IOPS and 21 GB/s throughput, and is ideal for big data, DevOps, and research workloads. To bridge the gap between different operating systems and their unique permission models, organizations often use solutions like "User Mapping" for seamless file sharing via NFS. In this post, I show how to use Network File System (NFS) protocol version 3 and different authentication methods available to concurrently share data stored on FSx for OpenZFS data to your Linux and Windows clients. That enables cross-platform data accessibility, enhanced security, and efficient data management, leading to improved productivity and resource optimization. Background on NFS and Amazon FSx for OpenZFS NFS is a native protocol on Linux and you can mount FSx for OpenZFS on Linux by using the standard Linux mount command and the domain name system (DNS) name associated with the volume. You can access data on your FSx for OpenZFS file system by using NFS protocol versions (v3, v4.0, v4.1 and v4.2). Linux clients natively support NFS client versions 3 and 4.x and let you mount the file system using the standard Linux mount command. Windows clients support NFS version 2 and 3 and require installation of the NFS client. You utilize NFS version 3 when connecting Linux and Windows clients concurrently to FSx for OpenZFS, since that is the NFS version supported by both platforms. The AWS post "New – Amazon FSx for OpenZFS" provides information on setting up an FSx for OpenZFS file system. Solution walkthrough The following is a summary of the steps to implement this solution:

Install and Configure the NFS client for Windows: this is required to mount FSx for OpenZFS on Windows clients using the NFS protocol. Mount the file system: you must mount the FSx for OpenZFS file system on both Windows and Linux clients in order to access the data. Select and configure the identity mapping and authentication method: you will choose between User Name Mapping server, Active Directory (AD) integration, or anonymous authentication to map and authenticate users to access the files on FSx for OpenZFS.

1. Install and configure the NFS client on Windows You will need to install and configure NFS client on Windows. Following are the steps to install the NFS client on Windows. This can be accomplished either via GUI or Windows powershell. The following is an example using the GUI on Windows Server 2019 (this process also works in Windows Server 2022): Installation

Open the Server Manager on the Windows server. On the dashboard, select Quick Start, then Add Roles and Features, then Next in the dialog box. For Installation Type, select Role-based or feature-based installation and select Next. In the Server Selection screen, select your server name and select Next. On Server Roles, expand File and Storage Services. On File and Storage Services, select Storage Services and select Next. That will bring you to the final step. On the Features dialog, select Client for NFS and select Install.

Figure 1: Installing the NFS client on Windows Server Configuration After installing the NFS client on Windows, you will need to configure it. Use the NFS client on Windows Server GUI to customize client settings, default file permissions, and security model. The default settings (shown in the following) work for most environments. However, you must consider if the default file permissions address your security posture or match with the defaults on your Linux distribution.

Figure 2: NFS client on Windows Server – default file permissions 2. Mount the file system Access the data by mounting the file system on the Linux and Windows clients.

To mount the file system, you need its DNS name. Using the FSx console, navigate to the Network & security tab of your FSx for OpenZFS file system and copy its DNS name. In the following screenshot, I show the DNS name of one of my FSx for OpenZFS file systems:

Figure 3: Capturing the DNS name of the file system 2. Next, mount the file system on the Linux server using the file system DNS name and the following mount command (a basic mount command): mount -t nfs -o vers=3 fs-04fcff18e33270111.fsx.us-west-2.amazonaws.com:/fsx/sync_vol /fsxsync

Figure 4: Mounting the file system on Linux 3. Finally, also mount the file system on the Windows server using the following command: mount \\fs-04fcff18e33270111.fsx.us-west-2.amazonaws.com\fsx\sync_vol\ Z:

Figure 5: Mounting the file system on Windows Refer to the FSx for OpenZFS documentation for steps to optimize file system performance. 3. Select and configure the identity mapping and NFS authentication method Linux and Windows use different account and security systems. Linux represents users with user identifiers (UIDs) and group identifiers (GIDs). Windows represent users and groups with a unique security identifier (SID). User mapping is the process of converting UIDs and GIDs from Linux to SIDs from Windows and vice versa. User mapping provides a clean default permission set for Windows users to transparently access, modify, and create files. After installing and configuring the Services for NFS as described in the "Background on NFS and Amazon FSx for OpenZFS" section, you must select and configure the appropriate identity mapping and authentication method. You can use User Name Mapping server, Active Directory (AD) integration, or anonymous authentication. AUTH_SYS or AUTH_UNIX: using UID and GID identifiers for account mapping AUTH_SYS or AUTH_UNIX account mapping is the process of matching the Linux UIDs and GIDs to corresponding Windows user and group SIDs. The Microsoft Client for NFS supports two methods for account mapping: either use the %windir%\system32\etc\passwd method for standalone servers or AD for servers joined to a domain. 3.1 Standalone servers: the /etc/passwd method The /etc/passwd method creates a one-to-one mapping of a Linux UIDs and GIDs to a Windows user and group SID. 1. First, provide a User Name Mapping server using the NFS Client for Windows. In our example, the mapping server is the server containing the password file.

Figure 6: NFS client on Windows Server – User Name Mapping server 2. Next, place a password (passwd) file on the Windows path %SYSTEMROOT%\system32\drivers\etc. Each Windows user/SID on the file is matched to a Linux user based on the UID and GID in the file. 3. Restart the NFS client on Windows after configuring the /etc/passwd file and mapping server. You can use the powershell commands nfsadmin client stop and nfsadmin client start for that. Make sure that the file system is unmounted on Windows before you restart the NFS client.

Figure 7: Restarting the NFS client The following is an example of user mapping using the /etc/passwd file placed on the Windows client at %SYSTEMROOT%\system32\drivers\etc.

Figure 8: Windows passwd file example Each line has the following fields separated by a colon: Windows username, Linux UID, Linux GID, description, Windows home directory. In this example, I created a user called mary on both Linux and Windows. After logging onto the Windows server and mounting the file system, I can confirm that mary's UID and GID (UID 1002 and GID 1007) are in effect by issuing the mount command (as follows). The NFS client mapped the UID and GID using the password file I provided.

Figure 9: mount point effective UID and GID 4. Next, create a file on Linux and check it on Windows. Using the account mary on Linux, I create the text file called mary-file-linux.txt on my FSx for OpenZFS file system mounted on /sync_vol. In the following, you can see the ownership, group membership, and permissions on the file mary-file-linux.txt.

Figure 10: file permissions – Linux 5. Now, check that file from the Windows side. Logged in as mary on Windows, I open the drive where I mapped the FSx for OpenZFS file system. I can see that the file is accessible and preserved the same ownership, group membership, and permissions from Linux. In the red boxes in Figure 11 you can see the file permissions, user ID, and group ID assigned by Windows.

Figure 11: file permissions – Windows file properties 6. Next, create a file on Windows and check it on Linux. Using the account mary on Windows, I create a text file and save it on the Z: drive (the FSx for OpenZFS share).

Figure 12: sample text file saved on Z: From Windows, I can see that the NFS client for windows assigned permissions using the Linux standard: Owner, Group, Other, and R, W, X. The permissions assigned use the default file permissions set in the NFS Client for Windows (Figure 3). In addition, it assigned the UID and GID from the passwd file found on the Windows side.

Figure 13: File permissions – Windows file properties 7. Next, check the same file on the FSx for OpenZFS Linux mountpoint. The content of the file can be seen and confirmed that the permissions and ownership shown on the Windows side match the Linux side.

Figure 14: File permissions – Linux Furthermore, note that the user names do not have to necessarily match from Linux to Windows and vice versa. For example, in the following password file on the Windows side, I am mapping the Windows user jeff to a UID 1004. UID 1004 in my example is a Linux user called phill. Windows uses the UID for the mapping, not the Linux user name (phill in this case).

Figure 15: User name mapping – jeff to UID 1004 (phill) 3.2 Servers joined to an AD 1. For servers joined to an AD, you must select the identity mapping source in the NFS client to use your AD domain ("example.com" in my case).

Figure 16: NFS client on Windows Server – AD domain name 2. Next, you must update the user common name (CN) attributes in the AD Organizational Unit (OU). Use the ADSI editor to update the "gidNumber" and "uidNumber" on the Windows user to match the Linux UID and GID of the corresponding Linux user. Follow these steps: a. Type "adsi" in the Windows search bar and open the editor in the AD domain server.

Figure 17: ADSI editor (AD editor) b. Navigate to the Users subtree in the ADSI editor, selecting the desired user in the OU from your domain. Right-click on the desired user, and select properties.

Figure 18: ADSI editor – changing the CN of a user In my example, I update the CN attributes gidNumber and uidNumber of the Windows user brian to match the UID and GID of the Linux user brian with UID 1013 and GID 1005. c. First, I change the uidNumber attribute to 1013.

Figure 19: ADSI editor – changing the uidNumber attribute d. Next, I change the gidNumber to 1005.

Figure 20: ADSI editor – changing the gidNumber attribute 3. I repeat the process for every user I want to map from Linux to Windows, and then I restart the NFS client when done. I use the powershell commands nfsadmin client stop and nfsadmin client start for the NFS client restart. Make sure to unmount the file system on Windows before restarting the NFS client. 4. Next, I create a file on Linux logged as user brian and check its ownership, group membership, and permissions.

Figure 21: Linux file details 5. Finally, to confirm, I check the file on Windows logged as user brian from the example AD domain. I can confirm that the Linux ownership, group membership, and permissions match the ones from the user brian, on Linux, as expected.

Figure 22: validating the user mapping using AD 3.3 AUTH_NONE: anonymous authentication You can use anonymous authentication to map the user ID and group ID of a Linux user to your Windows client. Anonymous authentication is a generic way to provide read/write access to the file system. But note that it does not provide a strict mechanism to arbitrate write access to files. Furthermore, you cannot setup permissions on a user/group level. Therefore, it is not the recommended method. It should be considered only in situations in which security is not a concern. You must add the following registry keys and then reboot to setup anonymous authentication. New-ItemProperty HKLM:\SOFTWARE\Microsoft\ClientForNFS\CurrentVersion\Default -Name AnonymousUID -Value -PropertyType "DWord" New-ItemProperty HKLM:\SOFTWARE\Microsoft\ClientForNFS\CurrentVersion\Default -Name AnonymousGID -Value -PropertyType "DWord" After adding the keywords and rebooting, you can mount the file system on Windows using the anonymous option (-o anon) as follows. Note that the UID and GID assigned -2, which means the anonymous access is in use.

Figure 23: anonymous authentication Cleaning up If you followed along and would like to remove resources used in this solution to avoid incurring any unwanted future charges, you can unmount the file systems and delete the FSx for OpenZFS file system by following the FSx for OpenZFS user guide. Conclusion In this post, I highlighted the method of using the NFS client on Windows Server to access data on the FSx for OpenZFS file system, enabling data sharing across Linux and Windows clients. I explored the process of mounting the file system, securing it through authentication, and tuning for performance. The main takeaway is the potential to use the NFS protocol for shared file storage using FSx for OpenZFS across both Linux and Windows platforms. The benefits include cross-platform data access, heightened security, and effective data management, which enhance productivity and optimize resources. By adopting this strategy, you not only harness the power of FSx for OpenZFS but also effectively bridge the gap between different operating systems. Next, for a deeper understanding of the FSx for OpenZFS service, you can delve into the references section. References

FSx for OpenZFS – OpenZFS User Guide Amazon FSx for OpenZFS performance Nfsadmin Utility

Virgil Ennes Virgil Ennes is a Specialty Sr. Solutions Architect at AWS. Virgil enjoys helping customers take advantage of the agility, costs savings, innovation, and global reach that AWS provides. He is mostly focused on Storage, AI, Blockchain, Analytics, IoT, and Cloud Economics. In his spare time, Virgil enjoys spending time with his family and friends, and also watching his favorite football club (GALO).

AWS End User Messaging 10DLC Campaign Types and Quotas for SMS

The following 10DLC Campaigns, or, Use Cases outlined in Table 2 are currently supported by AWS End User Messaging. As part of the process to register for sending SMS to US based phone numbers you must select at least one Campaign that will be associated with the 10DLC number you procure. If you require more than one Use Case then you will need more than one 10DLC, or you can select the standard mixed use case which supports lower volumes of messages. Throughput is determined based on the company vetting score of the registered sender of the message and what is being sent, not on the amount of numbers associated with the Campaign. For a breakdown of vetting scores and quotas see below: Throughput and Volume Quotas for 10DLC Vetted Companies *Note that by default each number associated to a 10DLC campaign supports 1 MPS. In order to increase your numbers to match what your campaign qualifies for by carriers you will be required to submit a MPS increase request. Table 1

Use Case Type Vetting Score Message Parts per Second (MPS) (AT&T Limits) Maximum daily messages (T-Mobile & Sprint)

Standard 10DLC Campaign Use Cases Select the campaign that most closely aligns with your use case(s). Table 2

Account Notifications Status notifications about an account that the recipient is a part of or owns

Delivery Notifications Notifications about the status of a delivery of a product or service

Higher Education Messaging Messaging originating from colleges, universities, or other post-secondary education institutions

Low Volume Small throughput, any combination of use-cases. Examples include: test, demo accounts

Marketing Messaging Promotional content related to sales or other offers

Mixed Use Cases Covers multiple use cases such as Account Notifications and Delivery Notifications. Mixed campaigns have lower throughput than dedicated ones

Polling and Voting – Not for Political Use Delivering messages containing customer surveys or other voting related actions. Not for political use

Public Service Announcements (PSA) Messages intended to raise awareness of a particular topic

Security Alerts Notifications related to a compromised software or hardware system that requires recipients to take an action

Two Factor Authentication(2FA) or One-Time Password(OTP) Authentication, account verifications, or one-time passcode

Special Use Cases Currently End User Messaging supports only the following special use cases. These may require different registration processes and/or fees than the Standard Use Cases above

Charity / 501(c)(3) Nonprofit Communications from a registered company classified as a 501(c)(3). Does not include religious organizations

Tyler Holmes Tyler is a Senior Specialist Solutions Architect. He has a wealth of experience in the communications space as a consultant, an SA, a practitioner, and leader at all levels from Startup to Fortune 500. He has spent over 14 years in sales, marketing, and service operations, working for agencies, consulting firms, and brands, building teams and increasing revenue.

Build production-ready generative AI applications for enterprise search using Haystack pipelines and Amazon SageMaker JumpStart with LLMs

This blog post is co-written with Tuana Çelik from deepset. Enterprise search is a critical component of organizational efficiency through document digitization and knowledge management. Enterprise search covers storing documents such as digital files, indexing the documents for search, and providing relevant results based on user queries. With the advent of large language models (LLMs), we can implement conversational experiences in providing the results to users. However, we need to ensure that the LLMs limit the responses to company data, thereby mitigating model hallucinations. In this post, we showcase how to build an end-to-end generative AI application for enterprise search with Retrieval Augmented Generation (RAG) by using Haystack pipelines and the Falcon-40b-instruct model from Amazon SageMaker JumpStart and Amazon OpenSearch Service. The source code for the sample showcased in this post is available in the GitHub repository Solution overview To restrict the generative AI application responses to company data only, we need to use a technique called Retrieval Augmented Generation (RAG). An application using the RAG approach retrieves information most relevant to the user's request from the enterprise knowledge base or content, bundles it as context along with the user's request as a prompt, and then sends it to the LLM to get a response. LLMs have limitations around the maximum word count for the input prompts, so choosing the right passages among thousands or millions of documents in the enterprise has a direct impact on the LLM's accuracy. The RAG technique has become increasingly important in enterprise search. In this post, we show a workflow that takes advantage of SageMaker JumpStart to deploy a Falcon-40b-instruct model and uses Haystack to design and run a retrieval augmented question answering pipeline. The final retrieval augmentation workflow covers the following high-level steps:

The user query is used for a retriever component, which does a vector search, to retrieve the most relevant context from our database. This context is embedded into a prompt that is designed to instruct an LLM to generate an answer only from the provided context. The LLM generates a response to the original query by only considering the context embedded into the prompt it received.

SageMaker JumpStart SageMaker JumpStart serves as a model hub encapsulating a broad array of deep learning models for text, vision, audio, and embedding use cases. With over 500 models, its model hub comprises both public and proprietary models from AWS's partners such as AI21, Stability AI, Cohere, and LightOn. It also hosts foundation models solely developed by Amazon, such as AlexaTM. Some of the models offer capabilities for you to fine-tune them with your own data. SageMaker JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning (ML) with SageMaker. Haystack Haystack is an open-source framework by deepset that allows developers to orchestrate LLM applications made up of different components like models, vector DBs, file converters, and countless other modules. Haystack provides pipelines and Agents, two powerful structures for designing LLM applications for various use cases including search, question answering, and conversational AI. With a big focus on state-of-the art retrieval methods and solid evaluation metrics, it provides you with everything you need to ship a reliable, trustworthy application. You can serialize pipelines to YAML files, expose them via a REST API, and scale them flexibly with your workloads, making it easy to move your application from a prototype stage to production. Amazon OpenSearch OpenSearch Service is a fully managed service that makes it simple to deploy, scale, and operate OpenSearch in the AWS Cloud. OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, security monitoring, and observability applications, licensed under the Apache 2.0 license. In recent years, ML techniques have become increasingly popular to enhance search. Among them are the use of embedding models, a type of model that can encode a large body of data into an n-dimensional space where each entity is encoded into a vector, a data point in that space, and organized such that similar entities are closer together. A vector database provides efficient vector similarity search by providing specialized indexes like k-NN indexes. With the vector database capabilities of OpenSearch Service, you can implement semantic search, RAG with LLMs, recommendation engines, and search rich media. In this post, we use RAG to enable us to complement generative LLMs with an external knowledge base that is typically built using a vector database hydrated with vector-encoded knowledge articles. Application overview The following diagram depicts the structure of the final application.

In this application, we use the Haystack Indexing Pipeline to manage uploaded documents and index documents and the Haystack Query Pipeline to perform knowledge retrieval from indexed documents. The Haystack Indexing Pipeline includes the following high-level steps:

We use OpenSearch as our DocumentStore and a Haystack indexing pipeline to preprocess and index our files to OpenSearch. Haystack FileConverters and PreProcessor allow you to clean and prepare your raw files to be in a shape and format that your natural language processing (NLP) pipeline and language model of choice can deal with. The indexing pipeline we've used here also uses sentence-transformers/all-MiniLM-L12-v2 to create embeddings for each document, which we use for efficient retrieval. The Haystack Query Pipeline includes the following high-level steps:

We send a query to the RAG pipeline. An EmbeddingRetriever component acts as a filter that retrieves the most relevant top_k documents from our indexed documents in OpenSearch. We use our choice of embedding model to embed both the query and the documents (at indexing) to achieve this. The retrieved documents are embedded into our prompt to the Falcon-40b-instruct model. The LLM returns with a response that is based on the retrieved documents.

For model deployment, we use SageMaker JumpStart, which simplifies deploying models through a simple push of a button. Although we've used and tested Falcon-40b-instruct for this example, you may use any Hugging Face model available on SageMaker. The final solution is available on the haystack-sagemaker repository and uses the OpenSearch website and documentation (for OpenSearch 2.7) as our example data to perform retrieval augmented question answering on. Prerequisites The first thing to do before we can use any AWS services is to make sure we have signed up for and created an AWS account. Then you should create an administrative user and group. For instructions on both steps, refer to Set Up Amazon SageMaker Prerequisites. To be able to use the Haystack, you'll have to install the farm-haystack package with the required dependencies. To accomplish this, use the requirements.txt file in the GitHub repository by running pip install requirements.txt. Index documents to OpenSearch Haystack offers a number of connectors to databases, which are called DocumentStores. For this RAG workflow, we use the OpenSearchDocumentStore. The example repository includes an indexing pipeline and AWS CloudFormation template to set up an OpenSearchDocumentStore with documents crawled from the OpenSearch website and documentation pages. Often, to get an NLP application working for production use cases, we end up having to think about data preparation and cleaning. This is covered with Haystack indexing pipelines, which allows you to design your own data preparation steps, which ultimately write your documents to the database of your choice. An indexing pipeline may also include a step to create embeddings for your documents. This is highly important for the retrieval step. In our example, we use sentence-transformers/all-MiniLM-L12-v2 as our embedding model. This model is used to create embeddings for all our indexed documents, but also the user's query at query time. To index documents into the OpenSearchDocumentStore, we provide two options with detailed instructions in the README of the example repository. Here, we walk through the steps for indexing to an OpenSearch service deployed on AWS. Start an OpenSearch service Use the provided CloudFormation template to set up an OpenSearch service on AWS. By running the following command, you'll have an empty OpenSearch service. You can then either choose to index the example data we've provided or use your own data, which you can clean and preprocess using the Haystack Indexing Pipeline. Note that this creates an instance that is open to the internet, which is not recommended for production use.

```bash aws cloudformation create-stack --stack-name HaystackOpensearch --template-body file://cloudformation/opensearch-index.yaml --parameters ParameterKey=InstanceType,ParameterValue=r5.large.search ParameterKey=InstanceCount,ParameterValue=3 ParameterKey=OSPassword,ParameterValue=Password123! ```Bash

Allow approximately 30 minutes for the stack launch to complete. You can check its progress on the AWS CloudFormation console by navigating to the Stacks page and looking for the stack named HaystackOpensearch. Index documents into OpenSearch Now that we have a running OpenSearch service, we can use the OpenSearchDocumentStore class to connect to it and write our documents to it. To get the hostname for OpenSearch, run the following command:

```bash aws cloudformation describe-stacks --stack-name HaystackOpensearch --query "Stacks[0].Outputs[?OutputKey=='OpenSearchEndpoint'].OutputValue" --output text ```Bash

```bash export OPENSEARCH_HOST='your_opensearch_host' export OPENSEARCH_PORT=443 export OPENSEARCH_USERNAME=admin export OPENSEARCH_PASSWORD=Password123! ```Bash

Then, you can use the opensearch_indexing_pipeline.py script to preprocess and index the provided demo data. If you would like to use your own data, modify the indexing pipeline in opensearch_indexing_pipeline.py to include the FileConverter and PreProcessor setup steps you require. Implement the retrieval augmented question answering pipeline Now that we have indexed data in OpenSearch, we can perform question answering on these documents. For this RAG pipeline, we use the Falcon-40b-instruct model that we've deployed on SageMaker JumpStart. You also have the option of deploying the model programmatically from a Jupyter notebook. For instructions, refer to the GitHub repo.

Deploy your model on SageMaker JumpStart, and take note of the endpoint name.

This will start a command line utility that waits for a user's question. For example, let's ask "How can I install the OpenSearch cli?"

This result is achieved because we have defined our prompt in the Haystack PromptTemplate to be the following:

```python question_answering = PromptTemplate(prompt="Given the context please answer the question. If the answer is not contained within the context below, say 'I don't know'.\n" "Context: {join(documents)};\n Question: {query};\n Answer: ", output_parser=AnswerParser(reference_pattern=r"Document\[(\d+)\]")) ```Python

Further customizations You can make additional customizations to different elements in the solution, such as the following:

The data – We've provided the OpenSearch documentation and website data as example data. Remember to modify the opensearch_indexing_pipeline.py script to fit your needs if you chose to use your own data. The model – In this example, we've used the Falcon-40b-instruct model. You are free to deploy and use any other Hugging Face model on SageMaker. Note that changing a model will likely mean you should adapt your prompt to something it's designed to handle. The prompt – For this post, we created our own PromptTemplate that instructs the model to answer questions based on the provided context and answer "I don't know" if the context doesn't include relevant information. You may change this prompt to experiment with different prompts with Falcon-40b-instruct. You can also simply pull some of our prompts from the PromptHub. The embedding model – For the retrieval step, we use a lightweight embedding model: sentence-transformers/all-MiniLM-L12-v2. However, you may also change this to your needs. Remember to modify the expected embedding dimensions in your DocumentStore accordingly. The number of retrieved documents – You may also choose to play around with the number of documents you ask the EmbeddingRetriever to retrieve for each query. In our setup, this is set to top_k=5. You may experiment with changing this figure to see if providing more context improves the accuracy of your results.

Production readiness The proposed solution in this post can accelerate the time to value of the project development process. You can build a project that is easy to scale with the security and privacy environment on the AWS Cloud. For security and privacy, OpenSearch Service provides data protection with identity and access management and cross-service confused proxy prevention. You may employ fine-grained user access control so that the user can only access the data they are authorized to access. Additionally, SageMaker provides configurable security settings for access control, data protection, and logging and monitoring. You can protect your data at rest and in transit with AWS Key Management Service (AWS KMS) keys. You can also track the log of SageMaker model deployment or endpoint access using Amazon CloudWatch. For more information, refer to Monitor Amazon SageMaker with Amazon CloudWatch. For the high scalability on OpenSearch Service, you may adjust it by sizing your OpenSearch Service domains and employing operational best practices. You can also take advantage of auto scaling your SageMaker endpoint—you can automatically scale SageMaker models to adjust the endpoint both when the traffic is increased or the resources are not being used. Clean up To save costs, delete all the resources you deployed as part of this post. If you launched the CloudFormation stack, you can delete it via the AWS CloudFormation console. Similarly, you can delete any SageMaker endpoints you may have created via the SageMaker console. Conclusion In this post, we showcased how to build an end-to-end generative AI application for enterprise search with RAG by using Haystack pipelines and the Falcon-40b-instruct model from SageMaker JumpStart and OpenSearch Service. The RAG approach is critical in enterprise search because it ensures that the responses generated are in-domain and therefore mitigating hallucinations. By using Haystack pipelines, we are able to orchestrate LLM applications made up of different components like models and vector databases. SageMaker JumpStart provides us with a one-click solution for deploying LLMs, and we used OpenSearch Service as the vector database for our indexed data. You can start experimenting and building RAG proofs of concept for your enterprise generative AI applications, using the steps outlined in this post and the source code available in the GitHub repository.

About the Authors Tuana Celik is the Lead Developer Advocate at deepset, where she focuses on the open-source community for Haystack. She leads the developer relations function and regularly speaks at events about NLP and creates learning materials for the community. Roy Allela is a Senior AI/ML Specialist Solutions Architect at AWS based in Munich, Germany. Roy helps AWS customers—from small startups to large enterprises—train and deploy large language models efficiently on AWS. Roy is passionate about computational optimization problems and improving the performance of AI workloads. Mia Chang is an ML Specialist Solutions Architect for Amazon Web Services. She works with customers in EMEA and shares best practices for running AI/ML workloads on the cloud with her background in applied mathematics, computer science, and AI/ML. She focuses on NLP-specific workloads, and shares her experience as a conference speaker and a book author. In her free time, she enjoys hiking, board games, and brewing coffee. Inaam Syed is a Startup Solutions Architect at AWS, with a strong focus on assisting B2B and SaaS startups in scaling and achieving growth. He possesses a deep passion for serverless architectures and AI/ML. In his leisure time, Inaam enjoys quality moments with his family and indulges in his love for biking and badminton. David Tippett is the Senior Developer Advocate working on open-source OpenSearch at AWS. His work involves all areas of OpenSearch from search and relevance to observability and security analytics.

This post is the first in a series dedicated to the art and science of practical data mesh implementation (for an overview of data mesh, read the original whitepaper The data mesh shift). The series attempts to bridge the gap between the tenets of data mesh and its real-life implementation by deep-diving into the functional and non-functional capabilities essential to a working operating model, laying out the decisions that need to be made for each capability, and describing the key business and technical processes required to implement them. Taken together, the posts in this series lay out some possible operating models for data mesh within an organization.

Kudzu Kudzu—or kuzu (クズ)—is native to Japan and southeast China. First introduced to the southeastern United States in 1876 as a promising solution for erosion control, it now represents a cautionary tale about unintended consequences, as Kudzu's speed of growth outcompetes everything from native grasses to tree systems by growing over and shading them from the sunlight they need to photosynthesize—eventually leading to species extinction and loss of biodiversity. The story of Kudzu offers a powerful analogy to the dangers and consequences of implementing data mesh architectures without fully understanding or appreciating how they are intended to be used. When the "Kudzu" of unmanaged pseudo-data products (methods of sharing data that masquerade as data products while failing to fulfill the myriad obligations associated with them) has overwhelmed the local ecosystem of true data products, eradication is costly and prone to failure, and can represent significant wasted effort and resources, as well as lost time. Desert While Kudzu was taking over the south in the 1930s, desertification caused by extensive deforestation was overwhelming the Midwest, with large tracts of land becoming barren and residents forced to leave and find other places to make a living. In the same way, overly restrictive data governance practices that either prevent data products from taking root at all, or pare them back too aggressively (deforestation), can over time create "data deserts" that drive both the producers and consumers of data within an organization to look elsewhere for their data needs. At the same time, unstructured approaches to data mesh management that don't have a vision for what types of products should exist and how to ensure they are developed are at high risk of creating the same effect through simple neglect. This is due to a common misconception about data mesh as a data strategy, which is that it is effectively self-organizing—meaning that once presented with the opportunity, data owners within the organization will spring to the responsibilities and obligations associated with publishing high-quality data products. In reality, the work of a data producer is often thankless, and without clear incentive strategies, organizations may end up with data deserts that create more data governance issues as producers and consumers go elsewhere to seek out the data they need to perform work. Bonsai Bonsai (盆栽) is an art form originating from an ancient Chinese tradition called penjing (盆景), and later shaped by the minimalist teachings of Zen Buddhism into the practice we know and recognize today. The patient practice of Bonsai offers useful analogies to the concepts and processes required to avoid the chaos of Kudzu as well as the specter of organizational data deserts. Bonsai artists carefully observe the naturally occurring buds that are produced by the tree and encourage those that add to the overall aesthetics of the tree, while pruning those that don't work well with their neighbors. The same ideas apply equally well to data products within a data mesh—by encouraging the growth and adoption of those data products that add value to our data mesh, and continuously pruning those that do not, we maximize the value and sustainability of our data mesh implementations. In a similar vein, Bonsai artists must balance their vision for the shape of the tree with a respect for the natural characteristics and innate structure of the species they have chosen to work with—to ignore the biology of the tree would be disastrous to the longevity of the tree, as well as to the quality of the art itself. In the same way, organizations seeking to implement successful data mesh strategies must respect the nature and structure (legal, political, commercial, technology) of their organizations in their implementation. Of the key capabilities proposed for the implementation of a sustainable data mesh operating model, the one that is most relevant to the problems we've described—and explore later in this post—is data product portfolio management. Overview of data product portfolio management Data mesh architectures are, by their nature, ideal for implementation within federated organizations, with decentralized ownership of data and clear legal, regulatory, or commercial boundaries between entities or lines of business. The same organizational characteristics that make data mesh architectures valuable, however, also put them at risk of turning into one of the twin nightmares of Kudzu or data deserts. To define the shape and nature of an organizational data mesh, a number of key questions need to be answered, including but not limited to:

What are the key data domains within the organization? What are the key data products within these domains needed to solve current business problems? How do we iterate on this discovery process to add value while we are mapping our domains? Who are the consumers in our organization, and what logical, regulatory, physical, or commercial boundaries might separate them from producers and their data products? How do we encourage the development and maintenance of key data products in a decentralized organization? How do we monitor data products against their SLAs, and ensure alerting and escalation on failure so that the organization is protected from bad data? How do we enable those we see as being autonomous producers and consumers with the right skills, the right tools, and the right mindset to actually want to (and be able to) take more ownership of independently publishing data as a product and consuming it responsibly? What is the lifecycle of a data product? When do new data products get created, and who is allowed to create them? When are data products deprecated, and who is accountable for the consequences to their consumers? How do we define "risk" and "value" in the context of data products, and how can we measure this? Whose responsibility is it to justify the existence of a given data product?

To answer questions such as these and plan accordingly, organizations must implement data product portfolio management (DPPM). DPPM does not exist in a vacuum—by its nature, DPPM is closely related to and interdependent with enterprise architecture practices like business capability management and project portfolio management. DPPM itself may therefore also be considered, in part, an enterprise architecture practice. As an enterprise architecture practice, DPPM is responsible for its implementation, which should reside within a function whose remit is appropriately global and cross-functional. This may be within the CDO office for those organizations that have a CDO or equivalent central data function, or the enterprise architecture team in organizations that do not. Goals of DPPM

Protect value – DPPM protects the value of the organizational data strategy by developing, implementing, and enforcing frameworks to measure the contribution of data products to organizational goals in objective terms. Examples may include associated revenue, savings, or reductions in operational losses. Earlier in their lifecycle, data products may be measured by alternative metrics, including adoption (number of consumers) and level of activity (releases, interaction with consumers, and so on). In the pursuit of this goal, the DPPM capability is accountable for engaging with the business to continuously priorities where data as a product can add value and align delivery priority accordingly. Strategies for measuring value and prioritizing data products are explored later in this post. Manage risk – All data products introduce risk to the organization—risk of wasted money and effort through non-adoption, risk of operational loss associated with improper use, and risk of failure on the part of the data product to meet requirements on availability, completeness, or quality. These risks are exacerbated in the case of proliferation of low-quality or unsupervised data products. DPPM seeks to understand and measure these risks on an individual and aggregated basis. This is a particularly challenging goal because what constitutes risk associated with the existence of a particular data product is determined largely by its consumers and is likely to change over time (though like entropy, is only ever likely to increase). Guide evolution – The final goal of DPPM is to guide the evolution of the data product landscape to meet overarching organizational data goals, such as mutually exclusive or collectively exhaustive domains and data products, the identification and enablement of single-threaded ownership of product definitions, or the agile inclusion of new sources of data and creation of products to serve tactical or strategic business goals. Some principles for the management of data mesh evolution, and the evaluation of data products against organizational goals, are explored later in this post.

Challenges of DPPM In this section, we explore some of the challenges of DPPM, and the pragmatic ways some of these challenges could be addressed. Infancy Data mesh as a concept is still relatively new. As such, there is little standardization associated with practical operating models for building and managing data mesh architectures, and no access to fully fledged out-of-the-box reference operating models, frameworks, or tools to support the practice of DPPM. Some elements of DPPM are supported in disparate tools (for example, some data catalogs include basic community features that contribute to measuring value), but not in a holistic way. Over time, standardization of the processes associated with DPPM will likely occur as a side-effect of commoditization, driven by the popularity and adoption of new services that take on and automate more of the undifferentiated heavy lifting associated with mesh supervision. In the meantime, however, organizations adopting data mesh architectures are left largely to their own devices around how to operate them effectively. Resistance The purest expression of democracy is anarchy, and the more federated an organization is (itself a supporting factor in choosing data mesh architectures), the more resistance may be observed to any forms of centralized governance. This is a challenge for DPPM, because in some way it must come together in one place. Just as the Bonsai artist knows the vision for the entire tree, there must be a cohesive vision for and ability to guide the evolution of a data mesh, no matter how broadly federated and autonomous individual domains or data products might be. Balancing this with the need to respect the natural shape (and culture) of an organization, however, requires organizations that implement DPPM to think about how to do so in a way that doesn't conflict with the reality of the organization. This might mean, for example, that DPPM may need to happen at several layers—at minimum within data domains, possibly within lines of business, and then at an enterprise level through appropriate data committees, guilds, or other structures that bring stakeholders together. All of this complicates the processes and collaboration needed to perform DPPM effectively. Maturity Data mesh architectures, and therefore DPPM, presume relatively high levels of data maturity within an organization—a clear data strategy, understanding of data ownership and stewardship, principles and policies that govern the use of data, and a moderate-to-high level of education and training around data within the organization. A lack of data maturity within the organization, or a weak or immature enterprise architecture function, will face significant hurdles in the implementation of any data mesh architecture, let alone a strong and useful DPPM practice. In reality, however, data maturity is not uniform across organizations. Even in seemingly low-maturity organizations, there are often teams who are more mature and have a higher appetite to engage. By leaning into these teams and showing value through them first, then using them as evangelists, organizations can gain maturity while benefitting earlier from the advantages of data mesh strategies. The following sections explore the implementation of DPPM along the lines of people, process, and technology, as well as describing the key characteristics of data products—scope, value, risk, uniqueness, and fitness—and how they relate to data mesh practices. People To implement DPPM effectively, a wide variety of stakeholders in the organization may need to be involved in one capacity or another. The following table suggests some key roles, but it's up to an individual organization to determine how and if these map to their own roles and functions.

Senior Leadership A Chief Data Officer Ultimately accountable for organizational data strategy and implementation. Approves changes to DPPM principles and operating model. Acts as chair of, and appoints members to, the data council.

. R Data Council** Stakeholder body representing organizational governance around data strategy. Acts as steering body for the governance of DPPM as a practice (KPI monitoring, maturity assessments, auditing, and so on). Approves changes to guidelines and methodologies. Approves changes to data product portfolio (discussed later in this post). Approves and governs centrally funded and prioritized data product development activities.

Enterprise Architecture AR Head of Enterprise Architecture Responsible for development and enforcement of data strategy. Accountable and responsible for the design and implementation of DPPM as an organizational capability.

. R Domain Architect Responsible for the implementing screening, data product analysis, periodic evaluation, and optimal portfolio selection practices. Responsible for the development of methodologies and their selection criteria.

Legal & Compliance C Legal & Compliance Officer Consults on permissibility of data products with reference to local regulation. Consults on permissibility of data sharing with reference to local regulation or commercial agreements.

. C Data Privacy Officer Consults on permissibility of data use with reference to local data privacy law. Consults on permissibility of cross-entity or border data sharing with reference to data privacy law.

Information Security RC Information Security Officer Consults on maturity assessments (discussed later in this post) for information security-relevant data product capabilities. Approves changes to data product technology architecture. Approves changes to IAM procedures relating to data products.

Business Functions A Data Domain Owner Ultimately accountable for the appropriate use of domain data, as well as its quality and availability. Accountable for domain data products. Approves changes to the domain data model and domain data product portfolio.

c R Data Domain Steward Responsible for implementing data domain responsibilities, including operational (day-to-day) governance of domain data products. Approves use of domain data in new data products, and performs regular (such as yearly) attestation of data products using domain data.

. A Data Owner Ultimately accountable for the appropriate use of owned data (for example, CRM data), as well as its quality and availability.

. R Data Steward Responsible for implementing data responsibilities. Approves use of owned data in new data products, and performs regular (such as yearly) attestation of data products using owned data.

. AR Data Product Owner Accountable and responsible for the design, development, and delivery of data products against their stated SLOs. Contributes to data product analysis and portfolio adjustment practices for own data products.

** The data council typically consists of permanent representatives from each function (data domain owners), enterprise architecture, and the chief data officer or equivalent. Process The following diagram illustrates the strategic, tactical, and operational practices associated with DPPM. Some considerations for the implementation of these practices is explored in more detail in this post, though their specific interpretation and implementation is dependent on the individual organization.

Boundaries When reading this section, it's important to bear in mind the impact of boundaries—although strategy development may be established as a global practice, other practices within DPPM must respect relevant organizational boundaries (which may be physical, geographical, operational, legal, commercial, or regulatory in nature). In some cases, the existence of boundaries may require some or all tactical and operational practices to be duplicated within each associated boundary. For example, an insurance company with a property and casualty legal entity in North America and a life entity in Germany may need to implement DPPM separately within each entity. Strategy development This practice deals with answering questions associated with the overall data mesh strategy, including the following:

The overall scope (data domains, participating entities, and so on) of the data mesh The degree of freedom of participating entities in their definition and implementation of the data mesh (for example, a mesh of meshes vs. a single mesh) The distribution of responsibilities for activities and capabilities associated with the data mesh (degree of democratization) The definition and documentation of key performance indicators (KPIs) against which the data mesh should be governed (such as risk and value) The governance operating model (including this practice)

Organizational guidelines for operational processes around pre-screening and screening of data products Well-defined KPIs that guide methodology development and selection for practices like data product analysis, screening, and optimal portfolio selection Allocation of organizational resources (people, budget, time) to the implementation of tactical processes around methodology development, optimal portfolio selection, and portfolio adjustment

Key considerations In this section, we discuss some key considerations for strategy development. Data mesh structure

This diagram illustrates the analogous relationship between data products in a data mesh, and the structure of the mesh itself. The following considerations relate to screening, data product analysis, and optimal portfolio selection.

Trunk (core data products) – Core data products are those that are central to the organization's ability to function, and from which the majority of other data products are derived. These may be data products consumed in the implementation of key business activities, or associated with critical processes such as regulatory reporting and risk management. Organizational governance for these data products typically favors availability and data accuracy over agility.

Branch (cross-domain data products) – Cross-domain data products represent the most common cross-domain use cases for data (for example, joining customer data with product data). These data products may be widely used across business functions to support reporting and analytics, and—to a lesser extent—operational processes. Because these data products may consume a variety of sources, organizational governance may favor a balanced view on agility vs. reliability, accepting some degree of risk in return for being able to adapt to changes in data sources. Data product versioning can offer mitigation of risks associated with change. Leaf (everything else) – These are the myriad data products that may arise within a data mesh, either as permanent additions to support individual teams and use cases or as temporary data products to fill data gaps or support time-limited initiatives. Because the number of these data products may be high and risks are typically limited to a single process or a small part of the organization, organizational governance typically favors a light touch and may prefer to govern through guidelines and best practices, rather than through active participation in the data product lifecycle.

Data products vs. data definitions The following figure illustrates how data definitions are defined and inherited throughout the lineage of data products.

In a data mesh architecture, data products may inherit data from each other (one data product consumes another in its data pipeline) or independently publish data within (or related to) the same domain. For example, a customer data product may be inherited by a customer support data product, while another the customer journey data product may directly publish customer-relevant data from independent sources. When no standards are applied to how domain data attributes are used and published, data products even within the same data domain may lose interoperability because it becomes difficult or impossible to join them together for reporting or analytics purposes. To prevent this, it can be useful to distinguish between data products and data definitions. Typically, organizations will select a single-threaded owner (often a data owner or steward, or a domain data owner or steward) who is responsible for defining minimal data definitions for common and reusable data entities within data domains. For example, a data owner responsible for the sales and marketing data domain may identify a customer data product as a reusable data entity within the domain and publish a minimal data definition that all producers of customer-relevant data must incorporate within their data products, to ensure that all data products associated with customer data are interoperable. DPPM can assist in the identification and production of data definitions as part of its data product analysis activities, as well as enforce their incorporation as part of oversight of data product development. Service management thinking These considerations relate to data product analysis, periodic evaluation, and methodology selection. Data products are services provided to the organization or externally to customers and partners. As such, it may make sense to adapt a service management framework like ITIL, in combination with the ITIL Maturity Model, for use in evaluating the fitness of data products for their scope and audience, as well as in describing the roles, processes, and acceptable technologies that should form the operating model for any data product. At the operational level, the stakeholders required to implement each practice may change depending on the scope of the data product. For example, the release management practice for a core data product may require involvement of the data council, whereas the same practice for a team data product may only involve the team or functional head. To avoid creating decision-making bottlenecks, organizations should aim to minimize the number of stakeholders in each case and focus on single-threaded owners wherever possible. The following table proposes a subset of capabilities and how they might be applied to data products of different scopes. Suggested target maturity levels, between 1 and 5, are included for each scope. (1= Initial, 5= Optimizing)

Methodology development This practice deals with the development of concrete, objective frameworks, metrics, and processes for the measurement of data product value and risk. Because the driving factors behind risk and value are not necessarily the same between products, it may be necessary to develop several methodologies or variants thereof. Key deliverables include the following:

Well-defined frameworks for measuring risk and value of data products, as well as for determining the optimal portfolio of data products Operationally feasible, measurable metrics associated with value and risk

Key considerations A key consideration for assessing data products is that of consumer value or risk vs. uniqueness. The following diagram illustrates how value and risk of a data product are driven by its consumers.

Data products don't inherently present risk or add value, but rather indirectly pose—in an aggregated fashion—the risk and value created by their consumers. In a consumer-centric value and risk model, governance of consumers ensures that all data use meets the following requirements:

Is associated with a business case justifying the use of data (for example, new business, cost reduction through business process automation, and so on) Is regularly evaluated with reference to the risk associated with the use case (for example, regulatory reporting

The value and risk associated with the linked data products are then calculated as an aggregation. Where organizations already track use cases associated with data, either as part of data privacy governance or as a by-product of the access approval process, these existing systems and databases can be reused or extended. Conversely, where data products overlap with each other, their value to the organization is reduced accordingly, because redundancies between data products represent an inefficient use of resources and increase organizational complexity associated with data quality management. To ensure that the model is operationally feasible (see the key deliverables of methodology development), it may be sufficient to consider simple aggregations, rather than attempting to calculate value and risk attribution at a product or use case level. Optimal portfolio selection This practice deals with the determination of which combination of data products (existing, new, or potential) would best meet the organization's current and known future needs. This practice takes input from data product analysis and data product proposals, as well as other enterprise architecture practices (for example, business architecture), and considers trade-offs between data-debt and time-to-value, as well as other considerations such as redundancy between data products to determine the optimal mix of permanent and temporary data products at any given point in time. Because the number of data products in an organization may become significant over time, it may be useful to apply heuristics to the problem of optimal portfolio selection. For example, it may be sufficient to consider core and cross-domain data products (trunk and branches) during quarterly portfolio reviews, with other data products (leaves) audited on a yearly basis. Key deliverables include the following:

A target state definition for the data mesh, including all relevant data products An indication of organizational priorities for use by the portfolio adjustment practice

The following are key considerations regarding the data product half-life:

Long-term or strategic data products – These data products fill a long-term organizational need, are often associated with key source systems in various domains, and anchor the overall data strategy. Over time, as an organization's data mesh matures, long-term data products should form the bulk of the mesh. Time-bound data products – These data products fill a gap in data strategy and allow the organization to move on data opportunities until core data products can be updated. An example of this might be data products created and used in the context of mergers and acquisitions transactions and post-acquisition, to provide consistent data for reporting and business intelligence until mid-term and long-term application consolidation has taken place. Time-bound data products are considered as data-debt and should be managed accordingly. Purpose-driven data products – These data products serve a narrow, finite purpose. Purpose-driven data products may or may not be time-bound, but are characterized primarily by a strict set of consumers known in advance. Examples of this might include:

Data products developed to support system-of-record harmonization between lines of business (for example, deduplication of customer records between insurance lines of business using separate CRM systems Data products created explicitly for the monitoring of other data products (data quality, update frequency, and so on)

Portfolio adjustment This practice implements the feasibility analysis, planning and project management, as well as communication and organizational change management activities associated with changes to the optimal portfolio. As part of this practice, a gap analysis is conducted between the current and target data product portfolio, and a set of required actions and estimated time and effort prepared for review by the organization. During such a period, data products may be marked for development (new data products to fill a need), changes, consolidation (merging two or more data products into a single data product), or deprecation. Several iterations of optimal portfolio selection and portfolio adjustment may be required to find an appropriate balance between optimality and feasibility of implementation. Key deliverables include the following:

A gap analysis between the current and target data product portfolio, as well as proposed remediation activities High-level project plans and effort or budget assessments associated with required changes, for approval by relevant stakeholders (such as the data council)

Data product proposals This practice organizes the collection and prioritization of requests for new, or changes to existing, data products within the organization. Its implementation may be adapted from or managed by existing demand management processes within the organization. Key deliverables include a registry of demand against new or existing data products, including metadata on source systems, attributes, known use cases, proposed data product owners, and suggested organizational priority. Methodology selection This practice is associated with the identification and application of the most appropriate methodologies (such as value and risk) during data product analysis, screening, and optimal portfolio selection. The selection of an appropriate methodology for the type, maturity, and scope of a data product (or an entire portfolio) is a key element in avoiding either a "Kudzu" mesh or a "data desert." Key deliverables include reusable selection criteria for mapping methodologies to data products during data product analysis, screening, and optimal portfolio selection. Pre-screening This optional practice is primarily a mechanism to avoid unnecessary time and effort in the comparatively expensive practice of data product analysis by offering simple self-service applications of guidelines to the evaluation of data products. An example might include the automated approval of data products that fall under the classification of personal data products, requiring only attestation on the part of the requester that they will uphold the relevant portions of the guideline that governs such data products. Key deliverables include tools and checklists for the self-service evaluation of data products against guidelines and automated registration of approved data products. Data product analysis This practice incorporates guidelines, methodologies, as well as (where available) metadata relating to data products (performance against SLOs, service management metrics, degree of overlap with other data products) to establish an understanding of the value and risk associated with individual data products, as well as gaps between current and target capability maturities, and compliance with published product definitions and standards. Key deliverables include a summary of findings for a particular data product, including scores for relevant value, risk, and maturity metrics, as well as operational gaps requiring remediation and recommendations on next steps (repair, enhance, decommission, and so on). Screening This optional practice is a mechanism to reduce complexity in optimal portfolio selection by ensuring the early removal of data products from consideration that fail to meet value or risk targets, or have been identified as redundant to other data products already available in the organization. Key deliverables include a list of data products that should be slated for removal (direct-to-decommissioning). Data product development This practice is not performed directly under DPPM, but is managed in part by the portfolio adjustment practice, and may be governed by standards that are developed as part of DPPM. In the context of DPPM, this practice is primarily associated with ensuring that data products are developed according to the specifications agreed as part of portfolio adjustment. Key deliverables include project management and software or service development deliverables and artefacts. Data product decommissioning This practice manages the decommissioning of data products and the migration of affected consumers to new or other data products where relevant. Unmanaged decommissioning of data products, especially those with many downstream consumers, can threaten the stability of the entire data mesh, as well as have significant consequences to business functions. Key deliverables include a decommissioning plan, including stakeholder assessment and sign-off, timelines, migration plans for affected consumers, and back-out strategies. Periodic evaluation This practice manages the calendar and implementation of periodic reviews of the data mesh, both in its entirety as well as at the data product level, and is primarily an exercise in project management. Key deliverables include the following:

yearly review calendar, published and made available to all data product owners and affected stakeholders Project management deliverables and artefacts, including evidence of evaluations having been performed against each data product

Technology Although most practices within DPPM don't rely heavily on technology and automation, some key supporting applications and services are required to implement DPPM effectively:

Data catalog – Core to the delivery of DPPM is the organizational data catalog. Beyond providing transparency into what data products exist within an organization, a data catalog can provide key insights into data lineage between data products (key to the implementation of portfolio adjustment) and adoption of data products by the organization. The data catalog can also be used to capture and make available both the documented as well as the realized SLO for any given data product, and—through the use of a business glossary—assist in the identification of redundancy between data products. Service management – Service management solutions (such as ServiceNOW) used in the context of data product management offer important insights into the fitness of data products by capturing and tracking incidents, problems, requests, and other metrics against data products. Demand management – A demand management solution supports self-service implementation and automation of data product proposal and pre-screening activities, as well as prioritization activities associated with selection and development of data products.

Conclusion Although this post focused on implementing DPPM in the context of a data mesh, this capability—like data product thinking—is not exclusive to data mesh architectures. The practices outlined here can be practiced at any scale to ensure that the production and use of data within the organization is always in line with its current and future needs, that governance is implemented in a consistent way, and that the organization can have Bonsai, not Kudzu. For more information about data mesh and data management, refer to the following:

How to unlock the full value of data? Manage it like a product The data mesh shift

In upcoming posts, we will cover other aspects of data mesh operating models, including data mesh supervision and service management models for data product owners.

Maximilian Mayrhofer is a Principal Solutions Architect working in the AWS Financial Services EMEA Go-to-Market team. He has over 12 years experience in digital transformation within private banking and asset management. In his free time, he is an avid reader of science fiction and enjoys bouldering.

Faris Haddad is the Data & Insights Lead in the AABG Strategic Pursuits team. He helps enterprises successfully become data-driven.

Zero-shot and few-shot prompting for the BloomZ 176B foundation model with the simplified Amazon SageMaker JumpStart SDK

Amazon SageMaker JumpStart is a machine learning (ML) hub offering algorithms, models, and ML solutions. With SageMaker JumpStart, ML practitioners can choose from a growing list of best performing and publicly available foundation models (FMs) such as BLOOM, Llama 2, Falcon-40B, Stable Diffusion, OpenLLaMA, Flan-T5/UL2, or FMs from Cohere and LightOn. In this post and accompanying notebook, we demonstrate how to deploy the BloomZ 176B foundation model using the SageMaker Python simplified SDK in Amazon SageMaker JumpStart as an endpoint and use it for various natural language processing (NLP) tasks. You can also access the foundation models thru Amazon SageMaker Studio. The BloomZ 176B model, one of the largest publicly available models, is a state-of-the-art instruction-tuned model that can perform various in-context few-shot learning and zero-shot learning NLP tasks. Instruction tuning is a technique that involves fine-tuning a language model on a collection of NLP tasks using instructions. To learn more about instruction tuning, refer to Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart. Zero-shot learning in NLP allows a pre-trained LLM to generate responses to tasks that it hasn't been specifically trained for. In this technique, the model is provided with an input text and a prompt that describes the expected output from the model in natural language. Zero-shot learning is used in a variety of NLP tasks, such as the following:

Multilingual text and sentiment classification Multilingual question and answering Code generation Paragraph rephrasing Summarization Common sense reasoning and natural language inference Question answering Sentence and sentiment classification Imaginary article generation based on a title Summarizing a title based on an article

Few-shot learning involves training a model to perform new tasks by providing only a few examples. This is useful where limited labeled data is available for training. Few-show learning is used in a variety of tasks, including the following:

Text summarization Code generation Name entity recognition Question answering Grammar and spelling correction Product description and generalization Sentence and sentiment classification Chatbot and conversational AI Tweet generation Machine translation Intent classification

About Bloom The BigScience Large Open-science Open-access Multilingual (BLOOM) language model is a transformer-based large language model (LLM). BLOOM is an autoregressive LLM trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for by casting them as text generation tasks. With its 176 billion parameters, BLOOM is able to generate text in 46 natural languages and 13 programming languages. For almost all of them, such as Spanish, French, and Arabic, BLOOM is the first language model with over 100 billion parameters ever created. Researchers can download, run, and study BLOOM to investigate the performance and behavior of recently developed LLMs down to their deepest internal operations. Solution overview In this post, we show how to use the state-of-the-art instruction-tuned BloomZ 176B model from Hugging Face for text generation. You can use the BloomZ 176B model with few-shot learning and zero-shot learning for many NLP tasks, without fine-tuning the model. There is no need to train a new model because models like BloomZ 176B have a significant number of parameters such that they can easily adapt to many contexts without being retrained. The BloomZ 176B model has been trained with a large amount of data, making to applicable for many general-purpose tasks. The code for all the steps in this demo is available in the following notebook. Instruction tuning The size and complexity of LLMs have exploded in the last few years. LLMs have demonstrated remarkable capabilities in learning the semantics of natural language and producing human-like responses. Many recent LLMs are fine-tuned with a powerful technique called instruction tuning, which helps the model perform new tasks or generate responses to novel prompts without prompt-specific fine-tuning. An instruction-tuned model uses its understanding of related tasks or concepts to generate predictions to novel prompts. Because this technique doesn't involve updating model weights, it avoids the time-consuming and computationally expensive process required to fine-tune a model for a new, previously unseen task. Instruction tuning involves fine-tuning a language model on a collection of NLP tasks using instructions. In this technique, the model is trained to perform tasks by following textual instructions instead of specific datasets for each task. The model is fine-tuned with a set of input and output examples for each task, allowing the model to generalize to new tasks that it hasn't been explicitly trained on as long as prompts are provided for the tasks. Instruction tuning helps improve the accuracy and effectiveness of models and is helpful in situations where large datasets aren't available for specific tasks. Prompt engineering for zero-shot and few-shot NLP tasks on BLOOM models Prompt engineering deals with creating high-quality prompts to guide the model towards the desired responses. Prompts need to be designed based on the specific task and dataset being used. The goal here is to provide the model with necessary information to generate high-quality responses while minimizing noise. This could involve keywords, additional contexts, questions, and more. A well-designed prompt can make the model more creative and generalized so that it can easily adapt to new tasks. Prompts can also help incorporate domain knowledge on specific tasks and improve interpretability. Prompt engineering can greatly improve the performance of zero-shot and few-shot learning models. Creating high-quality prompts requires careful consideration of the task at hand, as well as a deep understanding of the model's strengths and limitations. In the following table, the BloomZ 176B model is used to generate responses for various zero-shot and few-shot NLP tasks. The first column shows the task, the second column contains the prompt provided to the model (where the template text is bold and the non-bold text is the example input) and the third column is the response from the model when queried against the prompt For all the following tasks, we set the hyperparameters as 600 to be the max_length, num_return_sequences, top_k to be 250, top_p to be 0.95, and no sampling.

Multilingual text or sentiment classification (Chinese to English) 一个传奇的开端,一个不灭的神话,这不仅仅是一部电影,而是作为一个走进新时代的标签,永远彪炳史册。Would you rate the previous review as positive, neutral or negative? positive

Multilingual question answering (English to Chinese) Explain to me in Traditional Chinese what is the backpropagation 梯度反向传播算法是一种机器学习算法,用于训练人工神经网络。该算法使用梯度下降来最小化损失函数。

Code generation Write a binary search tree with O(log(n)) computational complexity. {code_start} def binary_search(a, x): low = 0 high = len(a) – 1 while low x: high = mid – 1 else: return mid return -1′ … else: return mid return low' Output is truncated

Paragraph rephrase Amazon Web Services (AWS) has announced nine major new updates for its cloud-based machine learning platform, SageMaker. SageMaker aims to provide a machine learning service which can be used to build, train, and deploy ML models for virtually any use case. During this year's re:Invent conference, AWS made several announcements to further improve SageMaker's capabilities.\n\nHow would you rephrase that briefly using English?,Amazon Web Services (AWS) has announced nine major new updates for its cloud-based machine learning platform, SageMaker. SageMaker aims to provide a machine learning service which can be used to build, train, and deploy ML models for virtually any use case. During this year's re:Invent conference, AWS made several announcements to further improve SageMaker's capabilities. \nThe above sentence is very complicated. Please provide me a simplified synonymous version consisting of multiple sentences: Amazon SageMaker gets a major update Amazon Web Services (AWS) has announced nine major new updates for its cloud-based machine learning platform, SageMaker. SageMaker aims to provide a machine learning service that can be used to build, train and deploy ML models for virtually any use case

Summarization Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\n\n===\nWrite a summary of the previous text in English: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.

Common sense reasoning and natural language inference Given that The world cup has kicked off in Los Angeles, United States. Does it follow that The world cup takes place in United States. Yes or no?, The world cup has kicked off in Los Angeles, United States. Using only the above description and what you know about the world, is \"The world cup takes place in United States\" definitely correct? Yes or no?,

Question answering question: \what are the key features of new Kindle?\\n\nContext: \The newest and most innovative Kindle yet lets you take notes on millions of books and documents, write lists and journals, and more. For readers who have always wished they could write in their eBooks, Amazon's new Kindle lets them do just that. The Kindle Scribe is the first Kindle for reading and writing and allows users to supplement their books and documents with notes, lists, and more. Here's everything you need to know about the Kindle Scribe, including frequently asked questions. The Kindle Scribe makes it easy to read and write like you would on paper The Kindle Scribe features a 10.2-inch, glare-free screen (the largest of all Kindle devices), crisp 300 ppi resolution, and 35 LED front lights that automatically adjust to your environment. Further personalize your experience with the adjustable warm light, font sizes, line spacing, and more. It comes with your choice of the Basic Pen or the Premium Pen, which you use to write on the screen like you would on paper. They also attach magnetically to your Kindle and never need to be charged. The Premium Pen includes a dedicated eraser and a customizable shortcut button. The Kindle Scribe has the most storage options of all Kindle devices: choose from 8 GB, 16 GB, or 32 GB to suit your level of reading and writing. \\n\nAnswer take notes on millions of books and documents, write lists and journals, and more

Sentence and sentiment classification Review:\nThis moive is so great and once again dazzles and delights us\nIs this movie review sentence negative or positive?\nOPTIONS:\n-positive \n-negative Positive

Imaginary article generation based on title Title: \"University has new facility coming up\"\\nGiven the above title of an imaginary article, imagine the article.\\n""" The £1.2m project will see the creation of a new teaching and learning centre at the University of the Highlands and Islands (UHI) in Inverness. It will be built on the site of the former Inverness College of Commerce. UHI said the new facility would be used for teaching and learning, as well as student support and administration. The university said it would also be used for conferences and events. The new building will be built on the site of the former Inverness College of Commerce The university said the new building would be built on the site of the former Inverness College of Commerce, which closed in 2013. It said the new facility would be used for teaching and learning, as well as student support and administration. It would also be used for conferences and events. UHI said the new facility would be built on the site of the former Inverness College of Commerce The university said the new building would be built on the site of the former Inverness College of Commerce, which closed in 2013. It said the new facility would be used for teaching and learning, as well as student support and administration. … It would also be used for conferences and events. UHI said the new facility would be built on the site of the former Inverness…

Summarizing a title based on the article \'The newest and most innovative Kindle yet lets you take notes on millions of books and documents, write lists and journals, and more. For readers who have always wished they could write in their eBooks, Amazon's new Kindle lets them do just that. The Kindle Scribe is the first Kindle for reading and writing and allows users to supplement their books and documents with notes, lists, and more. Here's everything you need to know about the Kindle Scribe, including frequently asked questions. The Kindle Scribe makes it easy to read and write like you would on paper The Kindle Scribe features a 10.2-inch, glare-free screen (the largest of all Kindle devices), crisp 300 ppi resolution, and 35 LED front lights that automatically adjust to your environment. Further personalize your experience with the adjustable warm light, font sizes, line spacing, and more. It comes with your choice of the Basic Pen or the Premium Pen, which you use to write on the screen like you would on paper. They also attach magnetically to your Kindle and never need to be charged. The Premium Pen includes a dedicated eraser and a customizable shortcut button. The Kindle Scribe has the most storage options of all Kindle devices: choose from 8 GB, 16 GB, or 32 GB to suit your level of reading and writing. \n\n \\n\\nGive me a good title for the article above. Amazon's Kindle Scribe: Everything you need to know

Summarization [Original]: Amazon scientists, in collaboration with researchers from the University of Sheffield, are making a large-scale fact extraction and verification dataset publicly available for the first time. The dataset, comprising more than 185,000 evidence-backed claims, is being made available to hopefully catalyze research and development that addresses the problems of fact extraction and verification in software applications or cloud-based services that perform automatic information extraction. [Summary]: Amazon and University researchers make fact extraction and verification dataset publicly available. ### [Original]: Prime members in the U.S. can get even more delivered to their door with a Prime membership. Members can now enjoy one year of Grubhub+ valued at $9.99 per month for free—at no added cost to their Prime membership. To activate this deal, visit amazon.com/grubhub. This new offer includes unlimited, $0 food delivery fees on orders over $12 as well as exclusive perks for Grubhub+ members and rewards like free food and order discounts. Plus, diners can "eat good while doing good" by opting into Grubhub's Donate the Change program, a donation-matching initiative that raised more than $25 million in 2021 alone, benefiting more than 20 charitable organizations across the country. [Summary]: Prime members in the U.S. can enjoy one year of Grubhub+ for free, with no food-delivery fees on eligible orders. ### [Original]: Amazon scientists, in collaboration with researchers from the University of Sheffield, are making a large-scale fact extraction and verification dataset publicly available for the first time. The dataset, comprising more than 185,000 evidence-backed claims, is being made available to hopefully catalyze research and development that addresses the problems of fact extraction and verification in software applications or cloud-based services that perform automatic information extraction. [Summary]: [Summary]: Amazon and University researchers make fact extraction and verification dataset publicly available.

Code generation description: a orange button that says stop code: Stop ### description: a blue box that contains yellow circles with red borders code: ### description: a Headline saying Welcome to AI code: code: Welcome to AI '

Name entity recognition [Text]: Fred is a serial entrepreneur. Co-founder and CEO of Platform.sh, he previously co-founded Commerce Guys, a leading Drupal ecommerce provider. His mission is to guarantee that as we continue on an ambitious journey to profoundly transform how cloud computing is used and perceived, we keep our feet well on the ground continuing the rapid growth we have enjoyed up until now. [Name]: Fred [Position]: Co-founder and CEO [Company]: Platform.sh ### [Text]: Microsoft (the word being a portmanteau of "microcomputer software") was founded by Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. Steve Ballmer replaced Gates as CEO in 2000, and later envisioned a "devices and services" strategy. [Name]: Steve Ballmer [Position]: CEO [Company]: Microsoft ### [Text]: Franck Riboud was born on 7 November 1955 in Lyon. He is the son of Antoine Riboud, the previous CEO, who transformed the former European glassmaker BSN Group into a leading player in the food industry. He is the CEO at Danone. [Name]: Franck Riboud [Position]: CEO [Company]: Danone ### [Text]: David Melvin is an investment and financial services professional at CITIC CLSA with over 30 years' experience in investment banking and private equity. He is currently a Senior Adviser of CITIC CLSA. [Name]: Fred [Position]: Co-founder and CEO [Company]: Platform.sh

Question answering Context: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production. Question: When was NLP Cloud founded? Answer: 2021 ### Context: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then. Question: What did NLP Cloud develop? Answer: API ### Context: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice. Question: When can plans be stopped? Answer: Anytime ### Context: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended. Question: Which plan is recommended for GPT-J? Answer:""" Answer: GPU plan'

Grammar and spelling correction I love goin to the beach. Correction: I love going to the beach. ### Let me hav it! Correction: Let me have it! ### It have too many drawbacks. Correction: It has too many drawbacks. ### I do not wan to go Correction: Correction: I don't want to go

Product description and generalization Generate a product description out of keywords. Keywords: shoes, women, $59 Sentence: Beautiful shoes for women at the price of $59. ### Keywords: trousers, men, $69 Sentence: Modern trousers for men, for $69 only. ### Keywords: gloves, winter, $19 Sentence: Amazingly hot gloves for cold winters, at $19. ### Keywords: t-shirt, men, $39 Sentence: Sentence: Cool t-shirt for men, for $39 only.

Sentence and sentiment classification Message: When the spaceship landed on Mars, the whole humanity was excited Topic: space ### Message: I love playing tennis and golf. I'm practicing twice a week. Topic: sport ### Message: Managing a team of sales people is a tough but rewarding job. Topic: business ### Message: I am trying to cook chicken with tomatoes. Topic: Topic: food

Chatbot and conversational AI This is a discussion between a [human] and a [robot]. The [robot] is very nice and empathetic. [human]: Hello nice to meet you. [robot]: Nice to meet you too. ### [human]: How is it going today? [robot]: Not so bad, thank you! How about you? ### [human]: I am ok, but I am a bit sad… [robot]: Oh? Why that? ### [human]: I caught flu [robot]: [robot]: Oh no, I am sorry to hear that!

Tweet generation keyword: markets tweet: Take feedback from nature and markets, not from people ### keyword: children tweet: Maybe we die so we can come back as children. ### keyword: startups tweet: Startups should not worry about how to put out fires, they should worry about how to start them. ### keyword: nlp tweet: tweet: NLP is a tool, not a goal.

Machine translation Hugging Face a révolutionné le NLP. Translation: Hugging Face revolutionized NLP. ### Cela est incroyable! Translation: This is unbelievable! ### Désolé je ne peux pas. Translation: Sorry but I cannot. ### NLP Cloud permet de deployer le NLP en production facilement. Translation: Translation: NLP Cloud makes it easy to deploy NLP in production.

Intent classification I want to start coding tomorrow because it seems to be so fun! Intent: start coding ### Show me the last pictures you have please. Intent: show pictures ### Search all these files as fast as possible. Intent: search files ### Can you please teach me Chinese next week? Intent: Intent: teach me chinese

Access the BloomZ 176B instruction-tuned model in SageMaker SageMaker JumpStart provides two ways to get started using these instruction-tuned Bloom models: Amazon SageMaker Studio and the SageMaker SDK. The following sections illustrate what each of these options look like and how to access them. Access the model with the simplified SageMaker JumpStart SDK The simplified SageMaker JumpStart SDK facilitates training and deploying built-in SageMaker JumpStart models with a couple lines of code. This gives you access to the entire library of SageMaker JumpStart models, including the latest foundation models and image generation models, without having to supply any inputs besides the model ID. You can take advantage of the model-specific default values we provide to specify the configuration, such as the Docker image, ML instance type, model artifact location, and hyperparameters, among other fields. These attributes are only default values; you can override them and retain granular control over the AWS models you create. As a result of these changes, the effort to write Python workflows to deploy and train SageMaker JumpStart models has been reduced, enabling you to spend more time on the tasks that matter. This feature is available in all Regions where JumpStart is supported, and can be accessed with the SageMaker Python SDK version 2.154.0 or later. You can programmatically deploy an endpoint through the SageMaker SDK. You will need to specify the model ID of your desired model in the SageMaker model hub and the instance type used for deployment. The model URI, which contains the inference script, and the URI of the Docker container are obtained through the SageMaker SDK. These URIs are provided by SageMaker JumpStart and can be used to initialize a SageMaker model object for deployment. Deploy the model and query the endpoint This notebook requires ipywidgets. Install ipywidgets and then use the execution role associated with the current notebook as the AWS account role with SageMaker access. Choose the pre-trained model We choose the bloomz-176b-fp16 pre-trained model:

The notebook in the following sections uses BloomZ 176B as an example. For a complete list of SageMaker pre-trained models, refer to Built-in Algorithms with pre-trained Model Table. Retrieve artifacts and deploy an endpoint With SageMaker, we can perform inference on the pre-trained model without fine-tuning it first on a new dataset. We start by retrieving the deploy_image_uri, deploy_source_uri, and model_uri for the pre-trained model. To host the pre-trained model, we create an instance of sagemaker.model.Model and deploy it. This may take a few minutes. Now we can deploy the model using the simplified SageMaker JumpStart SDK with the following lines of code:

#if no instance for this model id is available, use a smaller id model = JumpStartModel(model_id=model_id)

# ml.p4de.24xlarge is used by default. You can add the kwarg # instance_type to change this setting. predictor = model.deploy()

We use SageMaker large model inference (LMI) containers to host the BloomZ 176B model. LMI is an AWS-built LLM software stack (container) that offers easy-to-use functions and performance gain on generative AI models. It's embedded with model parallelism, compilation, quantization, and other stacks to speed up inference. For details, refer to Deploy BLOOM-176B and OPT-30B on Amazon SageMaker with large model inference Deep Learning Containers and DeepSpeed. Note that deploying this model requires a p4de.24xlarge instance and the deployment usually takes about 1 hour. If you don't have quota for that instance, request a quota increate on the AWS Service Quotas console. Query the endpoint and parse the response using various parameters to control the generated text The input to the endpoint is any string of text formatted as JSON and encoded in utf-8 format. The output of the endpoint is a JSON file with generated text. In the following example, we provide some sample input text. You can input any text and the model predicts the next words in the sequence. Longer sequences of text can be generated by calling the model repeatedly. The following code shows how to invoke an endpoint with these arguments:

predictor = retrieve_default(model_id=model_id, model_version="*", endpoint_name=endpoint=name) response = predictor.predict("How to make a pasta?") print(response["generated_text"])Python

We get the following output: ['How to make a pasta? boil a pot of water and add salt. Add the pasta to the water and cook until al dente. Drain the pasta.'] Access the model in SageMaker Studio You can also access these models through the JumpStart landing page in Studio. This page lists available end-to-end ML solutions, pre-trained models, and example notebooks. At the time of publishing the post, BloomZ 176B is only available in the us-east-2 Region.

You can choose the BloomZ 176B model card to view the notebook.

You can then import the notebook to run the notebook further.

Clean up To avoid ongoing charges, delete the SageMaker inference endpoints. You can delete the endpoints via the SageMaker console or from the SageMaker Studio notebook using the following commands: predictor.delete_model() predictor.delete_endpoint() Conclusion In this post, we gave an overview of the benefits of zero-shot and few-shot learning and described how prompt engineering can improve the performance of instruction-tuned models. We also showed how to easily deploy an instruction-tuned BloomZ 176B model from SageMaker JumpStart and provided examples to demonstrate how you can perform different NLP tasks using the deployed BloomZ 176B model endpoint in SageMaker. We encourage you to deploy a BloomZ 176B model from SageMaker JumpStart and create your own prompts for NLP use cases. To learn more about SageMaker JumpStart, check out the following:

Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart Run text generation with Bloom and GPT models on Amazon SageMaker JumpStart Generate images from text with the stable diffusion model on Amazon SageMaker JumpStart Run image segmentation with Amazon SageMaker JumpStart Run text classification with Amazon SageMaker JumpStart using TensorFlow Hub and Hugging Face models Amazon SageMaker JumpStart models and algorithms now available via API Incremental training with Amazon SageMaker JumpStart Transfer learning for TensorFlow object detection models in Amazon SageMaker Transfer learning for TensorFlow text classification models in Amazon SageMaker Transfer learning for TensorFlow image classification models in Amazon SageMaker

About the Authors Rajakumar Sampathkumar is a Principal Technical Account Manager at AWS, providing customers guidance on business-technology alignment and supporting the reinvention of their cloud operation models and processes. He is passionate about cloud and machine learning. Raj is also a machine learning specialist and works with AWS customers to design, deploy, and manage their AWS workloads and architectures. Dr. Xin Huang is an Applied Scientist for Amazon SageMaker JumpStart and Amazon SageMaker built-in algorithms. He focuses on developing scalable machine learning algorithms. His research interests are in the area of natural language processing, explainable deep learning on tabular data, and robust analysis of non-parametric space-time clustering. He has published many papers in ACL, ICDM, KDD conferences, and Royal Statistical Society: Series A journal. Evan Kravitz is a software engineer at Amazon Web Services, working on SageMaker JumpStart. He enjoys cooking and going on runs in New York City.

Deploy multi-Region Amazon RDS for SQL Server using cross-Region read replicas with a disaster recovery blueprint – Part 1

Disaster recovery and high availability planning play a critical role in ensuring the resilience and continuity of business operations. When considering disaster recovery strategies on AWS, there are two primary options: in-Region disaster recovery and cross-Region disaster recovery. The choice between in-Region and cross-Region disaster recovery depends on various factors, including the criticality of the applications and data, regulatory requirements, geographic distribution of users, cost and complexity. To implement a multi-Region disaster recovery solution on AWS, you must first identify the critical components of your infrastructure and determine the required Recovery Point Objective (RPO) and Recovery Time Objective (RTO) for each component. The RPO is the maximum amount of data loss that is acceptable, and the RTO is the maximum amount of time that can elapse before the system must be restored. As of July 2023, AWS Cloud spans across 99 Availability Zones within 31 geographic Regions around the world. AWS has also announced plans for 15 more Availability Zones and 5 more AWS Regions in Canada, Israel, Malaysia, New Zealand, and Thailand. Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud. You can select from popular engines like MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server. In this series, we are focusing on the needs of critical applications that require the best possible availability and disaster recovery across different AWS Regions. In this first post, we guide you through the process of establishing a failover strategy for applications utilizing Amazon RDS for SQL Server, Amazon Route53 and Amazon Simple Storage Service(Amazon S3) across two AWS Regions. In the next post, we will demonstrate how to actually perform a failover using this plan. Solution overview This solution is deployed in two AWS Regions with active-passive strategy where primary (active) Region host the workload and serve the traffic. The secondary (passive) Region is used for the disaster recovery. Amazon Route53 public hosted zone with failover policy is created to route internet traffic between primary and secondary Regions. Amazon Route 53 private hosted zone CNAME records to store RDS for SQL Server endpoints. The application connects to the database using these records. The following diagram illustrates the key components of this architecture.

In our example, the primary AWS Region is referred as us-east-1 and secondary AWS Region as us-east-2. In addition, note the following:

The Amazon RDS for SQL Server Multi-AZ DB instance is deployed in the primary AWS Region and the cross-Region read replica is in the secondary Region. Amazon RDS for SQL Server configures cross-Region read replicas (asynchronous replication) using Distributed Availability Groups. Application stack deployed in both Regions and maintain the same release version. Amazon Route53 public hosted zone that serve internet traffic is configured with 'failover' routing policies. Amazon Route53 private hosted zone configured with 'simple' routing policies for application to RDS for SQL Server instance connectivity.

Standby takes over primary Amazon Route 53 public hosted zone failover policies are implemented using 'standby takes over primary' approach. In this approach, Route53 health check for primary Region verifies accessibility of a HTTP endpoint. That endpoint belongs to an Amazon S3 file that is stored in secondary Region. For example, if you have an Amazon S3 bucket called examplebucket123 in AWS Region us-east-2 and file name is initiate_failover.dr, the corresponding endpoint to access this S3 file would be: This health check is deemed healthy when the HTTP response received from the designated endpoint returns a status code of 4xx or 5xx (That means, the Route 53 health check agents NOT able to resolve that Amazon S3 file endpoint). Conversely, if the response returns a status code of 2xx or 3xx, the health check fails (That means, the Route 53 health check agents were able resolve the Amazon S3 file endpoint). This method is called Route53 Invert health check. This method not only allows you to manually control failover between primary and secondary AWS Regions but also prevent accidental failovers if the resource in the standby Region fails. In event of disaster recovery, upload this file on S3 bucket in secondary Region. This will intentionally cause the Route53 health check for primary Region to fail. Refer this document for list of HTTP status codes. For a comprehensive understanding of different approaches of designing reliable failover mechanisms, refer Creating Disaster Recovery Mechanisms Using Amazon Route 53. It is important to note that automatic failover of internet traffic to the secondary Region is not recommended. In situations like a brief network glitch in primary Region, automatic failover can result into longer downtime. Prerequisites To implement this solution, you need the following:

An application stack and Amazon RDS for SQL Server multi-AZ instance in the primary Region. An application stack and Amazon RDS for SQL Server read replica instance in secondary Region(cross-replication). For instructions, refer Use cross-Region read replicas with Amazon Relational Database Service for SQL Server. Any server level objects (for example logins, SQL agent jobs and more) that are created in the primary DB instance after the creation of the read replica are not automatically replicated, and you must create them manually in the read replica. Hence make sure these objects are in sync between primary and secondary Regions. Refer to the Amazon RDS documentation for more details.

Navigate to Amazon Relational Database service (RDS) console in primary Region. Capture endpoint of the primary RDS for SQL Server instance.

Similarly, navigate to Amazon RDS Console in secondary Region. Capture endpoint of RDS for SQL Server cross-Region read replica. Navigate to Amazon Route53 console. Create a new private hosted zone.

Associate the VPCs for both Regions to the private hosted zone. This is necessary because private hosted zone routes traffic within an Amazon VPC.

Once private hosted zone is created, add two CNAME records with simple routing policy. In our example, we created below CNAME records:

rdsprimarydb.rds_sqlserver_private_hosted_zone – Connects to RDS for SQL Server in primary Region. rdssecondarydb.rds_sqlserver_private_hosted_zone – Connects to RDS for SQL Server cross-Region read replica.

After adding both CNAME records, private hosted zone should look like following screenshot.

Update your application configuration in primary Region and use CNAME record rdsprimarydb.rds_sqlserver_private_hosted_zone as the database host name in database connection string. Similarly update application in secondary Region using CNAME record rdssecondarydb.rds_sqlserver_private_hosted_zone. This step ensure that applications are not directly using RDS endpoints. Hence during failover application changes would not require. The following python code is an example code connecting to RDS for SQL Server instance using private hosted zone CNAME record:

Create new Amazon S3 buckets in both primary and secondary Regions. These S3 buckets are dedicated to host disaster recovery file. To ensure the security and integrity of the health check process, it is crucial to restrict access to this bucket exclusively for authorized personnel. You may leave these buckets empty at this point when both primary and secondary Regions are healthy. Navigate to Amazon Route53 console and create Route53 public hosted zone to manage internet traffic for your public domain.

After creating public hosted zone, navigate to Amazon Route53 health checks console. Create two new Route53 health checks one for primary Region failover and another is for secondary Region failover. These health checks will be attached to primary and secondary Region failover records.

Enter health check name for primary Region. Then select Endpoint option for what to monitor. In monitor and endpoint section, select Domain name and protocol HTTPS. Under domain name, specify Amazon S3 bucket endpoint of secondary Region. For example: examplebucket123.s3.us-east-2.amazonaws.com Under Path, specify the disaster recovery file name. For example: initiate_failover.dr Specify failover threshold and request interval in advanced configuration section. Check option for 'Invert health check status' and hit next. Submit create health check.

Repeat the same steps to create health check record for secondary Region. For the primary Region health check, specify the S3 file endpoint from the secondary Region and vice versa. In the event that the primary Region becomes inaccessible, you will be able to modify the S3 file in the secondary Region and initiate the failover. After health checks are created for primary and secondary Regions, Amazon Route53 will initiate the health check and report the status.

Navigate to the Route53 public hosted zone created at step 13 and complete the following steps:

Create record with failover routing policy. Click on Define failover record. Associate this record with application target. For example, Route53 support Application load balancer, API gateway, VPC endpoint or S3 endpoints and more. In this example, application load balance target is selected. Choose primary Region. Choose the target load balancer. Specify failover record type as Primary. Choose health check record created for primary region. Disable evaluate target health option. Submit

Repeat the similar steps to create another record for secondary record type.

Once public hosted zone configuration complete, it should look like below screenshot:

This completes deployment of disaster recovery blueprint with Amazon Route53, Amazon S3 and Amazon RDS for SQL Server. At this point, you should be able to access your application through internet traffic.

Clean up In the next post of this series, we are going to use this blueprint and show you how to perform cross-Region failover. Hence if you are planning to continue, preserve all resources created under this deployment. To delete the resources created to implement this solution, complete the following steps:

Delete public and private hosted zone you created. Change the application configuration to its original state. Delete the Amazon S3 bucket you created.

Summary In this post, we provided guidance on how to implement a cross-Region disaster recovery blueprint. The 'standby takes over primary' approach in Amazon Route53 public hosted zone policies empowers organizations to maintain control over the failover process and manually initiate failover when the primary Region becomes inaccessible. In the next post of this series, we will break down the process of promoting RDS for SQL Server in the AWS secondary Region and performing a cross-Region failover using the blueprint we have deployed. If you have any comments or feedback, leave them in the comments section.

About the author Ravi Mathur is a Sr. Solutions Architect at AWS. He works with customers providing technical assistance and architectural guidance on various AWS services. He brings several years of experience in software engineering and architecture roles for various large-scale enterprises.

Deploy multi-Region Amazon RDS for SQL Server using cross-Region read replicas with a disaster recovery blueprint – Part 2

In our previous post, we deployed multi-Region disaster recovery blueprint using Amazon Route 53, Amazon Relational Database Service (Amazon RDS) for SQL Server and Amazon Simple Storage Service (Amazon S3). In this post we walk you through the process of promoting RDS for SQL Server in the AWS secondary Region and performing a cross-Region failover using the blueprint we have deployed. A quick recap from our previous post At high level we performed the following steps:

Verify Amazon RDS for SQL Server database running with cross Region read replica. Created Amazon Route 53 private hosted zone CNAME records to configure application connectivity with RDS instance. Configured Amazon Route53 public hosted zone to manage internet traffic between primary and secondary AWS Regions. Created separate Amazon S3 buckets in both the Regions to host disaster recovery file. Tested connectivity for internet traffic using public domain name.

The architecture of this solution is depicted in the following diagram. When both primary and secondary Regions are up and running, Amazon Route53 routing internet traffic to active Region (us-east-1 in this example). No internet traffic routed to passive Region (us-east-2 in this example).

Key Considerations for Cross-Region Failover: Initiating cross-Region failover requires a thorough consideration of various factors. Here are a few essential points to consider

One or more AWS services that your application relies on in the primary Region are not responding. The application tier deployed in the secondary Region is fully updated and prepared to assume the primary role. Amazon Route 53 Application Recovery Controller provides capability of multi-Region readiness checks. Refer to Readiness check in Amazon Route 53 Application Recovery Controller for more details. The only option to restore customer services is by switching live traffic to the secondary Region. Replica lag for RDS for SQL Server read replica in secondary Region is under acceptable range for the business (RPO). Initiating a cross-Region failover with non-zero replica lag can potentially lead to data loss. Refer Troubleshooting a SQL Server read replica problem to learn more.

The following diagram illustrates the architecture during disaster recovery event:

Cross-Region Failover Steps: Let's see how this solution works by following the order of events

Declare a disaster recovery event and disable writes on primary RDS for SQL Server instance. Promote the RDS for SQL Server cross-Region read replica to a stand-alone DB instance in secondary Region. After the successful promotion of the RDS for SQL Server read replica, conduct an end-to-end validation, often referred to as a smoke test. This validation process ensures that the application is functioning correctly and ready to accept external traffic in the secondary AWS Region. Create and upload disaster recovery file on Amazon S3 bucket deployed with the disaster recovery blueprint. Use the same file name specified in HTTP endpoint of Route53 public record health check. Uploading this file will fail the Route 53 health checks for the primary Region and the failover to the secondary Region will initiate. Once the failover to the secondary Region is complete, verify the health of the application stack and databases and ensure they are functioning properly and receiving external traffic. At this step, your application downtime is complete. Enable in-Region high availability again by modifying the RDS instance in secondary Region and enable multi-AZ option. This is also a prerequisite of creating cross-Region read replica on RDS for SQL Server instance. Once the AWS Region primary Region become available, recreate cross-Region disaster recovery solution by manually creating new cross-Region RDS for SQL Server read replica. Modify the Route53 private hosted zone rds_sqlserver_private_hosted_zone and update rdsprimarydb.rds_sqlserver_private_hosted_zone CNAME record to point to newly created read replica in primary Region.

Implement the failover Use the following steps to initiate cross Region failover:

Declare disaster recovery event and disable writes on RDS for SQL Server instance in primary Region. For example, if application is accessible, stop it to prevent accidental writes on current primary database while the DR failover in-progress. In secondary Region, verify RDS for SQL Server read replica lag using Replica Lag Amazon CloudWatch metric. You may also run this query on primary database to get information about replica lag for all read replicas. Proceed to the next steps only if cross-Region replica lag is zero or under acceptable range for the business (RPO). Initiating cross-Region failover with non-zero replica lag can potentially lead to data loss. Navigate to Amazon Route 53 console. Select public hosted zone and verify the routing policies.

Navigate to Amazon Route 53 health checks dashboard. Note down URL for the S3 file endpoint for primary Region health check record. At this stage, health checks for both the Regions must have healthy status.

Navigate to private hosted zone rds_sqlserver_private_hosted_zone and verify the CNAME entries. Application stack in secondary Region connected to RDS instance using CNAME record rdssecondarydb.rds_sqlserver_private_hosted_zone.

In secondary Region, promote the cross-Region RDS for SQL Server read replica

You may also automate this step by creating an AWS Lambda function. You may find referenced python code on this GitHub repo. For instructions on creating a function, see the AWS Lambda Developer Guide. Promoting the RDS for SQL Server instance doesn't change RDS endpoint URL. So, you don't have to update Route53 private hosted zone CNAME records. Your application should be able to connect with promoted RDS for SQL Server instance using rdssecondarydb.rds_sqlserver_private_hosted_zone record. Perform an internal smoke test or application health checks and ensures that the application is functioning correctly and ready to accept internet traffic in secondary Region. Once application stack in secondary Region is ready to take internet traffic, initiate the failover process. Refer step 6 to obtain recovery file URL. Upload this recovery file on Amazon S3 bucket in secondary Region. In our example, file name is initiate_failover.dr.

Uploading this file on Amazon S3 bucket causes Route53 health check for primary Region to fail (Remember we enabled invert health check status option in Route 53 health checks). Once primary Region is marked unhealthy, Route53 initiates the failover of internet traffic to secondary Region. Failover delays are depended on health check settings, request interval and failure threshold.

Once failover is complete, your application start receiving internet traffic in secondary AWS Region.

At this step, your applications are recovered from the downtime but running in single AWS availability zone. To setup in-Region high availability, modify the RDS instance in the secondary Region and enable multi-AZ for Amazon RDS for SQL Server. Once the AWS primary Region become available again, recreate cross-Region disaster recovery solution by manually creating new cross-Region RDS for SQL Server read replica. Update Route 53 private hosted zone rds_sqlserver_private_hosted_zone and edit CNAME record value rdsprimarydb.rds_sqlserver_private_hosted_zone with new cross-Region RDS read replica endpoint.

Clean up To delete the resources created to implement this solution, complete the following steps:

Summary In this post, we explore the failover process for an application deployed in a multi-Region setup, employing an active/passive strategy. Amazon Route 53 public hosted zone policies failover traffic between the primary and the secondary Regions. Amazon Route53 private hosted zones policies take care of routing traffic to the appropriate database endpoints. It gives a uniform common endpoint for applications to use and application configuration doesn't need to change during the failures. You may use AWS Lambda function to script the manual tasks required for RDS instance promotion. You can manually trigger the function or use events. Try out this solution in your AWS account and if you have any comments or questions, leave them in the comments section.

AWS Weekly Roundup – Amazon MWAA, EMR Studio, Generative AI, and More – August 14, 2023

While I enjoyed a few days off in California to get a dose of vitamin sea, a lot has happened in the AWS universe. Let's take a look together! Last Week's Launches Here are some launches that got my attention: Amazon MWAA now supports Apache Airflow version 2.6 – Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is a managed orchestration service for Apache Airflow that you can use to set up and operate end-to-end data pipelines in the cloud. Apache Airflow version 2.6 introduces important security updates and bug fixes that enhance the security and reliability of your workflows. If you're currently running Apache Airflow version 2.x, you can now seamlessly upgrade to version 2.6.3. Check out this AWS Big Data Blog post to learn more. Amazon EMR Studio adds support for AWS Lake Formation fine-grained access control – Amazon EMR Studio is a web-based integrated development environment (IDE) for fully managed Jupyter notebooks that run on Amazon EMR clusters. When you connect to EMR clusters from EMR Studio workspaces, you can now choose the AWS Identity and Access Management (IAM) role that you want to connect with. Apache Spark interactive notebooks will access only the data and resources permitted by policies attached to this runtime IAM role. When data is accessed from data lakes managed with AWS Lake Formation, you can enforce table and column-level access using policies attached to this runtime role. For more details, have a look at the Amazon EMR documentation. AWS Security Hub launches 12 new security controls – AWS Security Hub is a cloud security posture management (CSPM) service that performs security best practice checks, aggregates alerts, and enables automated remediation. With the newly released controls, Security Hub now supports three additional AWS services: Amazon Athena, Amazon DocumentDB (with MongoDB compatibility), and Amazon Neptune. Security Hub has also added an additional control against Amazon Relational Database Service (Amazon RDS). AWS Security Hub now offers 276 controls. You can find more information in the AWS Security Hub documentation. Additional AWS services available in the AWS Israel (Tel Aviv) Region – The AWS Israel (Tel Aviv) Region opened on August 1, 2023. This past week, AWS Service Catalog, Amazon SageMaker, Amazon EFS, and Amazon Kinesis Data Analytics were added to the list of available services in the Israel (Tel Aviv) Region. Check the AWS Regional Services List for the most up-to-date availability information. For a full list of AWS announcements, be sure to keep an eye on the What's New at AWS page. Other AWS News Here are some additional blog posts and news items that you might find interesting: AWS recognized as a Leader in 2023 Gartner Magic Quadrant for Contact Center as a Service with Amazon Connect – AWS was named a Leader for the first time since Amazon Connect, our flexible, AI-powered cloud contact center, was launched in 2017. Read the full story here. Generate creative advertising using generative AI – This AWS Machine Learning Blog post shows how to generate captivating and innovative advertisements at scale using generative AI. It discusses the technique of inpainting and how to seamlessly create image backgrounds, visually stunning and engaging content, and reducing unwanted image artifacts. AWS open-source news and updates – My colleague Ricardo writes this weekly open-source newsletter in which he highlights new open-source projects, tools, and demos from the AWS Community. Upcoming AWS Events Check your calendars and sign up for these AWS events: Build On Generative AI – Your favorite weekly Twitch show about all things generative AI is back for season 2 today! Every Monday, 9:00 US PT, my colleagues Emily and Darko look at new technical and scientific patterns on AWS, inviting guest speakers to demo their work and show us how they built something new to improve the state of generative AI. In today's episode, Emily and Darko discussed the latest models LlaMa-2 and Falcon, and explored them in retrieval-augmented generation design patterns. You can watch the video here. Check out show notes and the full list of episodes on community.aws. AWS NLP Conference 2023 – Join this in-person event on September 13–14 in London to hear about the latest trends, ground-breaking research, and innovative applications that leverage natural language processing (NLP) capabilities on AWS. This year, the conference will primarily focus on large language models (LLMs), as they form the backbone of many generative AI applications and use cases. Register here. AWS Global Summits – The 2023 AWS Summits season is almost coming to an end with the last two in-person events in Mexico City (August 30) and Johannesburg (September 26). AWS Community Days – Join a community-led conference run by AWS user group leaders in your region: West Africa (August 19), Taiwan (August 26), Aotearoa (September 6), Lebanon (September 9), and Munich (September 14). AWS re:Invent (November 27 – December 1) – Join us to hear the latest from AWS, learn from experts, and connect with the global cloud community. Registration is now open. You can browse all upcoming in-person and virtual events. That's all for this week. Check back next Monday for another Weekly Roundup! — Antje P.S. We're focused on improving our content to provide a better customer experience, and we need your feedback to do so. Take this quick survey to share insights on your experience with the AWS Blog. Note that this survey is hosted by an external company, so the link doesn't lead to our website. AWS handles your information as described in the AWS Privacy Notice.

Antje Barth Antje Barth is a Principal Developer Advocate for generative AI at AWS. She is co-author of the O'Reilly books Generative AI on AWS and Data Science on AWS. Antje frequently speaks at AI/ML conferences, events, and meetups around the world. She also co-founded the Düsseldorf chapter of Women in Big Data.

Data consolidation for analytical applications using logical replication for Amazon RDS Multi-AZ clusters

Amazon Relational Database Service (Amazon RDS) Multi-AZ deployments provide enhanced availability and durability for your RDS database instances. You can deploy highly available, durable PostgreSQL databases in three Availability Zones using Amazon RDS Multi-AZ DB cluster deployments with two readable standby DB instances. With a Multi-AZ DB cluster, applications gain automatic failovers in typically under 35 seconds, up to two times faster transaction commit latency compared to Amazon Multi-AZ DB instance deployments without compromising durability, and additional read capacity. In this post, we walk through a use case of data consolidation for analytical applications using logical replication in the context of RDS Multi-AZ DB clusters. We enable and utilize logical replication in an RDS for PostgreSQL Multi-AZ DB cluster and also demonstrate logical replication consumption recovery after a DB instance failover. AWS announced support for PostgreSQL logical replication with Amazon RDS Multi-AZ DB cluster deployments. Amazon RDS for PostgreSQL supports streaming data changes using PostgreSQL's logical replication. You can now set up logical replication to send data from your Amazon RDS for PostgreSQL Multi-AZ DB cluster to other data systems, or receive changes from other systems into your Amazon RDS for PostgreSQL Multi-AZ DB cluster. Logical replication is supported for RDS Multi-AZ DB clusters running Amazon RDS for PostgreSQL version 14.8-R2 and higher and 15.3-R2 and higher. Logical Replication Overview PostgreSQL offers two types of replication – physical replication and logical replication. Both methods serve the purpose of creating redundant copies of data to ensure high availability and fault tolerance. Physical replication is also known as streaming replication. In this method, the actual data files (segments) and changes made to them are replicated from the primary database server to one or more standby servers. The replication process occurs at the binary level, meaning the entire data blocks are copied, which makes it efficient for handling large databases with high write loads. However, even small changes to a large block will result in replicating the whole block. In the replication topology, the primary and standby servers must be of the same PostgreSQL major version for compatibility and cannot be used for partial replication or selective replication of specific tables or data subsets. Logical replication, on the other hand, works at a higher level of abstraction. It replicates changes by understanding the logical structure of the data, which allows for more flexibility in terms of what is replicated and how it is consumed on the standby server. Logical replication gives you fine-grained control over both data replication and security. You can use logical replication between different platforms (for example, Linux to Windows) and different major versions of PostgreSQL, providing more flexibility during upgrades. You can also handle schema changes, such as table and column modifications, without much difficulty. Additionally, you can replicate individual rows or even specific columns, making it more efficient when dealing with selective replication or partial replication scenarios. Logical replication operates on a publish and subscribe model, utilizing change records from the PostgreSQL write-ahead log (WAL). The source, known as the publisher, transmits these changes for the specified tables to one or more recipients (subscribers) replicating the changes and ensuring synchronization between the publisher and the subscriber. The publisher's set of changes is identified through a publication, and subscribers access these changes by establishing a subscription that defines the connection to the publisher's database and its publications. PostgreSQL uses a mechanism called a replication slot to track the progress of a subscription, such as what changes still need to be sent to a subscriber. In Amazon RDS for PostgreSQL, users can manually create a logical replication slot on the writer DB instance of a Multi-AZ DB cluster. This offers more control and flexibility, and is useful for different change data capture (CDC) systems or troubleshooting existing setups. It is worth noting that the CREATE PUBLICATION/SUBSCRIPTION commands automatically set up the replication slots by default, which cover most use cases and simplifies the process. Manually creating the replication slot requires selecting a decoding plugin, which determines the format used to stream changes. RDS Multi-AZ DB clusters support three plugins: pgoutput (what PostgreSQL uses for logical replication), wal2json, and test_decoding (which is used for troubleshooting and is not recommended for production use) giving you the flexibility to capture changes in different formats for your CDC architectures and use cases. Logical replication use cases There are several common use cases for using logical replication with PostgreSQL, including:

Real-time data streaming and event-driven architectures –Using logical replication allows each change in data to be captured as a discrete event. For instance, changes such as insert, update, or delete statements in your database can each generate a distinct event. These events contain valuable information such as the modified data, changed values, timestamps, and the type of operation performed. This makes logical replication useful for implementing event-driven architectures, building data pipelines, and synchronizing data with other applications or services. To process the received events, you can subscribe to the replication stream and use messaging systems like Amazon Kinesis Data Streams, Amazon Managed Streaming for Apache Kafka (Amazon MSK), or Amazon MQ. These messaging systems provide scalable and fault-tolerant event processing capabilities, allowing you to handle a large volume of events and distribute them to multiple consumers. Additionally, consider leveraging event-driven frameworks or systems that are optimized for efficient event processing. For instance, serverless computing services like AWS Lambda can automatically run your code in response to events, ensuring scalability and fault tolerance in your architecture. Data distribution – Logical replication enables you to distribute data across multiple PostgreSQL instances or even different database systems. This is useful for scenarios where you need to share specific tables or subsets of data with other applications or databases. By using logical replication for data distribution, you can achieve better scalability, improved performance, data sharing, and integration across multiple environments and systems. Logical replication provides flexibility in choosing what data to replicate and where to replicate it, enabling you to design and implement a distributed architecture that meets your specific requirements. Data integration and ETL – Logical replication can be used for data integration and extract, transform, load (ETL) processes. You can replicate selected tables or entire databases to downstream systems where data can be transformed, merged, or loaded into other systems. You could also use logical replication to consolidate multiple databases into a single one (for example, for Data Analytics).

Solution overview In this example solution, we walk through a demonstration of enabling and utilizing logical replication in an RDS for PostgreSQL Multi-AZ DB cluster. We discuss the creation of a logical replication slot, the configuration of a Kinesis data stream, support for logical replication slots post-failover, and how to consume the replication slot data using a Python script. The following diagram illustrates our solution architecture.

Prerequisites To implement logical replication for an RDS for PostgreSQL Multi-AZ DB cluster, you must have:

An AWS Account. The AWS Command Line Interface (AWS CLI). A client running the PostgreSQL psql client and PgBench tool with Python installed. RDS for PostgreSQL Multi-AZ cluster does not support AWS free tier therefore consider costs for Amazon RDS pricing and AWS Kinesis Data Streams pricing.

Create a custom parameter group and enable logical replication First, we create a DB cluster parameter group and enable logical replication and increase the number of logical replication workers.

# Create a DB cluster parameter group aws rds create-db-cluster-parameter-group \ --db-cluster-parameter-group-name demo-pg-15-etl \ --db-parameter-group-family postgres15 \ --description "Demo RDS Multi-AZ Cluster ETL using logical replication" \ --region us-west-2

# Modify the demo DB cluster parameter group to enable logical replication and set the maximum logical replication workers to a value higher than your number of consumers aws rds modify-db-cluster-parameter-group \ --db-cluster-parameter-group-name demo-pg-15-etl \ --parameters "ParameterName=max_logical_replication_workers,ParameterValue=10,ApplyMethod=pending-reboot" \ "ParameterName=rds.logical_replication,ParameterValue=1,ApplyMethod=pending-reboot" \ --region us-west-2Bash

For more information on how to perform this step using the console, see Modifying parameters in a DB cluster parameter group and Working with parameter groups for Multi-AZ DB clusters. Create an RDS for PostgreSQL Multi-AZ cluster and DB cluster parameter group Our next step is to create an RDS for PostgreSQL Multi-AZ cluster. To do this, we use the AWS Command Line interface (AWS CLI). We configure the new DB cluster to use the custom parameter group we created previously.

# Create a demo DB cluster aws rds create-db-cluster \ --db-cluster-identifier demo-multi-az-cluster-etl-source \ --db-cluster-parameter-group-name demo-pg-15-etl \ --engine postgres \ --engine-version 15.3 \ --db-subnet-group-name default \ --master-user-password $PGPASSWORD \ --master-username $PGUSER \ --db-cluster-instance-class db.m6gd.large \ --allocated-storage 100 \ --storage-type io1 \ --iops 1000 \ --region us-west-2Bash

For instructions on how to perform this step using the AWS Management Console, see Creating a Multi-AZ cluster Create a Kinesis data stream While the DB cluster is rebooting, we can create the Kinesis data stream, which will be the target for logical replication. Kinesis Data Streams allows you to ingest, process, and analyze real-time streaming data and can be used to stream data into other targets such as Amazon Redshift.

# Create Kinesis data stream aws kinesis create-stream \ --stream-name etl_demo \ --shard-count 1 \ --region us-west-2Bash

For instructions on how to perform this step using the console, see Creating a Stream via the AWS Management Console. Create test tables and data After we have configured the DB cluster, we need to create test data. In this example, we use pgbench to create dummy tables and data:

# Create some dummy tables and data using pgbench pgbench -is 10 \ -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASESQL

Create a role and replication slot To enable logical replication, we must create a database role for replication and grant the role the necessary permissions. After the role is created, we create a logical replication slot on our source DB cluster. For this example, we use the wal2json decoding output.

# Connect to the source DB instance and create a role for replicaton psql -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASE

postgres=> CREATE USER repluser; CREATE ROLE postgres=> GRANT rds_replication to repluser; GRANT ROLE postgres=> \password repluser Enter new password for user "repluser": Enter it again:

# Create logical replication slot on source DB cluster SELECT * FROM pg_create_logical_replication_slot('etl_slot_wal2json','wal2json');SQL

For more information, see Create user and Logical Decoding. Consume the replication stream Finally, we consume the logical replication stream using the following Python script. The script connects to the DB cluster endpoint, starts consuming the replication stream, and forwards each record to the Kinesis data stream. The script also prints the payload to the standard output. The script has a simple retry mechanism to support reconnecting to the DB cluster endpoint if there is an interruption. To run this script, you need a client with network access to the RDS for PostgreSQL Multi-AZ DB cluster with Python and the PostgreSQL client tools installed.

$ cat consume_slot.py import boto3 import json import time import pdb import psycopg2 from psycopg2.extras import LogicalReplicationConnection import os

my_slot_name = 'etl_slot_wal2json' my_stream_name = 'etl_demo' kinesis_client = boto3.client('kinesis', region_name='us-west-2')

host = os.getenv('PGHOST') user = os.getenv('PGUSER') port = os.getenv('PGPORT') password = os.getenv('PGPASSWORD') database = os.getenv('PGDATABASE')

def consume_replication_stream(): my_connection = psycopg2.connect( f"dbname={database} host={host} port={port} user={user} password={password}", connection_factory=LogicalReplicationConnection) cur = my_connection.cursor() cur.drop_replication_slot(my_slot_name) cur.create_replication_slot(my_slot_name, output_plugin='wal2json') cur.start_replication(slot_name=my_slot_name, options={'pretty-print': 1}, decode=True) print(f'Successfully connected to slot {my_stream_name}') cur.consume_stream(consume)

def main(): retries = 10 for i in range(retries): try: consume_replication_stream() break # Success! So we break out the loop except Exception as e: print(f"Error occurred: {e}. Retrying...") time.sleep(30) # Wait for 30 seconds before retrying if i == retries - 1: # If we've retried up to the maximum retries allowed, raise the exception to be handled by the next level raise

Start a DML workload To demonstrate that data is streaming, we need to run a DML workload. In this example, we use pgbench to run the built-in tpcb-like workload for 300 seconds (5 minutes):

pgbench -T300 \ -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASE \Bash

After the workload begins, you should see the stream payload being processed in the terminal standard output:

$ python3 consume_slot.py Successfully connected to slot etl_demo { "change": [ ] } { "change": [ { "kind": "update", "schema": "public", "table": "pgbench_accounts", "columnnames": ["aid", "bid", "abalance", "filler"], "columntypes": ["integer", "integer", "integer", "character(84)"], "columnvalues": [674192, 7, 2261, " "], "oldkeys": { "keynames": ["aid"], "keytypes": ["integer"], "keyvalues": [674192] } }Bash

You can also view the payload via the Kinesis Data Viewer:

For more information on how to perform this step using the console, see Using Data Viewer in the Kinesis Console. Demonstrate DB cluster failover Now that we have validated that the logical replication stream is being consumed successfully, we can introduce a failover of the DB cluster to demonstrate that the replication slot is copied to the read replica nodes in the DB cluster and that post-failover consumption of the stream can continue. Prior to failover, let's review the slot configuration:

slot_name | plugin | slot_type | datoid | database | temporary | active | active_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | wal_status | safe_wal_size | two_phase ---------------------------------------------+----------+-----------+--------+----------+-----------+--------+------------+--------+--------------+-------------+---------------------+------------+---------------+----------- rds_us_west_2_db_knjbk2urhjnkn636iixihmgksq | | physical | | | f | t | 25517 | 338176 | 7874 | 1/B4000000 | | reserved | | f etl_slot_wal2json | wal2json | logical | 5 | postgres | f | t | | | 308164 | 1/A8210E68 | 1/A8210EA0 | reserved | | f rds_us_west_2_db_by3onqaew4kwmrlfn3zjpuigbq | | physical | | | f | t | 28390 | 338172 | 7874 | 1/B4000000 | | reserved | | f (3 rows)SQL

While our sample workload is running and being consumed, we initiate a failover:

# Failover DB cluster aws rds failover-db-cluster \ --db-cluster-identifier demo-multi-az-cluster-etl-source \ --region us-west-2 Bash

For further information on how to perform this step using the console, see Failover process for Multi-AZ DB Clusters. When the failover is complete, we can validate that the logical replication stream reconnects and continues to consume the data:

{ "kind": "insert", "schema": "public", "table": "pgbench_history", "columnnames": ["tid", "bid", "aid", "delta", "mtime", "filler"], "columntypes": ["integer", "integer", "integer", "integer", "timestamp without time zone", "character(22)"], "columnvalues": [71, 9, 175688, 3147, "2023-07-04 08:20:06.966371", null] } ] } Error occurred: terminating connection due to administrator command . Retrying... Error occurred: connection to server at "demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com" (35.163.140.202), port 5432 failed: Connection timed out Is the server running on that host and accepting TCP/IP connections? . Retrying... Successfully connected to slot etl_demo { "change": [ { "kind": "update", "schema": "public", "table": "pgbench_accounts", "columnnames": ["aid", "bid", "abalance", "filler"], "columntypes": ["integer", "integer", "integer", "character(84)"], "columnvalues": [17309, 1, 2376, " "], "oldkeys": { "keynames": ["aid"], "keytypes": ["integer"], "keyvalues": [17309] } }Bash

We can also validate the replication slot is still available after the failover:

psql -h demo-multi-az-cluster-etl-source.cluster-abcd123dummy.us-west-2.rds.amazonaws.com \ -p 5432 \ -U $PGUSER \ -d $PGDATABASE \ -c "SELECT * FROM pg_replication_slots;"

slot_name | plugin | slot_type | datoid | database | temporary | active | active_pid | xmin | catalog_xmin | restart_lsn | confirmed_flush_lsn | wal_status | safe_wal_size | two_phase ---------------------------------------------+----------+-----------+--------+----------+-----------+--------+------------+--------+--------------+-------------+---------------------+------------+---------------+----------- rds_us_west_2_db_knjbk2urhjnkn636iixihmgksq | | physical | | | f | t | 904 | 271194 | 7874 | 1/94000000 | | reserved | | f rds_us_west_2_db_7jhd2ofzzow66dzv2xpsxpanfq | | physical | | | f | t | 3468 | 271202 | 7874 | 1/94000000 | | reserved | | f etl_slot_wal2json | wal2json | logical | 5 | postgres | f | f | | | 269331 | 1/900ABF88 | 1/900ABFC0 | reserved | | f (3 rows)SQL

Cleanup Don't forget to delete any unnecessary resources and drop unused replication slots. To drop unused replication slots, connect to the publisher and run the following SQL command:

SELECT pg_drop_replication_slot(slot_name)FROM pg_replication_slots WHERE slot_name IN (SELECT slot_name FROM pg_replication_slots where active = 'f');SQL

For further information on the pg_drop_replication_slot function, see Replication Management Functions. You can use the following AWS CLI command to delete the RDS for PostgreSQL Multi-AZ DB cluster with an optional DB snapshot.

aws rds delete-db-cluster \ --db-cluster-identifier demo-multi-az-cluster-etl-source \ --final-db-snapshot-identifier demo-multi-az-cluster-etl-source \ --region us-west-2Bash

For further information on how to perform this step using the console, see Deleting a Multi-AZ DB cluster. You can use the following AWS CLI command to delete the RDS for PostgreSQL Multi-AZ DB cluster parameter group.

You can use the following AWS CLI command to delete the Amazon Kinesis Data Stream.

Best practices When enabling logical replication, there are a number of best practices you should consider:

Be aware of unsupported SQL commands – Logical replication is based on the logical changes made to the data. As a result, it doesn't support automatically replicating schema changes such as ALTER TABLE and some other DDL changes. For further information on limitations of PostgreSQL replication see the next section, Current limitations in PostgreSQL logical replication. Clean up unused replication slots – If a replication slot is no longer needed, you should drop the replication slot as soon as possible to prevent accumulation of WAL files. An unused replication slot can prevent vacuum from cleaning up dead tuples and resetting the XID age horizon. Monitor disk space usage – Logical replication slots retain WAL files until they are consumed by all subscribers. This can lead to significant disk space usage if a consumer falls behind or stops consuming changes. For information on monitoring Amazon RDS using Amazon CloudWatch see, Overview of Amazon RDS and Amazon CloudWatch. Monitor replication lag – It's important to monitor the logical replication lag. High replication lag can lead to delayed updates to the consumer and increased disk space usage on the source. Monitoring logical replication lag is the responsibility of the user, for further information on gathering replication status information see, pg_stat_replication. Monitor the number of logical replication consumers – As you increase the number of replication consumers, you need to ensure that the number of consumers doesn't exceed the configured value of max_logical_replication_workers.

Current limitations in PostgreSQL logical replication Bear in mind that logical replication has certain restrictions or limitations:

Schema changes – The database schema and DDL commands are not replicated. The initial schema can be copied by hand using pg_dump --schema-only. Subsequent schema changes would need to be kept in sync manually. As a best practice, schema changes should be committed first by the subscriber, then by the publisher. Sequence data – Though logical replication replicates sequence data in serial or identity columns, in the event of switchover or failover to the subscriber database, you must update the sequences to the latest values. Large objects – Large objects are not replicated. It's also easier to store a reference to a large object on some external storage like Amazon Simple Storage Service (Amazon S3) and replicate that reference rather than storing and replicating the object itself. Truncate – Replication of TRUNCATE commands is supported, but some care must be taken when truncating groups of tables connected by foreign keys. A workaround of TRUNCATE could be DELETE. To avoid accidental TRUNCATE operations, you can REVOKE TRUNCATE privileges from tables. Partitioned tables – Logical replication treats partitioned tables as regular tables. Logical replication doesn't work at the base table level, but at the child table level. If you're using partitions, the partition hierarchy must be the same on both sides of a logical replication setup. Foreign tables, views, and materialized views – Replication is only supported by tables, including partitioned tables. Attempts to replicate other types of relations, such as views, materialized views, or foreign tables, will result in an error.

There is ongoing work in the open-source PostgreSQL developer community to address these limitations. Conclusion In this post, we provided an overview of PostgreSQL logical replication, how to enable and use logical replication with RDS for PostgreSQL Multi-AZ DB clusters, how to consume logical replication to an Amazon Kinesis Data Stream in the context of data consolidation (ETL) use cases and best practices and limitations for PostgreSQL logical replication. RDS Multi-AZ cluster deployments offer enhanced availability and durability for production database workloads. The introduction of PostgreSQL logical replication with RDS Multi-AZ clusters enables granular replication and synchronization of individual tables, providing flexibility and efficiency. As we discussed, this feature supports various use cases, such as real-time data streaming, event-driven architectures, smooth database upgrades, data distribution, data integration and ETL processes, and more. Incorporating logical replication with RDS Multi-AZ clusters can bring significant benefits to your PostgreSQL database workloads. To enhance availability, durability, and data synchronization capabilities for your applications, it's time to explore logical replication with your RDS Multi-AZ cluster topology. We encourage you to learn more by building a solution using the sample implementation provided in this post and a dataset relevant to your business. If you have questions or suggestions, leave a comment.

About the Authors Sean Massey is a Database Engineer and a Subject Matter Expert in Amazon RDS, specializing in PostgreSQL. Sean's relentless dedication to customer success is reflected in his track record of resolving critical issues in Amazon Aurora and Amazon RDS for PostgreSQL database systems. Shayon Sanyal is a Principal Database Specialist Solutions Architect and specializes in the Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL open-source database engines. In his current role, he spends his time working with customers to help design scalable, secure, and robust cloud-native architectures. Vijay Karumajji is a Principal Database Solutions Architect with Amazon Web Services. He works with our customers to provide guidance and technical assistance on database projects, helping them improve the value of their solutions when using AWS.

Accelerate SaaS Delivery onto DoD Networks with Game Warden from Second Front Systems

Blog updated on 7/8/2024 By Dylan Sims, Director of Solutions Engineering – Second Front Systems By Chris Bryant, Content Producer – Second Front Systems By Zach Green, Solutions Architect – AWS

Software-as-a-service (SaaS) products have revolutionized private sector business operations in recent years, with 94% of enterprises using cloud services as of 2022, according to zippia.com. Despite the commercial sector's demonstrated success with SaaS, the United States Department of Defense (DoD) has been slow to adopt this new delivery model, pushing many companies to deliver on-premises and hybrid solutions in addition to its purely SaaS offerings. Despite the ease of use and favorable unit economics inherent to SaaS offerings, significant bureaucratic and cybersecurity barriers exist which prevent these products from reaching the DoD market. The core issues associated with delivering software to the U.S. military are centered around contracting pathways and a web of difficult to navigate cybersecurity requirements. Game Warden on AWS removes many of the barriers to entry for software companies seeking to meet any DoD Impact Level (IL) or classification. Game Warden To solve this, Second Front Systems built the Game Warden platform, a DoD-compliant DevSecOps Platform-as-a-Service (PaaS) that accelerates software delivery onto DoD networks while supporting modern DevOps practices and adhering to stringent cybersecurity controls. Game Warden is built on AWS GovCloud (US) and provides a pathway for containerized applications to receive a Certificate to Field (CTF) on the DoD's Non-Secure Internet Protocol Router Network (NIPRNet). Game Warden enables hosted applications to inherit an Authority to Operate (ATO) while running on the platform. Second Front Systems is an AWS Partner that helps organizations streamline software delivery with its fully managed and compliance DecSecOps platform Game Warden, which is available in AWS Marketplace. Behind the Scenes Game Warden leverages a suite of AWS services for development, compliance, operations, and monitoring. Second Front Systems collaboration with AWS has helped industry-leading software companies scale their business across DoD by demystifying the ATO process and abstracting compliance requirements through an inherited security model.

Figure 1 – Game Warden's shared responsibility model. Here are some products powered by Game Warden on top of AWS that are working on DoD contracts today:

Decision Lens: Decision Lens develops integrated planning software, modernizing how the government prioritizes, plans, and funds. Customers across the DoD, intelligence community, federal civilian agencies, and state and local governments have achieved sustained operational advantages through their long-term planning, continuous medium-term prioritization, and short-term funding execution. Collaboration.Ai: Collaboration.Ai builds products that enable intelligent collaboration and harness untapped networks with an emphasis on human connections. Its innovation management platform, CrowdVector, is the technology behind the well-known AFWERX Challenge, the U.S. Air Force Guardians and Airmen Innovation Network (GAIN), and the NASA Spark program. Systems Innovation Engineering (SIE): SIE built the Supplier Capabilities Analytics and Reporting (SCAR) application to help commercial and federal customers evaluate supply chain options, identify and reduce risk, and build resiliency through requirements trade-off and risk analysis.

Game Warden's inherited security model and automated tooling is reducing ATO timelines from months to weeks, and will soon unlock new production environments in AWS Secret and Top Secret regions.

Figure 2 – Comparison of Game Warden vs. traditional ATO timelines. Solution Overview Game Warden can be thought of as a set of interconnected systems that provide an accelerated pathway for deployment of containerized applications to DoD networks. The key systems include a DevSecOps pipeline, security and compliance architecture, and hosting platform. Let's dive into each system of Game Warden and how they integrate with AWS to provide a secure, DoD-compliant hosting environment and continuous Authorization to Operate (cATO) pipeline for modern SaaS applications. DevSecOps Pipeline Game Warden integrates with an organization's existing CI/CD pipelines—automating vulnerability and malware scanning, container hardening, and deployments to hosting environments. This security and release pipeline is also an accredited pathway for cATO. The Game Warden DevSecOps pipeline is a series of streamlined phases:

Applications are developed and built external to Game Warden or using Game Warden Builder. Application components are packaged into Cloud-Native Computing Foundation (CNCF) containers pushed to the Game Warden Registry. Game Warden's pipeline is automatically triggered with each image push. Images are scanned for malware and common vulnerabilities and exploits, hardened, and deployed to a development environment for functionality testing. Each container image is hardened with custom scripts that implement Secure Technical Implementation Guide (STIG) controls. After the image has met all of the DoD's cybersecurity requirements, it can be promoted to production environments at Impact Levels 2, 4, 5, 6, AWS Secret Region, or AWS Top Secret Region.

The Game Warden pipeline emulates the Risk Management Framework process for securing software, while supporting modern CI/CD and DevSecOps practices. Software developers can fully automate their application deployments into the Game Warden development environment, with new releases taking just a few seconds before they're ready for testing. When a new version of the application is ready for release, its security posture is reviewed and the release is promoted into an accredited production hosting environment typically within 24 hours. The following diagram displays the workflow of a containerized application going through the Game Warden CI/CD pipeline. For added coverage in the development phase, learn about the upcoming Game Warden Builder.

Figure 3 – Overview of the platform pipeline. Security and Compliance Architecture Using a combination of AWS services and platform features, Game Warden meets the compliance requirements at all DoD Impact Levels and classifications. The platform's inherited security model saves customers substantial time and money by removing the infrastructure and platform compliance burden. The DoD adheres to a select set of compliance frameworks that have overlapping requirements and security goals. Game Warden is adherent to these frameworks, allowing Second Front Systems to establish and maintain a continuous Authorization to Operate. Customers operating on Game Warden benefit from inherited compliance with these standards while their application workloads are running on the Game Warden platform. DoD adopted compliance frameworks include:

DoD CC SRG National Institute of Standards and Technology (NIST) 800-171, 800-53 Center of Internet Security (CIS) Benchmarks DoD DevSecOps Reference Architecture

To aid in understanding and mapping components to compliance frameworks, AWS offers the AWS Services in Scope by Compliance Program, a directory that lists different compliance frameworks and the particular AWS services that meet each standard. Game Warden uses many AWS services and relies on this repository to ensure the services employed across the platform meet stringent DoD requirements. Game Warden leverages popular AWS services like Amazon CloudWatch, AWS CloudTrail, and Amazon Virtual Private Cloud (VPC) to monitor network, system, and API activity on the platform. Amazon GuardDuty does real-time checks for cybersecurity anomalies and is used in tandem with CloudWatch, AWS Lambda, and Amazon Simple Notification Service (SNS) to automate responses and alerting.

Figure 4 – Game Warden's security architecture on AWS. Hosting Platform In addition to providing a rapid pathway for SaaS offerings to be positioned to achieve a cATO, Game Warden includes a fully managed hosting environment built on AWS that includes development, staging, and production environments and has a connection to NIPRnet for IL4 and IL5 deployments. Game Warden also provides a pathway to deploy software to classified networks through AWS' built-in cross domain solution, AWS Diode, and managed production environments in AWS Secret and Top Secret Regions for IL6 and Top Secret / Sensitive Compartmented Information (SCI) compliance. In Game Warden, application workloads run on Big Bang, a DoD-accredited DevSecOps platform on Kubernetes, and are used in tandem with Amazon Elastic Kubernetes Service (Amazon EKS) to simplify the setup and management of secure Kubernetes clusters. Game Warden supports both multi-tenant and single-tenant use cases, enabling customers to choose between individual clusters for their end users or a shared environment to reduce consumption costs and reduce deployment complexity. After Game Warden engineers deploy applications into production, site reliability engineers (SREs) manage Day 2 operations using AWS services like CloudWatch to monitor CPU and memory utilization for core services. Game Warden also includes an observability stack in each environment which is used by customers to view application and cluster logs, and metrics. All customers benefit from 24/7 helpdesk and incident response support as tenants on Game Warden's fully managed hosting platform. Second Front Systems assumes most platform and infrastructure administrative and site reliability responsibilities, enabling customers to put more resources towards building great products. Game Warden uses AWS Identity and Access Management (IAM) to grant service-level permissions to pods in each cluster, allowing customer applications to integrate directly with popular AWS services.

Figure 5 – Game Warden's platform architecture on AWS. Conclusion The combination of Game Warden and AWS makes delivering modern software onto U.S. Department of Defense (DoD) networks faster and easier than ever before. This opens up exciting opportunities for commercial companies to unlock access to the vast DoD IT market. To help your organization build and deploy DoD-compliant software, contact Second Front Systems. You can also learn more about Game Warden in AWS Marketplace. .

Second Front Systems – AWS Partner Spotlight Second Front Systems is an AWS Partner that helps organizations streamline software delivery with its fully managed and compliance DecSecOps platform Game Warden. Contact Partner | Partner Overview | AWS Marketplace

TAGS: Authority to Operate, AWS Partner Guest Post, AWS Partner References, AWS Partner Solutions Architects (SA), AWS Partner Success Stories, SaaS on AWS, Second Front Systems, Software as a Service

Cost considerations and common options for AWS Network Firewall log management

When you're designing a security strategy for your organization, firewalls provide the first line of defense against threats. Amazon Web Services (AWS) offers AWS Network Firewall, a stateful, managed network firewall that includes intrusion detection and prevention (IDP) for your Amazon Virtual Private Cloud (VPC). Logging plays a vital role in any firewall policy, as emphasized by the National Institute of Standards and Technology (NIST) Guidelines on Firewalls and Firewall Policy. Logging enables organizations to take proactive measures to help prevent and recover from failures, maintain proper firewall security configurations, and gather insights for effectively responding to security incidents. Determining the optimal logging approach for your organization should be approached on a case-by-case basis. It involves striking a balance between your security and compliance requirements and the costs associated with implementing solutions to meet those requirements. This blog post walks you through logging configuration best practices, discusses three common architectural patterns for Network Firewall logging, and provides guidelines for optimizing the cost of your logging solution. This information will help you make a more informed choice for your organization's use case. Stateless and stateful rules engines logging When discussing Network Firewall best practices, it's essential to understand the distinction between stateful and stateless rules. Note that stateless rules don't support firewall logging, which can make them difficult to work with in use cases that depend on logs. To verify that traffic is forwarded to the stateful inspection engine that generates logs, you can add a custom-defined stateless rule group that covers the traffic you need to monitor, or you can set a default action for stateless traffic to be forwarded to stateful rule groups in the firewall policy, as shown in the following figure.

Figure 1: Set up stateless default actions to forward to stateful rule groups

Alert logs and flow logs Network Firewall provides two types of logs:

Alert — Sends logs for traffic that matches a stateful rule whose action is set to Alert or Drop. Flow — Sends logs for network traffic that the stateless engine forwards to the stateful rules engine.

To grasp the use cases of alert and flow logs, let's begin by understanding what a flow is from the view of the firewall. For the network firewall, network flow is a one-way series of packets that share essential IP header information. It's important to note that the Network Firewall flow log differs from the VPC flow log, as it captures the network flow from the firewall's perspective and it is summarized in JSON format. For example, the following sequence shows how an HTTP request passes through the Network Firewall.

When you're using a stateful rule to block egress HTTP traffic, the TCP connection will be established initially. When an HTTP request comes in, it will be evaluated by the stateful rule. Depending on the rule's action, the firewall may send a TCP reset to the sender when a Reject action is configured, or it may drop the packets to block them if a Drop action is configured. In the case of a Drop action, shown in Figure 3, the Network Firewall decides not to forward the packets at the HTTP layer, and the closure of the connection is determined by the TCP timers on both the client and server sides.

In the given example, the Network Firewall generates a flow log that provides information like IP addresses, port numbers, protocols, timestamps, number of packets, and bytes of the traffic. However, it doesn't include details about the stateful inspection, such as whether the traffic was blocked or allowed. Figure 4 shows the inbound flow log.

The alert log entry complements the flow log by containing stateful inspection details. The entry includes information about whether the traffic was allowed or blocked and also provides the hostname associated with the traffic. This additional information enhances the understanding of network activities and security events, as shown in Figure 6.

In summary, flow logs provide stateless information and are valuable for identifying trends, like monitoring IP addresses that transmit the most data over time in your network. On the other hand, alert logs contain stateful inspection details, making them helpful for troubleshooting and threat hunting purposes. Keep in mind that flow logs can become excessive. When you're forwarding traffic to a stateful inspection engine, flow logs capture the network flows crossing your Network Firewall endpoints. Because log volume affects overall costs, it's essential to choose the log type that suits your use case and security needs. If you don't need flow logs for traffic flow trends, consider only enabling alert logs to help reduce expenses. Effective logging with alert rules When you write stateful rules using the Suricata format, set the alert rule to be evaluated before the pass rule to log allowed traffic. Be aware that:

You must enable strict rule evaluation order to allow the alert rule to be evaluated before the pass rule. Otherwise the order of evaluation by default is pass rules first, then drop, then alert. The engine stops processing rules when it finds a match. When you use pass rules, it's recommended to add a message to remind anyone looking at the policy that these rules do not generate messages. This will help when developing and troubleshooting your rules.

For example, the rules below will allow traffic to a target with a specific Server Name Indication (SNI) and log the traffic that was allowed. As you can see in the pass rule, it includes a message to remind the firewall policy maker that pass rules don't alert. The alert rule evaluated before the pass rule logs a message to tell the log viewer which rule allows the traffic. This way you can see allowed domains in the logs.

alert tls $HOME_NET any -> $EXTERNAL_NET any (tls.sni; content:"www.example.com"; nocase; startswith; endswith; msg:"Traffic allowed by rule 72912"; flow:to_server, established; sid:82912;) pass tls $HOME_NET any -> $EXTERNAL_NET any (tls.sni; content:"www.example.com"; nocase; startswith; endswith; msg:"Pass rules don't alert"; flow:to_server, established; sid:72912;)

This way you can see allowed domains in the alert logs.

Log destination considerations Network Firewall supports the following log destinations:

Amazon Simple Storage Service (Amazon S3) Amazon CloudWatch Logs Amazon Kinesis Data Firehose

You can select the destination that best fits your organization's processes. In the next sections, we review the most common pattern for each log destination and walk you through the cost considerations, assuming a scenario in which you generate 15 TB Network Firewall logs in us-east-1 Region per month. Amazon S3 Network Firewall is configured to inspect traffic and send logs to an S3 bucket in JSON format using Amazon CloudWatch vended logs, which are logs published by AWS services on behalf of the customer. Optionally, logs in the S3 bucket can then be queried using Amazon Athena for monitoring and analysis purposes. You can also create Amazon QuickSight dashboards with an Athena-based dataset to provide additional insight into traffic patterns and trends, as shown in Figure 8.

Figure 8: Architecture diagram showing AWS Network Firewall logs going to S3

Cost considerations Note that Network Firewall logging charges for the pattern above are the combined charges for CloudWatch Logs vended log delivery to the S3 buckets and for using Amazon S3. CloudWatch vended log pricing can influence overall costs significantly in this pattern, depending on the amount of logs generated by Network Firewall, so it's recommended that your team be aware of the charges described in Amazon CloudWatch Pricing – Amazon Web Services (AWS). From the CloudWatch pricing page, navigate to Paid Tier, choose the Logs tab, select your Region and then under Vended Logs, see the information for Delivery to S3. For Amazon S3, go to Amazon S3 Simple Storage Service Pricing – Amazon Web Services, choose the Storage & requests tab, and view the information for your Region in the Requests & data retrievals section. Costs will be dependent on storage tiers and usage patterns and the number of PUT requests to S3. In our example, 15 TB is converted and compressed to approximately 380 GB in the S3 bucket. The total monthly cost in the us-east-1 Region is approximately $3800. Long-term storage There are additional features in Amazon S3 to help you save on storage costs:

If after implementing Network Firewall logging, you determine that your log files are larger than 128 KB, you can take advantage of S3 Intelligent-Tiering, which provides automatic cost savings for files with unknown access patterns. There is additional cost savings potential in the S3 Intelligent-Tiering optional Archive Access and Deep Archive Access tiers, depending on your usage patterns. S3 Lifecycle management can help automatically move log files into cost-saving infrequent access storage tiers after a user-defined amount of time after creation. If logs need to be kept for compliance but are too small to take advantage of S3 Intelligent-Tiering, consider aggregating and placing them in the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive tiers as a way to save on storage costs. Metadata charges might apply and can influence costs depending on the number of objects being stored.

Analytics and reporting Athena and QuickSight can be used for analytics and reporting:

Athena can perform SQL queries directly against data in the S3 bucket where Network Firewall logs are stored. In the Athena query editor, a single query can be run to set up the table that points to the Network Firewall logging bucket. After data is available in Athena, you can use Athena as a data source for QuickSight dashboards. You can use QuickSight to visualize data from your Network Firewall logs, taking advantage of AWS serverless services. Please note that using Athena to scan firewall data in S3 might increase costs, as can the number of authors, users, reports, alerts, and SPICE data used in QuickSight.

Amazon CloudWatch Logs In this pattern, shown in Figure 9, Network Firewall is configured to send logs to Amazon CloudWatch as a destination. Once the logs are available in CloudWatch, CloudWatch Log Insights can be used to search, analyze, and visualize your logs to generate alerts, notifications, and alarms based on specific log query patterns.

Figure 9: Architecture diagram using CloudWatch for Network Firewall Logs

Cost considerations Configuring Network Firewall to send logs to CloudWatch incurs charges based on the number of metrics configured, metrics collection frequency, the number of API requests, and the log size. See Amazon CloudWatch Pricing for additional details. In our example of 15 TB logs, this pattern in the us-east-1 Region results in approximately $6900. CloudWatch dashboards offers a mechanism to create customized views of the metrics and alarms for your Network Firewall logs. These dashboards incur an additional charge of $3 per month for each dashboard. Contributor Insights and CloudWatch alarms are additional ways that you can monitor logs for a pre-defined query pattern and take necessary corrective actions if needed. Contributor Insights are charged per Contributor Insights rule. To learn more, go to the Amazon CloudWatch Pricing page, and under Paid Tier, choose the Contributor Insights tab. CloudWatch alarms are charged based on the number of metric alarms configured and the number of CloudWatch Insights queries analyzed. To learn more, navigate to the CloudWatch pricing page and navigate to the Metrics Insights tab. Long-term storage CloudWatch offers the flexibility to retain logs from 1 day up to 10 years. The default behavior is never expire, but you should consider your use case and costs before deciding on the optimal log retention period. For cost optimization, the recommendation is to move logs that need to be preserved long-term or for compliance from CloudWatch to Amazon S3. Additional cost optimization can be achieved through S3 tiering. To learn more, see Managing your storage lifecycle in the S3 User Guide. AWS Lambda with Amazon EventBridge, as shown in the following sample code, can be used to create an export task to send logs from CloudWatch to Amazon S3 based on an event rule, pattern matching rule, or scheduled time intervals for long-term storage and other use cases.

GROUP_NAME = "/AnfwDemo/Anfw/Alert" DESTINATION_BUCKET = "cwexportlogs-blog" PREFIX = "network-logs" NDAYS = 1 nDays = int(NDAYS)

currentTime = datetime.datetime.now() StartDate = currentTime - datetime.timedelta(days=nDays) EndDate = currentTime - datetime.timedelta(days=nDays - 1)

fromDate = int(StartDate.timestamp() * 1000) toDate = int(EndDate.timestamp() * 1000)

def lambda_handler(event, context): client = boto3.client('logs') response = client.create_export_task( logGroupName=GROUP_NAME, fromTime=fromDate, to=toDate, destination=DESTINATION_BUCKET, destinationPrefix=BUCKET_PREFIX ) print(response)Plain text

Figure 10 shows how EventBridge is configured to trigger the Lambda function periodically.

Figure 10: EventBridge scheduler for daily export of CloudWatch logs

Analytics and reporting CloudWatch Insights offers a rich query language that you can use to perform complex searches and aggregations on your Network Firewall log data stored in log groups as shown in Figure 11. The query results can be exported to CloudWatch dashboard for visualization and operational decision making. This will help you quickly identify patterns, anomalies, and trends in the log data to create the alarms for proactive monitoring and corrective actions.

Figure 11: Network Firewall logs ingested into CloudWatch and analyzed through CloudWatch Logs Insights

Amazon Kinesis Data Firehose For this destination option, Network Firewall sends logs to Amazon Kinesis Data Firehose. From there, you can choose the destination for your logs, including Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and an HTTP endpoint that's owned by you or your third-party service providers. The most common approach for this option is to deliver logs to OpenSearch, where you can index log data, visualize, and analyze using dashboards as shown in Figure 12. In the blog post How to analyze AWS Network Firewall logs using Amazon OpenSearch Service, you learn how to build network analytics and visualizations using OpenSearch in detail. Here, we discuss only some cost considerations of using this pattern.

Figure 12: Architecture diagram showing AWS Network Firewall logs going to OpenSearch

Cost considerations The charge when using Kinesis Data Firehose as a log destination is for CloudWatch Logs vended log delivery. Ingestion pricing is tiered and billed per GB ingested in 5 KB increments. See Amazon Kinesis Data Firehose Pricing under Vended Logs as source. There are no additional Kinesis Data Firehose charges for delivery unless optional features are used. For 15 TB of log data, the cost of CloudWatch delivery and Kinesis Data Firehose ingestion is approximately $5400 monthly in the us-east-1 Region. The cost for Amazon OpenSearch Service is based on three dimensions:

Instance hours, which are the number of hours that an instance is available to you for use The amount of storage you request The amount of data transferred in and out of OpenSearch Service

Storage pricing depends on the storage tier and type of instance that you choose. See pricing examples of using OpenSearch Service. When creating your OpenSearch domain, see Sizing Amazon OpenSearch Service domains to help you right-size your OpenSearch domain. Other cost optimization best practices include choosing the right storage tier and using AWS Graviton2 instances to improve performance. For instance, allocating approximately 15 TB of UltraWarm storage in the us-east-1 Region will result in a monthly cost of $4700. Keep in mind that in addition to storage costs, you should also account for compute instances and hot storage. In short, the estimated total cost for log ingestion and storage in the us-east-1 Region for this pattern is at least $10,100. Leveraging OpenSearch will enable you to promptly investigate, detect, analyze, and respond to security threats. Summary The following table shows a summary of the expenses and advantages of each solution. Since storing logs is a fundamental aspect of log management, we use the monthly cost of using Amazon S3 as the log delivery destination as our baseline when making these comparisons.

Pattern Log delivery and storage cost as a multiple of the baseline cost Functionalities Dependencies

Amazon S3, Athena, QuickSight 1 The most economical option for log analysis. The solution requires security engineers to have a good analytics skillset. Familiarity with Athena query and query running time will impact the incident response time and the cost.

Amazon CloudWatch 1.8 Log analysis, dashboards, and reporting can be implemented from the CloudWatch console. No additional service is needed. The solution requires security engineers to be comfortable with CloudWatch Logs Insights query syntax. The CloudWatch Logs Insights query will impact the incident response time and the cost.

Amazon Kinesis Data Firehose, OpenSearch 2.7+ Investigate, detect, analyze, and respond to security threats quickly with OpenSearch. The solution requires you to invest in managing the OpenSearch cluster.

You have the flexibility to select distinct solutions for flow logs and alert logs based on your requirements. For flow logs, opting for Amazon S3 as the destination offers a cost-effective approach. On the other hand, for alert logs, using the Kinesis Data Firehose and OpenSearch solution allows for quick incident response. Minimizing the time required to address ongoing security challenges can translate to reduced business risk at different costs. Conclusion This blog post has explored various patterns for Network Firewall log management, highlighting the cost considerations associated with each approach. While cost is a crucial factor in designing an efficient log management solution, it's important to consider other factors such as real-time requirements, solution complexity, and ownership. Ultimately, the key is to adopt a log management pattern that aligns with your operational needs and budgetary constraints. Network security is an iterative practice, and by optimizing your log management strategy, you can enhance your overall security posture while effectively managing costs. For more information about working with Network Firewall, see What is AWS Network Firewall?

Sharon Li Sharon is an Enterprise Solutions Architect at Amazon Web Services based in Boston, with a passion for designing and building secure workloads on AWS. Prior to her current role at AWS, Sharon worked as a software development engineer at Amazon, where she played a key role in bringing security into the development process.

Larry Tewksbury Larry is an AWS Technical Account Manager based in New Hampshire. He works with enterprise customers in the Northeast to understand, scale, and optimize their cloud operations. Outside of work, he enjoys spending time with his family, hiking, and tech-based hobbies.

Shashidhar Makkapati Shashidhar is an Enterprise Solutions Architect at Amazon Web Services, based in Charlotte, NC. With over two decades of experience as an enterprise architect, he has a keen focus on cloud adoption and digital transformation in the financial services industry. Shashidhar supports enterprise customers in the US Northeast. In his free time, he enjoys reading, traveling, and spending time with his family.

Implementing automatic drift detection in CDK Pipelines using Amazon EventBridge

The AWS Cloud Development Kit (AWS CDK) is a popular open source toolkit that allows developers to create their cloud infrastructure using high level programming languages. AWS CDK comes bundled with a construct called CDK Pipelines that makes it easy to set up continuous integration, delivery, and deployment with AWS CodePipeline. The CDK Pipelines construct does all the heavy lifting, such as setting up appropriate AWS IAM roles for deployment across regions and accounts, Amazon Simple Storage Service (Amazon S3) buckets to store build artifacts, and an AWS CodeBuild project to build, test, and deploy the app. The pipeline deploys a given CDK application as one or more AWS CloudFormation stacks. With CloudFormation stacks, there is the possibility that someone can manually change the configuration of stack resources outside the purview of CloudFormation and the pipeline that deploys the stack. This causes the deployed resources to be inconsistent with the intent in the application, which is referred to as "drift", a situation that can make the application's behavior unpredictable. For example, when troubleshooting an application, if the application has drifted in production, it is difficult to reproduce the same behavior in a development environment. In other cases, it may introduce security vulnerabilities in the application. For example, an AWS EC2 SecurityGroup that was originally deployed to allow ingress traffic from a specific IP address might potentially be opened up to allow traffic from all IP addresses. CloudFormation offers a drift detection feature for stacks and stack resources to detect configuration changes that are made outside of CloudFormation. The stack/resource is considered as drifted if its configuration does not match the expected configuration defined in the CloudFormation template and by extension the CDK code that synthesized it. In this blog post you will see how CloudFormation drift detection can be integrated as a pre-deployment validation step in CDK Pipelines using an event driven approach. Services and frameworks used in the post include CloudFormation, CodeBuild, Amazon EventBridge, AWS Lambda, Amazon DynamoDB, S3, and AWS CDK. Solution overview Amazon EventBridge is a serverless AWS service that offers an agile mechanism for the developers to spin up loosely coupled, event driven applications at scale. EventBridge supports routing of events between services via an event bus. EventBridge out of the box supports a default event bus for each account which receives events from AWS services. Last year, CloudFormation added a new feature that enables event notifications for changes made to CloudFormation-based stacks and resources. These notifications are accessible through Amazon EventBridge, allowing users to monitor and react to changes in their CloudFormation infrastructure using event-driven workflows. Our solution leverages the drift detection events that are now supported by EventBridge. The following architecture diagram depicts the flow of events involved in successfully performing drift detection in CDK Pipelines.

The user starts the pipeline by checking code into an AWS CodeCommit repo, which acts as the pipeline source. We have configured drift detection in the pipeline as a custom step backed by a lambda function. When the drift detection step invokes the provider lambda function, it first starts the drift detection on the CloudFormation stack Demo Stack and then saves the drift_detection_id along with pipeline_job_id in a DynamoDB table. In the meantime, the pipeline waits for a response on the status of drift detection. The EventBridge rules are set up to capture the drift detection state change events for Demo Stack that are received by the default event bus. The callback lambda is registered as the intended target for the rules. When drift detection completes, it triggers the EventBridge rule which in turn invokes the callback lambda function with stack status as either DRIFTED or IN SYNC. The callback lambda function pulls the pipeline_job_id from DynamoDB and sends the appropriate status back to the pipeline, thus propelling the pipeline out of the wait state. If the stack is in the IN SYNC status, the callback lambda sends a success status and the pipeline continues with the deployment. If the stack is in the DRIFTED status, callback lambda sends failure status back to the pipeline and the pipeline run ends up in failure. Solution Deep Dive The solution deploys two stacks as shown in the above architecture diagram

The CDK Pipelines stack defines a pipeline with a CodeCommit source and drift detection step integrated into it. The pre-requisite stack deploys following resources that are required by the CDK Pipelines stack.

A Lambda function that implements drift detection step A DynamoDB table that holds drift_detection_id and pipeline_job_id An Event bridge rule to capture "CloudFormation Drift Detection Status Change" event A callback lambda function that evaluates status of drift detection and sends status back to the pipeline by looking up the data captured in DynamoDB.

The pre-requisites stack is deployed first, followed by the CDK Pipelines stack. Defining drift detection step CDK Pipelines offers a mechanism to define your own step that requires custom implementation. A step corresponds to a custom action in CodePipeline such as invoke lambda function. It can exist as a pre or post deployment action in a given stage of the pipeline. For example, your organization's policies may require its CI/CD pipelines to run a security vulnerability scan as a prerequisite before deployment. You can build this as a custom step in your CDK Pipelines. In this post, you will use the same mechanism for adding the drift detection step in the pipeline. You start by defining a class called DriftDetectionStep that extends Step and implements ICodePipelineActionFactory as shown in the following code snippet. The constructor accepts 3 parameters stackName, account, region as inputs. When the pipeline runs the step, it invokes the drift detection lambda function with these parameters wrapped inside userParameters variable. The function produceAction() adds the action to invoke drift detection lambda function to the pipeline stage. Please note that the solution uses an SSM parameter to inject the lambda function ARN into the pipeline stack. So, we deploy the provider lambda function as part of pre-requisites stack before the pipeline stack and publish its ARN to the SSM parameter. The CDK code to deploy pre-requisites stack can be found here. export class DriftDetectionStep extends Step implements pipelines.ICodePipelineActionFactory { constructor( private readonly stackName: string, private readonly account: string, private readonly region: string ) { super(`DriftDetectionStep-${stackName}`); }

public produceAction( stage: codepipeline.IStage, options: ProduceActionOptions ): CodePipelineActionFactoryResult { // Define the configuraton for the action that is added to the pipeline. stage.addAction( new cpactions.LambdaInvokeAction({ actionName: options.actionName, runOrder: options.runOrder, lambda: lambda.Function.fromFunctionArn( options.scope, `InitiateDriftDetectLambda-${this.stackName}`, ssm.StringParameter.valueForStringParameter( options.scope, SSM_PARAM_DRIFT_DETECT_LAMBDA_ARN ) ), // These are the parameters passed to the drift detection step implementaton provider lambda userParameters: { stackName: this.stackName, account: this.account, region: this.region, }, }) ); return { runOrdersConsumed: 1, }; } } TypeScript Configuring drift detection step in CDK Pipelines Here you will see how to integrate the previously defined drift detection step into CDK Pipelines. The pipeline has a stage called DemoStage as shown in the following code snippet. During the construction of DemoStage, we declare drift detection as the pre-deployment step. This makes sure that the pipeline always does the drift detection check prior to deployment. Please note that for every stack defined in the stage; we add a dedicated step to perform drift detection by instantiating the class DriftDetectionStep detailed in the prior section. Thus, this solution scales with the number of stacks defined per stage. export class PipelineStack extends BaseStack { constructor(scope: Construct, id: string, props?: StackProps) { super(scope, id, props);

const repo = new codecommit.Repository(this, 'DemoRepo', { repositoryName: `${this.node.tryGetContext('appName')}-repo`, });

const pipeline = new CodePipeline(this, 'DemoPipeline', { synth: new ShellStep('synth', { input: CodePipelineSource.codeCommit(repo, 'main'), commands: ['./script-synth.sh'], }), crossAccountKeys: true, enableKeyRotation: true, }); const demoStage = new DemoStage(this, 'DemoStage', { env: { account: this.account, region: this.region, }, }); const driftDetectionSteps: Step[] = []; for (const stackName of demoStage.stackNameList) { const step = new DriftDetectionStep(stackName, this.account, this.region); driftDetectionSteps.push(step); } pipeline.addStage(demoStage, { pre: driftDetectionSteps, }); TypeScript Demo Here you will go through the deployment steps for the solution and see drift detection in action. Deploy the pre-requisites stack Clone the repo from the GitHub location here. Navigate to the cloned folder and run script script-deploy.sh You can find detailed instructions in README.md Deploy the CDK Pipelines stack Clone the repo from the GitHub location here. Navigate to the cloned folder and run script script-deploy.sh. This deploys a pipeline with an empty CodeCommit repo as the source. The pipeline run ends up in failure, as shown below, because of the empty CodeCommit repo.

Next, check in the code from the cloned repo into the CodeCommit source repo. You can find detailed instructions on that in README.md This triggers the pipeline and pipeline finishes successfully, as shown below.

The pipeline deploys two stacks DemoStackA and DemoStackB. Each of these stacks creates an S3 bucket.

Demonstrate drift detection Locate the S3 bucket created by DemoStackA under resources, navigate to the S3 bucket and modify the tag aws-cdk:auto-delete-objects from true to false as shown below

Now, go to the pipeline and trigger a new execution by clicking on Release Change

The pipeline run will now end in failure at the pre-deployment drift detection step.

Cleanup Please follow the steps below to clean up all the stacks.

Navigate to S3 console and empty the buckets created by stacks DemoStackA and DemoStackB. Navigate to the CloudFormation console and delete stacks DemoStackA and DemoStackB, since deleting CDK Pipelines stack does not delete the application stacks that the pipeline deploys. Delete the CDK Pipelines stack cdk-drift-detect-demo-pipeline Delete the pre-requisites stack cdk-drift-detect-demo-drift-detection-prereq

Conclusion In this post, I showed how to add a custom implementation step in CDK Pipelines. I also used that mechanism to integrate a drift detection check as a pre-deployment step. This allows us to validate the integrity of a CloudFormation Stack before its deployment. Since the validation is integrated into the pipeline, it is easier to manage the solution in one place as part of the overarching pipeline. Give the solution a try, and then see if you can incorporate it into your organization's delivery pipelines. About the author:

Damodar Shenvi Wagle is a Senior Cloud Application Architect at AWS Professional Services. His areas of expertise include architecting serverless solutions, CI/CD, and automation.

TAGS: Amazon DynamoDB, Amazon EventBridge, AWS CDK, AWS CloudFormation, AWS CodeBuild, AWS CodePipeline, AWS Lambda, CICD Pipeline

Load test your applications in a CI/CD pipeline using CDK pipelines and AWS Distributed Load Testing Solution

Load testing is a foundational pillar of building resilient applications. Today, load testing practices across many organizations are often based on desktop tools, where someone must manually run the performance tests and validate the results before a software release can be promoted to production. This leads to increased time to market for new features and products. Load testing applications in automated CI/CD pipelines provides the following benefits:

Early and automated feedback on performance thresholds based on clearly defined benchmarks. Consistent and reliable load testing process for every feature release. Reduced overall time to market due to eliminated manual load testing effort. Improved overall resiliency of the production environment. The ability to rapidly identify and document bottlenecks and scaling limits of the production environment.

In this blog post, we demonstrate how to automatically load test your applications in an automated CI/CD pipeline using AWS Distributed Load Testing solution and AWS CDK Pipelines. The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define cloud infrastructure in code and provision it through AWS CloudFormation. AWS CDK Pipelines is a construct library module for continuous delivery of AWS CDK applications, powered by AWS CodePipeline. AWS CDK Pipelines can automatically build, test, and deploy the new version of your CDK app whenever the new source code is checked in. Distributed Load Testing is an AWS Solution that automates software applications testing at scale to help you identify potential performance issues before their release. It creates and simulates thousands of users generating transactional records at a constant pace without the need to provision servers or instances. Prerequisites To deploy and test this solution, you will need:

AWS Command Line Interface (AWS CLI): This tutorial assumes that you have configured the AWS CLI on your workstation. Alternatively, you can use also use AWS CloudShell. AWS CDK V2: This tutorial assumes that you have installed AWS CDK V2 on your workstation or in the CloudShell environment.

Solution Overview In this solution, we create a CI/CD pipeline using AWS CDK Pipelines and use it to deploy a sample RESTful CDK application in two environments; development and production. We load test the application using AWS Distributed Load Testing Solution in the development environment. Based on the load test result, we either fail the pipeline or proceed to production deployment. You may consider running the load test in a dedicated testing environment that mimics the production environment. For demonstration purposes, we use the following metrics to validate the load test results.

Average Response Time – the average response time, in seconds, for all the requests generated by the test. In this blog post we define the threshold for average response time to 1 second. Error Count – the total number of errors. In this blog post, we define the threshold for for total number of errors to 1.

For your application, you may consider using additional metrics from the Distributed Load Testing solution documentation to validate your load test. Architecture diagram

AWS CDK code for the CI/CD pipeline, including AWS Identity and Access Management (IAM) roles and policies. The pipeline has the following stages:

Source: fetches the source code for the sample application from the AWS CodeCommit repository. Build: compiles the code and executes cdk synth to generate CloudFormation template for the sample application. UpdatePipeline: updates the pipeline if there are any changes to our code or the pipeline configuration. Assets: prepares and publishes all file assets to Amazon S3 (S3). Development Deployment: deploys application to the development environment and runs a load test. Production Deployment: deploys application to the production environment.

The AWS Lambda (Lambda) function in the architecture contains a 500 millisecond sleep statement to add latency to the API response.

Typescript code for starting the load test and validating the test results. This code is executed in the 'Load Test' step of the 'Development Deployment' stage. It starts a load test against the sample restful application endpoint and waits for the test to finish. For demonstration purposes, the load test is started with the following parameters:

Concurrency: 1 Task Count: 1 Ramp up time: 0 secs Hold for: 30 sec End point to test: endpoint for the sample RESTful application. HTTP method: GET

Load Testing service deployed via the AWS Distributed Load Testing Solution. For costs related to the AWS Distributed Load Testing Solution, see the solution documentation.

Implementation Details For the purposes of this blog, we deploy the CI/CD pipeline, the RESTful application and the AWS Distributed Load Testing solution into the same AWS account. In your environment, you may consider deploying these stacks into separate AWS accounts based on your security and governance requirements. To deploy the solution components

Follow the instructions in the the AWS Distributed Load Testing solution Automated Deployment guide to deploy the solution. Note down the value of the CloudFormation output parameter 'DLTApiEndpoint'. We will need this in the next steps. Proceed to the next step once you are able to login to the User Interface of the solution. Clone the blog Git repository

Update the Distributed Load Testing Solution endpoint URL in loadTestEnvVariables.json. Deploy the CloudFormation stack for the CI/CD pipeline. This step will also commit the AWS CDK code for the sample RESTful application stack and start the application deployment. cd pipeline && cdk bootstrap && cdk deploy --require-approval neverBash Follow the below steps to view the load test results:

Open the AWS CodePipeline console. Click on the pipeline named "blog-pipeline". Observe that one of the stages (named 'LoadTest') in the CI/CD pipeline (that was provisioned by the CloudFormation stack in the previous step) executes a load test against the application Development environment.

Click on the details of the 'LoadTest' step to view the test results. Notice that the load test succeeded.

Change the response time threshold In this step, we will modify the response time threshold from 1 second to 200 milliseconds in order to introduce a load test failure. Remember from the steps earlier that the Lambda function code has a 500 millisecond sleep statement to add latency to the API response time.

From the AWS Console and then go to CodeCommit. The source for the pipeline is a CodeCommit repository named "blog-repo". Click on the "blog-repo" repository, and then browse to the "pipeline" folder. Click on file 'loadTestEnvVariables.json' and then 'Edit'. Set the response time threshold to 200 milliseconds by changing attribute 'AVG_RT_THRESHOLD' value to '.2'. Click on the commit button. This will start will start the CI/CD pipeline. Go to CodePipeline from the AWS console and click on the 'blog-pipeline'. Observe the 'LoadTest' step in 'Development-Deploy' stage will fail in about five minutes, and the pipeline will not proceed to the 'Production-Deploy' stage.

Click on the details of the 'LoadTest' step to view the test results. Notice that the load test failed.

Log into the Distributed Load Testing Service console. You will see two tests named 'sampleScenario'. Click on each of them to see the test result details.

From the AWS Console, go to CloudFormation and delete the stacks 'Production-Deploy-Application' and 'Development-Deploy-Application'.

Delete the CI/CD pipeline. cd pipeline && cdk destroyBash Delete the Distributed Load Testing Service CloudFormation stack.

From CloudFormation console, delete the stack for Distributed Load Testing service that you created earlier.

Conclusion In the post above, we demonstrated how to automatically load test your applications in a CI/CD pipeline using AWS CDK Pipelines and AWS Distributed Load Testing solution. We defined the performance bench marks for our application as configuration. We then used these benchmarks to automatically validate the application performance prior to production deployment. Based on the load test results, we either proceeded to production deployment or failed the pipeline. About the Authors

Usman Umar Usman Umar is a Sr. Applications Architect at AWS Professional Services. He is passionate about developing innovative ways to solve hard technical problems for the customers. In his free time, he likes going on biking trails, doing car modifications, and spending time with his family.

Krishnakumar Rengarajan Krishnakumar Rengarajan is a Senior DevOps Consultant with AWS Professional Services. He enjoys working with customers and focuses on building and delivering automated solutions that enable customers on their AWS cloud journey.

Best Practices from Pragma for Navigating the API Economy with Observability and AWS

The term "API economy" refers to businesses delivering digital services to end users, other company services, or partners. But what is the API economy? You could use APIs to improve productivity with services already built and exposed by other companies, or you could create internal services and expose them through APIs to help you reuse these services, reduce coupling, and improve maintainability. You could also develop services and expose them to a general audience or partners who need them. All of these options could be delivered via API-as-a-service, which gives you financial benefits and strategic advantages. We call this the "API economy," which allows you to establish many business models to develop and publish APIs. This post discusses the necessary aspects to achieve an observability model in the API economy, including a practice example with an architecture design and related technologies. Pragma is an AWS Advanced Tier Services Partner and AWS Marketplace Seller with Competencies in DevOps and Digital Customer Experience. Pragma has worked with big retail, banking, and insurance companies in LATAM for over 30 years. Its approach is to work alongside organizations and stakeholders to assess their business and technology needs and recommend tailor-made solution roadmaps that are focused on the end-user experience. Exploring the API Economy: API-as-a-Service API-as-a-service can be consumed by a company's internal services to transform and add value before going to end users or a partner who, having the required user authorization, works with that information and delivers their value proposition to end users. Amazon Web Services (AWS) is one of the examples of how API economy can be established. AWS uses APIs to expose core service capabilities and customers, partners, and internal AWS employees make use of these APIs, creating an API economy. A little background on how it came into existence. As Amazon was growing, the team started transitioning to a service-oriented architecture and built services in a decoupled, API-access fashion. Internal teams inside of Amazon were expected to be able to consume peer internal development team services through these APIs. As more developments were made, AWS made the APIs available to external users as well. To have a thriving API economy model, a well-defined observability architecture is required to optimize the business or improve external quality attributes. Here are some considerations that should be taken into account:

Percentage of successful and failed transactions. Number of requests by second, minutes, hours, months, or year. Configuration changes and their impact on the architecture. API consumption plan per customer. Infrastructure cost. Most and least used APIs. When an error happens, detect the error, its cause, and how to solve it.

Enabling the API Economy Based on Pragma's extensive experience, consider the following steps to establish an API economy model. Step 1: Understanding and Definitions Conducting thorough market research and understanding the target consumer is crucial to create an API economy model. Define the value proposition of the API while keeping in mind legal and regulatory restrictions. API architecture should consider business goals, processes, technical restrictions, and the current architecture context. Quality metrics should be assessed to ensure the API is reliable, secure, and efficient. Establishing a propagation and deployment strategy includes identifying the first customers who will consume the APIs and determining how they'll do so. Careful consideration should be given to the load the API will receive, taking into account the projected customer base. By projecting the load, it's possible to ensure the API can handle the expected volume of requests without experiencing performance issues. Step 2: Business Model Selection When devising an API economy model, consider diverse business models that can be utilized to maximize profits and benefits. Here are some of the most significant ones that have proven effective in the industry:

Free: The benefit is not monetary. It could be strategic, as when the company wants end users to take advantage of their service while tracking how they use it. Freemium: Users can consume the service for free with some limitations and then pay for added features. For example, the company delivers a free tier for users to see their credit card expenses, but if users want to create a budget that sends an alert when they exceed it, they have to pay an additional cost. Subscription: Customers subscribe to a standard service that could have extra payment options if the limit is exceeded. For example, users could request to know their credit score three times every month, but if the number of requests exceeds this limit they have to pay for the additional requests. Pay-per-use: The price is directly based on usage, with a specific driver such as the number of requests to the API or MB consumed. Depending on their quantity, users pay a unit/value. Revenue share: There could be a partnership between two companies, and the profits related to the API usage are distributed depending on the number of accesses to the service.

Step 3: Design Integration and Observability Prioritize designing an architecture that lays the foundation for building a scalable and reliable API economy. This involves creating a cohesive structure, defining design patterns, and adopting best practices that enable seamless communication and data exchange between components. Pragma uses dashboards that offer business and technical perspectives to ensure it can observe all APIs. These dashboards allow Pragma to analyze data and take actions related to service pricing and API consumption. A critical aspect of this architecture is observability design, which focuses on gaining insights into the system's behavior and performance. Observability involves configuring services and APIs to collect logs, metrics, and traces which can be analyzed to detect issues and make informed decisions. An example of how APIs can be shared is AWS Data Exchange, which allows customers to find third-party datasets. The exchange happens through Amazon API Gateway which integrates with provider APIs, allowing data providers to distribute and monetize API-accessible data and subscribers to find, subscribe to, and use third-party APIs in the cloud. It's imperative to know which API you'll expose and who will use it; such as:

Public API – exposed to the general public. Private API – exposed to the company's internal users. Partners API – exposed to third parties that have or will have an agreement with the company.

Step 4: Technology Selection to Implement API Lifecycle To create successful APIs, you must establish the technology stack at this stage, choosing the appropriate tools and frameworks to develop, deploy, and manage APIs efficiently. The technology stack should align with the organization's requirements and scalability needs. Additionally, defining a baseline for API codification ensures consistent and standardized API development, making it easier for developers to collaborate. The necessary infrastructure should also be put in place to support API development. A testing strategy ensures the quality of the API economy model, aligning with quality attributes metrics, strategy, and business goals. On top of that, a user-centric approach ensures a seamless and effective user experience, as the goal is to deliver reliable, user-friendly, and responsive APIs. Step 5: Implementation Taking a strategic approach to implementing an effective API economy model is important, and you first need to create APIs that align with the overall strategy. This involves identifying key functionalities and services offered through the APIs, and once the APIs have been developed the next step is to publish them. To streamline development, CI/CD pipelines should be established to automate the building, testing, and deployment of APIs. Embracing an API-oriented DevSecOps approach drives agility, faster development cycles, and improved reliability in software development and delivery. Finally, it's essential to establish governance to ensure the API economy model is continuously evolving based on user feedback and observability. This involves monitoring the performance of the APIs and making adjustments to enhance the user experience and improve overall strategy. Pragma's Approach Observability in the API economy is critical because a company needs to know if the execution is effective according to their chosen business model. Based on the insights gained, recommendations must be made to improve both profits and customer experience. Observability has three mainstays—logs, traces, and metrics—but we should consider other critical aspects related to monitoring, such as costs, infrastructure, application performance management), transactional auditory, log management, synthetic monitoring, real user monitoring, and security. Based on Pragma's experience, we'll now showcase an example of observability in the API economy and explain step by step how to implement it and how you could cater to business and technical needs. Building the Architecture Clone this repository, which was done with AWS CloudFormation. Now, change in the template api_economy_template.yaml, the domain PaymentUserPoolDomain (section for the domain you want), and username in ExampleUser (section for your email). Once you have run the template, you'll have in your AWS console the next deployed architecture.

Figure 1 – High-level architecture. This high-level architecture works with APIs that can be exposed to multiple customers with one API key and is connected to Amazon Cognito to manage the authorization process. In addition, the architecture has different services, which we'll use to answer some business and technical questions. One of the relevant services of this architecture is Amazon DevOps Guru, a machine learning (ML) service designed to detect anomalous behavior patterns by identifying problems and recommending how to solve them. Amazon EventBridge is scheduled to run an AWS Lambda function every minute, allowing DevOps Guru to detect whether the service is healthy. Next, uncomment the third line (time.sleep(120)) to modify the latency of this service, and after 10-20 minutes DevOps Guru will start to detect problems and make recommendations to improve the latency. In the following image, DevOps Guru shows an insight with high severity due to invocations timing out.

Figure 2 – DevOps Guru high severity. On the same page, DevOps Guru shows recommendations to solve the problem, such as a rollback (remember the error was caused by uncommenting the line in payment service), which is the right solution to the problem. After the rollback, wait a few minutes and DevOps Guru will show the insight was closed.

Figure 3 – Recommend rollback. Now, we'll see how to answer other technical and business questions. For that, uncomment the third line to affect the latency again. Then, get a token—you can get one by using the URL: Change REGION for the region in which you are working in the AWS console; example us-east-1. Next, change YOUR_DOMAIN and YOUR_CLIENT_ID for the data you can find in Cognito > User pools > App Integration (tab). Note the token generated above should be used by the authenticated user only, as this token is tied to an individual user account. Hence, it should be kept as a secret. If the API is called from a web or mobile application, AWS Amplify can simplify the overall authentication, including the token lifecycle management. Additionally, you can use AWS WAF and AWS Shield to block malicious requests and prevent from other web exploits and attacks. The next step is to log in and get the URL and paste in notes to get id_token. Take into account the URL contains id_token, access_token, expires_in, token_type,and you must choose just id_token. Take into account that to log in, the email address is what you put in the CloudFormation stack and the password was sent to that email. Next, import TestAPIEconomy.postman_collection.json of the GitHub in Postman, fill in the next variables, and save:

TOKEN: id_token you got previously X-API-KEY: API key you can find in API Gateway > Select your API > API Keys. Example: G4qilyc7EF3Yj7S904Sib9m6dwE6LxKAcavpeRI6 URL: API Invoke URL in stage prod. Example:

Send the request from the Postman, and you should receive the following text as a response with status 200: "Your payment has been made successfully." Create run collections with 20 iterations and a delay of 60,000, run them manually, and wait until they finish. After the previous process, observe the request trace, percentage of successful transactions, percentage of error transactions, and he transaction quantity per minute in AWS X-Ray for the API Gateway and AWS Lambda service.

Figure 4 – AWS X-Ray percentage of successful and error transactions. The error we can see now in the API Gateway is because we configured a usage plan that allows just 20 requests; after that, the limit will be exceeded and the response code will be 429: "Too many requests."

Figure 5 – Too many requests. Alternatively, with the last requests DevOps Guru also detects the anomalous latency and 4XX Error of "Too many requests," as shown in the following image. Besides, it gives some recommendations to solve this problem.

Figure 6 – Behavior anomalous in API Gateway. Next, you could see the API consumption plan per customer in API Gateway > Payment > Usage Plans > Payment_Usage_Plan > API Keys (tab) > Usage.

Figure 7 – API consumption plan per customer. Cleanup Delete all cloud formation stacks and the associated cloud watch logs group to avoid incurring additional costs. Conclusion In this post, we covered the concepts of API economy and the steps you should take to establish an API economy model effectively. You have learned the mainstays of observability and the importance of having well-defined observability architecture to have a thriving API economy model at scale. We have used AWS services such as Amazon DevOps Guru and AWS X-Ray to provide answers to improve the business. Learn more about Pragma in AWS Marketplace. .

Pragma – AWS Partner Spotlight Pragma is an AWS Partner that has worked with big retail, banking, and insurance companies in LATAM for over 30 years. Its approach is to work alongside organizations and stakeholders to assess their business and technology needs and recommend tailor-made solution roadmaps that are focused on end-user experience. Contact Pragma | Partner Overview | AWS Marketplace

TAGS: AWS Competency Partners, AWS Partner Guest Post, AWS Partner Solutions Architects (SA), AWS Partner Success Stories, AWS Service Delivery Partners, AWS Well-Architected Partners, PRAGMA

Securing Amazon API Gateway with secure ciphers using Amazon CloudFront

Enterprise customers have differing requirements based on a number of reasons including security, compliance, governance, and the industry they operate. In certain situations, customers are expected to adhere to certain protocols and standards that are non-negotiable. In this post, we explore how customers can specify TLS protocols and associated ciphers that are allowed from viewers on Amazon API Gateway. The TLS protocol provides protection against network security concerns, such as tampering between client and server communication. It is recommended to use the latest version of TLS to make sure that your API can support the most recent ciphers which offer the highest level of security. API Gateway provides numerous options for selecting TLS ciphers for the APIs provisioned for your custom domain. However, enterprise customers may require further flexibility based on their organizational policies and governance requirements. This post is intended to assist customers in this particular situation to customize API Gateway endpoints to support specific TLS cipher suites that meet their organizational compliance requirements.

Walkthrough What are Cipher Suites and how do they relate to TLS/SSL? Cipher Suites are a collection of Cryptographic algorithms (Ciphers) that allow network connections to be secure. A typical TLS/SSL Cipher suite provides a set of steps and the order under which the contained algorithms are executed. A considerable amount of processing is involved when establishing the HTTPS connection between the client and server applications. This process is referred to as a TLS/SSL handshake. During the SSL/TLS handshake, the client application provides the server application with the maximum TLS version along with the cipher suites it supports. Then, the server application makes a decision based on the best options available for both the server and the client on the right TLS version and cipher suites to be used throughout the session. Cipher suites for TLS 1.2 and TLS 1.3 TLS 1.2 and TLS 1.3 are the prevalent standards used today. The cipher suites for TLS 1.2 and TLS 1.3 are different, TLS 1.2 uses 37 cipher suites whilst TLS 1.3 uses only five cipher suites. TLS 1.3 is still considered more secure and the reason for this lies in how these cipher suites are created. If you look deeper into the composition of a TLS 1.2 cipher suite, then you find the following four ciphers: · Key exchange algorithm for protecting information when creating symmetric keys for exchange · Signature algorithm which validates the type of certificate · Bulk data encryption algorithm to encrypt messages between client and server · Message authentication code (MAC) algorithm to perform message authentication using hashes and signatures between the clients and the server An example of how the different algorithms in TLS 1.2 Cipher Suite are organized is shown here:

Looking at the composition of the cipher suite for TLS 1.3, you notice only two ciphers: one for MAC and the other for bulk data encryption. An example for a TLS 1.3 Cipher is shown here:

With TLS 1.3, the client-server TLS/SSL handshake doesn't require negotiations for key exchange and signature. This reduces the messages exchanged during the TLS/SSL handshake to just one round trip in TLS 1.3 as compared to two round trips in TLS 1.2, making TLS 1.3 faster. TLS 1.3 is also safer than TLS 1.2, as it has removed support of older, weaker, and more vulnerable algorithms which were supported in TLS 1.2 and the previous versions of TLS. TLS support in API Gateway When implementing API endpoints for your organizational needs, you can utilize API Gateway to quickly provision different types of highly scalable APIs. The three types of APIs supported by API Gateway are REST, HTTP, and WebSocket APIs. You can create an API Gateway by following the steps mentioned in the AWS documentation. Security becomes a prime concern when it comes to exposing the created API Endpoints to the client applications. API Gateway allows developers to choose a minimum version of TLS protocol to be enforced on your custom domain to provide enhanced security. Adding the security policy is supported using either AWS Management Console, AWS Command line Interface (AWS CLI), or by using AWS SDKs. To create a custom domain for API Gateway, 1. Log in to the AWS Console. 2. Navigate to API Gateway console by either searching for API Gateway or traversing through the Services menu. 3. In the API Gateway console, select "Custom domain names" as shown in the image below.

Currently, when creating the custom domain name, API Gateway provides options to choose a security policy between TLS 1.2 or TLS 1.0 as the minimum TLS version for the custom domain. The following image shows the TLS ciphers supported by an API with regional endpoint or a custom domain name with regional endpoint. Utilities like sslscan are effective at finding the TLS ciphers supported by the server.

TLS ciphers supported by an API Gateway with regional endpoint or a custom domain name with regional endpoint

API Gateway support for TLS 1.3 Currently, out of the box support for TLS 1.3 and its associated cipher suites occurs when deploying the API using edge-optimized endpoints or fronting the API using an edge-optimized custom domain name. In both cases, an AWS managed Amazon CloudFront distribution gets created using the security policy set by CloudFront, which has support for TLS 1.3 cipher suites. However, in both cases, since the CloudFront distribution created for the API is managed by AWS, it is not possible to customize the TLS version and the cipher suites used. Currently, the recommended (default) security policy for CloudFront distributions using custom SSL certificates is TLSv1.2_2021. The cipher suites supported with TLSv1.2_2021 and other security policies is highlighted in the AWS documentation for supported protocols and ciphers for CloudFront. The following image shows the TLS ciphers supported by an API with edge-optimized endpoint or using an edge-optimized custom domain name.

TLS ciphers supported by an API Gateway with edge-optimized endpoint or using an edge-optimized custom domain name

Exposing API Gateway via a custom CloudFront distribution Organizations that need flexibility in selecting specific security policies and associated TLS ciphers benefit from using an API Gateway with a regional endpoint and fronting the endpoints via a custom CloudFront distribution. Doing this allows the organizations to not only select their preferred security policy and TLS ciphers, but also use additional features provided by CloudFront including choosing default cache behavior, selecting minimum origin SSL protocol, selecting the supported HTTP versions, and adding response headers to decide on the CORS policy to be used, among others. An example architecture to achieve this looks similar to the following.

Sample architecture showing CloudFront distribution in front of a regional API Gateway

Refer to the steps highlighted in the AWS documentation for CloudFront to create the CloudFront distribution. As per the previous architecture, placing the CloudFront distribution in front of a regional API Gateway lets us configure the right Security Policy with the needed TLS 1.3 ciphers to be allowed for communication from the clients to the CloudFront. The communication between the CloudFront and the origin (Regional API Gateway) will occur based on the minimum origin SSL protocol set between CloudFront and the Origin. You can choose to limit the access to your REST Regional API Gateway endpoint exclusively to CloudFront by adding a custom header (X-Origin-Verify) to the request when CloudFront passes the request from client to API Gateway using AWS WAF in between, as highlighted in this AWS Well-Architected Lab. The same can be achieved with HTTP API Gateway endpoint using an API Gateway Lambda authorizer as highlighted in this post. The following image shows the TLS ciphers supported by the custom CloudFront distribution fronting the Regional API Gateway endpoint with security policy selected as TLSv1.

TLS ciphers supported by fronting the Regional API Gateway endpoint with security policy as TLSv1

Limiting API endpoints to support only strong cipher suites Now that we have achieved fine control on the level of security policies and associated cipher suites supported by CloudFront fronted API, let's see how this can help organizations where API endpoints are required to support only strong cipher suites and not allow handshakes from clients publishing weak cipher suites. To give a background for this use case, let's first define weak and strong ciphers. A weak cipher is one where the algorithm used to encrypt/decrypt uses a key which is not long enough, thus making it vulnerable to attacks. On the other hand, a strong cipher uses an algorithm with a longer key, thus making it more robust. You can read more about strong and weak ciphers on the owasp site. Both regional and edge optimized API Gateway endpoints have support for weak cipher suites. This makes them inappropriate to be used in organizations which have a mandate of using API endpoints supporting only strong cipher suites. The classification of cipher suites supported by regional API Gateway endpoints is shown in the following (source: ssllabs). Note that the Regional API Gateway endpoints support only TLS 1.2 as well as weak cipher suites.

Regional API Gateway endpoints support only TLS 1.2 as well as weak cipher suites

The edge optimized API Gateway endpoint falls in the same category as regional endpoints, where even though it supports TLS 1.3 along with TLS 1.2, it still has weaker cipher suites in the supported cipher suites. The classification of cipher suites in order of preference for edge optimized API Gateway endpoint is shown in the following image (source: ssllabs).

TLS version and Cipher suites supported by edge optimized API Gateway endpoint

To overcome this issue, you can use the same solution of fronting the API Gateway with CloudFront. To allow your API to use only strong cipher suites, you can configure a custom SSL certificate for the domain which can then be used as an alternate domain name for the CloudFront distribution, followed by selecting the Security policy to be the latest version. In this scenario, the security policy to be selected is TLSv1.2_2021 (recommended), as shown in the following image.

Selecting Security Policy to be used between CloudFront and clients

Looking at the classification of cipher suites where the endpoint custom domain is added as an alternate domain to the CloudFront distribution, you find that it has support for both TLS 1.3 and TLS 1.2, but it has limited its cipher suites to support only strong cipher suites. This makes the above arrangement of API Gateway fronted by CloudFront distribution with your custom domain name certificate with a security policy of TLSv1.2_2021 a perfect choice for organizations which require their API endpoints to support only the strong cipher suites for both TLS 1.3 and TLS 1.2. The following image (source: ssllabs) shows the classification of cipher suites for this arrangement.

TLS version and Cipher suites supported by API Gateway fronted by CloudFront Distribution with custom domain name certificate

Conclusion In this post, we saw how fronting API Gateway with custom CloudFront distribution provides customers with the right control and flexibility to adjust the TLS versions and the associated cipher suites for their APIs to be compliant with their organizational policies. Furthermore, by using the needed TLS ciphers, the solution also enables customers to use other associated benefits with CloudFront not limited to added reliability, low latency, and high throughput network connectivity with the help of its global edge network. Visit creating a CloudFront distribution and Amazon CloudFront Pricing to get started with CloudFront. To learn more about the best practices of building applications with CloudFront, visit AWS content delivery blogs.

Kushagra Tiwari (Kush) Kush is a Senior Applications Architect at AWS with expertise in Serverless and Containers. He has designed and architected many scalable and resilient solutions for both cloud native and hybrid products across multiple domains, including Media Systems, Finance, Insurance, Defence and Security amongst others in his 17+ years of Software experience. He is based out of Sydney and is a tea enthusiast with collection of tea leaves from over 20 countries.

Robbie Cooray Robbie Cooray is a Senior Solutions Architect at AWS since 2019, based out of Melbourne Australia. He is 11x certified, specialised provides guidance to customers in building software using serverless technologies. Robbie is passionate about designing, building and modernising enterprise solutions for retail and enterprise customers.

Simplify business intelligence identity management with Amazon QuickSight and AWS IAM Identity Center

April 2025: This post was reviewed and updated for accuracy. Amazon QuickSight is now an AWS IAM Identity Center enabled application. This capability allows administrators that subscribe to QuickSight to use IAM Identity Center to enable their users to login using their existing credentials. QuickSight is a scalable, serverless, machine learning (ML)-powered business intelligence (BI) solution that makes it simple to connect to your data, create interactive dashboards, get access to ML-enabled insights, enable natural language querying of your data, and share visuals and dashboards with tens of thousands of internal and external users, either within QuickSight itself or embedded into another application. IAM Identity Center helps you securely create or connect your workforce identities and manage their access centrally across AWS accounts and applications. IAM Identity Center is the recommended approach for workforce authentication and authorization on AWS for organizations of varying size and type. With IAM Identity Center, you can create and manage user identities in AWS, or connect your existing identity source, including Microsoft Active Directory, Okta, Ping Identity, JumpCloud, Google Workspace, and Azure Active Directory (Azure AD). With this release, administrators can create a new QuickSight account and select IAM Identity Center to configure QuickSight with their organization's identity provider or with the IAM Identity Center identity store without requiring additional single sign-on configuration in QuickSight. They can use a supported identity provider's groups to assign QuickSight roles (administrator, author and reader) to users. Amazon QuickSight users can seamlessly sign-in to QuickSight with their organization's identity provider from the AWS access portal or directly from QuickSight and share assets with users and groups. In this post, we show how to simplify your BI identity management with QuickSight and IAM Identity Center. Use case overview AnyCompany Inc. is a fictitious organization in the healthcare space. They have an application for hospitals across different regions of the country that manages business operations. AnyCompany has thousands of healthcare employees accessing the application portal. Being in the healthcare and life sciences industry, AnyCompany handles sensitive data and centrally securing user access is critical to meet business and compliance requirements. AnyCompany already configured Okta with IAM Identity Center. This configuration enables identity and access management for IAM Identity Center enabled applications. This post will go through the steps for AnyCompany to sign up to QuickSight with IAM Identity Center. Solution Overview When enabled with an external identity provider like Okta, signing up to QuickSight with IAM Identity Center enables the following sign-in flows:

The user loads the QuickSight application URL and is automatically redirected to the sign-in page of the configured IdP. Upon successful authentication, the user is redirected to QuickSight. The following diagram illustrates this flow.

The user loads the AWS access portal URL and is automatically redirected to the sign-in page of the configured IdP. After authentication, the user chooses the QuickSight application tile in the AWS access portal and is redirected to QuickSight. The following diagram illustrates this flow.

The user navigates to the application portal hosted by the IdP and is authenticated. They select the IAM Identity Center AWS access portal. The AWS access portal loads and shows enabled applications like QuickSight. The user chooses the QuickSight application tile, and is automatically redirected to QuickSight.

Prerequisites To complete this walkthrough, you must have the following:

Organizations and IAM Identity Center enabled. Okta configured as an IdP in IAM Identity Center. For instructions, refer to Okta. An AWS account for QuickSight in the same organization as IAM Identity Center. This account should not currently be subscribed to QuickSight. The administrator needs AWS Identity and Access Management (IAM) administrator access or QuickSight and IAM Identity Center permissions. For more information, refer to IAM policy examples for Amazon QuickSight.

Subscribe to QuickSight with IAM Identity Center To subscribe to QuickSight, complete the following steps:

Sign in to your AWS account and open QuickSight from the AWS Management Console. You can find it under Analytics or by searching for QuickSight.

Enter a notification email address for the QuickSight account owner or group. This email address receives service and usage notifications. Select the identity option that you want to subscribe with. For this post, we select Use AWS IAM Identity Center. Enter an account name. Choose Configure.

Next, you assign groups in IAM Identity Center to roles in QuickSight (admin, author and reader.) This step enables your users to access the QuickSight application. If you are using an external identity provider, the groups that you plan to use to assign users to roles in QuickSight need to be assigned to the IAM Identity Center application. For more details on how to do this with Okta, refer to this step from the prerequisite configuration of Okta and IAM Identity Center. If you cannot find a group from your external identity provider from QuickSight, the group likely hasn't yet been assigned to the IAM Identity Center application in your external identity provider. If you are not using an external identity provider, your IAM Identity Center groups are ready to use in QuickSight. A common way to create groups for role group assignation is to create a group of users for each role: QuickSightAdmins, QuickSightAuthors and QuickSightReaders. In Okta, you can create rules based on attributes in your external identity provider to automatically add users to those groups. Search for a group from IAM Identity Center for each QuickSight role.Note that you can add additional groups after your sign up is complete from the Manage Users page in the QuickSight administration console. From this page, click on Manage role groups. Select an IAM role to control QuickSight access to your AWS resources. For more information, see Accessing AWS resources.

Review the choices that you made, then choose Finish. Choose Go to Amazon QuickSight.

Validate user sign-in to QuickSight Now you can validate that users are able to sign-in to QuickSight using both the IdP-initiated flow and SP-initiated flow. Verify IdP initiated flow: Complete the following steps to test the IdP-initiated flow:

Navigate to the Okta console and log in with a user who is a member of one of the groups that you assigned to be QuickSight Admins, Authors or Readers.

Once logged in on the dashboard, users will be able to see the assigned Identity Center application.

Choose the AWS IAM Identity Center application from the list of applications available.

This will bring you to the IAM Identity Center landing page with a list of applications.

Choose the QuickSight application from the list to navigate to QuickSight.

Validate the SP-initiated flow: To validate the SP-initiated flow, complete the following steps:

Navigate to On the landing page, enter the account name used during QuickSight sign up, then choose Next.

Conclusion This post provided step-by-step instructions to sign up for QuickSight with IAM Identity Center and assign groups to roles in QuickSight. QuickSight automatically detects the IAM Identity Center instance in the account's organization and doesn't require additional single sign-on configuration in QuickSight. Amazon QuickSight users can seamlessly sign-in to QuickSight with their organization's identity provider from the AWS access portal or directly from QuickSight and share assets with users and groups. For more details, see Configure your Amazon QuickSight account with IAM Identity Center. For additional discussions and help getting answers to your questions, check out the QuickSight Community.

About the authors Camille Taylor is a Sr Technical Product Manager focused on QuickSight Administration and Identity Management at AWS. She has been a consultant advising Fortune 500 companies on how to derive value from their data and scale adoption of their business intelligence investments across industries. Earlier in her career, she worked in financial services consumer product valuations and analytics. In her spare time, she likes to cycle, run and engage with the latest analytical consumer tech. Srikanth Baheti is a Specialized World Wide Principal Solutions Architect for Amazon QuickSight. He started his career as a consultant and worked for multiple private and government organizations. Later he worked for PerkinElmer Health and Sciences & eResearch Technology Inc, where he was responsible for designing and developing high traffic web applications, highly scalable and maintainable data pipelines for reporting platforms using AWS services and Serverless computing. Raji Sivasubramaniam is a Sr. Solutions Architect at AWS, focusing on Analytics. Raji is specialized in architecting end-to-end Enterprise Data Management, Business Intelligence and Analytics solutions for Fortune 500 and Fortune 100 companies across the globe. She has in-depth experience in integrated healthcare data and analytics with wide variety of healthcare datasets including managed market, physician targeting and patient analytics. Sumit Dhuwalia is a Technical Account Manager at AWS focused on helping ISV customers build, deploy, and run secure, resilient, and cost-effective workloads on AWS. He has a background in analytics consulting and has helped several Fortune 500 companies across industries implement solutions spanning the entire data lifecycle, from data governance / management to advanced analytics using machine learning. Outside of work, he keeps himself busy with all things finance and reading non-fiction. Ron Cully is a Principal Product Manager at AWS where he has spent over 7 years in identity leading work on AWS IAM Identity Center, and previously on AWS Directory Service. Ron has over 25 years of industry experience in product and program management of networking and directory related products. He is passionate about delivering secure, reliable solutions that help make it easier for customers to have trustworthy experiences while taking advantage of cloud computing. Praney Mahajan is a Senior Technical Account Manager at AWS who partners with key enterprise customers as their strategic advisor. He is passionate about bridging technical solutions with business outcomes, with expertise in database, analytics, cost optimization, and operational efficiency to help customers achieve sustainable growth through innovative cloud solutions. He enjoys going on long drives with his family and playing cricket in his free time.

Bob's Used Books: A .NET Sample Application – Part 3: Infrastructure

Introduction Welcome to the third and final post in the Bob's Used Books blog post series. In the first post I discussed how to get started with Bob's Used Books and described the different debug and deployment modes you can use to test and run the application. In the second post I discussed the architecture of Bob's Used Books and provided some insight into the decisions that were made whilst building the sample application. In this post I will dive into the AWS Cloud Development Kit (AWS CDK) project that provisions AWS resources and deploys the sample application. This post references v1.0.0 of Bob's Used Books. The Bob's Used Books v1.0.0 GitHub repository can be found here. Overview The AWS CDK is an Infrastructure-as-Code (IaC) framework that enables developers to represent AWS resources and services as application code. When that application code is executed against an AWS account the resources and services defined within that code are automatically provisioned within the AWS account. IaC is often used in conjunction with DevOps to automate the provisioning of application environments as part of a CI/CD pipeline. IaC provides the following benefits:

Application environments can be version controlled by committing IaC scripts to source control Environment creation and deletion can be automated, reducing the risk of human errors Environments can be created and deleted on demand, enabling agile software development workflows and greater cost control

When an AWS CDK project is synthesized it generates an AWS CloudFormation template which is then passed to the CloudFormation service for processing. Although developers can create CloudFormation scripts directly, the AWS CDK provides higher-level constructs that typically enable developers to achieve the same outcomes with less effort. Bob's Used Books uses the AWS CDK to provision a number of AWS resources and services, including:

An Amazon Virtual Private Cloud (Amazon VPC) to host the application and its components An Amazon Simple Storage Service (Amazon S3) bucket to store book cover images An Amazon CloudFront distribution for low latency delivery of application assets An Amazon Cognito user pool and admin user for Customer Identity and Access Management (CIAM) An Amazon Relational Database Service (Amazon RDS) for SQL Server for the application backend An Amazon Elastic Cloud Compute (Amazon EC2) instance to act as the web server when the application is deployed to AWS A number of Amazon Identity and Access Management (IAM) roles, policies, and permissions that enable the application's components to communicate with each other.

As you can see, Bob's Used Books takes advantage of numerous AWS services. This is very common for applications that run in the cloud. Further, almost all applications these days have multiple non-production environments like Test and UAT in addition to a production environment. Using an IaC framework like the AWS CDK guarantees that each of those environments is provisioned quickly and consistently. With all that in mind, let's take a look at how Bob's Used Books uses the AWS CDK. CDK Stacks As discussed in the overview, applications that run in the cloud often depend upon multiple services and resources. The AWS CDK makes it easy to group these services and resources together into stacks. A stack is a group of services and resources that can be managed as a single unit. When a stack is deployed, all of the resources defined in that stack are deployed; when a stack is deleted, all the resources in that stack are deleted. For small applications it might make sense to maintain a single stack that contains all the resources for that application. As the application grows to include more services and resources though, maintaining a monolithic stack could become difficult. At that point it makes sense to split that monolithic stack into multiple stacks. Bob's Used Books has been organized into four stacks:

CoreStack – Defines an Amazon S3 bucket, an Amazon Cognito user pool, and a CloudFront distribution NetworkStack – Defines an Amazon VPC and associated resources DatabaseStack – Defines an Amazon RDS for SQL Server database EC2ComputeStack – Defines an Amazon EC2 instance that acts as the web server and deploys the application to that web server

Organizing the resources for Bob's Used Books into stacks provides greater control over resource deployment. When you are in Integrated Debugging mode (see the first post in this series for details on Integrated Debugging) you only need to deploy the Amazon S3 bucket, the Amazon CloudFront distribution, and the Amazon Cognito user pool. The application still runs locally in your development environment and doesn't require the networking resources, the database, or the web server. You deploy the resources required for Integrated Debugging by deploying CoreStack with the following command: cdk deploy BobsBookstoreCore When you want to simulate a production environment you need to deploy all of the stacks. You can do that with the following command: cdk deploy BobsBookstoreEC2 But wait, doesn't that just provision the web server and deploy the application? What about the network resources and the database? The AWS CDK project in Bob's Used Books takes advantage of a feature of the AWS CDK framework called cross-stack references. The EC2ComputeStack stack has references to resources defined in the other stacks. When you deploy the EC2ComputeStack the AWS CDK recognizes those dependencies and ensures they are also deployed. Let's take a closer look at how Bob's Used Books uses cross-stack references to manage resources. Cross-Stack References The four stacks that are defined by Bob's Used Books are instantiated in the Main method of Bookstore.Cdk/Program.cs. When EC2ComputeStack is initialized it is passed an instance of EC2ComputeStackProps: var ec2Stack = new EC2ComputeStack(app, $"{Constants.AppName}EC2", new EC2ComputeStackProps { Env = env, Vpc = networkStack.Vpc, Database = databaseStack.Database, ImageBucket = coreStack.ImageBucket, WebAppUserPool = coreStack.WebAppUserPool }); C# EC2ComputeStackProps implements the IStackProps interface and is populated with the resources created in NetworkStack, DatabaseStack, and CoreStack. For example, EC2ComputeStack deploys an Amazon EC2 instance into the VPC that was created by NetworkStack. It uses the Amazon Cognito user pool created in CoreStack to generate a user pool client app for the web server, and it uses the Amazon S3 bucket and RDS for SQL Server database to generate the appropriate access permissions for the web server. When you run cdk deploy BobsBookstoreEC2 the AWS CDK recognizes that EC2ComputeStack is dependent upon the VPC created in NetworkStack, the database created in DatabaseStack, and the bucket and user pool created in CoreStack, and it ensures they are provisioned first. Application Deployment In addition to defining compute resources for the solution, EC2ComputeStack also deploys the application to the web server. NOTE: Application deployments are typically facilitated by CI/CD pipelines rather than an IaC framework like AWS CDK, however we want to provide the .NET development community with a simple, self-contained development experience and deploying the application via the CDK achieved that goal. The Bookstore.Cdk project has a folder called EC2Artifacts that contains the following files:

bobsbookstore.conf – An Apache configuration file that defines the virtual host for Bob's Used Books. bobsbookstore.service – A Linux service that starts the application via the dotnet CLI. ssl.conf – A configuration file that is used as part of the self-signed certificate configuration. configure_ec2_web_app.sh – A bash script that configures the EC2 instance that hosts Bob's Used Books.

EC2ComputeStack uploads these files, along with the application outputs that are produced when the application is published, to an S3 bucket (this is a different bucket from the one which is defined in CoreStack). This is achieved by using the Asset class defined in the Amazon.CDK.S3.Assets namespace and is implemented in the UploadAssestsToS3 method. Once the EC2 instance is provisioned a UserData script is created that copies the files from the S3 bucket to the EC2 instance and executes configure_ec2_web_app.sh to configure the EC2 instance. configure_ec2_web_app.sh installs the latest updates, installs and configures Apache, unzips and copies Bob's Used Books to var/www/bobsbookstore, and starts bobsbookstore.service. All of these steps are performed in the ConfigureUserData method of EC2ComputeStack. Clean Up If you have deployed any of the CDK stacks that are included in Bob's Used Books you can delete the AWS services and resources that were created by opening a command-line prompt, navigating to the application solution folder, and running the following command: cdk destroy BobsBookstore* This is an important step to ensure you don't incur unnecessary costs. Conclusion Bob's Used Books takes advantage of the AWS SDK for .NET to define and provision the required AWS resources. The AWS CDK for .NET is a powerful Infrastructure-as-Code framework that enables .NET developers to represent their AWS infrastructure using a programming language with which they are familiar and productive. This is the final post in the introductory series to Bob's Used Books. The first post in this series describes how to get up and running with the application and the different debug and deployment modes that are available. The second post in the series dives deep into the architecture of Bob's Used Books and provides insight into our thought processes as we designed and built the application. This is not, however, the last you will hear about Bob's Used Books! On the contrary, we plan on using Bob's Used Books to demonstrate common .NET modernization scenarios, to demonstrate integrating with additional AWS services, and for inclusion in upcoming training and enablement content. You can download Bob's Used Books from our GitHub repository.

Brad Webber Brad is a Senior Microsoft Solutions Architect at Amazon Web Services (AWS). He started writing code when Visual Basic 6 was still a thing. These days he spends most of his time helping organizations modernize their .NET workloads and migrate them to the cloud.

Developing autonomous robots is hard. Because it requires both hardware and software to be developed, integrated and tested, doing so in a physical environment is difficult and costly. Using simulations, robot developers can test their code quickly, iterating many times before finally deploying software to the hardware for testing in the real world. Having a high-fidelity simulator allows developers to test vision, perception, and recognition systems for their robots with high confidence. Open 3D Engine (O3DE) is a community driven, open source simulator that provides that level of high-fidelity realistic rendering needed for robotic simulations. An Introduction to O3DE O3DE is a 3D engine capable of creating and running realistic 3D worlds for gaming. To achieve photo-realistic rendering, it uses a configurable, multi-threaded rendering engine known as Atom, which supports features such as physically based rendering (PBR), ray-tracing, post-processing, and forward + rendering. O3DE also is capable of simulating real-world physics through its Nvidia PhysX based physics engine. O3DE is based on a modular system architecture known as the Gems system. This allows you to select and configure any number of packages (Gems) for a project depending on its needs. Gems can come from the O3DE project itself, third-party providers, or custom made. With the recent addition of the ROS 2 gem, this enables O3DE to work with robotics simulations that use the Robotic Operating System (ROS) to develop robots. By providing scene rendering and physics modeling, robotics developers can test and train perception models that closely resemble the real world in a pure simulation environment. In order to speed up the development cycle, developers can scale out their simulations by running them in parallel in the cloud. Being able to run simulations in the cloud adds performance, cost effectiveness, and scalability to the robot development process by providing access to high performance GPU instances on demand without the need for costly investment in physical hardware or on-site infrastructure. AWS RoboMaker supports GPU-based simulation jobs by running any simulator in an OCI compliant container such as Docker. This tutorial will go over creating a Docker image from an O3DE sample application (Robot Vacuum Sample) and deploying it as a simulation job in AWS RoboMaker. Prerequisites This tutorial requires an AWS account login with Amazon ECR and AWS RoboMaker access IAM permissions. It also requires a development system, or an AWS Cloud9 development environment with the following requirements:

Linux Ubuntu 20.04 or 22.04 At least 60 GB of free disk space AWS CLI installed Docker engine installed.

Walkthrough Build the container images The Robot Vacuum Sample application comes with a Dockerfile that is used to build container images for both the simulation as well as the navigation stack. Note: All of the command line examples in this walkthrough will be based on a workspace folder $WORKSPACE, which can exist anywhere on the host system that is accessible to the currently logged in user. It is recommended to set the environment variable $WORKSPACE to $HOME/o3de-sample.

Set the environment variable for $WORKSPACE to use for this walkthrough. export WORKSPACE=$HOME/o3de-sampleBash Clone the Robot Vacuum Sample application repository. mkdir -p $WORKSPACE git clone $WORKSPACE/RobotVacuumSample Bash Build a simulation docker image named o3de_robot_vacuum_simulation. cd $WORKSPACE/RobotVacuumSample/Docker docker build --build-arg IMAGE_TYPE=simulation -t o3de_robot_vacuum_simulation . Bash Build the navigation stack docker image named o3de_robot_vacuum_navstack. cd $WORKSPACE/RobotVacuumSample/Docker docker build --build-arg IMAGE_TYPE=navstack -t o3de_robot_vacuum_navstack . Bash If your environment meets the minimum O3DE hardware requirements, you can run these docker containers by following the docker readme.

Build and push the AWS RoboMaker container images to Amazon ECR The container images created for the demo are designed to run locally on a Linux host machine. In order to run on AWS RoboMaker, the container images need to have the NICE DCV plugins installed. NICE DCV is a high-performance remote display protocol that will be used to stream the simulation from AWS RoboMaker.

Extend the simulation docker image above to enable NICE DCV by creating a file $WORKSPACE/Dockerfile.simulation.robomaker and copying the following content into it. FROM o3de_robot_vacuum_simulation:latest ENV WORKSPACE=/data/workspace WORKDIR $WORKSPACE ENV DEBIAN_FRONTEND=noninteractive

# Install NICE DCV (from RUN wget RUN gpg --import NICE-GPG-KEY RUN wget RUN tar -xvzf nice-dcv-2023.0-15487-ubuntu2204-x86_64.tgz \ && mkdir -p /etc/modprobe.d \ && apt-get install -y /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-server_2023.0.15487-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gl_2023.0.1039-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gltest_2023.0.318-1_amd64.ubuntu2204.deb

# Install xterm for direct console access RUN apt update && apt-get install -y xterm Bash Build a new docker image named o3de_robot_vacuum_simulation_robomaker. docker build -t o3de_robot_vacuum_simulation_robomaker -f $WORKSPACE/Dockerfile.simulation.robomaker $WORKSPACEBash Create a repository in the Amazon ECR for the new simulation docker image. aws ecr create-repository --repository-name o3de-robot-vacuum-simulationBash The command will return a JSON result for the new repository: { "repository": { "repositoryArn": "arn:aws:ecr:us-west-2:000000000000:repository/o3de-robot-vacuum-simulation", "registryId": "000000000000", "repositoryName": "o3de-robot-vacuum-simulation", "repositoryUri": "000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-simulation", "createdAt": 1674082385.0, "imageTagMutability": "MUTABLE", "imageScanningConfiguration": { "scanOnPush": false }, "encryptionConfiguration": { "encryptionType": "AES256" } } }

JSON Set an environment variable for the simulation Amazon ECR repository with the repositoryUri results from the JSON output. export ECR_SIMULATION_URI=000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-simulationBash Log into Amazon ECR from your local Docker runtime. aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_SIMULATION_URIBash Tag the docker image o3de_robot_vacuum_simulation_robomaker with the same repository URI. docker tag o3de_robot_vacuum_simulation_robomaker $ECR_SIMULATION_URIBash Push the new simulation docker image to Amazon ECR. docker push $ECR_SIMULATION_URIBash Note: Due to the size of the container image (~20 GB) it may take some time to complete and upload depending on your internet connectivity. Extend the navigation docker image above to enable NICE DCV by creating a file $WORKSPACE/Dockerfile.navstack.robomaker and copying the following content into it. FROM o3de_robot_vacuum_navstack:latest ENV WORKSPACE=/data/workspace WORKDIR $WORKSPACE ENV DEBIAN_FRONTEND=noninteractive

# Install NICE DCV (from RUN wget RUN gpg --import NICE-GPG-KEY RUN wget RUN tar -xvzf nice-dcv-2023.0-15487-ubuntu2204-x86_64.tgz \ && mkdir -p /etc/modprobe.d \ && apt-get install -y /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-server_2023.0.15487-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gl_2023.0.1039-1_amd64.ubuntu2204.deb \ /data/workspace/nice-dcv-2023.0-15487-ubuntu2204-x86_64/nice-dcv-gltest_2023.0.318-1_amd64.ubuntu2204.deb RUN wget

# Install xterm for direct console access RUN apt update && apt-get install -y xterm Bash Build a new container image named o3de_robot_vacuum_navstack_robomaker. docker build -t o3de_robot_vacuum_navstack_robomaker -f $WORKSPACE/Dockerfile.navstack.robomaker $WORKSPACEBash Create a repository in Amazon ECR for the new navigation stack docker image. aws ecr create-repository --repository-name o3de-robot-vacuum-navstackBash The command will return a JSON result for the new repository: { "repository": { "repositoryArn": "arn:aws:ecr:us-west-2:000000000000:repository/o3de-robot-vacuum-navstack", "registryId": "000000000000", "repositoryName": "o3de-robot-vacuum-navstack", "repositoryUri": "000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-navstack", "createdAt": 1674088085.0, "imageTagMutability": "MUTABLE", "imageScanningConfiguration": { "scanOnPush": false }, "encryptionConfiguration": { "encryptionType": "AES256" } } } JSON Set an environment variable for the navigation stack Amazon ECR repository with the repositoryUri results from the json output. export ECR_NAVSTACK_URI=000000000000.dkr.ecr.us-west-2.amazonaws.com/o3de-robot-vacuum-navstackBash Tag the docker image o3de_robot_vacuum_navstack_robomaker with the same repository URI. docker tag o3de_robot_vacuum_navstack_robomaker $ECR_NAVSTACK_URIBash Push the new navigation stack docker image to Amazon ECR. docker push $ECR_NAVSTACK_URIBash

Create an AWS RoboMaker simulation The next step is to create an AWS RoboMaker simulation using the newly created container images. The workflow involves defining the general parameters for the simulation such as an IAM role, compute type (set to CPU and GPU), and the simulation duration. You will also need to create both a Robot application and Simulation application for the simulation. Below is a step-by-step walk through of this process.

Click on 'Create simulation job' from the 'Simulation jobs' page. (verify your selected AWS region)

Configure the simulation job details and configure the 'Compute type' to 'CPU and GPU'.

Figure 2 – Configure the simulation details such as the job duration, Failure behavior, IAM role, Compute type, and Simulation/GPU Unit Limits.

Create a new robot application named 'o3de-robot-vacuum-navstack' and select 'o3de-robot-vacuum-navstack' from ECR by clicking on 'Browse ECR'. Check on 'Run with streaming session', and set 'Launch command' to '/data/workspace/LaunchNavStack.bash'.

Figure 3 – Define a new robot application based on the uploaded 'o3de-robot-vacuum-navstack' docker image

Create a new simulation application named 'o3de-robot-vacuum-simulation' and select 'o3de-robot-vacuum-simulation' from ECR by clicking on 'Browse ECR'. Check on 'Run with streaming session', and set 'Launch command' to '/data/workspace/LaunchSimulation.bash'.

Figure 4 – Define a new simulation application based on the uploaded 'o3de-robot-vacuum-simulation' docker image

Click on 'Create' to create and launch the simulation job.

Figure 6 – The status will show 'Preparing' while the simulation job is being created and launched before you are able to take control of it.

View and control the new simulation job Once the status of the simulation job changes to 'Running', the 'Connect' buttons on each tool will be available to view and/or control the simulation job.

The 'Connect' for the Simulation application will launch the O3DE launcher that is running the simulation client.

The simulation client presents a 3rd person view of the Robot Vacuum that has been placed inside of a loft scenario. At this point, the robot is awaiting commands that will be sent to it using ROS 2. To control the robot vacuum, open the RViz 3D visualization tool which is provided as part of the ROS package. This is done by clicking 'Connect' for the Robot Application.

Through the RViz tool, you can specify points to direct the Robot Vacuum to navigate to. It will use its simulated Lidar sensor to best navigate to the desired point without colliding with any obstacle. This is done by clicking '2D Goal Pose' in RViz and specifying a point in the visualized map.

As the robot navigates the room, it maps its environment with its Lidar sensor which will also be visualized in RViz. In an autonomous robot vacuum scenario, a container with the actual automated robot code will replace the sample one in this demonstration to control the robot automatically. Cleaning up The container images created in this tutorial total around 20 gigabytes of data. If you no longer need this sample robot vacuum application, you can clear the following artifacts:

The o3de-robot-vacuum-navstack and o3de-robot-vacuum-simulation repositories in ECR The o3de-robot-vacuum-navstack robot application in AWS RoboMaker under 'Robot Applications' The o3de-robot-vacuum-simulation application in AWS RoboMaker under 'Simulation Applications'

Conclusion This concludes this tutorial on how to run a robotics simulation using O3DE in AWS RoboMaker. We walked through the steps required to prepare an O3DE container image with a robotics simulation application and executed that container in AWS RoboMaker after a few configuration steps. Now that you have completed running this sample, you can try to create your own robotic simulation project with O3DE and have it run on AWS RoboMaker. To learn more about O3DE, please visit to find out more information.

Steve Pham Steve Pham is a Senior Software Development Engineer for the Game Engine & Developer Services team at AWS. As part of the development team for O3DE for the past 7 years, he has been focused on its evolution from a windows-based 3D game engine into a broader, multi-platform 3D engine.

Matt Hansen Matt Hansen is a Principal Solutions Architect specializing in Robotics and Simulation at Amazon Web Services (AWS).

We are pleased to announce that AWS Serverless Application Model (AWS SAM) now supports building serverless GraphQL APIs with AWS AppSync. AWS SAM provides a short-hand syntax for defining AWS serverless resources. AWS AppSync enables developers to connect their applications to data and events with secure, serverless, and performant GraphQL and Pub/Sub APIs. GraphQL APIs allow developers to reduce IO latency by reducing round trips and constraining payloads to only the data needed. This blog post will demonstrate building a GraphQL API with the recently released AWS::Serverless::GraphQLApi resource. New GraphQLApi Resource With the AWS SAM GraphQLApi resource, you can declare everything needed for a GraphQL API with a single resource definition. Review the AWS::Serverless::GraphQLApi documentation for details on the resource type and its properties.

AWS::Serverless::GraphQLApi: The top-level resource, including properties for API keys, caching, custom domains, GraphQL schema, logging, and tracing. Auth: A property defining nested authorization details, including AWS Lambda authorizer, Amazon Cognito User Pool, or OIDC provider DataSource: A property defining nested datasource details like AWS Lambda or Amazon DynamoDB. Function: A property to configure code, runtime, and other pipeline function properties. Resolver: A property to configure code, runtime, pipeline function execution order, and other resolver properties.

SAM Application Walkthrough This post pairs with the pre-built demonstration application to create and view social posts. Reference the git repository README for detailed instructions on how to deploy and test the application. The following diagram provides a high-level AWS architecture of the sample application.

Figure 1: The example AWS AppSync application architecture Suppose you were to define this application with AWS CloudFormation. In that case, you'd need to configure distinct CloudFormation AppSync resources for the GraphQLApi, GraphQLSchema, ApiKey, DataSource including AWS Identity and Access Management (IAM) Role and Policy, three FunctionConfiguration(s), and two pipeline Resolver(s). Instead, with the new AWS SAM transform, you can define this application with a single AWS::Serverless::GraphQLApi resource and its inline properties. Prerequisites It's beneficial to have familiarity with GraphQL, JavaScript, and NoSQL . You will need the following:

Access to an AWS account with permissions to create CloudFormation stacks, AWS AppSync APIs, AWS IAM roles and policies, and DynamoDB tables. Install the AWS Serverless Application Model Command Line Interface (AWS SAM CLI) with version >= 1.88.0. Install a GIT client in your work environment. Install the curl client utility in your environment. Install Node.js in your environment (to run the subscription example).

Step 1: Clone the application and review the SAM template Review the template.yml file on GitHub or clone the application by executing the following commands in your CLI terminal to get started.

Read through the template, and you'll see that only two AWS SAM resources are defined, a GraphQL API and a DynamoDB table referenced as a datasource for API pipeline functions. AWS SAM simplifies your effort as a developer by taking care of the following undifferentiated heavy lifting for you:

Generating a well-scoped IAM policy and role for datasources to access DynamoDB. The createPostItem and getPostFromTable functions reference the DynamoDBPostsTable resource, but you don't have to define custom IAM resources. The GraphQLPostsAPI property SchemaUri references a relative path to the sam_graphql_api/schema.graphql file. When you deploy this template, AWS SAM CLI packages this schema for you, uploads it to S3, and then fetches the file from S3 when you deploy your template. Revisions to the file are versioned in S3. The GraphQLPostsAPI Functions dictionary defines named functions like createPostItem with a CodeUri property that references a relative path to the sam_graphql_api/createPostItem.js function code. AWS SAM CLI takes care of packaging, versioning, and uploading function files to S3 No code is defined for pipeline Resolvers mutation and query, addPost, and getPost. AWS SAM generates the pipeline resolver JS code for you, corresponding to the Runtime you define as APPSYNC_JS. This API uses a simple API Key for authorization, an easy Auth type for starting a new GraphQL API. The Auth and API Key definition only takes two lines of configuration each. AWS SAM generates these with default values so you can start testing your API immediately.

Step 2: Installing the Application The next step is to deploy this application to your AWS account so that you can explore the resources it creates and interact with the GraphQL API. Run the following command and accept all the defaults.

As AWS SAM deploys your application, it will first upload your GraphQL schema and functions to Amazon S3. Then it makes an updated copy of your template so that the code URIs reference the Amazon S3 location. Finally, AWS SAM executes your template to create resources in your AWS account, like the following screenshot. Take note of the region where you installed this application.

Figure 2: Output list of AWS resources created from the AWS SAM deploy command Step 3: Testing the Application When your deployment completes, you'll see two outputs printed to your console, providing both the application's GraphQL API endpoint and API key. Save these, as you'll need both values to test your API in the following steps.

Figure 3: CloudFormation outputs APIEndpoint and ApiKeyValue for use in testing Now you can run the following curl commands to create and retrieve a post. You'll need to use the API Endpoint and API Key captured from SAM CLI output values. Copy and run the command below, replacing placeholder angle brackets with your output values.

curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"Anonymous\", content: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\", title: \"A simple post\") { author content id } }" }' \

Capture the value of the post id field from the response output and replace the POST_ID placeholder, then issue the following command to retrieve your post details.

curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "{ getPost(id: \" \") { id author title content version ups downs } }" }' \

Step 4: Testing the subscription To test a subscription on your API, you can update the src/exports.js file with your API endpoint, API key, and the region where you installed your template.

const awsmobile = { aws_project_region: " ", // region from `sam deploy` aws_appsync_graphqlEndpoint: " ", aws_appsync_region: " ", aws_appsync_authenticationType: "API_KEY", aws_appsync_apiKey: " ", }; export default awsmobile;

Once you've updated the src/exports.js file, you can run the following commands to start a web server that hosts a web page subscribed to your API

Once the web application runs locally, open the address in a browser. Next, execute some AddPost mutations against your API so that you can see them displayed on the single-page application in real-time. Again, you'll need to replace the placeholders with your API endpoints and key before you run the commands. For this step, open a second terminal; the original terminal you opened needs to continue running the web server.

curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"AWS\", content: \"Simplifies serverless resource creation\", title: \"SAM\") { author title content id ups downs version} }" }' \ curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"AWS\", content: \"Awesome managed GraphQL\", title: \"AppSync\") { author title content id ups downs version} }" }' \ curl \ --request POST \ --header 'Content-Type: application/json' \ --header 'x-api-key: ' \ --data '{ "query": "mutation AddPost { addPost(author: \"AWS\", content: \"Speedy noSQL\", title: \"DynamoDB\") { author title content id ups downs version} }" }' \

If everything worked as expected, you'll see a page similar to the following.

Figure 4: Real time subscrition result. Step 5: Cleanup To avoid accruing charges for the resources you created following the instructions in this blog post, use the following command to delete your application.

Conclusion This post reviewed the new AWS SAM features to quickly build and deploy serverless GraphQL applications with AWS AppSync. We highlighted time-saving shortcuts like generated IAM policies for datasources and file packaging to improve the developer experience for the schema file and functions. The AWS SAM resource AWS::Serverless::GraphQLApi helps developers quickly build serverless GraphQL applications. AWS AppSync provides a managed GraphQL runtime to access data or events from one or more data sources with a single API and create engaging real-time experiences by publishing data from any event source to subscribed clients with built-in security, monitoring, caching, logging, and tracing. For more information, reference the AWS Serverless Application Model (AWS SAM) and AWS AppSync documentation. For more learning resources, visit and

Kevin Schwarz Kevin is Sr. Solutions Architect. Kevin brings over 20 years of technical experience building business capabilities with depth in financial services, serverless, resiliency and IoT. Outside of work, Kevin enjoys spending time with his wife and kids, running, working out, and gardening.

Navigating the investment landscape can be challenging for anyone responsible for managing pension funds. Leaders must keep up with the latest investment trends and opportunities, all while supporting the needs of stakeholders. However, traditional investment processes can be slow and outdated, posing obstacles when it comes to managing large sums of money on behalf of workers. What if there was a transformative solution for managing pension funds? Amazon Web Services (AWS) offers personalized investment solutions for investment firms using cutting-edge machine learning (ML) technologies. With AWS, pension fund managers can benefit from improved risk management through a highly reliable cloud infrastructure and operational support. Additionally, AWS supports 143 security standards and compliance certifications, more than any other offering, which can help firms in protecting sensitive data, meeting compliance requirements, maintaining business continuity, and more. These factors can help firms minimize financial risks and maximize investment opportunities. By utilizing cloud-based services, pension managers can harness advanced data analytics, gaining valuable insights that inform investment strategies. This empowers leaders to make well-informed decisions that align with the needs of stakeholders. Pension fund management from past to present The history of the pension fund industry dates back centuries, but the modern pension fund industry can be traced back to the 19th century when pension plans were established for government employees in the United States and Europe. Today, the pension fund industry is a global industry with trillions of dollars in assets under management. Before the advent of cloud technology, pension fund investment firms operated through manual processes involving a lot of paperwork, phone calls, and in-person meetings. Investment firms conducted research and analysis to identify investment opportunities; traders executed trades through brokers and trading floors; portfolio managers decided which assets to buy and sell; and reports were generally provided through paper statements or in-person meetings. As the pension fund industry has evolved over the centuries, so too has the technology used to manage and invest vast amounts of money. In the past, investment firms relied on manual processes and paper-based reporting to make informed decisions about which assets to buy and sell. However, the challenges facing pension funds today need a more sophisticated approach, one that uses data analytics and cloud technology to maximize returns while minimizing risk. How can cloud technology help pension fund managers? Pension funds manage and analyze vast amounts of data, such as investment data, member data, and regulatory data. They need to use this data to make informed investment decisions, manage risks, and comply with regulatory requirements. AWS provides a suite of data analytics tools, such as Amazon Redshift, Amazon EMR, Amazon Glue, and Amazon QuickSight, which enable pension fund managers to process and analyze large volumes of data quickly and simply. These tools can also help identify patterns and trends that are not readily noticeable to human analysts, allowing pension fund managers to make more informed investment decisions. The National Bank of Canada's Global Equity Derivatives Group (GED) used AWS to process and analyze hundreds of terabytes of financial data, conduct data manipulations in one minute instead of days, and scale and optimize its operations. Pension funds must also manage risk. The complexity of financial instruments has made it difficult to effectively manage different types of risks, such as market risk, credit risk, and operational risk. High performance computing (HPC) on AWS offers cloud-based services that provide access to HPC resources such as networking, storage, and compute on demand. By utilizing HPC, pension funds can perform complex risk simulations and stress tests to manage and mitigate risk more efficiently without the need to manage and maintain the on-premises infrastructure. For example, the Bank of Montreal (BMO) used AWS to build an elastic platform for calculating risk metrics, run multiple risk metrics and regulatory stress calculations in parallel, and scale computational capacity for future regulatory requirements. Using HPC on AWS, pension funds can analyze vast amounts of data, run simulations and models, and identify potential risks and opportunities more accurately. Additionally, HPC can reduce the time needed for complex calculations, resulting in faster decision-making and shorter time-to-market for new investment products. Through HPC on AWS, pension funds can simulate different market scenarios and evaluate their impact on the fund's portfolio. Cybersecurity is another significant challenge for the pension fund industry. Pension funds deal with vast amounts of sensitive data, including personal information of members and investment data. As cyber threats become more sophisticated, pension funds must have robust cybersecurity measures in place. AWS offers various cloud-based services that can help pension funds strengthen their cybersecurity strategies. These services include Amazon GuardDuty, AWS Security Hub, AWS Identity and Access Management (IAM), and Amazon Inspector. GuardDuty and Security Hub can provide continuous monitoring of the pension fund's AWS environment for threats and vulnerabilities, while IAM allows administrators to manage user access to resources securely. Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure. Finally, pension funds must meet increasing demand for personalized investment solutions. Members want more control over their retirement savings, and they expect pension funds to provide tailored investment solutions that meet their individual needs. AWS provides a suite of services, such as Amazon Bedrock, Amazon SageMaker, and Amazon Personalize, which can help pension funds develop personalized investment solutions for their members. These technologies use machine learning (ML) and artificial intelligence (AI) to provide investment advice based on an individual's goals, risk tolerance, and financial situation. What's next for pension fund management in the cloud? To embrace the benefits of cloud technology, pension fund managers can assess the following strategies for using the cloud:

Establish a digital transformation strategy – Develop a comprehensive plan for using technology to improve operations, reduce costs, and enhance the customer experience. Invest in data analytics tools – Implement tools such as AI and ML to help manage and analyze data more efficiently and accurately. Prioritize cybersecurity – Make sure that robust cybersecurity measures are in place to protect sensitive member and investment data. Embrace cloud technology – Develop a cloud strategy that aligns with business goals and objectives and assess the suitability of cloud providers. Customers can use the AWS Cloud Adoption Framework (AWS CAF) to identify their cloud enabled strategic business outcomes. Provide personalized investment solutions – Invest in robo-advisory technologies to provide personalized investment advice to members. You can build a conversational interface such as chatbots and virtual assistants to provide personalized experience for your customers using Amazon Bedrock, a fully managed service that makes foundational models (FMs) from leading AI startups accessible via an API. Foster a culture of innovation – Encourage employees to experiment with new technologies and approaches and provide resources and support to help them do so. AWS Skill Builder provides resources and training to help customers acquire the necessary skills and knowledge to build and deploy applications on the AWS. It offers a range of learning paths, hands-on labs, and self-paced courses designed to enhance proficiency in AWS services, architectures, and best practices.

Conclusion As the pension fund industry evolves to meet the dynamic needs of their members, technology can help address some of these emerging needs. AI, ML, advanced analytics, and robo-advisory technologies can help pension funds manage data, mitigate risk, improve cybersecurity, and provide personalized investment solutions. As the pension funds industry continues to evolve, pension fund managers can embrace technology to provide better retirement savings solutions for their members. Related resources:

AWS for Capital Markets hub AWS Financial Services Competency Partners AWS Solutions Library: Financial data management solutions AWS Solutions Library: Risk management platforms

TAGS: cybersecurity, digital transformation, federal financial, financial services, public sector

John Hakim John Hakim is a senior solutions architect on the Canadian public sector financial services team at Amazon Web Services (AWS). He develops technical strategies that empower his financial services clients to develop quickly and make the most of cloud economics. With John's guidance, clients can seamlessly implement new cloud-based systems or migrate existing on-premises workloads. John makes sure his clients stay competitive in this fast-paced industry by providing guidance and support every step of the way.

Integrating IBM MQ with Amazon SQS and Amazon SNS using Apache Camel

This post is written by Joaquin Rinaudo, Principal Security Consultant and Gezim Musliaj, DevOps Consultant. IBM MQ is a message-oriented middleware (MOM) product used by many enterprise organizations, including global banks, airlines, and healthcare and insurance companies. Customers often ask us for guidance on how they can integrate their existing on-premises MOM systems with new applications running in the cloud. They're looking for a cost-effective, scalable and low-effort solution that enables them to send and receive messages from their cloud applications to these messaging systems. This blog post shows how to set up a bi-directional bridge from on-premises IBM MQ to Amazon MQ, Amazon Simple Queue Service (Amazon SQS), and Amazon Simple Notification Service (Amazon SNS). This allows your producer and consumer applications to integrate using fully managed AWS messaging services and Apache Camel. Learn how to deploy such a solution and how to test the running integration using SNS, SQS, and a demo IBM MQ cluster environment running on Amazon Elastic Container Service (ECS) with AWS Fargate. This solution can also be used as part of a step-by-step migration using the approach described in the blog post Migrating from IBM MQ to Amazon MQ using a phased approach. Solution overview The integration consists of an Apache Camel broker cluster that bi-directionally integrates an IBM MQ system and target systems, such as Amazon MQ running ActiveMQ, SNS topics, or SQS queues. In the following example, AWS services, in this case AWS Lambda and SQS, receive messages published to IBM MQ via an SNS topic:

The cloud message consumers (Lambda and SQS) subscribe to the solution's target SNS topic. The Apache Camel broker connects to IBM MQ using secrets stored in AWS Secrets Manager and reads new messages from the queue using IBM MQ's Java library. Only IBM MQ messages are supported as a source. The Apache Camel broker publishes these new messages to the target SNS topic. It uses the Amazon SNS Extended Client Library for Java to store any messages larger than 256 KB in an Amazon Simple Storage Service (Amazon S3) bucket. Apache Camel stores any message that cannot be delivered to SNS after two retries in an S3 dead letter queue bucket.

The next diagram demonstrates how the solution sends messages back from an SQS queue to IBM MQ:

A sample message producer using Lambda sends messages to an SQS queue. It uses the Amazon SQS Extended Client Library for Java to send messages larger than 256 KB. The Apache Camel broker receives the messages published to SQS, using the SQS Extended Client Library if needed. The Apache Camel broker sends the message to the IBM MQ target queue. As before, the broker stores messages that cannot be delivered to IBM MQ in the S3 dead letter queue bucket.

Deploy the broker service to allow reading messages from and writing to existing IBM MQ queues. Once the consumer or producer is migrated, migrate its counterpart to the newly selected service (SNS or SQS).

Next, you will learn how to set up the solution using the AWS Cloud Development Kit (AWS CDK). Deploying the solution Prerequisites

Step 1: Cloning the repository Clone the repository using git: git clone Step 2: Setting up test IBM MQ credentials This demo uses IBM MQ's mutual TLS authentication. To do this, you must generate X.509 certificates and store them in AWS Secrets Manager by running the following commands in the app folder:

Generate X.509 certificates: ./deploy.sh generate_secretsBash Set up the secrets required for the Apache Camel broker (replace with, for example, dev): ./deploy.sh create_secrets broker Bash Set up secrets for the mock IBM MQ system: ./deploy.sh create_secrets mockBash Update the cdk.json file with the secrets ARN output from the previous commands:

If you are using your own IBM MQ system and already have X.509 certificates available, you can use the script to upload those certificates to AWS Secrets Manager after running the script. Step 3: Configuring the broker The solution deploys two brokers, one to read messages from the test IBM MQ system and one to send messages back. A separate Apache Camel cluster is used per integration to support better use of Auto Scaling functionality and to avoid issues across different integration operations (consuming and reading messages). Update the cdk.json file with the following values:

accountId: AWS account ID to deploy the solution to. region: name of the AWS Region to deploy the solution to. defaultVPCId: specify a VPC ID for an existing VPC in the AWS account where the broker and mock are deployed. allowedPrincipals: add your account ARN (e.g., arn:aws:iam::123456789012:root) to allow this AWS account to send messages to and receive messages from the broker. You can use this parameter to set up cross-account relationships for both SQS and SNS integrations and support multiple consumers and producers.

Make sure you have the correct AWS_PROFILE and AWS_REGION environment variables set for your development account. Run yarn cdk bootstrap –-qualifier mq / to bootstrap CDK. Run yarn install to install CDK dependencies. Finally, execute yarn cdk deploy '*-dev' –-qualifier mq --require-approval never to deploy the solution to the dev environment.

Step 5: Testing the integrations Use AWS System Manager Session Manager and port forwarding to establish tunnels to the test IBM MQ instance to access the web console and send messages manually. For more information on port forwarding, see Amazon EC2 instance port forwarding with AWS System Manager.

In a command line terminal, make sure you have the correct AWS_PROFILE and AWS_REGION environment variables set for your development account. In addition, set the following environment variables:

IBM_ENDPOINT: endpoint for IBM MQ. Example: network load balancer for IBM mock mqmoc-mqada-1234567890.elb.eu-west-1.amazonaws.com. BASTION_ID: instance ID for the bastion host. You can retrieve this output from Step 4: Bootstrapping and deploying the solution listed after the mqBastionStack deployment. Use the following command to set the environment variables: export IBM_ENDPOINT=mqmoc-mqada-1234567890.elb.eu-west-1.amazonaws.com export BASTION_ID=i-0a1b2c3d4e5f67890Bash Run the script test/connect.sh. Log in to the IBM web console via using the default IBM user (admin) and the password stored in AWS Secrets Manager as mqAdapterIbmMockAdminPassword.

In the IBM MQ console, access the local queue manager QM1 and DEV.QUEUE.1. Send a message with the content Hello AWS. This message will be processed by AWS Fargate and published to SNS. Access the SQS console and choose the snsIntegrationStack-dev-2 prefix queue. This is an SQS queue subscribed to the SNS topic for testing. Select Send and receive message. Select Poll for messages to see the Hello AWS message previously sent to IBM MQ.

Access the SQS console and choose the queue with the prefix sqsPublishIntegrationStack-dev-3-dev. Select Send and receive messages. For Message Body, add Hello from AWS. Choose Send message. In the IBM MQ console, access the local queue manager QM1 and DEV.QUEUE.2 to find your message listed under this queue.

Step 6: Cleaning up Run cdk destroy '*-dev' to destroy the resources deployed as part of this walkthrough. Conclusion In this blog, you learned how you can exchange messages between IBM MQ and your cloud applications using Amazon SQS and Amazon SNS. If you're interested in getting started with your own integration, follow the README file in the GitHub repository. If you're migrating existing applications using industry-standard APIs and protocols such as JMS, NMS, or AMQP 1.0, consider integrating with Amazon MQ using the steps provided in the repository. If you're interested in running Apache Camel in Kubernetes, you can also adapt the architecture to use Apache Camel K instead. For more serverless learning resources, visit Serverless Land.

This post was contributed by Laurent Fabre, Chief Technology Officer and his team from Reezocar, and Ilan Gleiser, Pr. Specialist, Global Impact Computing, AWS and our team. Computer vision has enormous potential to revolutionize the way we think about sustainability and the circular economy. One of the key ways in which computer vision is already being applied to these areas is using powerful graphics processing units (GPUs) and AWS cloud computing services. One of the primary applications of computer vision in the circular economy is in product lifecycles. By using advanced computer vision algorithms and machine learning models, manufacturers and supply chain managers can more accurately track and analyze the lifecycle of products, from raw materials sourcing all the way through to end-of-life disposal. This allows companies to better understand the environmental impacts of their products and make more informed decisions about how to design and produce products that are more sustainable and can be easily reused or recycled. Another area in which computer vision is being applied to the circular economy is in waste reduction. By using advanced image recognition algorithms and computer vision technologies, waste management companies can better sort and categorize recyclable materials, making it easier and more cost-effective to recover and reuse valuable resources. In addition to these applications, computer vision is also being used to identify opportunities for energy and resource savings, and to monitor and analyze environmental impacts at both a local and global scale. For example, satellite imagery and other forms of remote sensing data can be used to track deforestation, monitor ocean pollution levels, and even create early warning systems to predict the impact of natural disasters on ecosystems. In this post, we'll explore how Reezocar is using computer vision to change the way they detect car damage and price used vehicles for re-sale in secondary markets. This technology can be used to determine the useful life of a car and potentially reduce the need for landfill waste, therefore aligning with the goals of the circular economy: designing-out waste and pollution from the environment. Secondary markets play a critical role in achieving a circular economy by extending the lifespan of products and reducing waste. The benefits of secondary markets go beyond environmental impact, as they also generate economic opportunities, job creation, and community empowerment. In this context, computer vision is revolutionizing the way we detect car damage. By leveraging the power of machine learning, computer vision can detect even the smallest dents and scratches on a car's body. Overall, the applications of computer vision in the circular economy are wide-ranging and growing rapidly. By enabling more efficient and sustainable resource use, computer vision has the power to drive real change in our economy and help create a more sustainable future for us all. Who is Reezocar? Reezocar is an online platform for buying and selling used cars in France. It offers customers a safe purchase guarantee and a bespoke shopping help service. With access to over six-million car ads, Reezocar makes it easy to select makes and models that match a buyer's search criteria. Customers also have the option of purchasing a certified car, with a 15-day money back guarantee included. Reezocar uses GPU-accelerated machine learning algorithms, convolutional neural networks, and a damage estimation model to calculate car prices. This system helps customers get the best deal on their purchase, as well as an accurate assessment of the vehicle's condition. The result is a fair price estimate of the car and reassurance that they're getting a good value for their money. More than 10,000 customers have already been won over by the Reezocar experience, making it one of the top on-line marketplaces for buying and selling used cars. With its reliable customer service and advanced technology, Reezocar is quickly becoming the go-to destination for those looking to purchase a used car. Reezocar is committed to its environmental mission and employs various strategies to uphold its climate-conscious values. Here are four key approaches they employ:

Extending Vehicle Lifespan: By refurbishing selected vehicles, they have successfully prolonged their lifespan by up to five years, using approximately 5% of their manufacturer's suggested retail price (MSRP) as refurbishing budget. This initiative has prevented over 30,000 tons of waste from ending up in landfills. Carbon Offset Initiatives: To counterbalance the CO2 emissions produced by these vehicles, they consistently engage in carbon offset practices through their partnership with ReforestAction. Through this collaboration, they have planted 33,000 trees, effectively trapping an estimated 5,000 tons of CO2. Discouraging New Car Production: Recognizing that the most environmentally friendly car is the one that doesn't need to be built, they actively discourage the production of new cars by prioritizing the refurbishment of used vehicles. This approach has resulted in an estimated avoidance of 40,000 tons of material, considering the increasing weight of modern cars. Efficient Supply Chain: By advocating for a shorter supply chain, they significantly reduce energy consumption during the delivery of vehicles. Additionally, this approach helps them avoid the costs associated with waste management while minimizing the pollution caused by car-related materials such as plastics, thus safeguarding the surrounding ecosystem. With a decade of experience and numerous satisfied customers, Reezocar remains dedicated to leading the way in climate action.

The rise of computer vision For decades, car buyers and dealers have relied on manual inspection to detect body damage on used cars. Until recently, physical inspections were the only way to get a full picture of the car's condition. Even then, most damage could only be detected with close examination from a trained eye. The technology leverages a combination of machine learning algorithms and convolutional neural networks to detect dents in vehicles, resulting in a faster, more streamlined, and accurate process. Initially, the technology employs machine learning algorithms to estimate an initial value of the car based on features such as its model and gearbox type (automatic vs. standard). Next, Computer vision is used to process the car's image and identify any form of damage, like scratches, dents, or severe degradation. Any detected damage will be factored into the initial value estimate by subtracting the repair costs. The proposed computer vision system determines the specific parts of the car that are damaged, which helps identify the necessary repairs and their costs. After computing the repair cost, Reezocar generates a post-repair estimate of the car's value. This estimate considers factors like age, condition, and make to ensure maximum accuracy. With these tools at their disposal, dealerships can offer more competitive pricing and decide whether a car should be refurbished or sent to the landfill. Reezocar reference architecture

Reezocar's Reference architecture aims to detect car dents using computer vision, as illustrated in Figure 1. Their approach consists of three steps: acquiring a dataset, fine tuning a convolutional neural network, and measuring the success of the model. First, they acquired and combined a synthetic and a real-world dataset to ensure a diverse and robust sample dataset. Using Amazon EC2 instances, they generated a synthetic dataset by algorithmically deforming 3D CAD models of cars. This helped to create a labeled synthetic dataset of images. Afterwards, they used Amazon SageMaker Ground Truth to annotate a set of real-world images. To achieve this, Reezocar uses Amazon SageMaker Ground Truth to acquire and annotate data, and then employs machine learning models running on GPUs to train a damage estimation model. By using Amazon SageMaker Ground Truth, Reezocar was able to avoid the need to manage their own data labeling workforce, which would have slowed down innovation and increased costs. This approach allowed Reezocar to focus on their core competencies and optimize their resources for maximum efficiency. Second, they leveraged GPUs to fine tune a convolutional neural network (CNN) such as Mask R-CNN or Detectron to do object segmentation and damage detection. This CNN model is used to segment car parts and detect car dents. Reezocar adopted a gradual approach to tackle the dent detection problem. They aimed to ensure that their system could effectively differentiate between damaged and undamaged cars. Once they were satisfied with the performance of the initial model, they submitted images with low confidence for undamaged cars and images of damaged cars to Amazon SageMaker Ground Truth to obtain around 4000 more detailed and accurate annotations, which indicate which parts are damaged. This annotated dataset was then used to train an object detection model that could accurately identify the damaged parts of a car. Reezocar uses AWS Batch to run inference and augment their data with damage information. Reezocar triggers AWS Batch conditionally by leveraging AWS Step Functions and an Amazon SQS queue. The Step Functions workflow showcased in Reference 1 checks for the arrival of a certain number of events related to the car damage data in the SQS queue. Once the required number of events is present, AWS Step Functions triggers AWS Batch to process the data using the proposed computer vision algorithm on P4d instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of the batch jobs submitted. P4d instances are a specific type of Amazon EC2 instance family that is optimized for high-performance computing and includes powerful NVIDIA GPUs. These instances provide the necessary computational power for running machine learning models and computer vision algorithms efficiently, making them suitable for Reezocar's car damage detection system. This combination of Step Functions, SQS, and AWS Batch allows Reezocar to efficiently process the data. Evaluating the model is in the eye of the beholder Finally, to evaluate the performance of their dent detection models, Reezocar used a combination of accuracy and mean Average Precision (mAP) metrics. The accuracy metric was used to assess the classifier's performance, while the mAP metric was used to evaluate the object detection model's performance. They set a target accuracy of 0.85 and a target mAP of 0.9 to ensure high levels of accuracy and reliability in their results. These metrics allowed Reezocar to measure the success of their object detection model and assess the quality of the system's output. The process is illustrated in Figure 2.

Figure 2: The damage detection system developed by Reezocar combines the generation of a synthetic dataset with real-world data labeling to ensure a diverse and robust dataset. The system is designed to detect relevant images that require labeling, which contributes to the overall efficiency and effectiveness of the damage detection system.

Reezocar's objective is to determine the refurbishing cost of cars, which involves an additional step to detect the severity of the damage in parts. The severity assessment process is illustrated in Figure 3, and the severity estimation is used to determine the repair cost of the damaged part. Table 1 shows that the repair cost estimation method involves applying a percentage of the initial price of the part, which varies according to the severity of the damage. In other words, different percentages are used based on the severity of the damage to estimate the repair cost accurately.

Figure 3: Damage severity assessment pipeline. The severity assessment is based on the normal consistency of the damaged part.

Table 1: Repair cost. Reezocar uses a percentage of the initial price of the damaged part to estimate the repair cost. The repair cost estimation process involves applying different percentages based on the severity of the damage. Through the implementation of their damage detection pipeline, Reezocar has achieved a high degree of automation with a precision rate of 86.7%. However, due to challenging light conditions and reflective materials, some cars are being mislabeled as damaged. Reezocar has identified a mislabeling rate of 15.7%, and are currently labeling data in order to retrain the damage detection model. The team was able to achieve a notable performance improvement by using Amazon Sagemaker Ground Truth to obtain 4000 labeled images, resulting in an increase in their mean Average Precision (MAP) from 0.22 to 0.3. This success has validated the effectiveness of their data-labeling pipeline and has motivated the company to pursue further data acquisition for labeling to enhance the performance of their system. As part of this effort, Reezocar is actively collecting images of damaged cars and labeling them to reach their goal of a MAP of 0.9. In the figures below, we see some qualitative results of the proposed damage detection model. For example, in Figure 4, an image of a car with damaged parts is displayed, and the proposed model accurately detects and segments the damaged parts, as shown in the second row, first column. Additionally, they used a car-part segmentation model (See Row 1; Column 3 of Figure 4) to identify which specific part is damaged.

Figure 5 shows an image of an undamaged car, while Figure 6 showcases the performance of the proposed method on scratched parts of a car, providing a closer look at the results.

Conclusion The use of computer vision to detect car dents is quickly becoming a game-changer in the automotive industry. Machine learning and convolutional neural networks are allowing for a more accurate detection of car dents than ever before, leading to improved repair and maintenance processes. Companies like Reezocar are leveraging computer vision-based damage estimation models to accurately and efficiently calculate car prices. Thanks to these models, Reezocar is now able to calculate the car price with a Mean Absolute Percentage (MAP) error of just 6%, improving the accuracy and efficiency of their pricing process. This technology is not only helping to extend the life of cars but also to reduce landfill waste. It's also helping to make the car buying process simpler and more transparent. As computer vision technology continues to improve, the way we detect and repair car dents will likely change as well. By combining the power of Amazon EC2 instances, NVIDIA GPUs, AWS Batch linked by Elastic Fabric Adapter (EFA), and machine learning powered computer vision models, Reezocar can accurately detect car dents, ultimately extending the lifespan of selected cars by up to 5 years. By refurbishing these cars and reselling them in the secondary market, Reezocar helps reduce landfill waste and promote circularity.

Laurent Fabre Laurent Fabre is the CTO of Reezocar, a leading online platform for buying and selling used cars. Laurent brings over two decades of experience in the IT industry. He is a highly skilled Cybersecurity expert, having trained at Airbus, where he developed expertise in cryptography and secure programming techniques. He also studied mathematics, with a focus on operations research, which has enabled him to leverage data analytics and metrics to solve complex problems and achieve optimal solutions. Throughout his career, Laurent has taken on numerous mission-critical tasks for both civilian and military-grade projects, often on short notice. Outside of work, Laurent indulges his passion for interpretive dancing of quantum physics, which allows him to combine his love of science and art.

Atef Shaar Atef Shaar is currently a Lead Data at Reezocar. He is developing new machine-learning-based applications for the automobile industry. Additionally, he is managing a team of data engineers and data scientists. Prior to this work, Atef Shaar worked as a Research Engineer at Télécom Paris. He graduated with a Ph.D. degree from Télécom Paris in 2018. He was a visiting research student at the National University of Singapore in 2015. He completed a master's study in International Business at Grenoble Graduate School of Business, after earning an engineering degree. His research work is related to machine learning and its application in multiple domains including marketing, distributed storage systems, and the automotive industry.

Julien Maksoud Julien Maksoud is a highly skilled engineer with over a decade of experience in the Oil & Gas industry. As a consultant, he supported various companies in Europe and Africa in analyzing their data using machine learning-based methods, providing valuable insights to decision-makers. Julien's passion for solving complex problems using machine learning led him to pursue an advanced Master's degree from Télécom Paris, which equipped him with a deeper understanding of data science, including machine learning and deep learning frameworks. Currently, Julien is working as a Data Engineer at Reezocar, where he leverages his expertise to create data pipelines on AWS for ETL and data analysis. His experience in both data engineering and data science enables him to assist with model deployment and maintenance as well. Julien is committed to delivering results that enable organizations to make informed decisions based on data-driven insights.

Tarek Ben Charrada Tarek Ben Charrada graduated with distinction, ranking 28th out of over 2800 candidates in the highly competitive "concours d'entrée aux grandes écoles" to go and earn his engineering degree from Ecole polytechnique de Tunisie. He then pursued his PhD in 3D reconstruction from a single image, demonstrating his commitment to advancing the field of computer vision and computer graphics. Today, Tarek's expertise is sought-after as a data scientist, where he leverages his skills in computer vision, speech processing and differentiable privacy to push the boundaries of what's possible in these exciting fields.

TAGS: AWS Batch, AWS ParallelCluster, HPC, Machine Learning, ML, simulations, Sustainability

Ilan Gleiser Ilan Gleiser is a Principal GenAI Specialist at AWS WWSO Frameworks team focusing on developing scalable Artificial General Intelligence architectures and optimizing foundation model training and inference. With a rich background in AI and machine learning, Ilan has published over 20 blogs and delivered 100+ prototypes globally over the last 5 years. Ilan holds a Master's degree in mathematical economics.

Francis Laurens Francis Laurens is a Senior Solutions Architect at AWS focusing on helping customers to find out the right architecture for their needs. He has a strong startup background having worked for top French Startups. During his time there, his technical proficiency in data topics helped out those startups on their path to success. He has started to use AWS in production 10 years ago when he was leading one of the first migration to Amazon Redshift in Europe.

Intelligent video and audio Q&A with multilingual support using LLMs on Amazon SageMaker

Digital assets are vital visual representations of products, services, culture, and brand identity for businesses in an increasingly digital world. Digital assets, together with recorded user behavior, can facilitate customer engagement by offering interactive and personalized experiences, allowing companies to connect with their target audience on a deeper level. Efficiently discovering and searching for specific content within digital assets is crucial for businesses to optimize workflows, streamline collaboration, and deliver relevant content to the right audience. According to a study, by 2021, videos already make up 81% of all consumer internet traffic. This observation comes as no surprise because video and audio are powerful mediums offering more immersive experiences and naturally engages target audiences on a higher emotional level. As companies accumulate large volumes of digital assets, it becomes more challenging to organize and manage them effectively to maximize their value. Traditionally, companies attach metadata, such as keywords, titles, and descriptions, to these digital assets to facilitate search and retrieval of relevant content. But this requires a well-designed digital asset management system and additional efforts to store these assets in the first place. In reality, most of the digital assets lack informative metadata that enables efficient content search. Additionally, you often need to do an analysis of different segments of the whole file and discover the concepts that are covered there. This is time consuming and requires a lot of manual effort. Generative AI, particularly in the realm of natural language processing and understanding (NLP and NLU), has revolutionized the way we comprehend and analyze text, enabling us to gain deeper insights efficiently and at scale. The advancements in large language models (LLMs) have led to richer representations of texts, which provides better search capabilities for digital assets. Retrieval Augmented Generation (RAG), built on top of LLMs and advanced prompt techniques, is a popular approach to provide more accurate answers based on information hidden in the enterprise digital asset store. By taking advantage of embedding models of LLMs, and powerful indexers and retrievers, RAG can comprehend and process spoken or written queries and quickly find the most relevant information in the knowledge base. Previous studies have shown how RAG can be applied to provide a Q&A solution connecting with an enterprise's private domain knowledge. However, among all types of digital assets, video and audio assets are the most common and important. The RAG-based video/audio question answering solution can potentially solve business problems of locating training and reference materials that are in the form of non-text content. With limited tags or metadata associated of these assets, the solution is trying to make users interact with the chatbot and get answers to their queries, which could be links to specific video training ("I need link to Amazon S3 data storage training") links to documents ("I need link to learn about machine learning"), or questions that were covered in the videos ("Tell me how to create an S3 bucket"). The response from the chatbot will be able to directly answer the question and also include the links to the source videos with the specific timestamp of the contents that are most relevant to the user's request. In this post, we demonstrate how to use the power of RAG in building a Q&A solution for video and audio assets on Amazon SageMaker. Solution overview The following diagram illustrates the solution architecture.

Convert video to text with a speech-to-text model and text alignment with videos and organization. We store the data in Amazon Simple Storage Service (Amazon S3). Enable intelligent video search using a RAG approach with LLMs and LangChain. Users can get answers generated by LLMs and relevant sources with timestamps. Build a multi-functional chatbot using LLMs with SageMaker, where the two aforementioned solutions are wrapped and deployed.

For a detailed implementation, refer to the GitHub repo. Prerequisites You need an AWS account with an AWS Identity and Access Management (IAM) role with permissions to manage resources created as part of the solution. For details, refer to create an AWS account. If this is your first time working with Amazon SageMaker Studio, you first need to create a SageMaker domain. Additionally, you may need to request a service quota increase for the corresponding SageMaker processing and hosting instances. For preprocessing the video data, we use an ml.p3.2xlarge SageMaker processing instance. For hosting Falcon-40B, we use an ml.g5.12xlarge SageMaker hosting instance. Convert video to text with a speech-to-text model and sentence embedding model To be able to search through video or audio digital assets and provide contextual information from videos to LLMs, we need to convert all the media content to text and then follow the general approaches in NLP to process the text data. To make our solution more flexible to handle different scenarios, we provide the following options for this task:

Amazon Transcribe and Amazon Translate – If each video and audio file only contains one language, we highly recommend that you choose Amazon Transcribe, which is an AWS managed service to transcribe audio and video files. If you need to translate them into the same language, Amazon Translate is another AWS managed service, which supports multilingual translation. Whisper – In real-world use cases, video data may include multiple languages, such as foreign language learning videos. Whisper is a multitasking speech recognition model that can perform multilingual speech recognition, speech translation, and language identification. You can use a Whisper model to detect and transcribe different languages on video data, and then translate all the different languages into one language. It's important for most RAG solutions to run on the knowledge base with the same language. Even though OpenAI provides the Whisper API, for this post, we use the Whisper model from Hugging Face.

We run this task with an Amazon SageMaker Processing job on existing data. You can refer to data_preparation.ipynb for the details of how to run this task. Convert video data to audio data Because Amazon Transcribe can handle both video and audio data and the Whisper model can only accept audio data, to make both options work, we need to convert video data to audio data. In the following code, we use VideoFileClip from the library moviepy to run this job: from moviepy.editor import VideoFileClip

video = VideoFileClip(video_path) video.audio.write_audiofile(audio_path)Python Transcribe audio data When the audio data is ready, we can choose from our two transcribing options. You can choose the optimal option based on your own use case with the criteria we mentioned earlier. Option 1: Amazon Transcribe and Amazon Translate The first option is to use Amazon AI services, such as Amazon Transcribe and Amazon Translate, to get the transcriptions of the video and audio datasets. You can refer to the following GitHub example when choosing this option. Option 2: Whisper A Whisper model can handle audio data up to 30 seconds in duration. To handle large audio data, we adopt transformers.pipeline to run inference with Whisper. When searching relevant video clips or generating contents with RAG, timestamps for the relevant clips are the important references. Therefore, we turn return_timestamps on to get outputs with timestamps. By setting the parameter language in generate_kwargs, all the different languages in one video file are transcribed and translated into the same language. stride_length_s is the length of stride on the left and right of each chunk. With this parameter, we can make the Whisper model see more context when doing inference on each chunk, which will lead to a more accurate result. See the following code: from transformers import pipeline import torch

device = "cuda:0" if torch.cuda.is_available() else "cpu" pipe = pipeline( "automatic-speech-recognition", model=f"openai/{whisper_model}", device=device )

generate_kwargs = {"task":"transcribe", "language":f" "} prediction = pipe( file_path, return_timestamps=True, chunk_length_s=30, stride_length_s=(5), generate_kwargs=generate_kwargs )Python The output of pipe is the dictionary format data with items of text and chunks. text contains the entire transcribed result, and chunks consists of chunks with the timestamp and corresponding transcribed result (see the following screenshot). We use data in chunks to do further processing.

As the preceding screenshot shows, lot of sentences have been cut off and split into different chunks. To make the chunks more meaningful, we need to combine sentences cut off and update timestamps in the next step. Organize sentences We use a very simple rule to combine sentences. When the chunk ends with a period (.), we don't make any change; otherwise, we concatenate it with the next chunk. The following code snippet explains how we make this change: prev_chunk = None new_chunks = [] for chunk in chunks: if prev_chunk: chunk['text'] = prev_chunk['text'] + chunk['text'] chunk['timestamp'] = (prev_chunk['timestamp'][0], chunk['timestamp'][1])

if not chunk['text'].endswith('.'): prev_chunk = chunk else: new_chunks.append(chunk) prev_chunk = NonePython Compared to the original chunks produced by the audio-to-text converts, we can get complete sentences that are cut off originally.

Chunk sentences The text content in documents is normally organized by paragraph. Each paragraph focuses on the same topic. Chunking by paragraph may help embed texts into more meaningful vectors, which may improve retrieval accuracy. Unlike the normal text content in documents, transcriptions from the transcription model are not paragraphed. Even though there are some stops in the audio files, sometimes it can't be used to paragraph sentences. On the other hand, langchain provides the recursive chunking text splitter function RecursiveCharacterTextSplitter, which can keep all the semantically relevant content in the same chunk. Because we need to keep timestamps with chunks, we implement our own chunking process. Inspired by the post How to chunk text into paragraphs using python, we chunk sentences based on the similarity between the adjacent sentences with a sentence embedding approach. The basic idea is to take the sentences with the lowest similarity to adjacent sentences as the split points. We use all-MiniLM-L6-v2 for sentence embedding. You can refer the original post for the explanation of this approach. We have made some minor changes on the original source code; refer to our source code for the implementation. The core part for this process is as follows: # Embed sentences model_name = "all-minilm-l6-v2" model = SentenceTransformer(model_name) embeddings = model.encode(sentences_all) # Create similarities matrix similarities = cosine_similarity(embeddings)

# Let's apply our function. For long sentences i reccomend to use 10 or more sentences minmimas = activate_similarities(similarities, p_size=p_size, order=order)

# Create empty string split_points = [each for each in minmimas[0]] text = ''

if text == '' and (start_timestamp == current_timestamp[1]): start_timestamp = current_timestamp[0]

if num in split_points: para_chunks.append(text) para_timestamp.append([start_timestamp, current_timestamp[1]]) text = f'{each}. ' start_timestamp = current_timestamp[1] else: text+=f'{each}. '

if len(text): para_chunks.append(text) para_timestamp.append([start_timestamp, timestamps_all[-1][1]])Python To evaluate the efficiency of chunking with sentence embedding, we conducted qualitative comparisons between different chunking mechanisms. The assumption underlying such comparisons is that if the chunked texts are more semantically different and separate, there will be less irrelevant contextual information being retrieved for the Q&A, so that the answer will be more accurate and precise. At the same time, because less contextual information is sent to LLMs, the cost of inference will also be less as charges increment with the size of tokens. We visualized the first two components of a PCA by reducing high dimension into two dimensions. Compared to recursive chunking, we can see the distances between vectors representing different chunks with sentence embedding are more scattered, meaning the chunks are more semantically separate. This means when the vector of a query is close to the vector of one chunk, it may have less possibility to be close to other chunks. A retrieval task will have fewer opportunities to choose relevant information from multiple semantically similar chunks.

When the chunking process is complete, we attach timestamps to the file name of each chunk, save it as a single file, and then upload it to an S3 bucket. Enable intelligent video search using a RAG-based approach with LangChain There are typically four approaches to build a RAG solution for Q&A with LangChain:

Using the load_qa_chain functionality, which feeds all information to an LLM. This is not an ideal approach given the context window size and the volume of video and audio data. Using the RetrievalQA tool, which requires a text splitter, text embedding model, and vector store to process texts and retrieve relevant information. Using VectorstoreIndexCreator, which is a wrapper around all logic in the second approach. The text splitter, text embedding model, and vector store are configured together inside the function at one time. Using the ConversationalRetrievalChain tool, which further adds memory of chat history to the QA solution.

For this post, we use the second approach to explicitly customize and choose the best engineering practices. In the following sections, we describe each step in detail. To search for the relevant content based on the user input queries, we use semantic search, which can better understand the intent behind and query and perform meaningful retrieval. We first use a pre-trained embedding model to embed all the transcribed text into a vector space. At search time, the query is also embedded into the same vector space and the closest embeddings from the source corpus are found. You can deploy the pre-trained embedding model as shown in Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart to create the embeddings for semantic search. In our post, we adopt similar ways to create an intelligent video search solution using a RAG-based approach with the open-source LangChain library. LangChain is an open-source framework for developing applications powered by language models. LangChain provides a generic interface for many different LLMs. We first deploy an embedding model GPT-J 6B provided by Amazon SageMaker JumpStart and the language model Falcon-40B Instruct from Hugging Face to prepare for the solution. When the endpoints are ready, we follow similar steps described Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart to create the LLM model and embedding model for LangChain. The following code snippet shows how to create the LLM model using the langchain.llms.sagemaker_endpoint.SagemakerEndpoint class and transform the request and response payload for the LLM in the ContentHandler: from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint

def transform_input(self, prompt: str, model_kwargs={}) -> bytes: self.len_prompt = len(prompt) input_str = json.dumps({"inputs": prompt , "parameters": {**model_kwargs}}) return input_str.encode("utf-8")

def transform_output(self, output: bytes) -> str: response_json = output.read() res = json.loads(response_json) print(res) ans = res[0]['generated_text'][self.len_prompt:] return ans

sm_llm = SagemakerEndpoint( endpoint_name=_MODEL_CONFIG_["huggingface-falcon-40b"]["endpoint_name"], region_name=aws_region, model_kwargs=parameters, content_handler=content_handler, ) Python When we use a SageMaker JumpStart embedding model, we need to customize the LangChain SageMaker endpoint embedding class and transform the model request and response to integrate with LangChain. Load the processed video transcripts using the LangChain document loader and create an index. We use the DirectoryLoader package in LangChain to load the text documents into the document loader: loader = DirectoryLoader("./data/demo-video-sagemaker-doc/", glob="*/.txt") documents = loader.load()Python Next, we use the embedding models to create the embeddings of the contents and store the embeddings in a FAISS vector store to create an index. We use this index to find relevant documents that are semantically similar to the input query. With the VectorstoreIndexCreator class, you can just write a few lines of code to achieve this task: index_creator = VectorstoreIndexCreator( vectorstore_cls=FAISS, embedding=embeddings, text_splitter=CharacterTextSplitter(chunk_size=500, chunk_overlap=0), ) index = index_creator.from_loaders([loader])Python Now we can use the index to search for relevant context and pass it to the LLM model to generate an accurate response: index.query(question=question, llm=sm_llm)Python Build a multi-functional chatbot with SageMaker With the deployed LLM on SageMaker, we can build a multi-functional smart chatbot to show how these models can help your business build advanced AI-powered applications. In this example, the chatbot uses Streamlit to build the UI and the LangChain framework to chain together different components around LLMs. With the help of the text-to-text and speech-to-text LLMs deployed on SageMaker, this smart chatbot accepts inputs from text files and audio files so users can chat with the input files (accepts text and audio files) and further build applications on top of this. The following diagram shows the architecture of the chatbot.

When a user uploads a text file to the chatbot, the chatbot puts the content into the LangChain memory component and the user can chat with the uploaded document. This part is inspired by the following GitHub example that builds a document chatbot with SageMaker. We also add an option to allow users to upload audio files. Then the chatbot automatically invokes the speech-to-text model hosted on the SageMaker endpoint to extract the text content from the uploaded audio file and add the text content to the LangChain memory. Lastly, we allow the user to select the option to use the knowledge base when answering questions. This is the RAG capability shown in the preceding diagram. We have defined the SageMaker endpoints that are deployed in the notebooks provided in the previous sections. Note that you need to pass the actual endpoint names that are shown in your account when running the Streamlit app. You can find the endpoint names on the SageMaker console under Inference and Endpoints. Falcon_endpoint_name = os.getenv("falcon_ep_name", default="falcon-40b-instruct-12xl") whisper_endpoint_name = os.getenv('wp_ep_name', default="whisper-large-v2") embedding_endpoint_name = os.getenv('embed_ep_name', default="huggingface-textembedding-gpt-j-6b")Python When the knowledge base option is not selected, we use the conversation chain, where we add the memory component using the ConversationBufferMemory provided by LangChain, so the bot can remember the current conversation history: def load_chain(): memory = ConversationBufferMemory(return_messages=True) chain = ConversationChain(llm=llm, memory=memory) return chain

chatchain = load_chain()Python We use similar logic as shown in the earlier section for the RAG component and add the document retrieval function to the code. For demo purposes, we load the transcribed text stored in SageMaker Studio local storage as a document source. You can implement other RAG solutions using the vector databases based on your choice, such as Amazon OpenSearch Service, Amazon RDS, Amazon Kendra, and more. When users use the knowledge base for the question, the following code snippet retrieves the relevant contents from the database and provides additional context for the LLM to answer the question. We used the specific method provided by FAISS, similarity_search_with_score, when searching for relevant documents. This is because it can also provide the metadata and similarity score of the retrieved source file. The returned distance score is L2 distance. Therefore, a lower score is better. This gives us more options to provide more context for the users, such as providing the exact timestamps of the source videos that are relevant to the input query. When the RAG option is selected by the user from the UI, the chatbot uses the load_qa_chain function provided by LangChain to provide the answers based on the input prompt. docs = docsearch.similarity_search_with_score(user_input) contexts = []

for doc, score in docs: print(f"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}") if score <= 0.9: contexts.append(doc) source.append(doc.metadata['source'].split('/')[-1]) print(f"\n INPUT CONTEXT:{contexts}") prompt_template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.:\n\n{context}\n\nQuestion: {question}\nHelpful Answer:"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"]) chain = load_qa_chain(llm=llm, prompt=PROMPT) result = chain({"input_documents": contexts, "question": user_input}, return_only_outputs=True)["output_text"]

if len(source) != 0: df = pd.DataFrame(source, columns=['knowledge source']) st.data_editor(df)Python Run the chatbot app Now we're ready to run the Streamlit app. Open a terminal in SageMaker Studio and navigate to the cloned GitHub repository folder. You need to install the required Python packages that are specified in the requirements.txt file. Run pip install -r requirements.txt to prepare the Python dependencies. Then run the following command to update the endpoint names in the environment variables based on the endpoints deployed in your account accordingly. When you run the chatbot.py file, it automatically updates the endpoint names based on the environment variables. export falcon_ep_name= export wp_ep_name= export embed_ep_name= streamlit run app_chatbot/chatbot.py --server.port 6006 --server.maxUploadSize 6Bash

To access the Streamlit UI, copy your SageMaker Studio URL and replace lab? with proxy/[PORT NUMBER]/. For this post, we specified the server port as 6006, so the URL should look like ID>.studio. .sagemaker.aws/jupyter/default/proxy/6006/. Replace domain ID and region with the correct value in your account to access the UI. Chat with your audio file In the Conversation setup pane, choose Browse files to select local text or audio files to upload to the chatbot. If you select an audio file, it will automatically invoke the speech-to-text SageMaker endpoint to process the audio file and present the transcribed text to the console, as shown in the following screenshot. You can continue asking questions about the audio file and the chatbot will be able to remember the audio content and respond to your queries based on the audio content.

Use the knowledge base for the Q&A When you want to answer questions that require specific domain knowledge or use the knowledge base, select Use knowledge base. This lets the chatbot retrieve relevant information from the knowledge base built earlier (the vector database) to add additional context to answer the question. For example, when we ask the question "what is the recommended way to first customize a foundation model?" to the chatbot without the knowledge base, the chatbot returns an answer similar to the following screenshot.

When we use the knowledge base to help answer this question, the chatbot returns a different response. In the demo video, we read the SageMaker document about how to customize a model in SageMaker Jumpstart.

The output also provides the original video file name with the retrieved timestamp of the corresponding text. Users can go back to the original video file and locate the specific clips in the original videos.

This example chatbot demonstrates how businesses can use various types of digital assets to enhance their knowledge base and provide multi-functional assistance to their employees to improve productivity and efficiency. You can build the knowledge database from documents, audio and video datasets, and even image datasets to consolidate all the resources together. With SageMaker serving as an advanced ML platform, you accelerate project ideation to production speed with the breadth and depth of the SageMaker services that cover the whole ML lifecycle. Clean up To save costs, delete all the resources you deployed as part of the post. You can follow the provided notebook's cleanup section to programmatically delete the resources, or you can delete any SageMaker endpoints you may have created via the SageMaker console. Conclusion The advent of generative AI models powered by LLMs has revolutionized the way businesses acquire and apply insights from information. Within this context, digital assets, including video and audio content, play a pivotal role as visual representations of products, services, and brand identity. Efficiently searching and discovering specific content within these assets is vital for optimizing workflows, enhancing collaboration, and delivering tailored experiences to the intended audience. With the power of generative AI models on SageMaker, businesses can unlock the full potential of their video and audio resources. The integration of generative AI models empowers enterprises to build efficient and intelligent search solutions, enabling users to access relevant and contextual information from their digital assets, and thereby maximizing their value and fostering business success in the digital landscape. For more information on working with generative AI on AWS, refer to Announcing New Tools for Building with Generative AI on AWS.

About the authors Gordon Wang is a Senior AI/ML Specialist TAM at AWS. He supports strategic customers with AI/ML best practices across many industries. He is passionate about computer vision, NLP, generative AI, and MLOps. In his spare time, he loves running and hiking. Melanie Li is a Senior AI/ML Specialist TAM at AWS based in Sydney, Australia. She helps enterprise customers build solutions using state-of-the-art AI/ML tools on AWS and provides guidance on architecting and implementing ML solutions with best practices. In her spare time, she loves to explore nature and spend time with family and friends. Guang Yang is a Senior Applied Scientist at the Amazon Generative AI Innovation Center, where he works with customers across various verticals and applies creative problem solving to generate value for customers with state-of-the-art generative AI solutions. Harjyot Malik is a Senior Program Manager at AWS based in Sydney, Australia. He works with the APJC Enterprise Support teams and helps them build and deliver strategies. He collaborates with business teams, delving into complex problems to unearth innovative solutions that in return drive efficiencies for the business. In his spare time, he loves to travel and explore new places.

As customers accelerate their migrations to the cloud and transform their businesses, some find themselves in situations where they have to manage data analytics in a multi-cloud environment, such as acquiring a company that runs on a different cloud provider. Customers who use multi-cloud environments often face challenges in data access and compatibility that can create blockades and slow down productivity. When managing multi-cloud environments, customers must look for services that address these gaps through features providing interoperability across clouds. With the release of the Amazon Athena data source connector for Google Cloud Storage (GCS), you can run queries within AWS to query data in Google Cloud Storage, which can be stored in relational, non-relational, object, and custom data sources, whether that be Parquet or comma-separated value (CSV) format. Athena provides the connectivity and query interface and can easily be plugged into other AWS services for downstream use cases such as interactive analysis and visualizations. Some examples include AWS data analytics services such as AWS Glue for data integration, Amazon QuickSight for business intelligence (BI), as well as third-party software and services from AWS Marketplace. This post demonstrates how to use Athena to run queries on Parquet or CSV files in a GCS bucket. Solution overview The following diagram illustrates the solution architecture.

The Athena Google Cloud Storage connector uses both AWS and Google Cloud Platform (GCP), so we will be referencing both cloud providers in the architecture diagram. We use the following AWS services in this solution:

Amazon Athena – A serverless interactive analytics service. We use Athena to run queries on data stored on Google Cloud Storage. AWS Lambda – A serverless compute service that is event driven and manages the underlying resources for you. We deploy a Lambda function data source connector to connect AWS with Google Cloud Provider. AWS Secrets Manager – A secrets management service that helps protect access to your applications and services. We reference the secret in Secrets Manager in the Lambda function so we can run a query on AWS and it can access the data stored on Google Cloud Provider. AWS Glue – A serverless data analytics service for data discovery, preparation, and integration. We create an AWS Glue database and table to point to the correct bucket and files within Google Cloud Storage. Amazon Simple Storage Service (Amazon S3) – An object storage service that stores data as objects within buckets. We create an S3 bucket to store data that exceeds the Lambda function's response size limits.

The Google Cloud Platform portion of the architecture contains a few services as well:

Google Cloud Storage – A managed service for storing unstructured data. We use Google Cloud Storage to store data within a bucket that will be used in a query from Athena, and we upload a CSV file directly to the GCS bucket. Google Cloud Identity and Access Management (IAM) – The central source to control and manage visibility for cloud resources. We use Google Cloud IAM to create a service account and generate a key that will allow AWS to access GCP. We create a key with the service account, which is uploaded to Secrets Manager.

Prerequisites For this post, we create a VPC and security group that will be used in conjunction with the GCP connector. For complete steps, refer to Creating a VPC for a data source connector. The first step is to create the VPC using Amazon Virtual Private Cloud (Amazon VPC), as shown in the following screenshot.

Then we create a security group for the VPC, as shown in the following screenshot.

For more information about the prerequisites, refer to Amazon Athena Google Cloud Storage connector. Additionally, there are tables that highlight the specific data types that can be used such as CSV and Parquet files. There are also required permissions to run the solution. Google Cloud Platform configuration To begin, you must have either CSV or Parquet files stored within a GCS bucket. To create the bucket, refer to Create buckets. Make sure to note the bucket name—it will be referenced in a later step. After you create the bucket, upload your objects to the bucket. For instructions, refer to Upload objects from a filesystem.

The CSV data used in this example came from Mockaroo, which generated random test data as shown in the following screenshot. In this example, we use a CSV file, but you can also use Parquet files.

Additionally, you must create a service account to generate a key pair within Google Cloud IAM, which will be uploaded to Secrets Manager. For full instructions, refer to Create service accounts.

After you create the service account, you can create a key. For instructions, refer to Create and delete service account keys.

AWS configuration Now that you have a GCS bucket with a CSV file and a generated JSON key file from Google Cloud Platform, you can proceed with the rest of the steps on AWS.

On the Secrets Manager console, choose Secrets in the navigation pane. Choose Store a new secret and specify Other type of secret. Provide the GCP generated key file content.

The next step is to deploy the Athena Google Cloud Storage connector. For more information, refer to Using the Athena console.

On the Athena console, add a new data source. Select Google Cloud Storage.

For Data source name, enter a name. For Lambda function, choose Create Lambda function to be redirected to the Lambda console.

In the Application settings section, enter the information for Application name, SpillBucket, GCSSecretName, and LambdaFunctionName.

You also have to create an S3 bucket to reference the S3 spill bucket parameter in order to store data that exceeds the Lambda function's response size limits. For more information, refer to Create your first S3 bucket.

After you provide the Lambda function's application settings, you're redirected to the Review and create page.

Confirm that these are the correct fields and choose Create data source.

Now that the data source connector has been created, you can connect Athena to the data source.

On the Athena console, navigate to the data source. Under Data source details, choose the link for the Lambda function.

You can reference the Lambda function to connect to the data source. As an optional step and for validation, the variables that were put into the Lambda function can be found within the Lambda function's environment variables on the Configuration tab.

Because the built-in GCS connector schema inference capability is limited, it's recommended to create an AWS Glue database and table for your metadata. For instructions, refer to Setting up databases and tables in AWS Glue.

Query the data Now you can run queries on Athena that will access the data stored on Google Cloud Storage.

On the Athena console, choose the correct data source, database, and table within the query editor. RunSELECT * FROM [AWS Glue Database name].[AWS Glue Table name]in the query editor.

As shown in the following screenshot, the results will be from the bucket on Google Cloud Storage.

The data that is stored on Google Cloud Platform can be accessed through AWS and used for many use cases, such as performing business intelligence, machine learning, or data science. Doing so can help unblock developers and data scientists so they can efficiently provide results and save time. Clean up Complete the following steps to clean up your resources:

Delete the provisioned bucket in Google Cloud Storage. Delete the service account under IAM & Admin. Delete the secret GCP credentials in Secrets Manager. Delete the S3 spill bucket. Delete the Athena connector Lambda function. Delete the AWS Glue database and table.

Troubleshooting If you receive a ROLLBACK_COMPLETE state and "can not be updated error" when creating the data source in Lambda, go to AWS CloudFormation, delete the CloudFormation stack, and try recreating it. If the AWS Glue table doesn't appear in the Athena query editor, verify that the data source and database values are correctly selected in the Data pane on the Athena query editor console. Conclusion In this post, we saw how you can minimize the time and effort required to access data on Google Cloud Platform and use it efficiently on AWS. Using the data connector helps organizations become multi-cloud agnostic and helps accelerate business growth. Additionally, you can build out BI applications with the discoveries, relationships, and insights found when analyzing the data, which can further your organization's data analysis process.

About the Author Jonathan Wong is a Solutions Architect at AWS assisting with initiatives within Strategic Accounts. He is passionate about solving customer challenges and has been exploring emerging technologies to accelerate innovation.

AWS Women in Cloud program helps Brazilian on path to solutions architect role

I joined Amazon Web Services (AWS) as a Solutions Architect in September 2022, a few months after participating in AWS Women in Cloud (WiC), a workforce development initiative designed to inspire, motivate, and connect female learners to AWS training and cloud-related job opportunities. As I approach my one-year "Amaversary," as we Amazonians like to call it, I'm moved to share my journey with the hopes it inspires even one woman to see her future in my story. Since I was young, I dreamed of working for a big tech company, but this dream always seemed far from attainable. You see, I was born and raised in a city of 140,000 inhabitants in the interior of Rio de Janerio State in Brazil. Growing up in such a small and remote part of Brazil presented me with few opportunities for professional growth, especially when it came to tech. Education as my first step Determined not to let geography stand in the way of my dreams, I began studying information systems at UniFOA in 2013. There, I found great inspiration in one of my professors who taught the last class on Friday nights. He worked at a big tech company for years, and even though the class was late and he was tired, he still had a sparkle in his eyes every time he talked about his professional journey. He, too, came from a small town in Brazil and I was inspired by his story, which showed me that my dream was feasible to achieve. Thanks to this inspiration and my hard work, I secured a Systems Analyst role when I graduated in 2016. Though I entered the tech workforce, I was still living in my small hometown and my opportunities for growth felt limited. Over the next three years, I planned and dreamed for what was to come next. Making a big move In 2019, I decided to move to Brazil's largest city, São Paulo, where a single neighborhood has a population that exceeded that of my entire hometown. At first, everything felt daunting.. I had never been away from home and the move required a lot of courage and determination. But I eventually found a job as a developer at a consultancy firm, where I continued to sharpen my tech skills. I've always loved and had a strong affinity for the field of data and enjoyed helping people solve their problems. So, when I was offered a new position as a Data Engineer at a financial services company in 2020, I immediately accepted. Though I had heard of Amazon Web Services (AWS), I had little prior knowledge of cloud computing and had never worked in the AWS Cloud. My fascination with AWS's data services quickly grew and I decided to pursue the AWS Certified Cloud Practitioner certification so I could further specialize in cloud computing and add more value to my work as a Data Engineer. While taking courses at Paulista Faculty of Informatics and Administration I studied for the certification exam through AWS Academy, which provides higher education institutions around the world with a free, ready-to-teach AWS Cloud computing curriculum. Empowered by stories from Women in Cloud Not one to sit still, I next set a goal to become a Solutions Architect. Though this goal seemed like a stretch at the time, my history taught me that I could do anything I set my mind to. And, lucky for me, my AWS Academy professor had invited me to participate in the WiC event, held at the AWS Brazil office. Without hesitation, I accepted the invitation, as my dream was closer than ever: I was going to step into the AWS Brazil office for the first time! The all-day event included hands-on activities to practice my cloud skills, innovation workshops, and opportunities to hear from women who have excelled in their IT careers. Employers with available positions attended the event hoping to meet cloud-skilled candidates to fill these open roles. Leaving the event, I was deeply impacted, motivated, determined, and empowered. I completed the AWS Academy course, obtained my first AWS Certification, and began applying for Solutions Architect positions. Dreams become a reality WiC aims to build a bridge to employment by connecting cloud-skilled female job seekers with employers looking to hire for in-demand, open roles. And, about three months later, I was called to participate in the selection process for a Solutions Architect role at AWS. My heart almost leaped out of joy. I went through the process, and a few days later, I received the long-awaited email saying, "Congratulations, Inaiah. You have been approved for the Solutions Architect position at Amazon Web Services!" After all my hard work, my dream of becoming a Solutions Architect was finally coming true, thanks to the "Women in Cloud" event that allowed me to connect with other women in tech who I now get to call my colleagues. This year I attended the third edition of WiC in Brazil and had the privilege of sharing my story on stage. As I looked out into the audience of hundreds of women who were just like me a short time ago, one thing became clear: while I look forward to continuing to grow professionally, my passion is to impact and motivate other women with my experience. We are all capable of reaching our goals! About Women in Cloud AWS Women in Cloud (WiC) is a workforce development initiative to inspire, motivate, and connect female learners to AWS training and cloud-related job opportunities, promoting and growing women empowerment in IT. WIC has conducted three events in Brazil since June 2022, impacting over 1,000 women through networking, mentorship, training learners in AWS Academy Cloud Foundations, and connecting cloud-skilled talent with entry-level employment opportunities. Connect with Talent from AWS Education Programs AWS Education Programs are preparing graduates for cloud roles in cities and towns around the world. We designed our curriculum by working backward from employer requirements so that graduates are qualified and prepared with the skills needed. Find cloud-skilled talent to fill your open roles today.

While working with many of our customers, a recurring question has been "How can we be notified when users login to key accounts so we can take action if needed?" This post shows how to implement a flexible, simple, and serverless solution that creates notifications when sensitive accounts are logged in to. Alerting on high risk account access is advised as part of multi-account best practices. And although this approach is something we recommend, customers currently must develop their own method. In this post, we will walk you through a solution that will allow you to implement a flexible and scalable framework that can trigger notifications when users access accounts of interest, use specified roles, or leverage the root user principal in AWS account(s) or Organizational Units (OUs) they specify. The notifications generated can be emailed or sent to another AWS service to trigger additional automated actions, allowing you to reduce risk from unauthorized access to your environments. Prerequisites You will need to verify the following:

Your AWS Organization must have all features enabled. Make sure you do not have an explicit deny Service Control Policy (SCP) set at the Organization level that could prevent stacksets from being deployed. You must enable trusted access for stacksets to run the management account stack.

Key Concepts Before we dive deep into this solution, let's review some key concepts that are centric to the logic:

AWS root account user is the email address and password that were used to create an AWS account initially. The AWS root account user has the highest level of privileges and its use should be restricted to a small list of actions that require it. IAM identities are created using AWS Identity and Access Management (IAM) and take the form of either an IAM user with long-term credentials, or an IAM role with short-term credentials. AWS accounts can contain many users and roles. AWS Organizations allows for the creation and management of AWS accounts. Organizational Units (OUs) are containers for accounts. An OU can also contain other OUs. Root OU is the parent container for all the accounts for your organization. If you apply a policy to the root, it applies to all OUs and accounts in the organization. CloudFormation Stacksets allow you to deploy and manage configurations across multiple AWS accounts.

Solution The solution we will be walking you through details the steps needed to build a multi-account access notification system using AWS Organizations for account management, Amazon EventBridge to identify key login actions, AWS Lambda to act on the events, Amazon Simple Notification Service (SNS) to send notifications, and Amazon DynamoDB for storing configuration data. The solution is also fully compatible with AWS Control Tower.

The LoginNotification Configuration Database DynamoDB table is where the rules are stored to determine what login activities trigger a notification. By default, you will have a single rule in the table that causes the AWS root account to trigger a notification, which will look like the following:

Records in the LoginNotification Configuration Database have the following attributes: "id" is what the rule applies to. This can be an AWS account number, Organizational Unit, or Organizational root. There is a special ID called "DEFAULT" which applies to all accounts "inclusions" is a stringset field that contains a list of principals you want to include in your login notifications. Principals are evaluated against the "userIdentity" field in AWS Management Console sign-in events from CloudTrail. Items in the "inclusions" stringset are evaluated for a complete or partial match for the "userIdentity" field in the login event. "exclusions" is a stringset field that indicates principals which should not generate notifications. The same evaluation logic applies to exclusions as inclusions. There are 2 reserved words in the "inclusions" and "exclusions" list

ROOT means the AWS root account user. ALL means match all principals.

When a console login event is received it is evaluated in the following order:

By default, no notifications will be sent. If the account where the login occurred is not defined in the LoginNotification Configuration database then only the "DEFAULT" rule is applied. If the account where the login occurred is defined in, or is under an OU that is defined in the LoginNotifications Configuration database then all inclusions and exclusions that apply to the account and higher-level OUs are consolidated into a combined list If any of the entries in the combined "inclusions" list are a partial or complete match for the "userIdentity" from the event, a notification will be sent, unless there is an entry in the combined "exclusions" list that also matches the "userIdentity" from the event. "exclusions" take priority over "inclusions".

We recommend adding additional rows beyond the default entry that fit your own best practices. You may add rules at any of four different levels: Account, OU, Root OU, and Default. Exclusions always take precedence over inclusions. As an example, consider the DynamoDB table below:

In this example you can see two rows that represent specific accounts (rows 1 and 3), a row that represents an OU (row 2) and the default entry (row 4). Let us consider the effect on account "123456789012" in this set of entries. First, we would apply the DEFAULT rule which indicates we should include ROOT logins and any with "aperson" in the login credentials. If account "123456789012" was part of "OU-ABCD-EFGHIJKLM", we would also apply an inclusion for "athirdperson" due to that entry. And finally, since we have an entry for that account, we would apply the exclusion for "aperson" which would remove that string from our combined inclusions. We would be left with any logins for ROOT or for "anotherperson" for this account. Walkthrough In order to install this solution, you will need to deploy two CloudFormation stacks. These stacks can be created in any order.

CentralEventBus stack. This stack creates the EventBridge central event bus, DynamoDB tables, and Lambda functions necessary for this solution. This should be deployed in a security audit account. If you're using AWS Control Tower this should be deployed in your Audit account. ManagementAccount stack. This must be deployed in your AWS Organizations management account. This stack deploys the stackset that will deploy the EventBridge forwarding rules to your existing and new AWS accounts. It also deploys a cross account role that the LoginNotifierParser Lambda function uses to query AWS organizations to determine which rules apply to each account. This stack requires trusted account access with AWS Organizations to be enabled for your AWS Organization.

Step 1: Deploying the CentralEventBus stack We'll start with the CentralEventBus stack. You should deploy this to your Audit account, or a similar functioning account. In order to deploy the solution, sign in to the AWS Management Console and navigate to the CloudFormation console. Next, download the CentralEventBus template and save it, and deploy the solution template using Create Stack. The CentralEventBus template takes the following parameters:

AWSSSOHomeRegion indicates the home region for your IAM Identity Center (successor to AWS SSO). The default here is us-east-1. CacheExpiration is the length of time (in seconds) to cache results from AWS organization. To minimize calls to AWS Organizations and reduce execution times this solution caches information about the AWS Organizations hierarchy in a DynamoDB table. The default expiration is 300 seconds (5 minutes) EmailSubscription is the email address that will receive notification events. OrganizationID is your AWS Organizations identifier. This can be found by either navigating to the AWS Organizations console in the AWS console or by running the AWS CLI command "aws organizations describe-organization".

You will be taken to the CloudFormation Stacks screen for your CentralEventBus stack. The deployment may take a few minutes to complete. Once completed, you will see a CREATE_COMPLETE status.

After the stack has successfully completed, the email address you used in the stack details will receive an email requesting that you confirm the subscription to the SNS notification topic. Click the link in the email while logged into the account where you deployed the CloudFormation stack.

When you click on the "Confirm subscription" link you will see a confirmation screen indicating you have subscribed that email to the SNS notification topic for the login notifications.

Step 2: Deploying the ManagementAccount stack Next you will deploy the ManagementAccount stack. You should deploy this to your AWS Organizations management account. Be sure to verify you have covered the prerequisites as mentioned earlier in "Prerequisites". In order to deploy the solution, sign in to the AWS Management Console and navigate to the CloudFormation console. Next, download the ManagementAccountStack template and save it, and deploy the solution template using Create Stack. The ManagementAccount template takes the following parameters:

CentralEventBridgeAccountID is the account ID where the CentralEventBus stack is or will be deployed. CentralEventBridgeRegion is the region where the CentralEventBus stack was deployed. DeployTo is where you would want the EventBridge Notification rules to be deployed. This can either your AWS Organizations root ID or an OU. MonitoredRegions is a comma-separated list of the regions from which you want login events forwarded. "us-east-1" must be part of this list in order to forward AWS root user logins.

You will be taken to the CloudFormation Stacks screen for your ManagementAccount stack. The deployment may take some time to complete depending on how many accounts and regions are included. Once the stack is deployed you will see a CREATE_COMPLETE for the stack name you used.

Tips: It can be useful to run the AWS CLI command "sts get-caller-identity" from an account you're logged into in order to see the "UserID" information you may want to key off of for your inclusion/exclusion rules. Here is an example of the command output: [cloudshell-user@ip-10-0-0-6 ~]$ aws sts get-caller-identity { "UserId": "AROAYQDUR5EGT522XDYRZ:my-user", "Account": "1234567829012", "Arn": "arn:aws:sts:: 1234567829012:assumed-role/MyRole/my-user" } It also may be helpful to review "ConsoleLogin" events in the AWS CloudTrail console to help formulate your own best practice rules. Cleanup: In order to remove this deployed solution it is only necessary to delete both CloudFormation stacks created here. Additional costs will occur if the solution is not deleted. The cost to implement the solution will vary based on the number of regions and accounts monitored, as well as the amount of login activity. Please see the relevant service pricing pages for more information on pricing:

Amazon EventBridge Pricing AWS Lambda Pricing Amazon DynamoDB Pricing Amazon SNS Pricing

Conclusion In this post, we showed how you can implement a flexible, simple and serverless solution for alerting on sensitive account logins as part of a best practice recommendation. This is accomplished by utilizing existing AWS CloudTrail events, AWS Organizations, Amazon EventBridge, AWS Lambda, and Amazon SNS. This allows customers to send notifications to relevant support personnel or trigger additional automation tasks based on critical events, making remediation of potential security issues a simple task. Although we do not require AWS Control Tower for this solution, it is an account management best practice and works well with the architecture for the Login Notifier. You can read more about Control Tower in the documentation. About the Authors

Jeff Bodey Jeff Bodey is a Senior Solutions Architect based in Denver, Colorado. He works with customers to help them build well architected and scalable applications in AWS. Jeff enjoys playing with gadgets and probably has entirely too many in his home.

Jeff Stockamp Jeff Stockamp is a Principal Solutions Architect based in Seattle, Washington. Jeff helps guide customers as they build well architected-applications and migrate workloads to AWS. Jeff is a constant builder and spends his spare time building Legos with his son.

Thuy Uyen Tran Uyen Tran is a Greenfield Account Solutions Architect and also part of the AI/ML TFC at AWS. She is passionate about learning and facing technical challenges. Technology, innovation, and automation are her areas of interest.

Giuseppe Zappia Giuseppe Zappia is a Senior Solutions Architect at AWS, with over 20 years of experience in full stack software development, distributed systems design, and cloud architecture. In his spare time, he enjoys playing video games, programming, watching sports, and building things.

Giuseppe Zappia Giuseppe Zappia is a Senior Solutions Architect working with customers in the early stages of adopting AWS. In his spare time, he enjoys playing video games, programming, and building things.

This blog post is written by Brian Graf, Senior Developer Advocate, Amazon Lightsail and Sophia Parafina, Senior Developer Advocate. Amazon Lightsail is a virtual private server (VPS) for deploying both operating systems (OS) and pre-packaged applications, such as WordPress, Plesk, cPanel, PrestaShop, and more. When deploying these instances, you can run launch scripts with additional commands such as installation of applications, configuration of system files, or installing pre-requisites for your application. Where do I add a launch script? If you're deploying an instance with the Lightsail console, launch scripts can be added to an instance at deployment. They are added in the 'deploy instance' page:

The launch script must be added before the instance is deployed, because launch scripts can't retroactively run after deployment. Anatomy of a Windows Launch Script When deploying a Lightsail Windows instance, you can use a batch script or a PowerShell script in the 'launch script' textbox. Of the two options, PowerShell is more extensible and provides greater flexibility for configuration and control. If you choose to write your launch script as a batch file, you must add tags at the beginning and end of your code respectively. Alternatively, a launch script in PowerShell, must use the tags in a similar fashion. After the closing or tag, you must add a tag on the following line. The persist tag is used to determine if this is a run-once command or if it should run every time your instance is rebooted or changed from the 'Stop' to 'Start' state. If you want your script to run every time the instance is rebooted or started, then you must set the persist tag to 'true'. If you want your launch script to just run once, then you would set your persist tag to 'false'. Anatomy of a Linux Launch Script Like a Windows launch script, a Linux launch script requires specific code on the first row of the textbox to successfully execute during deployment. You must place '#!/bin/bash' as the first line of code to set the shell that executes the rest of the script. After first line of code, you can continue adding additional commands to achieve the results you want. How do I know if my Launch Script ran successfully? Although running launch scripts is convenient to create a baseline instance, it's possible that your instance doesn't achieve the desired end-state because of an error in your script or permissions issues. You must troubleshoot to see why the launch script didn't complete successfully. To find if the launch script ran successfully, refer to the instance logs to determine whether your launch script was successful or not. For Windows, the launch log can be found in: C:\ProgramData\Amazon\EC2-Windows\launch\Log\UserdataExecution.log. Note that ProgramData is a hidden folder, and unless you access the file from PowerShell or Command Prompt, you must use Windows File Explorer (`View > Show > Hidden items`) folders to see it. For Linux, the launch log can be found in: /var/log/cloud-init-output.log and can be monitored after your instance launches by tailing the log by typing the following in the terminal: tail -f /var/log/cloud-init-output.logPowerShell If you want to see the entire log file including commands that have already run before you opened the log file, then you can type the following in the terminal: less +F /var/log/cloud-init-output.logPowerShell On a Windows instance, an easy way to monitor the UserdataExecution.log is to add the following code in your launch script, which creates a shortcut to tail or watch the log as commands are executing: # Create a log-monitoring script to monitor the progress of the launch script execution

# Save the log-monitoring script to the desktop for the user

false PowerShell If the script was executed, then the last line of the log should say '{Timestamp}: User data script completed'. However, if you want more detail, you can build the logging into your launch script. For example, you can append a text or log file with each command so that you can read the output in an easy-to-access location: # Set the location for the log file. In this case, # it will appear on the desktop of your Lightsail instance $loc = "c:\Users\Administrator\Desktop\mylog.txt"

# Write text to the log file Write-Output "Starting Script" >> $loc

# Download and install Chocolatey to do unattended installations of the rest of the apps. iex ((New-Object System.Net.WebClient).DownloadString('

# You could run commands like this to output the progress to the log file:

# Install vscode and all dependencies choco install -y vscode --force --force-dependencies --verbose >> $loc

# Install git and all dependencies choco install -y git --force --force-dependencies --verbose >> $loc

# Completed Write-Output "Completed" >> $loc false PowerShell This code creates a log file, outputs data, and appends it along the way. If there is an issue, then you can see where the logs stopped or errors appeared. For Ubuntu and Amazon Linux 2 If the cloud-init-output.log isn't comprehensive enough, then you can re-direct the output from your commands to a log file of your choice. In this example, we create a log file in the /tmp/ directory and push all output from our commands to this file. # Create the log file touch /tmp/launchscript.log

# Add text to the log file if you so choose echo 'Starting' >> /tmp/launchscript.log

# Install software to manage independent software vendor sources sudo apt -y install software-properties-common >> /tmp/launchscript.log

# Add the repository for all PHP versions sudo add-apt-repository -y ppa:ondrej/php >> /tmp/launchscript.log

# Install Web server, mySQL client, PHP (and packages), unzip, and curl sudo apt -y install apache2 mysql-client-core-8.0 php8.0 libapache2-mod-php8.0 php8.0-common php8.0-imap php8.0-mbstring php8.0-xmlrpc php8.0-soap php8.0-gd php8.0-xml php8.0-intl php8.0-mysql php8.0-cli php8.0-bcmath php8.0-ldap php8.0-zip php8.0-curl unzip curl >> /tmp/launchscript.log

# Any final text you want to include echo 'Completed' >> /tmp/launchscript.logBash It's possible to check the logs before the launch script has finished executing. One way to follow along is to 'tail' the log file. This lets you stream all updates as they occur. You can monitor the log using: 'tail -f /tmp/launchscript.log'. Bash Using Launch Scripts from AWS Command Line Interface (AWS CLI) You can deploy their Lightsail instances from the AWS Command Line Interface (AWS CLI) instead of the Lightsail console. You can add launch scripts to the AWS CLI command as a parameter by creating a variable with the script and referencing the variable, or by saving the launch script as a file and referencing the local file location on your computer. The launch script is still written the same way as the previous examples. For a Windows instance with a PowerShell launch script, you can deploy a Lightsail instance with a launch script with the following code: # PowerShell script saved in the Downloads folder:

# Download and install Chocolatey to do unattended installations of the rest of the apps.

Write-Output "Completed" >> $locPowerShell AWS CLI code to deploy a Windows Server 2019 medium instance in the us-west-2a Availability Zone: aws lightsail create-instances \

--user-data file://~/Downloads/powershell_script.ps1PowerShell Clean up Remember to delete resources when you are finished using them to avoid incurring future costs. Conclusion You now have the understanding and examples of how to create and troubleshoot Lightsail launch scripts both through the Lightsail console and AWS CLI. As demonstrated in this blog, using launch scripts, you can increase your productivity and decrease the deployment time and configuration of your applications. For more examples of using launch scripts, check out the aws-samples GitHub repository. You now have all the foundational building blocks you need to successfully script automated instance configuration. To learn more about Lightsail, visit the Lightsail service page.

Loupe live shopping platform upgrades user experience with Amazon IVS

Designed for card collectors, live e-commerce platform Loupe connects buyers with small businesses across the US to provide on-demand access to sports cards 24/7. Loupe focuses on fostering a community through real-time chat and big, exciting, memorable moments. After completing a thorough vetting process, vendors can live stream sales via the platform, which supports web, iOS, and Android. To provide customers with a high-quality experience, Loupe powers its live streams using Amazon Interactive Video Service (Amazon IVS) on Amazon Web Services (AWS). "Unlike a static marketplace, Loupe is a fully interactive live experience. It's also a reliable destination for in-demand collectibles, built on community and trust. Our initial live stream solution wasn't allowing us to hit the high bar we wanted," explained Loupe founder and CEO, Eric Doty. "When we implemented Amazon IVS for our live streams, the video quality jumped substantially, and our users immediately noticed it. I wish we had it from day one." Released in October 2020, Loupe was inspired by the team's own lackluster card collecting experiences and the rise of "box breaking"—the term for someone buying cards from a seller who's opening them on a live stream. While revitalizing the sports card hobby, box break purchases often required several time-consuming steps across different platforms. Loupe unifies these steps, allowing customers to complete the purchasing journey in as little as 10 seconds without leaving the app.

"In a typical sports card break, the seller might live stream on a social platform, you message them to make a purchase, they reply and direct you to their website. You then find the item and purchase it with an online payment application, which sends the seller an email about the transaction, so they can open the product on the live stream. It's way too many unnecessary steps," noted Brett Makedonski, Loupe director of communications. "Our team has a background in video games and designing cutting-edge viewing experiences. We saw how the sports card industry is lagging and could be so much better. By streamlining the experience, collectors are more likely to buy in because we've removed a lot of the friction seen in other e-commerce platforms." Loupe developers leverage the Amazon IVS broadcast SDK and player SDK to custom integrate the live streaming service within the platform, and the IVS Mobile Broadcast SDK enables sellers to go live straight from the Loupe mobile application. Amazon IVS powers these live streams, which are configured to record to Amazon Simple Storage Service (Amazon S3) automatically. Loupe makes the files available for on-demand viewing or download through the Amazon CloudFront content delivery network in case sellers or buyers want to share highlights of the stream on their own social channels. About a month after the live stream, the video is archived to Amazon S3 Glacier storage, economically preserving the content while allowing the app to highlight more recent streams. Amazon Elastic Compute (Amazon EC2) and Amazon Container Service (Amazon ECS) allow Loupe to seamlessly run a scalable computing infrastructure to power its applications. The team also relies on Amazon CloudWatch for gathering and analyzing performance data. The migration from the previous live stream solution and tech stack was complete in about two months and has been live since June 2022. The move to Amazon IVS and AWS has allowed Loupe to double the max live stream resolution, increase the max bitrate by 8x, and increase the frame rate by 4x, in addition to gaining live stream stability and customization capabilities. Though sports cards are the primary offering, the functionality Loupe has built with AWS could easily apply to nearly any live streaming e-commerce or auction-based marketplace. "Our focus is on providing a quality experience to a specific audience that is enthusiastic, engaged, and discerning. They're willing to pay for premium collectibles—and many are repeat customers, but only when their expectations are met. High-quality video is central to our success, and we've seen a significant increase in viewership and business since implementing Amazon IVS," Doty concluded. With the integration of Amazon IVS, Loupe has created an elevated experience for the card collector community by streamlining the traditional buying and unboxing process. By providing customers with more personalized and engaging shopping experiences, innovative solutions like Amazon IVS will continue to help retailers stay relevant and competitive in an ever-evolving retail landscape. To learn more about building with interactive live video, check out:

Josh Walters Josh Walters manages growth marketing programs for Amazon Interactive Video Service, focusing on building, scaling, and improving initiatives that increase awareness among developers creating interactive live streaming experiences.

Importance and Impact of Compliance for SaaS Solutions on AWS

Legal requirements: SaaS companies are often subject to various regulations and laws related to data protection, privacy, and security. Compliance with these regulations is not only legally required, but can also help to avoid costly fines and legal actions. Trust and reputation: Customers trust SaaS companies with their sensitive data and expect that it will be protected. Compliance with industry standards and regulations can help to build and maintain trust. Accessing new markets: Compliance with certain standards and regulations may be required to access new markets or to work with certain customers, particularly in highly regulated industries. Risk management: Compliance can help SaaS companies to identify and manage risks associated with data protection and security. Requirements often include implementing policies, procedures, and controls to help mitigate risks and prevent data breaches.

Building with a plan for compliance can also influence the design and architecture of your SaaS solution, and compliance can help avoid technical debt down the road. In this post, we'll break down some of the factors SaaS providers need to consider in designing and building their compliant solution, and highlight resources and services within Amazon Web Services (AWS) that you can leverage in your journey. Compliance as the Foundation of Security Security and privacy frameworks provide the tools to build out cybersecurity programs, stand up policies and procedures, and implement necessary technical controls to safeguard the confidentiality, availability and integrity of information. These frameworks provide a blueprint for managing risk and reducing vulnerabilities and are designed to consider the risks organizations face and how attackers can exploit security weaknesses. For SaaS providers, these frameworks can be used to demonstrate their solutions meet the security requirements of customers, whether with government laws and regulations of their specific industries. In some cases, organizations choose to pursue an attestation or certification such as, Service Organization Control 2 (SOC 2) and/or International Organization for Standardization 27001 (ISO 27001) to improve business operations and boost their competitive advantage. In other cases, businesses must meet regulatory compliance obligations and comply with laws for how to handle payment cards, personally identifiable information (PII), and other sensitive data. Security requirements of frameworks generally map to a group of controls that each define some aspect of security. These controls may require self-attestation of compliance, or attestation by a third party, which may take the form of periodic reviews, or continual compliance with systems that monitor the state of compliance with a control. Achieving compliance will be an ongoing process, but regular monitoring and reporting can help make adhering to these frameworks a standard part of business operations. Good security controls, data privacy, and data management should be foundational components of a SaaS application from the beginning. Shared Responsibility Model and Compliance For SaaS providers building on AWS, it's important to understand the Shared Responsibility Model in terms of compliance. AWS is continuously audited and maintains certifications and accreditations across the globe, including SOC 2, ISO 27001, Federal Risk and Authorization Management Program (FedRAMP), Payment Card Industry Data Security Standard (PCI DSS), and others to help customers meet security and compliance requirements. When systems are built in the AWS cloud, AWS and its customers share those compliance responsibilities. Some controls can be inherited from AWS, some are shared, and some controls are the customers responsibility, as Figure 1 below describes. Just as AWS has a SOC 2 attestation, FedRAMP authorization, and ISO certification, SaaS providers that need to be compliant will have to go through the same compliance processes. It's important SaaS providers understand which components they are responsible for within the scope of services that comprise their environment.

Figure 1 – AWS Shared Responsibility Model. A few resources to mention that are important as you plan your SaaS architecture:

Customer Compliance Guides (CCG) provide a consolidated view of AWS security practices based on the configurable options for AWS services and compliance topics and control requirements mapped to common frameworks (SOC 2, ISO 27001, FedRAMP, National Institute of Standards and Technology Cybersecurity Framework (NIST CSF)). AWS Services in Scope by Compliance Program is a list of services assessed and approved for specific compliance frameworks and regulations. For example, for Health Insurance Portability and Accountability Act (HIPAA), customers may use any AWS service in an account designated as a HIPAA account, but only process, store, and transmit protected health information (PHI) in HIPAA-eligible services defined on this page and in the Business Associate Addendum (BAA). AWS Artifact provides access to AWS compliance reports and agreements between AWS and you as a SaaS provider which may be required by your compliance auditor. Some reports will list inheritable controls for "of the cloud," helping customers understand the scope of controls that are shared and their responsibility.

Compliance Impact on SaaS Architecture In the following sections, we'll examine how compliance concerns influence our thinking on a number of core SaaS concepts. In doing so, we'll refer back to the AWS whitepaper about SaaS Architecture Fundamentals and the SaaS Lens for the AWS Well-Architected Framework. Please refer to these guides for general SaaS guidance. Tenant Isolation Implications Tenant isolation is the process of protecting tenant resources, and denying any attempts to access other tenants' resources. The options for isolating tenant infrastructure, in SaaS, break down primarily into "silo" and "pool." Check out the SaaS whitepaper's "Full stack silo and pool" section for background reading.

Figure 2 – Silo vs. Pool model of tenant isolation. It's a misconception that compliance frameworks or regulations, particularly those covering highly regulated industries, prohibit the use of pooled resources. In fact, most frameworks offer no guidance on tenant isolation, even FedRAMP primarily specifies "a full application test which attempts to use provisional access of one tenant to compromise another tenant." (FedRAMP Penetration Test Guidance) Be prepared to prove you will "introduce isolation strategies across all layers of the architecture, providing specific constructs that ensure that any attempt to access a tenant resource is valid for the current tenant context." (SaaS Lens) If you're going to pool resources, the focus is on ensuring isolation policies are applied at every layer of our application. ln Figure 3, you can see how isolation policies need applied at our API, compute, and storage layers. AWS Identity and Access Management (IAM) is essential for isolation of pooled resources, and techniques like Dynamic Policy Generation and Attribute Based Access Control (ABAC) provide mechanism to apply isolation policies.

Figure 3 – Isolation policies in a SaaS solution. Despite the lack of explicit prohibition of pooled resources in most compliance frameworks, SaaS providers that serve highly-regulated industries should still consider customer preference. You may need silo resources to accommodate silo resource requests as higher cost pricing tiers. Compliant Tenant Onboarding In silo environments, you must ensure all tenant environments are running the same compliant version of software. As you onboard new tenants, and as we deploy new versions of our software, you need to be able to provide assurances the version every tenant is running meets our compliance standards.

Figure 4 – Compliant tenant onboarding. AWS Organizations and AWS Control Tower can help you centrally provision tenants and set up Service Control Policy (SCP) guardrails that can prevent accidental modification of accounts. Landing Zones Accelerator on AWS (LZA) can assist with setting up security configurations with log archive and audit accounts, so evidence of activity is available to auditors. Compliance Implications for Data Storage Discussions around data in a SaaS solution begin with understanding how you'll partition your tenant data. In some ways, this is similar to tenant isolation, but consider there is "often a temptation to view data partitioning and tenant isolation as interchangeable. These two concepts are not meant to be equivalent. When we talk about data partitioning, we are talking about how tenant data is stored for individual tenants." (SaaS Whitepaper) You need to consider how you partition data for compliance purposes just like tenant isolation. This may include making decisions to segregate data based on its sensitivity, in addition to isolation concerns.

Figure 5 – Segregating sensitive SaaS data. Data partitioning decisions may align with data encryption requirements, as well as backup and restore policies, data retention, data sovereignty, and offboarding, including decisions about how to configure storage and databases around the capabilities of the services you are using. For example, an Amazon Simple Storage Service (Amazon S3) bucket and its folders can be configured to encrypt with AWS Key Management Service (AWS KMS) by default, offering the option of data encrypted with tenant-specific keys.

Figure 6 – Encrypting tenant Amazon S3 data. Multi-Tenant Auditing, Logging, and Monitoring You need to inject tenancy into everything our application emits, including logs, traces, and metrics. For compliance purposes, it's important to ensure logs are aggregated into a centralized logging solution, and access to those logs is restricted in an audit account. For verifying the history of access to your resources, turning on AWS CloudTrail for all accounts and regions. Eensure access logs are turned on for services that produce them; for example, Logging calls to Amazon API Gateway and Logging Amazon Cognito API Calls. Managing Tenant Identity You also need to manage tenant users and their access to your SaaS solution; this includes ensuring users are authenticated and connected to a specific tenant context. This context contains the information you pass along to every layer of your application, like logging and monitoring as we just discussed, and in fact every layer of your solution. Our compliance story is strengthened by a single source of truth for tenant identity, rather than tenant services and logic spread throughout our solution. Leveraging an identity provider (IdP) can provide controls you can inherit around your identity management, simplifying your compliance story. Getting Started with Security and Compliance We've looked at some of the reasons SaaS compliance may be challenging, so let's examine the journey towards compliance. For many SaaS providers, this may mean making incremental progress even if you don't yet know what compliance frameworks your customers will require. Many controls and requirements defined by compliance frameworks are founded on the best practices for cloud architecture. AWS has several free resources that can ensure your solution is following best practices, which can help prepare your architecture in anticipation of future compliance requirements.

AWS Startup Security Baseline: Minimized set of free controls, customized for the needs of early stage startups. AWS Well-Architected Tool: AWS best practices right in your AWS console, with additional reviews like the SaaS Lens as well. AWS Foundational Technical Review (FTR): For AWS Partners, FTRs combine a self-assessment with an AWS validation of your SaaS workload. Center for Internet Security (CIS) AWS Benchmarks and CIS Controls: AWS CIS Benchmarks provide a secure configuration baseline agreed on by security experts from around the industry ,and CIS Controls are a comprehensive best-practice guide for system and network security.

Members of the AWS Partner Network (APN) can leverage programs, including the AWS Global Security and Compliance Accelerator (GSCA), that help SaaS providers meet their customers' compliance needs. The AWS SaaS Factory program provides expert guidance to SaaS providers at any stage of the SaaS journey. Summary In this post, we examined a number of aspects of compliance on AWS and reviewed concerns you should keep in mind as you design and build your SaaS applications, cross-referencing the best practices for SaaS with compliance requirements. We've provided a number of free resources you can leverage to align with compliance best practices while building our SaaS solution. Finally, we discussed how you can use AWS compliance services to monitor your solution. About AWS SaaS Factory AWS SaaS Factory helps organizations at any stage of the SaaS journey. Whether looking to build new products, migrate existing applications, or optimize SaaS solutions on AWS, we can help. Visit the AWS SaaS Factory Insights Hub to discover more technical and business content and best practices. SaaS builders are encouraged to reach out to their account representative to inquire about engagement models and to work with the AWS SaaS Factory team. Sign up to stay informed about the latest SaaS on AWS news, resources, and events. About AWS Global Security and Compliance Acceleration Global Security and Compliance Acceleration on AWS Program (GSCA) supports businesses globally that need to meet security, privacy, and compliance requirements for financial services, healthcare, privacy, and public sector. This includes both commercial and public sector workloads. AWS Partners with compliances needs are encouraged to reach out to their account representative to understand program requirements and to engage with the GSCA team.

TAGS: APN Programs, AWS Partner Solutions Architects (SA), AWS SaaS Factory, SaaS Insights Hub, SaaS Journey Framework, SaaS Lens, SaaS on AWS, Software as a Service

Integrationworx Launches Amazon QuickSight Objects Management Tool OverSight with Support from SaaS Factory

Integrationworx enables customers to address various business challenges by leveraging data. It offers a diverse range of services such as analytics, data integration, data management, training, and mentoring. Integrationworx has worked with multiple customers who employ formal software development lifecycle (SDLC) processes to manage their Amazon QuickSight-based business intelligence (BI) solutions across segregated Amazon Web Services (AWS) accounts. Throughout these engagements, Integrationworx witnessed, first-hand, the complexity and effort required to migrate Amazon QuickSight assets across operating environments. This led to the idea of providing a set of operations and management tools, as an add-on to QuickSight, that helps customers manage their QuickSight SDLC in a low-friction, reliable, and repeatable way. With support from AWS SaaS Factory, Integrationworx built and launched its OverSight (OVS) solution on AWS. Oversight is a as a software-as-a-service (SaaS) application designed to enable development and DevOps teams to migrate QuickSight objects across AWS accounts. It supports the creation and management of a structured and repeatable dashboard development lifecycle (DDLC).

"We had great confidence in our delivery team to architect and implement OverSight from the very beginning. Our partnership with AWS SaaS Factory immediately turbo-charged our team with guidance and best-practices that would have taken us much more time to discover on our own, accelerating our time to market two-fold." ~ Glen Douglas, Chief Architect and Product Manager at Integrationworx

The AWS SaaS Factory team spoke with Glen Douglas, OverSight Chief Architect and Product Manager at Integrationworx, to learn more about OverSight, the value its new solution brings to customers, and the key lessons learned from the journey to SaaS on AWS. Check out OverSight in AWS Marketplace >> Q&A with Integrationworx AWS SaaS Factory: Can you tell us about yourself and role at Integrationworx? Glen Douglas: I've been working in the information technology industry for over 30 years, beginning in application development and gradually moving into roles of solution and enterprise architecture over the past 20. I am one of the five founding partners at Integrationworx, and among other roles I am the visionary and chief architect for OverSight. AWS SaaS Factory: Can you tell us a bit about the history of Integrationworx as a Systems Integrator? Glen Douglas: Integrationworx helps people and companies solve business problems using data. This includes empowering and transforming the business, dealing with data from mergers and acquisitions, managing data/information as an asset, discovering efficiencies, and identifying new markets and opportunities. Our service offerings include a wide range of data challenges, such as analytics, data integration, and data management. We pride ourselves in training and mentoring, and our relationship with AWS allows us to offer managed services if needed/desired. Integrationworx has covered all North America for over 18 years. AWS SaaS Factory: Who are your customers? Glen Douglas: OverSight is specifically targeted at AWS customers that use Amazon QuickSight to deliver business intelligence and analytics solutions:

Within their own organization and departments. As part of product and/or solution offering to their customers. As service offerings to other customers.

Our customers look to OverSight to provide a simple, fast, and reliable service to manage and copy QuickSight objects across AWS accounts they manage, without the need to learn complex APIs and command line interface (CLI) commands. This could be for a variety of use cases, including:

Support of formal SDLC processes across segregated environments. Supporting QuickSight-based solution deployments. General migration or duplication of QuickSight assets across environments.

AWS SaaS Factory: How does OverSight work and what are some of the key customer benefits? Glen Douglas: OverSight is a web application and set of integration services, which a customer can use to manage their QuickSight environments. The web application provides an easy-to-use interface and release management workflow for defining and managing groups of QuickSight assets called "Applications." Applications represent a logical container for managing, versioning, and distributing a set of QuickSight objects as a single unit of work. Once defined, OverSight users can create versioned "Releases" of their Applications representing a point-in-time snapshots of their assets, which in turn can be used as deployment packages and/or backups of their work. Once created, Releases can be "deployed" to another QuickSight account; all with the click of a button. OverSight customers realize a number of key benefits including:

Ready-to-use solution to quickly copy QuickSight objects across accounts. Turn-key release management tool to support their QuickSight SDLC. Version control, and backup and recovery of QuickSight objects. Zero investment of knowledge acquisition and retention of complicated QuickSight CLI and API. SaaS-based delivery of service for a monthly subscription.

AWS SaaS Factory: How can users get started using OverSight? Glen Douglas: Users can sign-up for a free 21-day trial of OverSight via AWS Marketplace, and be operating in a matter of minutes. End users can use the OverSight web application to configure one or more of their QuickSight (AWS) accounts to work with the OverSight integration service. With the configuration in place, end users can start defining their Applications and creating releases and deployments to other QuickSight accounts they have configured. AWS SaaS Factory: Can you walk us through the architecture? What are some of the AWS services you leveraged? Glen Douglas: The OverSight architecture is an adaptation of the AWS Serverless SaaS reference architecture, introduced to us by the SaaS Factory team. OverSight is based entirely on AWS serverless technologies, which allows us to minimize our operational costs and provide an extremely cost-effective solution to our customers. Some of the key services employed in the OverSight end-to-end architecture include:

Amazon CloudFront: Provides content delivery of web application(s) representing frontend user interface (UI) to customers and internal back-office management. AWS AppSync: Hosts GraphQL APIs used between our presentation layer and microservices. AWS Lambda: Hosts microservice workloads for managing core application functionality and interfacing to QuickSight. Amazon DynamoDB: Provides application data persistence using fast, flexible NoSQL database service. AWS Identity and Access Management (IAM): Securely manage trusted-relationships between OverSight service and customer AWS accounts and access to other AWS services and resources. Amazon Cognito: Hosts user/tenant pools and application authentication, authorization, and user management.

The solution employs numerous other AWS services including Amazon Simple Storage Service (Amazon S3), Amazon CloudWatch, Amazon API Gateway, AWS CodePipeline, and AWS CloudFormation. AWS SaaS Factory: What were some of the technical challenges when moving to a SaaS delivery model? Glen Douglas: Prior to engaging with AWS SaaS Factory, our first product release of OverSight was designed as a customer-hosted solution and employed a CLI-like interface implemented in AWS Step Functions. Moving from this customer-hosted architecture to a SaaS-based solution delivery required us to rethink the following:

Design and implementation of our security model to allow OverSight to manipulate QuickSight objects within the customer's AWS accounts. Supporting multi-tenancy within all layers of the solution (UI, services, security, and data persistence). Functional design and feature bundling to accommodate future usage and pricing models. Integration with AWS Marketplace.

AWS SaaS Factory: Can you share how AWS SaaS Factory helped you address these technical challenges? Glen Douglas: The transition of OverSight to a SaaS-based delivery model was drastically accelerated with the help of the SaaS Factory team and guidance they provided. Initial workshops provided a forum for our team to review architecture goals and vision, and receive feedback and suggestions on best practices for SaaS delivery implementation. A key factor in our success was the AWS SaaS Factory team's introduction of the Serverless SaaS reference architecture and solution, which immediately addressed a number of backlog items in our solution architecture. The reference architecture formed the basis for discussions on a variety of relevant design subjects, including support of multi-tenancy, cross-account security, and scalability. Regular touchpoint meetings with the SaaS Factory team provided a venue for deep-dive discussions on specific design patterns and reviews of our solution components as they evolved. AWS SaaS Factory: Can you walk us through how the SaaS Factory team supported your business efforts? Glen Douglas: The SaaS Factory team brought entirely different perspectives to both our approach to our go-to-market (GTM) strategy and solution architecture. Business workshops provided us with deep market research on the evolution of SaaS, as well as current trends and approaches to consider in our feature-based bundling and pricing. AWS SaaS Factory: What advice would you share with other organizations as they navigate to a SaaS delivery model? Glen Douglas: First, do your research to ensure your solution is suitable for a SaaS-based delivery model and be prepared to refactor or redesign aspects of your solution to leverage SaaS delivery capabilities. Take advantage of AWS best practices and leverage relationships with your AWS Partner Solution Architects to engage experts as needed. Seek out an AWS SaaS Factory team subject matter expert (SME) as they can provide both technical and business guidance on your SaaS journey, without re-inventing the wheel. Finally, look for opportunities to employ serverless technologies within your architecture to reduce operational costs and simplify performance efficiency within your offering. About AWS SaaS Factory AWS SaaS Factory helps organizations at any stage of the SaaS journey. Whether looking to build new products, migrate existing applications, or optimize SaaS solutions on AWS, we can help. Visit the AWS SaaS Factory Insights Hub to discover more technical and business content and best practices. SaaS builders are encouraged to reach out to their account representative to inquire about engagement models and to work with the AWS SaaS Factory team. Sign up to stay informed about the latest SaaS on AWS news, resources, and events. Integrationworx – AWS Partner Spotlight Integrationworx offers services and solutions for business data including data integration, data management and analytics. Contact Integrationworx | Partner Overview | AWS Marketplace

TAGS: AWS Partner References, AWS Partner Solutions Architects (SA), AWS SaaS Factory, Integrationworx, SaaS Factory Insights Hub, SaaS on AWS, Software as a Service

10DLC Registration Best Practices to Send SMS with AWS End User Messaging

Updated 08/04/2025 to include updates to abandoned cart use cases What is 10DLC? Ten-Digit Long Code, or more commonly shortened as 10DLC, is intended specifically for sending Application-to-Person (A2P) SMS in the United States only. If you don't send text messages to recipients in the US, then 10DLC doesn't apply to you. 10DLC was designed to cover the volume and throughput middle ground between toll-free numbers on the low end and short codes on the high end. All senders using 10DLC are required to register both their company and their campaign(s), which is managed by a third-party company called The Campaign Registry (TCR). TCR maintains an industry-wide database of companies and use cases that are authorized to send messages, to US registered handsets, using 10DLC phone numbers. How to Register for 10DLC Registration can be done within the AWS console as well as programmatic registration via the SMS V2 API.

Navigate to AWS End User Messaging Select "Registrations" from the left hand rail Click "Create registration" button

If you have not already registered a company then select registration type "US 10DLC brand registration" as the Registration type and give it a "Registration friendly name" you will recognize later and proceed with the best practices below. If you have already successfully registered a company and require additional vetting proceed to "Additional Vetting" below If you have already successfully registered a company and completed the additional vetting process proceed to "Campaign Registration" below

To help ensure your registration is approved during this vetting process follow these best practices when registering. Who Should Register for a 10DLC? The information provided during registration should be for the company from whom SMS messages will be sent from.

Example 1: Company X wants to send their customers alerts via SMS should their account be compromised and there is a need to reset passwords.

In this example the company being registered is Company X.

Example 2: Company Y is an Independent Software Vendor(ISV) with 100s of their customers using their software platform. Company Z wants to give their customers the ability to send SMS from within their platform.

In this example each of Company Y's customers who want to send SMS will need to provide their information. Each of these customers will need their own separate 10DLC for each use case that Company Y wants to enable for their customers. Company Y should define very clearly for their customers the types of messages that can be sent as each of their customers will be expected to send only messages that align with the Campaign(Use-Case) that they register for.

Example 3: Company Z is an Independent Software Vendor(ISV) with 100s of their customers using their software platform. Company Z wants to provide One-Time Password(OTP) codes via SMS.

In this example the company being registered will be Company Z.

10DLC Registration Best Practices As you progress through the steps of 10DLC registration follow these best practices to ensure a smooth process. Begin here if you have not registered your company(ies) yet. Company Registration Info and Additional Company and Contact Info

Best practices for Company Registration and Additional Company and Contact Info

Make sure to enter all information correctly. Dependent on the country in which you have a Tax ID, enter into the Tax ID field one of the following:

If you select "PUBLIC_PROFIT" as your "Legal form of organization" you MUST fill out the following fields and complete the external brand verification shown in the screenshots below in the section titled "Public Profit Brand Verification Email Process"

Stock symbol Stock exchange Brand verification email – Make sure to provide your personal company email. You will receive an email from [EMAIL] to complete the brand verification.

Select the vertical that most closely aligns with your business Make sure that your website is publicly accessible. Your registration will be denied if the reviewer cannot access the site. It is a hard requirement to have both a support email and phone number

Make sure your support email and support phone number are both active

Make sure that your Company name and Email/Website domains match

If you register the company Amazon Inc. but then list a support email of [EMAIL] your registration will likely be rejected if you are considered a large enough brand that should have a dedicated email domain.

Public Profit Brand Verification Email Process – Required if you selected "PUBLIC_PROFIT" as your "Legal form of organization" Once you submit your Brand Registration you will receive an email from [EMAIL] to complete the brand verification. This may take 1-3 days to arrive. Step 1: Example email you will receive below

Once you have completed and submitted your registration, as soon as you see your Brand Registration Status show as "Complete" you are ready to move on to "Brand Vetting." Read "Additional Company Vetting for Potential Increased Quotas" below for next steps.

Additional Company Vetting for Potential Increased Quotas Once you have completed the initial Company registration you have the following quotas assigned to your business:

AT&T: 1.25 Messages Per Second(MPS) or 75 Transactions Per Minute(TPM) T-Mobile = 2000 messages/day

The quotas above do not mean that you cannot message recipients who use other carriers, these are just limits that these carriers have published. If the throughput above isn't enough for your business's needs you can apply for US 10DLC brand vetting, for a $40 fee.

Click the "Create Registration" button again and select "US 10DLC brand vetting" as the "Registration type." Select the radio button for the brand you previously registered. This vetting will be applied to that brand.

If you have multiple brands you will need to do this for each of them

The Campaign Registry, a third-party provider, will then do a deeper vetting of the information you have already provided and will give your company a score that will determine the throughput and volume apportioned to you. Read here for a detailed breakdown of the possible scores and the quotas that are attached to them. Note: Vetting doesn't guarantee that your carrier throughput or daily volume will increase. It is possible for the vetting results to decrease carrier throughput and daily volume. 10DLC Campaign Registration Once you have completed the registration process and the optional additional vetting you will need to register your Campaigns, which should align with your use-case(s). If you would like more detail for each of the 10DLC Campaign types that End User Messaging supports you can read more here.

Provide a clear and comprehensive overview of the campaign's objectives and interactions the end-user would experience after opting in. Make sure to identify who the sender is, who the recipient is, and why messages are being sent to the intended recipient

Example: One-Time Password messages are sent by Company X to its customers for purposes of authentication to log into our application

Select the segment in which your company most closely operates in

You can either enter the URL if it is publicly accessible or upload a file that contains them The file can be up to 500KB and valid file formats are PNG, JPEG, and PDF Please read this blog for the boiler plate verbiage that needs to be included

You can either enter the URL if it is publicly accessible or upload a file that contains it The file can be up to 500KB and valid file formats are PNG, JPEG, and PDF Please read this blog for the boiler plate verbiage that needs to be included

The primary purpose of the Opt-in workflow is to demonstrate that the end user explicitly consents to receive text messages and understands the nature of the program. Your application is being reviewed by a 3rd party reviewer so make sure to provide clear and thorough information about how your end-users opt-in to your SMS service and any associated fees or charges. If the reviewer cannot determine how your opt-in process works then your application will be denied and returned.

This blog details the requirements that carriers have for a compliant SMS opt-in process Note: If you have a use case that is internal to your business, you are still required to demonstrate explicit opt-in consent from the recipients. There are no exceptions to having an opt-in workflow.

The description has to be between 40 – 2048 characters and must not contain leading or trailing spaces The Opt-in location must include the following at the point of opt-in, whether that be online, paper form, or verbal:

Program (brand) name Link to a publicly accessible Terms & Conditions page Link to a publicly accessible Privacy Policy page Message frequency disclosure. Customer care contact information Opt-out information "Message and data rates may apply" disclosure.

If your Opt-in process requires a log-in, is not yet published publicly, is a verbal opt-in, or if it occurs on printed sources such as fliers and paper forms then make sure to thoroughly document how this process is completed by the end-user receiving messages. Provide a screenshot of the Call to Action in such cases using the optional "Campaign opt-in screenshot" field below

Use this if your Opt-In Workflow requires a log-in, is not yet published publicly, is a verbal opt-in, or if it occurs on printed sources such as fliers and paper forms The supported file type are PNG, JPEG, and PDF and the maximum file size is 500KB

This is optional but if you plan on allowing for opt-in by texting into your originator you should indicate that keyword here

Provide the exact message that will be sent back to your end-users letting them know that they have successfully registered

"Welcome to AnyCo! Reply YES to confirm your subscription. Msg & data rates may apply. Text STOP to opt out and HELP for support."

Brand Name It is best practice, but not required, to do a "double opt-in" as seen in the example where the recipient will text back "YES" to confirm that they did want to register. Include "Msg & data rates may apply" as seen in the example Include opt-out language as seen in the example Include Help language as seen in the example

The "Help message" is the response that is required to be sent to end-users when they text the keyword "HELP" (or similar keywords). The purpose is to provide information to the end-user related to how they can get support or opt-out of the messaging program. The message has to be a minimum of 20 characters and a maximum of 160 characters The message must include:

Program (brand) name OR product description. Additional customer care contact information.

It is mandatory to include a phone number and/or email for end-user support

The following is an example of a HELP response that complies with the requirements of the US mobile carriers:

ExampleCorp Account Alerts: For help call 1-888-555-0142 or go to example.com. Msg&data rates may apply. Text STOP to cancel.

The "Stop message" is the response that is required to be sent to end-users when they text the keyword "STOP" (or similar keywords). End-users are required to be opted out of further messages when they text the STOP (or equivalent) keyword to your number and confirms with them that they will no longer receive messages for the program. The message has to be a minimum of 20 characters and a maximum of 160 characters The message must include:

Program (brand) name OR product description Confirmation that no further messages will be delivered

The following is an example of a compliant STOP response:

You are unsubscribed from ExampleCorp Account Alerts. No more messages will be sent. If you need help call 1-888-555-0142.

Campaign Capabilities Number capability: Choose whether or not the numbers you associate to an approved campaign can support voice outbound calling in addition to SMS. If you only require SMS you can leave the default selection of SMS-only. If you require voice calling, you should select voice as well. Selecting voice will increase the registration processing time. Message Type: The content of your messages need to align with the Campaign Type and Message Type that you select here — if it's misaligned your registration will be denied. You can't change the message type on a campaign after it's in an approved state.

Campaign Use Case End User Messaging supports all of the standard use cases available to be sent via 10DLC and a single Special use case for communications from a non-religious registered 501(c)(3) charity aimed at providing help and raising money for those in need. For a more detailed listing of the campaign use cases supported visit this page.

Select the Use case that most closely aligns to your use case.

All of the information that you provide during this process needs to align with this selection or your registration will be rejected Make sure to ONLY select a Sub use case if you select a use case of MIXED or LOW_VOLUME

Note: The "Low Volume" and "Mixed" campaigns have lower quotas which are the same as a company that does not opt for the increased vetting detailed above:

For each of the Yes/No drop down selections make sure to be truthful. These registrations are being done by humans who will be checking each of these. An untruthful answer can cause your registration to be rejected.

If you plan on using links within your messages remember that generic URL shorteners e.g. "bit.ly/LONGLINK" will be rejected. If you would like to use shorteners make sure that it is a branded shortener such as "any.co/LONGLINK"

NOTE: If you are planning on sending abandoned cart messages there are additional requirements. These requirements include:

Additional information in your Privacy Policy and Terms and Conditions Opt-in process must be double opt-in and include specific verbiage for the use case Content restrictions on time and frequency

Details about this can be found in: CTIA Guidelines 3.16 Section 6.2 of T-Mobile's code of conduct

Subscriber opt-in is automatically set to "Yes" on your behalf. Explicit opt-in is required of all end-users regardless of your use case.

You are required by carriers to opt-out end users at their request. This is generally done through the opt-out keyword 'STOP'. More information related to opt-outs and how to manage them effectively can be found here

Carriers require that your SMS numbers reply to the 'HELP' keyword or similar at all times regardless of the numbers opt-in status. More information related to HELP auto-response requirements can be found in End User Messaging best practices documentation here

If you are a 1st party lender you can get approval for transactional use cases (loan transaction receipts, OTPs, etc.). If your company is related to the lending business then you must mark this as "yes"

If you have supplied messaging examples with an embedded link you must mark this as a "yes." If this is misaligned with your content then your registration will be rejected

Note: Generic link shorteners such as Bitly or TinyURL should not be used and may cause your registration to be rejected. Make sure that any links in your sample messages are branded and consistent with your domain

If you have supplied messaging examples with an embedded phone number you must mark this as a "yes." If this is misaligned with your content then your registration will be rejected

There is a potential to be rejected or for the campaign to be suspended later if your content includes age gated material and you do not mark "yes" here If they are do they need to do anything different here?

Message Samples Sample messages should reflect actual messages to be sent under the campaign you are registering for. It is critical to ensure that there is consistency between the use case, your campaign description, and the content of the messages.

Sample messages should reflect actual messages to be sent under campaign Indicate any templated fields that are variable with brackets and make sure to be clear with what information may be replaced

Example: Hi, [FirstName] this is Amazon inc. letting you know that your delivery is ready

Each sample message has to be a minimum of 20 characters. If you plan to use multiple message templates for this 10DLC campaign, include them as well Sample messages should identify who is sending the message (brand name)

Make sure your messaging does not involve prohibited content such as cannabis, hate speech, etc. and that your use case is compliant with AWS Messaging Policy

What to do if your 10DLC campaigns are rejected If your Company registration or Campaign registration is rejected please follow the steps here to create a case and the AWS Support team will provide information about the reasons that your 10DLC campaign registration was rejected in your AWS Support case.

How to Set Up a Platform That Effectively Supports Your Development Teams

Many of my conversations with AWS customers are about their attempts to build developer experience platforms that simplify software development and operations, automate deployments, improve software quality, reduce costs, and ensure security and compliance. Unfortunately, not all platforms live up to their expectations. The most frequently cited problem is development teams rejecting the platform that is supposed to support them. Other problems include a lack of collaboration between the platform teams and the development teams and complex platform requirements. In this blog post, I will give you an overview of how you can set up a platform to effectively support your development teams. The Purpose of Platforms The main purpose of IT platforms is to provide delivery teams with simple, efficient, and stress-free toolchains to build and run software so that they can focus on solving business problems faster. A platform team is an enabling organizational unit

that provides tooling-as-a-service to other IT teams, mostly software and data engineering teams; that helps to onboard, support, and advise these teams on the platform's tools and services; that facilitates alignment between the users of the platform on the scope, roadmap, and priority of the platform's tools and services; and that is accountable for ensuring product development teams adhere to defined global guidelines regarding security, compliance, and financial aspects

Platform Scope Developer experience platforms can offer a wide range of tools and services designed to support the development and operations of transactional and analytical applications. Typical products include tools and services for software development and testing, CI/CD pipelines, monitoring and logging, containerization and orchestration, service and data discovery, analytics, security, collaboration, and billing and backup services. In some organizations, platforms provide hosting services for deploying and running applications. Platform Size As a rule of thumb, 10% to 20% of a company's product development should consist of platforms. This may vary over time and depends on the scope of the platform. Resources may be greater at the beginning of platform development and scaled down as the functionality becomes sufficient and the platform scope experiences fewer changes. Characteristics of a Platform Team Platform teams are product teams for internally used products. They are small, diverse, cross-functional, and self-organizing. They include every role required to design, build, and operate platform products. Effective teams include 6–10 people, with some leeway. The roles required depend on your organization's current operating model and maturity. Typical roles include product owners, software developers, and sometimes quality assurance (QA) engineers and IT administrators. Platform product owners should understand the socio-technical domain of developer tooling (probably as former users of the platform's products) and have ideation, communication, facilitation, and delivery management skills. Organizations sometimes underestimate the role of the product owner—this is a mistake. The product owner plays a key role in the success of a platform. Successful platform teams work backward from user needs. It is important to have team members with both operational and development backgrounds. Developers should be able to work across the technology stack of their team's responsibility with expertise in a specific area (these are called T- or V-shaped developers). In some organizations, developers are also responsible for testing their products. Other organizations add the role of a dedicated QA engineer, a software developer focused on test automation. I recommend empowering and training developers to quality assure their own products. The fewer roles there are on a team, the more flexible and effective it is. If the QA engineer is a dedicated role, they should be part of the platform team instead of a separate QA team. This allows them to take responsibility for a specific part of the platform over an extended period and run their services independently. How Do You Know If Your Platform Is Good? It is difficult to measure whether a platform is achieving its goal, which makes it important to gather the right data and anecdotes. A good indicator is the net promoter score (NPS), a metric based on asking platform users if they would recommend the service to a new member. Another indicator is the adoption rate of nonmandatory services in delivery teams. These two metrics are general indicators, but you can get very specific metrics about a new version of a platform service by deploying it in an A/B test. In this method, some delivery teams use the new service while a control group doesn't. You can measure and compare certain capabilities like cycle time, throughput, release frequency, or duration for both groups. How to Get Platform Adopted I have successfully used three strategies to overcome problems in getting platform services adopted:

Involve product team representatives in making decisions for the platform's development—but don't treat them as just a sounding board; grant them decision authority regarding the scope and prioritization of the platform's products. Rotate engineers between the user teams and platform teams. This way, the engineers who will use the platform are actively involved in building it and can ensure that it is designed to meet their needs. Conversely, the platform engineers have to eat their own dog food while working on the consuming engineering teams. A reasonable rotation length is a few weeks but no longer than a quarter. Avoid speculative design and gold plating at all costs. Don't generalize too soon. Only build a product when other consuming teams have committed to using it. It is even better to prototype platform products for a single team (perhaps with additional capacity from the platform teams) to learn and use in one context. Only move the product to platform ownership and generalize it when two other teams commit to using it.

Conclusion Platforms are an important part of larger product development organizations. Among other benefits, they can implement centralized and automated that policies improve your compliance and security. When set up correctly, they can improve the quality, cycle time, throughput, and motivation of your delivery teams. The principles I've given here should help you get the most of your platform and the team that creates and manages it. What are your experiences with developer experience platforms? I would be interested in hearing about some of them.

TAGS: Agility, Best Practices, Business Value, Cloud Transformation, Culture, DevOps, Digital Transformation, enterprise strategy, Migration, Organizational Agility

Matthias Patzak Matthias joined the Enterprise Strategist team in early 2023 after a stint as a Principal Advisor in AWS Solutions Architecture. In this role, Matthias works with executive teams on how the cloud can help to increase the speed of innovation, the efficiency of their IT, and the business value their technology generates from a people, process and technology perspective. Before joining AWS, Matthias was Vice President IT at AutoScout24 and Managing Director at Home Shopping Europe. In both companies he introduced lean-agile operational models at scale and led successful cloud transformations resulting in shorter delivery times, increased business value and higher company valuations

How Amazon Shopping uses Amazon Rekognition Content Moderation to review harmful images in product reviews

Customers are increasingly turning to product reviews to make informed decisions in their shopping journey, whether they're purchasing everyday items like a kitchen towel or making major purchases like buying a car. These reviews have transformed into an essential source of information, enabling shoppers to access the opinions and experiences of other customers. As a result, product reviews have become a crucial aspect of any store, offering valuable feedback and insights to help inform purchase decisions. Amazon has one of the largest stores with hundreds of millions of items available. In 2022, 125 million customers contributed nearly 1.5 billion reviews and ratings to Amazon stores, making online reviews at Amazon a solid source of feedback for customers. At the scale of product reviews submitted every month, it is essential to verify that these reviews align with Amazon Community Guidelines regarding acceptable language, words, videos, and images. This practice is in place to guarantee customers receive accurate information regarding the product, and to prevent reviews from including inappropriate language, offensive imagery, or any type of hate speech directed towards individuals or communities. By enforcing these guidelines, Amazon can maintain a safe and inclusive environment for all customers. Content moderation automation allows Amazon to scale the process while keeping high accuracy. It's a complex problem space with unique challenges and requiring different techniques for text, images, and videos. Images are a relevant component of product reviews, often providing a more immediate impact on customers than text. With Amazon Rekognition Content Moderation, Amazon is able to automatically detect harmful images in product reviews with higher accuracy, reducing reliance on human reviewers to moderate such content. Rekognition Content Moderation has helped to improve the well-being of human moderators and achieve significant cost savings.

Moderation with self-hosted ML models The Amazon Shopping team designed and implemented a moderation system that uses machine learning (ML) in conjunction with human-in-the-loop (HITL) review to ensure product reviews are about the customer experience with the product and don't contain inappropriate or harmful content as per the community guidelines. The image moderation subsystem, as illustrated in the following diagram, utilized multiple self-hosted and self-trained computer vision models to detect images that violate Amazon guidelines. The decision handler determines the moderation action and provides reasons for its decision based on the ML models' output, thereby deciding whether the image required a further review by a human moderator or could be automatically approved or rejected.

With these self-hosted ML models, the team started by automating decisions on 40% of the images received as part of the reviews and continuously worked on improving the solution through the years while facing several challenges:

Ongoing efforts to improve automation rate – The team desired to improve the accuracy of ML algorithms, aiming to increase the automation rate. This requires continuous investments in data labeling, data science, and MLOps for models training and deployment. System complexity – The architecture complexity requires investments in MLOps to ensure the ML inference process scales efficiently to meet the growing content submission traffic.

Replace self-hosted ML models with the Rekognition Content Moderation API Amazon Rekognition is a managed artificial intelligence (AI) service that offers pre-trained models through an API interface for image and video moderation. It has been widely adopted by industries such as ecommerce, social media, gaming, online dating apps, and others to moderate user-generated content (UGC). This includes a range of content types, such as product reviews, user profiles, and social media post moderation. Rekognition Content Moderation automates and streamlines image and video moderation workflows without requiring ML experience. Amazon Rekognition customers can process millions of images and videos, efficiently detecting inappropriate or unwanted content, with fully managed APIs and customizable moderation rules to keep users safe and the business compliant. The team successfully migrated a subset of self-managed ML models in the image moderation system for nudity and not safe for work (NSFW) content detection to the Amazon Rekognition Detect Moderation API, taking advantage of the highly accurate and comprehensive pre-trained moderation models. With the high accuracy of Amazon Rekognition, the team has been able to automate more decisions, save costs, and simplify their system architecture.

Improved accuracy and expanded moderation categories The implementation of the Amazon Rekognition image moderation API has resulted in higher accuracy for detection of inappropriate content. This implies that an additional approximate of 1 million images per year will be automatically moderated without the need for any human review. Operational excellence The Amazon Shopping team was able to simplify the system architecture, reducing the operational effort required to manage and maintain the system. This approach has saved them months of DevOps effort per year, which means they can now allocate their time to developing innovative features instead of spending it on operational tasks. Cost reduction The high accuracy from Rekognition Content Moderation has enabled the team to send fewer images for human review, including potentially inappropriate content. This has reduced the cost associated with human moderation and allowed moderators to focus their efforts on more high-value business tasks. Combined with the DevOps efficiency gains, the Amazon Shopping team achieved significant cost savings. Conclusion Migrating from self-hosted ML models to the Amazon Rekognition Moderation API for product review moderation can provide many benefits for businesses, including significant cost savings. By automating the moderation process, online stores can quickly and accurately moderate large volumes of product reviews, improving the customer experience by ensuring that inappropriate or spam content is quickly removed. Additionally, by using a managed service like the Amazon Rekognition Moderation API, companies can reduce the time and resources needed to develop and maintain their own models, which can be especially useful for businesses with limited technical resources. The API's flexibility also allows online stores to customize their moderation rules and thresholds to fit their specific needs. Learn more about content moderation on AWS and our content moderation ML use cases. Take the first step towards streamlining your content moderation operations with AWS.

About the Authors Shipra Kanoria is a Principal Product Manager at AWS. She is passionate about helping customers solve their most complex problems with the power of machine learning and artificial intelligence. Before joining AWS, Shipra spent over 4 years at Amazon Alexa, where she launched many productivity-related features on the Alexa voice assistant. Luca Agostino Rubino is a Principal Software Engineer in the Amazon Shopping team. He works on Community features like Customer Reviews and Q&As, focusing through the years on Content Moderation and on scaling and automation of Machine Learning solutions. Lana Zhang is a Senior Solutions Architect at AWS WWSO AI Services team, specializing in AI and ML for Content Moderation, Computer Vision, Natural Language Processing and Generative AI. With her expertise, she is dedicated to promoting AWS AI/ML solutions and assisting customers in transforming their business solutions across diverse industries, including social media, gaming, e-commerce, media, advertising & marketing.

New – Amazon EC2 M7a General Purpose Instances Powered by 4th Gen AMD EPYC Processors

In November 2021, we launched Amazon EC2 M6a instances, powered by 3rd Gen AMD EPYC (Milan) processors, running at frequencies up to 3.6 GHz, which offer you up to 35 percent improvement in price performance compared to M5a instances. Many customers who run workloads that are dependent on x86 instructions, such as SAP, are looking for ways to optimize their cloud utilization. They're taking advantage of the compute choice that EC2 offers. Today, we're announcing the general availability of new, general purpose Amazon EC2 M7a instances, powered by the 4th Gen AMD EPYC (Genoa) processors with a maximum frequency of 3.7 GHz, which offer up to 50 percent higher performance compared to M6a instances. This increased performance gives you the ability to process data faster, consolidate workloads, and lower the cost of ownership. M7a instances support AVX-512, Vector Neural Network Instructions (VNNI) and brain floating point (bfloat16). These instances feature Double Data Rate 5 (DDR5) memory, which enable high-speed access to data in-memory, and deliver 2.25 times more memory bandwidth compared to M6a instances for lower latency. M7a instances are SAP-certified and ideal for applications that benefit from high performance and high throughput, such as financial applications, application servers, simulation modeling, gaming, mid-size data stores, application development environments, and caching fleets. M7a instances feature sizes of up to 192 vCPUs with 768 GiB RAM. Here are the detailed specs:

Name vCPUs Memory (GiB) Network Bandwidth (Gbps) EBS Bandwidth (Gbps)

M7a instances have up to 50 Gbps enhanced networking and 40 Gbps EBS bandwidth, which is similar to M6a instances. But you have a new medium instance size, which enables you to right-size your workloads more accurately, offering 1 vCPUs, 4 GiB, and the largest size offering 192 vCPUs, 768 GiB. Additionally, with M7a instances you can attach up to 128 EBS volumes to an instance compared to up to 28 EBS volume attachments with M6a instances. The new instances are built on the AWS Nitro System, a collection of building blocks that offloads many of the traditional virtualization functions to dedicated hardware for high performance, high availability, and highly secure cloud instances. Now Available Amazon EC2 M7a instances are now available today in AWS Regions: US East (Ohio), US East (N. Virginia), US West (Oregon), and EU (Ireland). As usual with Amazon EC2, you only pay for what you use. For more information, see the Amazon EC2 pricing page. To learn more, visit the EC2 M7a instance and AWS/AMD partner page. You can send feedback to [EMAIL] AWS re:Post for EC2, or through your usual AWS Support contacts. — Channy

Channy Yun (윤석찬) Channy is a Lead Blogger of AWS News Blog and Principal Developer Advocate for AWS Cloud. As an open web enthusiast and blogger at heart, he loves community-driven learning and sharing of technology.

Introducing customer-defined partition keys for Amazon Timestream: Optimizing query performance

Amazon Timestream is a fully managed, scalable, and secure time series database designed for workloads such as infrastructure observability, user behavioral analytics, and Internet of Things (IoT) workloads. It's built to handle trillions of events per day, and designed to scale horizontally to meet your needs. With features like multi-measure records and scheduled queries, Timestream enables you to analyze time series data and gain valuable insights in a cost-effective way. The flexible data model of Timestream allows you to store and query data in a way that makes sense for your use case. Whether you are tracking performance metrics for a fleet of devices, or analyzing customer behavior in real time, Timestream can address your needs. And now, with the introduction of customer-defined partition keys, you can access your time series data faster than before due to query optimizations tailored to your specific needs. The need for customer-defined partition keys Many customers and internal services, like our internal Alexa Voice Service (AVS) team, uses Timestream because their use cases require highly scalable ingestion and optimized query execution. As their systems scaled over time, we learned that though the existing partitioning schema enabled scaling, there was still a need for a flexible options that could accommodate query patterns more attuned to their use cases. This differed from the fleet-wide trends tracking and analysis on measures we had initially optimized for. With this in mind, we are excited to announce the launch of customer-defined partition keys, a new feature that provides you the flexibility you need to speed up your queries and derive insights more efficiently based on your specific time series data-related needs. Partitioning is a technique used to distribute data across multiple physical storage units, allowing for faster and more efficient data retrieval. With customer-defined partition keys feature, customers can create a partitioning schema that better aligns with their query patterns and use cases. In Timestream, a record is a single data point in a time series. Each record contains three parts:

Timestamp – This usually indicates when data was generated for a given record Set of dimensions – This is metadata that uniquely identifies an entity (for example, a device) Measures – These represent the actual value being measured by a record and tracked in a time series

To learn more about Timestream key concepts, refer to Amazon Timestream concepts. By allowing you to choose a specific dimension to partition your data on, Timestream reduces the amount of data scanned during queries, thereby substantially reducing query latency for access patterns that match your partitioning schema. For instance, filtering by customer ID, device ID, or location was a very common access pattern that many of our customers use. By allowing you to choose any of such dimensions as your partition key, you can optimize your queries and get the most out of your time series data. With this added flexibility, Timestream will be able to better adapt to specific customer workloads, and we are excited to see the innovative ways you will use this new feature to extract more value from your data. How customer-defined partition keys work To get started, simply create a new table and select a specific partition key based on your most common access pattern. Usually, this is your main filtering condition in your query predicate (for example, customer ID or device ID). Identifying and selecting the right dimension by which to partition will ensure you get the best possible performance from the feature. Also, you can optionally configure your timestream table to accept only records containing non-null values for your partition key. This will further optimize partitioning the data in your table for the best query performance. When choosing the partition key type, don't forget to choose Dimension if you want to choose you own dimension. If you chose Measure name, Timestream will adopt the default partitioning schema, which is better suited to analyze fleet-wide variations of a specific measure over time.

When we talk about common access patterns, we're referring to the types of filters or predicates that are typically used when querying time series data. These filters may include things like a specific customer ID, device ID, or any other high-cardinality dimension that's important to your business. For this hypothetical use case where the partitioning key is DeviceModel, it could be something like the following code:

Select DeviceModel, segment, Event_name, count(event_name) as occurrences from events where event_type = 'voice chrome latency' and DeviceModel in ('xxx-xxx-xx' , 'yyy-yyy-yy', 'zzz-zzz-zz', '%-012') and time >= ago(1h) group by 1, 2,3 order by occurrencesSQL

For example, if you're using Timestream to store data from smart devices or IoT applications, you might want to partition your data based on the device model. This would allow you to quickly retrieve data for a specific set of devices without having to scan through all of the data in the table. Similarly, if you're storing data about customer interactions, consider partitioning your data based on the customer ID to quickly retrieve all interactions for a specific customer. This will also provide visibility of all customer-associated devices. In cases where you could have multiple different access patterns that could be derived from the same raw data, you could even generate a scheduled query with a different partition key optimized for different use cases, for example, one to understand behavior by device model and another for root cause analysis related to a specific customer issue. By understanding your most common access patterns, you can choose the dimension that will be most beneficial as a partition key. This will allow Timestream to optimize query performance for those specific access patterns, leading to faster and more efficient analysis of your time series data. After you choose your partitioning dimension, Timestream automatically partitions the ingested data to optimize the performance of queries that are based on such partitioning. It's important to carefully consider the query patterns and select the most appropriate dimension as the partition key because once the table is partitioned, it cannot be changed. To ensure the accuracy of your data and maximize query efficiency, we are also launching a new capability called schema enforcement. This will allow you to configure the table so that it will reject write requests that don't contain the dimension that is being used as the partition key. This will help ensure that your data is properly partitioned, leading to improved query performance and faster insights. It's recommended to set the enforcement level to REQUIRED, which ensures that you will get the full benefits of your partitioning schema. However, we understand that there may be cases where a small amount of data might not contain the partitioning dimensions, and it's still the customer's preference to accept those records. In such cases, setting the enforcement level to OPTIONAL can be useful. All records that don't contain the required dimension will still be written to the system, but will be collocated together. The enforcement level can be changed at any time during the table existence. Using customer-defined partition keys: A case study Let's look at an example of how our internal AVS device monitoring teams benefited from customer-defined partition keys feature to monitor the performance of millions of in-field connected device performance. AVS device monitoring team ingests data from millions of connected devices on a daily basis, including device metrics, system health metrics, and cloud-side metrics. The goal is to ensure a delightful customer experience across all their devices. With such a large volume of data, it's important to have an efficient way of analyzing data at the entity level. In this particular use case, an entity is an individual device or all the devices owned by a particular form factor segment . The device monitoring systems needs to analyze device and cloud side metrics near real time, detect any anomalies, and proactively mitigate these issues to minimize long-lasting impact on customer engagement. The following diagram illustrates the architecture that was implemented for the above use case.

One possible data model could include data for region, device type, health metric, measure and more. By defining a partition key on Device Type, for example, it optimizes entity-level queries, such as root cause analysis of issues or deviations based on this particular attribute. This partitioning schema efficiently collocates data in a way that closely resembles their specific query patterns, thereby reducing the amount of data scanned during queries. This improved query performance, allowing for faster insight generation or issue detection, so the team could act quickly to provide the best possible query latency. Let's look at an example data model.

This data model contains few dimensions with a cardinality that could be in the millions. In this data model, device monitoring team chose the device_type dimension as the partition key. Let's look at a few examples that would see performance benefits if data is partitioned by app_name, Device_type, or device_model (Timestream allows only one partition-key to be chosen on a table). With this schema, we could get very valuable insights, such as the following:

If your primary use case requires finding all devices used by a specific application ranked by times used, your ideal partition key is app_name. If your primary use case is to find all failed and successful request events for a specific segment and calculate the average latency or outcome experience by device type, your ideal partition key is segment. If your primary use case is to calculate latencies related to successful interactions, responses, and successful prompt detection for all domain events, your ideal partition key is Domain. If your goal is to generate dashboards that show the most problematic device type, based on failed events around the world, your ideal partition key is device_type)

By utilizing customer-defined-partition-key feature, in conjunction with other features like scheduled queries, we have observed improved levels of query performance and further cost optimization thanks to more efficient data access patterns. To learn more about scheduled queries, refer to Using scheduled queries in Timestream. Now it's to optimize your queries! Customer-defined partition keys feature can be a powerful tool for optimizing query performance on entity-level analysis. By understanding the most common access patterns and choosing a high-cardinality dimension that fits most data access needs, you can benefit from faster query times and improved insights. In the above use case for device monitoring, optimizing queries for specific dimension-level analysis helped the team to better understand their device performance and improve their service accordingly. With the added benefit of schema enforcement, you can ensure that your tables are properly configured according to your specific needs, so you can decide to either to reject or allow write requests that don't contain the dimension being used as the partition key. Get started with Amazon Timestream and Customer Defined Partition Keys for Free, taking advantage of our 30-day Free trial and start enhancing your time-series workloads!

Victor Servin is a Senior Product Manager for the Amazon Timestream team at AWS, bringing over 18 years of experience leading product and engineering teams in the Telco vertical. With an additional 5 years of expertise in supporting startups with Product Led Growth strategies and scalable architecture, Victor's data-driven approach is perfectly suited to drive the adoption of analytical products like Timestream. His extensive experience and commitment to customer success allows him to help customers to efficiently achieve their goals. Yogeesh Rajendra is a tenured Engineering Tech Lead at Alexa Amazon in Bellevue, WA, USA. With over 10 years of expertise in software development and AWS technologies, he excels in building highly scalable and robust distributed systems for customers. Yogeesh's focus is on handling high-volume, high-variance, and high-velocity data, ensuring that customer experiences on Alexa-enabled devices worldwide are seamless and never compromised. His technical leadership and collaborative approach enable him to deliver innovative solutions, continuously enhancing the capabilities and performance of Alexa devices. Ramsundar Muthusubramanian is a Senior Data Engineer at Alexa Amazon in Bellevue. With over 12 years of experience in Data Warehousing and Analytics, Ram had delivered big data projects like Realtime Engine Telemetrics, Demand Forecasting Data Analytics, Realtime Certification Analytics using a range of AWS Services like Redshift, Kinesis Streams, Dynamo DB, Kinesis Firehose, Lambda. Excels in designing data lake architectures and solving data problems with best data modeling practices. Seshadri Pillailokam is a Senior Software Dev Manager, Alexa Developer Experience and Customer Trust. He runs the engineering org that builds few monitoring products for Alexa using various AWS technologies. For the last few years, he has been solving security, and risk challenges through various big data technologies, to keep his customers secure. Previously he worked for over a decade on developer facing tools and technologies for Amazon, and others. Brutus Martin is a Software Development Manager, Alexa Developer Experience and Customer Trust. His team is responsible for building platforms, tools and systems that are required to monitor 3P contents & devices to ensure customer safety and trust and better experience. Brutus has 17 years of experience in building software services and products. His team's main expertise is in building distributed scalable and extensible systems to detect anomalies, analyzing digital contents across text, image, audio and video in terms of policy, security and functional infractions that could cause a trust busting experience for customers. Brutus also manages data team that integrates data from various systems across Alexa teams to build business and technical insights.

Security is fundamental for each product and service you are building with. Whether you are working on the back-end or the data and machine learning components of a system, the solution should be securely built. In 2022, we discussed security in our post Let's Architect! Architecting for Security. Today, we take a closer look at general security practices for your cloud workloads to secure both networks and applications, with a mix of resources to show you how to architect for security using the services offered by Amazon Web Services (AWS). In this edition of Let's Architect!, we share some practices for protecting your workloads from the most common attacks, introduce the Zero Trust principle (you can learn how AWS itself is implementing it!), plus how to move to containers and/or alternative approaches for managing your secrets. A deep dive on the current security threat landscape with AWS This session from AWS re:Invent, security engineers guide you through the most common threat vectors and vulnerabilities that AWS customers faced in 2022. For each possible threat, you can learn how it's implemented by attackers, the weaknesses attackers tend to leverage, and the solutions offered by AWS to avert these security issues. We describe this as fundamental architecting for security: this implies adopting suitable services to protect your workloads, as well as follow architectural practices for security. Take me to this re:Invent 2022 session!

Statistics about common attacks and how they can be launched

Zero Trust: Enough talk, let's build better security What is Zero Trust? It is a security model that produces higher security outcomes compared with the traditional network perimeter model. How does Zero Trust work in practice, and how can you start adopting it? This AWS re:Invent 2022 session defines the Zero Trust models and explains how to implement one. You can learn how it is used within AWS, as well as how any architecture can be built with these pillars in mind. Furthermore, there is a practical use case to show you how Delphix put Zero Trust into production. Take me to this re:Invent 2022 session!

AWS implements the Zero Trust principle for managing interactions across different services

A deep dive into container security on AWS Nowadays, it's vital to have a thorough understanding of a container's underlying security layers. AWS services, like Amazon Elastic Kubernetes Service and Amazon Elastic Container Service, have harnessed these Linux security-layer protections, keeping a sharp focus on the principle of least privilege. This approach significantly minimizes the potential attack surface by limiting the permissions and privileges of processes, thus upholding the integrity of the system. This re:Inforce 2023 session discusses best practices for securing containers for your distributed systems. Take me to this re:Inforce 2023 session!

Migrating your secrets to AWS Secrets Manager Secrets play a critical role in providing access to confidential systems and resources. Ensuring the secure and consistent management of these secrets, however, presents a challenge for many organizations. Anti-patterns observed in numerous organizational secrets management systems include sharing plaintext secrets via unsecured means, such as emails or messaging apps, which can allow application developers to view these secrets in plaintext or even neglect to rotate secrets regularly. This detailed guidance walks you through the steps of discovering and classifying secrets, plus explains the implementation and migration processes involved in transferring secrets to AWS Secrets Manager. Take me to this AWS Security Blog post!

An organization's perspectives and responsibilities when building a secrets management solution

Conclusion We're glad you joined our conversation on building secure architectures! Join us in a couple of weeks when we'll talk about cost optimization on AWS. To find all the blogs from this series, visit the Let's Architect! list of content on the AWS Architecture Blog.

Luca Mezzalira Luca is Principal Solutions Architect based in London. He has authored several books and is an international speaker. He lent his expertise predominantly in the solution architecture field. Luca has gained accolades for revolutionizing the scalability of front-end architectures with micro-frontends, from increasing the efficiency of workflows, to delivering quality in products.

Federica Ciuffo Federica is a Solutions Architect at Amazon Web Services. She is specialized in container services and is passionate about building infrastructure with code. Outside of the office, she enjoys reading, drawing, and spending time with her friends, preferably in restaurants trying out new dishes from different cuisines.

Vittorio Denti Vittorio Denti is a Machine Learning Engineer at Amazon based in London. After completing his M.Sc. in Computer Science and Engineering at Politecnico di Milano (Milan) and the KTH Royal Institute of Technology (Stockholm), he joined AWS. Vittorio has a background in distributed systems and machine learning. He's especially passionate about software engineering and the latest innovations in machine learning science.

Zamira Jaupaj Zamira is an Enterprise Solutions Architect based in the Netherlands. She is highly passionate IT professional with over 10 years of multi-national experience in designing and implementing critical and complex solutions with containers, serverless, and data analytics for small and enterprise companies.

Deploying Amazon VPC IP Address Manager (IPAM) using AWS Service Catalog

Planning, tracking, and monitoring IP addresses for large-scale networks can challenging. Network Administrators often use a combination of spreadsheets, confluence pages, and home-grown tools to track IP address assignments across Amazon Virtual Private Clouds (Amazon VPCs), AWS Regions, and AWS accounts. However, these methods are largely manual and prone to errors, and even a minor mistake can cause IP address conflicts that can cause issues in establishing bidirectional connectivity. This problem is amplified in large enterprise networks, where the AWS environment spans multiple AWS Organizational Units (OUs), AWS accounts, or even AWS Organizations. This is where Amazon VPC IP Address Manager (IPAM) comes in. IPAM simplifies IP address planning, tracking, and monitoring for your enterprise. With IPAM, you can release applications more quickly because developers don't have to wait for the networking team to manage IP addresses. You can find overlapping IP addresses and fix them before issues arise with network connectivity. IPAM can notify you if your IP address pools are nearing exhaustion—it lets you quickly and efficiently perform routine IP address management activities. If you combine IPAM with the power of Infrastructure-as-Code (IaC), then you can deploy IPAM across your environment quickly and in compliance with best practices. In this post, we describe a solution to turn on IPAM using AWS Service Catalog, and we walk you through it step-by-step. AWS Service Catalog uses AWS CloudFormation to abstract the underlying complexity and provides standardized deployments. Solution overview The solution described in this post uses AWS Service Catalog to provision VPC IPAM pools across multiple accounts. We create two AWS Service Catalog Portfolios: (1) IPAM Main Portfolio which comprises three Service Catalog products, and (2) IPAM Spoke Portfolio which comprises one Service Catalog product. A Service Catalog product is an IT service or application that you want to make available for deployment on AWS. Using AWS Resource Access Manager (AWS RAM), we share these products with specific AWS accounts, or the entire Organization. This makes the products available to both existing and future accounts in your organization. Users with access to the AWS Service Catalog can provision these products in a self-service manner. This solution uses CloudFormation to create the Service Catalog portfolios and its products, as shown in the following diagram (figure 1).

Figure 1: Amazon VPC IP Address Management using AWS Service Catalog

Portfolios and products overview IPAM Main Portfolio: This portfolio comprises three Service Catalog products and is shared with the NetworkHub/Networking account where the IPAM pools are to be provisioned.

Delegate IPAM Product: This product delegates the NetworkHub account as the IPAM Delegated Administrator account. IPAM CloudFormation Macro Product: This product deploys a CloudFormation Macro and the underlying AWS Lambda function which is used to perform the template processing for the macro. The Lambda function accepts the provided IPAM template (ipam-product.yml) and returns the processed template as a response. This CloudFormation Macro processes the ipam-product.yml base template to dynamically render a CloudFormation template for creating multiple IPAM resources from a single resource definition. It is primarily used to replicate the IPAM resource block for all provided AWS Regions in the pOperatingRegionList parameter value in the source template. It dynamically creates multiple IPAM Pool resources from the defined CIDR blocks in the template, for all provided pOperatingEnvironmentsList (Prod,NonProd). IPAM Product: This product is used to provision the IPAM Scope and IPAM Pools, as well as share the created IPAM Pools across multiple AWS accounts. The input is a base CloudFormation template with IPAM Scope, IPAM Pool, and AWS RAM resources. The previous CloudFormation Macro processes this template, and the processed template contains the resource blocks for each Regional and environment-specific pool described in the input parameters. It also creates resources blocks for sharing the IPAM pools across multiple AWS accounts.

IPAM Spoke Portfolio: This portfolio comprises one service catalog product and is shared across all spoke accounts.

IPAM Spoke Product: This product creates a VPC and its subnets in the spoke account using the provisioned IPAM pool to get the CIDRs for the VPC.

We assume that you have the following AWS accounts and OU structure in your environment:

Management Account – the root user for this account and the AWS Identity and Access Management (IAM) user or IAM administrator user for this account have full access to all resources within your organization. A Network Hub Account – the account which is to be used as the IPAM Delegated Administrator account.

Note that to follow along with this post, create a Network Hub Account if you don't have one already.

Resource sharing with Organizations is turned on. We assume you are familiar with the most common IPAM constructs.

An example walkthrough In this section, we consider a top-level pool (e.g., 10.0.0.0/8) that we use as an input for our IPAM solution. Two Regional pools are created from the CIDRs sourced from the top-level pool. A Production IPv4 pool and a Non-Production IPv4 pool are then created from the Regional pool. The following diagram (figure 2) shows an example of the IPAM pool hierarchy for IPv4 across multiple Regions within a top-level IPAM pool for the Organization. We do not discuss IPv6 in this post, but it follows the same pattern as IPv4.

Deployment instructions The deployment of IPAM has two phases: Phase 1: Deploy the AWS Service Catalog Portfolio in the Management Account Step 1 – Create an S3 bucket to hold the Service Catalog Portfolio CloudFormation templates

Download the s3Bucket.yml CloudFormation template to create an Amazon Simple Storage Service (Amazon S3) bucket. This template creates two S3 buckets. One bucket named control-tower-storage-{AccountId}-{Region} to hold the Service Catalog Product template files. And the second S3 bucket named control-tower-uploadbuckets3accesslogs-{AccountId}-{Region} to log the Amazon S3 server access logs of the control-tower-storage-{AccountId}-{Region} bucket. Server access logging provides detailed records for the requests that are made to the bucket. This access log information can be useful in security and access audits. Navigate to the CloudFormation console in the Management Account, and in the left pane select Stacks. Create a stack using the downloaded s3Bucket.yml template and provide your organization ID in the template parameters section. Once the stack is deployed, navigate to the Amazon S3 console and select the control-tower-storage-{AccountId}-{Region} bucket that you just created using the CloudFormation template. Upload the four Service Catalog product template files (ipam-delegate.yml, ipam-macro.yml, ipam-product.yml, ipam-spoke-product.yml) to this S3 bucket. The following diagram (figure 3) shows the CloudFormation templates uploaded to the S3 bucket.

Figure 3: IPAM Service Catalog Product templates uploaded to the S3 bucket

Step 2 – Enable trusted access for AWS Service Catalog in Organizations

Sign in to the AWS Organizations console of the Management Account. You must sign in as an IAM user, assume an IAM Role with admin privileges, or sign in as the root user (not recommended) in the Organization's management account. On the Services page, find the row for the Service Catalog service that you want to enable trusted access. Choose Enable trusted access. In the confirmation dialog box, check the box to Show the option to enable trusted access, enter enable in the box, and then choose Enable trusted access. The following diagram (figure 4) shows the trusted access enabled for AWS Service Catalog.

Step 3 – Deploy the Service Catalog Portfolios using CloudFormation

Download the ipamportfolio.yml template and navigate to the CloudFormation console to create a stack using the this template. Update the appropriate Amazon S3 URLs of the above four templates in the parameters section of the ipamportfolio.yml template, and provide the organization ID. This template creates the service catalog portfolios (IPAM Main Portfolio and IPAM Spoke Portfolio), creates the service catalog products (it does not provision the product yet – follow Phase 2 for provisioning the products) for IPAM automation, and shares the portfolio with the provided Organization. Validate the successful deployment of the resources via the CloudFormation console (as shown in the following diagram (figure 5)) and Portfolios tab on the AWS Service Catalog console (as shown in the following diagram (figure 6)).

Phase 2: Setup user access and provision the above created IPAM products using AWS Service Catalog The IPAM Service Catalog product comprises four main deployment steps: Step 1 – Delegate an IPAM Administrator Account using the Delegate IPAM Product in your Management Account

Sign in to your AWS Management Console of the Management Account. Navigate to the AWS Service Catalog console, and select Portfolios on the left navigation pane. Select the Local tab on the top and choose IPAM Main Portfolio that was created in Phase 1. On the portfolio details page, choose the Access tab. Choose Grant Access. Add the IAM Role, user, or group that you want the end users to use to launch the product. Select Grant Access. Login back to the Management Account using the IAM Role or User or Group granted access above, if not already logged-in via that IAM Role or User or Group. In the left navigation pane, choose Products. Select Delegate IPAM Product and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the product parameters. Select Launch Product. This product delegates the NetworkHub account as the IPAM delegated administrator account.

Step 2 – Deploy the CloudFormation Macro definition using the IPAM CloudFormation Macro Product in your NetworkHub Account

Sign in to your AWS Management Console of the NetworkHub Account via the AWS role that has Administrator access. Navigate to the AWS Service Catalog console, and select Portfolios on the left navigation pane. On the Imported tab, choose IPAM Main Portfolio that was shared by the Management Account. On the portfolio details page, choose the Access tab. Choose Grant Access. Add the IAM Role, user, or group that you want the end users to use to launch the product. Select Grant Access. Login back to the NetworkHub Account using the IAM Role or User or Group granted access above, if not already logged-in via that IAM Role or User or Group. In the left navigation pane, choose Products. In the left navigation pane, choose Products. Select IPAM CloudFormation Macro Product, and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the product parameters. Select Launch Product.

Note that this template does not take-in any input parameters and is used to process the IPAM template provided in the next section. Step 3 – Deploy the AWS Service Catalog IPAM Product in your NetworkHub Account

Sign in to your AWS Management Console of the NetworkHub Account via the AWS role that was granted access to the portfolios in the previous step. Navigate to the AWS Service Catalog console, and choose Portfolios. In the left navigation pane, choose Products. Select IPAM Product and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the following product parameters:

pOrgId: ID of Organization pOperatingRegionList: A list of Regions where the IPAM is allowed to manage IP address CIDRs.

pMainPoolCIDRIPv4List: IPv4 CIDR provisioned to the main IPAM pool (TOP-LEVEL-POOL).

pRegionalPool1CIDRIPv4List: The list of CIDRs for Regional pools (one per Region) provisioned from the main IPAM pool, given in order as pOperatingRegionList.

pOperatingEnvironmentsList: A list of Environments where the IPAM is allowed to manage IP address CIDRs

pPoolCIDRIPv4ListProd: The list of production pool CIDRs (one per Region) provisioned from the respective Regional IPAM pool, given in order as pOperatingRegionList.

pPoolCIDRIPv4ListNonProd: The list of non-production pool CIDRs (one per Region) provisioned from the respective Regional IPAM pool, given in order as pOperatingRegionList.

Step 4 – Deploy the Spoke VPC Product using the IPAM Spoke Product in your Spoke Account

Sign in to your AWS Management Console of the Spoke Account. Navigate to the AWS Service Catalog console, and choose Portfolios. On the Imported tab, choose Ipam Spoke Portfolio. On the portfolio details page, choose the Access tab. Choose Grant Access. Add the IAM Role, user, or group that you want the end users to use to launch the product. Select Grant Access.

Note that if you have a common IAM Role deployed across all your spoke accounts, the previous five steps can be eliminated by updating the IPAMSpokeProductLaunchConstraintRole parameter in the ipamportfolio.yml template, with the local IAM Role name. Then, un-comment the IPAMSpokeProductLaunchRoleConstraint resource and update your ipamportfolio.yml stack.

Login back to the Spoke Account using the IAM Role or User or Group granted access previously, if not already logged-in via that IAM Role or User or Group. In the left navigation pane, choose Products. Select IPAM Spoke Product and select Launch Product. On the Launch Product page, enter a name for your provisioned product and provide the following product parameters:

pVpcSize: Size for the VPC. Either small (supports 256 IP addresses), medium (supports 512 IP addresses), or large (supports 1024 IP addresses). pNumberOfAZs: Number of Availability Zones (AZs). The value must lie between two and four. IPAMRegion: Region of IPAM Scope

Select Launch Product. Validate the VPC creation. The VPCs are created based on the size that you provide (small, medium, or large) and the CIDRs in the IPAM pool provisioned for this Organization.

Cleanup steps All the resources deployed through CloudFormation templates should be deleted after successful testing and validation to avoid any unwanted costs.

Go to the Service Catalog console, identify the Provisioned Products from the left navigation pane, and terminate the product. This would terminate the corresponding CloudFormation stacks on your behalf. Go to the CloudFormation console, identify the stack used to deploy the Service Catalog Portfolio, and delete them. In spoke accounts, you can shut down the provisioned AWS Service Catalog product(s), which would terminate the corresponding CloudFormation stacks on your behalf.

Note that in a multi account setup, you must navigate through account boundaries and follow the previous steps where products were deployed. Conclusion In this blog post, we demonstrated how to deploy AWS VPC IPAM programmatically, by utilizing AWS Service Catalog. The AWS Service Catalog includes the IPAM portfolios and products which can be created using the provided CloudFormation templates. We also demonstrated how to create IPAM pools for different regions from a single resource definition using CloudFormation Macros, thereby eliminating the need for duplicating resources manually in the CloudFormation templates.

Mokshith Kumar Mokshith Kumar is a Senior Cloud Infrastructure Architect at AWS. He thrives on learning new technologies and solving complex customer challenges. He enjoys interacting with customers and strives to help accelerate their cloud adoption journey by offering technical guidance and implementing AWS solutions. He holds a master's degree specializing in Computer Networks from the University of Southern California. Off work, Mokshith is an avid swimmer and enjoys listening to music.

Raunak Tibrewal Raunak Tibrewal is a Senior Product Manager at AWS. He has spent 10+ years in the computer networking industry, and he is currently working in the Amazon VPC team building capabilities to simplify IP management for AWS customers. In his spare time, he enjoys traveling, hiking and other outdoor activities.

TAGS: Amazon VPC, Amazon VPC IP Address Manager, AWS CloudFormation, AWS Control Tower

AWS recognized as a Leader in 2023 Gartner Magic Quadrant for Contact Center as a Service with Amazon Connect

Gartner, a company that delivers actionable, objective insight to executives and their teams, has published the 2023 Gartner Magic Quadrant for Contact Center as a Service (CCaaS). Amazon Web Services (AWS) was named a Leader for the first time since Amazon Connect, our flexible, AI-powered cloud contact center, was launched in 2017. We believe this Leader placement reflects our rapid pace of innovation that enables companies of all sizes to deliver superior customer experiences at lower cost.

AWS's Ability to Execute and Completeness of Vision are the reasons why it was named a Leader in the CCaaS space. In the Gartner Critical Capabilities research, AWS positioned first in Agile Contact Center, second in Global Contact Center, and third in Digital Customer Service Center and Customer Engagement Center use cases. Since Amazon Connect was launched in 2017, we've continued to launch new capabilities for omnichannel customer experience, agent productivity, and analytics, insights, and optimization. Vice President of Amazon Connect Pasquale DeMaio reflects, "We're extremely excited about how we're enabling our customers to deliver bar raising experiences for their customers and agents. We've made huge investments over the last year to launch significant new capabilities like agent step-by-step guidance, case management, forecasting, capacity planning, and scheduling, and screen recording. We're energized by the momentum we've maintained and believe these expanded capabilities are a key factor to Gartner recognizing us as a Leader." Today, Amazon Connect customers like Priceline, Deliveroo, Unum, Capital One, Fujitsu, Intuit, John Hancock, New York Times, and National Australia Bank use Amazon Connect to deliver better customer experiences. The Gartner report offers insightful guidance as you evaluate the right cloud contact center solution for your business. Access a complimentary copy of the 2023 Gartner Magic Quadrant for CCaaS report here. This graphic was published by Gartner, Inc. as part of a larger research document and should be evaluated in the context of the entire document. The Gartner document is available upon request from AWS. GARTNER is a registered trademark and service mark of Gartner and Magic Quadrant is a registered trademark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. All rights reserved. Gartner does not endorse any vendor, product or service depicted in its research publications and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.

TAGS: Amazon Connect, AWS, CCaaS, Contact Center, Contact Centre, customer, Customer Experience, experience, Gartner, Magic, Quadrant

Leveraging AWS Analytic Services and HCLTech Frameworks for OLAP Solutions

Online analytical processing (OLAP) is a method of organizing datasets in a multi-dimensional format for quick analysis. This provides deeper insights for decision-makers by enabling them to analyze datasets, understand trends and patterns, perform root cause analysis, identify drivers, and act. Multi-dimension analysis is widely adopted by analysts, knowledge users, and power users for their decision support process. In this post, we will explore utilizing Amazon Web Services (AWS) analytic services and migration tools together with HCLTech frameworks to orchestrate OLAP solutions. HCLTech is an AWS Premier Tier Services Partner with Competencies in Migration, DevOps, SAP, Storage, Mainframe Modernization, and Cloud Operation Services. HCLTech is also a member of the AWS Managed Service Provider (MSP) and Well-Architected partner programs. OLAP Architecture and Components There are four types of online analytical processing:

Multi-dimensional OLAP (MOLAP): Stores aggregated and computed datasets in the proprietary multi-dimensional (in memory or on disk) cube format for analysis. Offers high performance and optimized storage. Relational OLAP (ROLAP): Stores datasets on relational data store to perform multi-dimensional analysis. Data aggregation and computation happens on the fly. Hybrid OLAP (HOLAP): Stores aggregated datasets in MOLAP cube and detail datasets in ROLAP database for analysis. Virtual OLAP (VOLAP): Enables multi-dimensional analysis over the datasets available in various data sources through federated queries, optimized columnar storage, and distributed and parallel processing.

AWS services for OLAP are comprised of four layers, as described below: Insights and Visualization This layer provides capabilities to interact with datasets using reports and dashboards. It enables end users to perform ad-hoc analysis, author reports, dashboards, scorecards, and charts from the underlying datasets with role-based access controls (RBAC). Amazon QuickSight addresses the capabilities in this layer Governance and Access This layer provides the capability to catalog the underlying datasets and enables users to perform search, discovery, and secure data access. It enables federated access to a variety of datasets available in the underlying persistent storages, and to centrally manage permissions on data being shared across your organization. Key AWS services include AWS Glue Data Catalog, Amazon Athena, and Amazon Redshift. Compute and Storage This layer provides capabilities to persist any kind of datasets and perform data computations (ELT/ETL) over the same. Depending on the data requirements (cold, warm, hot), data will be stored and processed by Amazon Simple Storage Service (Amazon S3), Amazon EMR, Amazon Redshift Serverless, and AWS Glue. Ingestion This layer provides the capability to bring all source and upstream datasets onto an AWS data lake or staging layer. It meets all the requirements of batch, real-time streaming, change data capture, and secure transfers.

Figure 1 – AWS services for OLAP. AWS Services for OLAP Solution Next, we will discuss the AWS services leveraged in the top three layers for the OLAP solution. Amazon QuickSight Capabilities Preparing the QuickSight SPICE Cube Amazon QuickSight uses SPICE (Super-fast, Parallel, In-memory Calculation Engine) to store data in a multi-dimensional format for faster processing of analytical queries. The cubes are encrypted and compressed for storage optimization. As part of the demonstration, we'll leverage a sample retail dataset which comprises product dimension, branches, daily sales transactions, and revenue target. As a broader process, sales data is copied to Amazon S3, loaded to Amazon Redshift Serverless data mart, and reported in QuickSight. Now, let's embed OLAP capability for the above process. While preparing your retail dataset for analysis in QuickSight, choose "Query Mode" as SPICE to import the dataset and build the in-memory cube. This imports the dataset from Amazon Redshift Serverless views/tables. Note that we need to regularly refresh the in-memory cube to update it with the latest/incremental records.

Figure 2 – Amazon QuickSight – SPICE configuration. Next, go to the dataset and select "Refresh Property" to see the status of current and historical refreshes. We can schedule the cube refresh in any given frequency, and many users can access a SPICE dataset concurrently without performance issues. Prepare Multi-Dimensional Analysis Preparing hierarchies, level, and drill-downs navigations over the datasets is seamless in QuickSight. Note that QuickSight provides the option to convert the date field to year, month, or day automatically. The above features are available in charts as well.

Figure 3 – Amazon QuickSight hierarchy, levels, drill down, and slice/dice. Amazon QuickSight provides ability to drill up and down levels, and also to slice and dice rows and columns as shown above. Drill Through Navigation from Summary to Detail Reports We can use the action options to drill through the navigation, from summary report to detail report, and pass the filter context for the same. This provides the flexibility of splitting the complexities of the reports while navigating seamlessly without losing the context.

Figure 4 – Amazon QuickSight – Drill through navigation. Aggregate and Table Calculations Amazon QuickSight provides rich sets of table calculations to discover how dimensions influence measures or each other. With the help of this table and aggregate calculation, we can address the mulit-dimensional expressions (MDX) equivalent expressions. Provided below are some of the business scenarios:

QTD: QuickSight calculate the quarter to date (QTD) automatically based on the time period hierarchy. YTD: To calculate the year to date (YTD) value, use the following table function:

PY Act: To calculate the prior year actual value for the given date:

Figure 5 – Amazon QuickSight – PY Act, Forward One Month, Trail One Month.

T1M: To calculate the trailing one month (T1M), which is the sum of a given month and previous month value:

F1M: To calculate the forward one month (F1M), which is the sum of a given month and next month value:

sum(actual)+periodOverPeriodLastValue(sum(actual),{sale_date},MONTH,-1) We can use Amazon QuickSight parameter control to customize the above calculations for specific months. Machine-Learning Powered Forecasting We can use QuickSight machine learning-powered forecasting to forecast complex, real-world scenarios such as data with multiple seasonality. Auto Narratives QuickSight automatically interprets the data, charts, and tables in the dashboard and provides a number of suggested insights in natural language. The suggested insights you can choose from are ready-made and come with words, calculations, and functions. These narratives can be customized per user requirements. QuickSight and Redshift Serverless Integrations Amazon QuickSight can perform ROLAP by directly integrating with Amazon Redshift using direct query mode. All of the QuickSight reports and insights developed in SPICE can be seamlessly repointed to its source Redshift tables, views, and materialized views. Redshift views or materialized views encapsulate complexities of underlying data models (tables and joins) and create a subset of contextual datasets that can act as a base view for OLAP analysis. We can embed window functions in the SQLs for complex analytical scenarios, pre-compute, aggregate, and store the processed result set in materialized view. Materialized views offer significant performance benefits compared to live table SQL or view queries. Also, we can extend the QuickSight insights from Redshift managed tables to Amazon S3 datasets using Amazon Redshift Spectrum.

Figure 6 – Amazon Redshift Spectrum view. We can query the S3 dataset by creating an external table from Redshift, as shown above. We can also combine datasets that include data stored in Redshift and in S3. Then, we can access them using the SQL syntax in Redshift. QuickSight and Amazon Athena Integrations Amazon Athena provides virtualization infrastructure where it submits a single SQL query to analyze data in relational, nonrelational, object, and custom data sources running on-premises or in the cloud. Athena accesses AWs Glue Data Catalog which maintains registry of sources metadata, and AWS Glue crawler continuously scans the source metadata and maintains up to date changes.

Figure 7 – Amazon QuickSight integration with Athena. By leveraging the Amazon QuickSight Athena Connector, we can compose federated queries and generate OLAP insights. QuickSight and Amazon EMR Integrations Amazon QuickSight supports big data connectors to connect with Apache Spark on Amazon EMR. We can leverage Spark tables or through results of Spark SQL queries. For this process, QuickSight requires your Spark server to be secured and authenticated using LDAP, which is available to Spark version 2.0. Spark SQL can also be configured to use AWS Glue Data Catalog as its meta store.

Figure 8 – QuickSight integration with Apache Spark on Amazon EMR. AWS Glue Crawlers can automatically infer schema from source data in S3 and store the associated metadata in the data catalog. QuickSight requires LDAP as authentication protocol for authenticating Spark cluster; hence, we need to install and configure OpenLDAP in Amazon EMR and enable Thrift Server. OLAP Solution Deployments on AWS In this section, we'll cover OLAP deployment types which involves provisioning of set of AWS analytic services based on business functional and technical requirements, along with HCLTech accelerators to fast track the deployments.

Figure 9 – AWS OLAP deployment types. Use Case Requirement #1 A customer wants to perform self-service business intelligence with multi-dimensional data analysis requiring quick query response and ad-hoc analysis over GBs of data volume, with acceptable data latency for reporting MOLAP is the appropriate deployment type, and you can leverage Amazon QuickSight SPICE which supports a maximum of 2,000 fields per cube with 24 million records (25 GB dataset) for standard edition and one billion records (one TB dataset) for enterprise edition. Use Case Requirement #2 A customer wants a managed report/scorecard in addition to dashboards and analysis over complex multi-dimensional data model, with TBs of datasets and zero data latency. ROLAP is the deployment type, and Amazon QuickSight's integration with Redshift Serverless leverages a massively parallel processing query optimizer and takes advantage of the columnar-oriented data storage. It can process complex analytic queries that often include multi-table joins, subqueries, and aggregation. Generally, customers keep the cold data, unstructured data, and historical transactional data in Amazon S3 for low-cost, efficient storage. Redshift Spectrum can create complex queries of churning both Redshift-managed tables containing hot data and S3 datasets. Use Case Requirement #3 A customer wants to perform analysis over a big data lake with petabytes of datasets. ROLAP is the deployment type and leverages the Amazon EMR SparkSQL connector with QuickSight. Amazon EMR provides a simple and cost-effective way to run highly distributed processing frameworks such as Spark. Use Case Requirement #4 A customer wants to perform hybrid data analysis over various data sources through virtualization. VOLAP is the deployment type, and QuickSight's integration with Amazon Athena can run OLAP analysis over federated queries churning datasets across S3, Redshift, and Amazon Relational Database Service (Amazon RDS). This offers zero footprint traversing of analytical queries from data mart to data lake. Advantages of AWS and HCLTech There are several advantages for customers when deploying or migrating OLAP workloads onto AWS:

Interoperability of analytical insights: Several AWS analytic services seamlessly integrate with your datasets, including Amazon QuickSight, Amazon Redshift, Amazon S3, Amazon Athena, Amazon EMR, and AWS Glue Data Catalog. AWS serverless and cost-effective deployment: With low-cost data storage, serverless performance at scale, and pay-as-you-go compute services, AWS offers fully managed and cost-effective infrastructure for managing OLAP workload deployments. Quicker deployment: AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (DMS) make it easy to migrate relational databases, data warehouses, and NoSQL databases to AWS and unlock analytic insights. HCLTech's ADvantage Migrate fast tracks AWS data lake/data mart implementation and data migration to AWS, optimizes production release cycles, and drives productivity by adopting modular design approaches (reusability of codes) and template-based data preparation and testing activities.

Conclusion In this post, we briefed you on orchestrating online analytical processing (OLAP) workloads using AWS analytics services and their key features in addressing OLAP requirements. Unlike legacy products that require separate infrastructure and licenses, AWS analytics services offer seamless OLAP analysis over the data residing in the data lake, data mart, data warehouse, with the flexibility to scale up and down at a lower cost. .

HCLTech – AWS Partner Spotlight HCLTech is an AWS Premier Tier Services Partner and MSP that serves hundreds of global enterprises to solve day-to-day and complex challenges with a dedicated full-stack business unit. Contact HCLTech | Partner Overview | AWS Marketplace | Case Studies

TAGS: AWS Competency Partners, AWS MSP Partner Program, AWS Partner Guest Post, AWS Partner Success Stories, AWS Premier Tier Services Partners, AWS Public Sector Partners, AWS Service Delivery Partners, AWS Solution Provider Partners, AWS Well-Architected Partners, HCLTech, Managed Service Provider

It's important to be aware of the case sensitivity of text data in PostgreSQL when performing queries or comparisons to ensure that the results match your intended expectations. Case also affects how the database performs sorting operations. By default, PostgreSQL is case sensitive when sorting or comparing string values. For example, PostgreSQL considers "amazon" and "Amazon" to be different values, and this impacts how they are sorted. If you want to search for a specific value regardless of its case, you need to use a case-insensitive comparison function or normalize the data to a consistent case. Case insensitivity in PostgreSQL allows for greater flexibility and ease of use when working with data. In this post, we discuss different options to deal with the case-insensitive data in PostgreSQL and their use cases, pros, and cons. Solution overview Case insensitivity in PostgreSQL can improve productivity, reduce errors, and make it easier to work with data and identifiers in a flexible and consistent way. However, there are some performance tradeoffs to consider. PostgreSQL provides different options to achieve case insensitivity in data. We discuss the following options in this post:

Using case-insensitive comparison functions Using ILIKE or ~~* operators for comparison Using the CITEXT data type Using a custom = (equal to) operator Using a case-insensitive ICU collation

Prerequisites If you want to follow along this post's examples, complete the following prerequisite steps:

Create an Amazon Aurora PostgreSQL-Compatible Edition cluster or an Amazon Relational Database Service (Amazon RDS) for PostgreSQL instance if you don't already have one. For instructions, refer to Create an Aurora PostgreSQL DB cluster or Create a PostgreSQL DB instance respectively. Create an Amazon Elastic Compute Cloud (Amazon EC2) instance to install the PostgreSQL client to access the Aurora PostgreSQL or RDS for PostgreSQL instance. For instructions, refer to Create your EC2 resources and launch your EC2 instance. Alternatively, you can set up connectivity between your RDS database and EC2 compute instance in 1-click Install the PostgreSQL client. On Amazon Linux, we can use the following commands to download the psql command line tool:

Option 1: Using case-insensitive comparison functions In this option, we discuss the PostgreSQL in-built comparison functions LOWER and UPPER. These functions allow you to convert a string to lowercase or uppercase at runtime, respectively. The lower function takes a string as an argument and returns a new string with all the characters in lowercase. For example:

postgres=# SELECT pg_catalog.lower('Print IN LOWER'); lower ---------------- print in lower (1 row)SQL

The upper function, on the other hand, takes a string as an argument and returns a new string with all the characters in uppercase. For example:

postgres=# SELECT pg_catalog.upper('Print in upper'); upper ---------------- PRINT IN UPPER (1 row)SQL

This is a straightforward way of dealing with case insensitivity in PostgreSQL. When performing a search on a text column, you can use the lower or upper function to convert the search term to lowercase or uppercase and then search for that term in the lowercased or uppercased text column. This ensures that the search is case insensitive, so you can find matches regardless of the case of the text data. Also, when importing data from external sources, it's common to use the lower and upper functions to standardize the case of text data. This helps ensure that all text data is in the same case format, making it easier to compare and manipulate. Benefits This option offers built-in functions and is simple to use. Drawbacks If you want to use an index for case-insensitive searches or comparisons, you'll need to create a specialized index such as an expression index or a specialized data type like citext instead of relying on a standard text index. Additionally, PostgreSQL can't use an expression index if the expression is not present, so you would need to add another index for case-sensitive searches. Complete the following steps:

CREATE TABLE public.case_test_opt1 ( id SERIAL PRIMARY KEY, first_name TEXT NOT NULL, last_name TEXT NOT NULL, email TEXT NOT NULL );SQL

Insert some sample data. The following query inserts 1 million records:

INSERT INTO public.case_test_opt1 (first_name, last_name, email) SELECT 'Foo', 'Bar' || n::text, 'foo.bar' || n::text || [EMAIL] FROM generate_series(1, 1000) n;SQL

Run the following query, which doesn't use the index created in the previous step because it's a normal index. You can see the full table scan (seq scan) as highlighted:

postgres=> EXPLAIN ANALYZE SELECT * FROM public.case_test_opt1 WHERE pg_catalog.UPPER(email) = [EMAIL]

QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Gather (cost=1000.00..16084.00 rows=5000 width=37) (actual time=0.428..171.612 rows=1000 loops=1) Workers Planned: 2 Workers Launched: 2 -> Parallel Seq Scan on case_test_opt1 (cost=0.00..14584.00 rows=2083 width=37) (actual time=0.237..163.207 rows=333 loops=3) Filter: (upper(email) = [EMAIL] Rows Removed by Filter: 333000 Planning Time: 0.082 ms Execution Time: 171.681 ms (8 rows) SQL

Run the same query to see if the functional index is utilized:

QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------------ Bitmap Heap Scan on case_test_opt1 (cost=139.18..7761.06 rows=5000 width=37) (actual time=0.331..1.314 rows=1000 loops=1) Recheck Cond: (upper(email) = [EMAIL] Heap Blocks: exact=1000 -> Bitmap Index Scan on case_opt1_email_upper_idx (cost=0.00..137.93 rows=5000 width=0) (actual time=0.200..0.200 rows=1000 loops=1) Index Cond: (upper(email) = [EMAIL] Planning Time: 0.149 ms Execution Time: 1.375 ms (7 rows)SQL

Option 2: Using ILIKE or ~~* operators for comparison In this option, we discuss PostgreSQL built-in string comparison operators ILIKE and ~~*. They are equivalent in functionality, but ILIKE is the standard SQL-compliant operator whereas ~~* is a PostgreSQL-specific operator. The ILIKE operator matches two strings, ignoring differences in letter case. It's equivalent to the LIKE operator but case insensitive. For example, the expression 'hello' ILIKE 'HelLO' would evaluate to TRUE:

postgres=# SELECT 'hello' ILIKE 'HelLO'; ?column? ---------- t (1 row)SQL

The ~~* operator is the PostgreSQL-specific version of ILIKE, and performs a case-insensitive string comparison similar to ILIKE. For example, the expression 'hello' ~~* 'HelLO' would evaluate to TRUE:

postgres=# SELECT 'hello' ~~* 'HelLO'; ?column? ---------- t (1 row)SQL

When accepting user input in a search form or filter, you can use the ILIKE or ~~* operator to search for matching records in a case-insensitive manner. This ensures that users can find the records they're looking for regardless of the case of the input. You can also use these operators when searching for specific text patterns in a large dataset. This is useful for scenarios such as finding all the product names that contain a certain keyword, regardless of the case of the keyword. Benefits This option offers built-in operators and is simple to use. Drawbacks You cannot use B-tree indexes, even expression indexes, with the ILIKE or ~~* operators. However, you can use PostgreSQL's GIN or GiST index types to build indexes for these types of operators. More details on GIN and GiST indexes including the differences can be found in Preferred Index Types for Text Search documentation. Keep the following in mind:

You must use the pg_trgm extension. This extension is available in Amazon Aurora and Amazon RDS, but it is not enabled by default. You can install it in your database using "CREATE EXTENSION pg_trgm;" GIN indexes can be significantly larger than B-tree indexes, especially for data types with many distinct values or long text fields. This can impact the speed of index updates and maintenance. Creating a GIN index, as well as inserts and updates, can take longer than when using a B-tree index. GIN indexes are optimized for certain query patterns, such as searching for multiple values or ranges within the indexed data. However, they may not be as effective for other types of queries, such as equality or range queries on a single column.

Using the same table that we created earlier (case_test_opt1), we can create a normal, functional, and GIN index to see the index creation time difference:

For this data set, the GIN index creation time took almost three times longer than the B-tree indexes from the previous example. The following query shows that PostgreSQL scans the GIN index when using the ILIKE operator:

postgres=> EXPLAIN ANALYZE SELECT * FROM public.case_test_opt1 WHERE email ILIKE 'FOO.BAR500@EXAMPLE%';

QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------- Bitmap Heap Scan on case_test_opt1 (cost=344.77..709.64 rows=99 width=37) (actual time=84.869..86.652 rows=1000 loops=1) Recheck Cond: (email ~~* 'FOO.BAR500@EXAMPLE%'::text) Heap Blocks: exact=1000 -> Bitmap Index Scan on case_opt1_email_tgrm_idx (cost=0.00..344.74 rows=99 width=0) (actual time=84.740..84.740 rows=1000 loops=1) Index Cond: (email ~~* 'FOO.BAR500@EXAMPLE%'::text) Planning Time: 0.256 ms Execution Time: 86.729 ms (7 rows) SQL

Option 3: Using the CITEXT data type citext is a case-insensitive text data type in PostgreSQL. It's an extension module that provides a case-insensitive version of the built-in text data type. When citext is used for a column, any string value inserted into that column is automatically converted to lowercase, and any queries on that column are case insensitive. To use citext, you must first enable the citext extension in your database using the following command:

After that, you can create a table with a citext column using following statement:

CREATE TABLE public.case_citext_opt3 ( id INT PRIMARY KEY, name CITEXT );SQL

If you're migrating data from a system that uses case-insensitive text types, using citext can simplify the migration process by allowing you to maintain the same data format. citext can help prevent data entry errors or inconsistencies in your data. With citext, you don't need to use lower and upper functions in your queries to perform case-insensitive searches or comparisons. This can make your queries simpler and easier to read. Benefits This option offers a built-in extension with the citext data type. Additionally, migration from other databases with case-insensitive data types is straightforward. Drawbacks A drawback to this option is that you can't perform case-sensitive searches if needed. Also, if you have a citext type on one side of an operator and a text on the other for a query, then it might not work as expected. So, we need to be careful with such queries. Option 4: Using a custom = operator PostgreSQL allows users to define custom operators to extend the built-in set of operators. Custom operators can be defined to perform specific operations that aren't supported by the built-in operators, or to provide shorthand notations for frequently used operations. In this option, we show how to create a new = operator to deal with the case-insensitive data. Complete the following steps:

Create a new schema to define the operator and grant necessary privileges on the schema to public:

CREATE SCHEMA util; GRANT USAGE ON SCHEMA util TO PUBLIC;SQL

Create a function to define the functionality of the operator. This function basically changes the case of right and left operands to lowercase so that comparison is done without modifying the query to use any built-in functions like lower or upper.

CREATE OR REPLACE FUNCTION util.case_ins_cmp(text, text) RETURNS BOOLEAN LANGUAGE sql immutable as $$ select pg_catalog.lower($1) operator(pg_catalog.=) pg_catalog.lower($2) $$;SQL

CREATE OPERATOR util.= ( LEFTARG = TEXT, RIGHTARG = TEXT, PROCEDURE = util.case_ins_cmp);SQL

CREATE TABLE public.case_test_opt4 ( id SERIAL PRIMARY KEY, first_name TEXT NOT NULL, last_name TEXT NOT NULL, email TEXT NOT NULL ); SQL

INSERT INTO public.case_test_opt4 (first_name, last_name, email) VALUES ('foo', 'bar', [EMAIL] ('foo', 'BAR', [EMAIL] ('foo', 'BaR', [EMAIL]

postgres=# SELECT * FROM public.case_test_opt4 WHERE last_name='bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] (1 row)SQL You can see only one row based on the case. Set the search_path to use the util schema before pg_catalog and check the results:

postgres=# SET search_path TO util, pg_catalog; SET postgres=# SELECT * FROM public.case_test_opt4 WHERE last_name='bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 2 | foo | BAR | [EMAIL] 3 | foo | BaR | [EMAIL] (3 rows)SQL

You can see all three rows regardless of case. Alternatively, you can use the absolute path of the operator using schema instead of setting the search_path:

postgres=# SELECT * FROM public.case_test_opt4 WHERE last_name operator(util.=) 'bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 2 | foo | BAR | [EMAIL] 3 | foo | BaR | [EMAIL] (3 rows)SQL

postgres=# CREATE INDEX ind_opt4 ON public.case_test_opt4(pg_catalog.lower(last_name)); CREATE INDEX postgres=# ANALYZE case_test_opt4; ANALYZE postgres=# EXPLAIN ANALYZE SELECT * FROM public.case_test_opt4 WHERE last_name OPERATOR (util. =) 'bar';

QUERY PLAN -------------------------------------------------------------------------------------------------------------------------- Index Scan using ind_opt4 on case_test_opt4 (cost=0.42..8.44 rows=1 width=40) (actual time=0.071..0.072 rows=3 loops=1) Index Cond: (lower(last_name) = 'bar'::text) Planning Time: 0.322 ms Execution Time: 0.084 ms (4 rows)SQL

Benefits One benefit of this option is that you might not need to change the application queries to add any built-in functions like lower and upper or add any operators like ~~* or ILIKE if you choose to add the schema (in which the operator is created) to the search_path and set it to the database. Additionally, you can set the search_path as a switch to make the data case insensitive if you want only part of your application to behave like case insensitive without making any changes to the application. Drawbacks You need to add the search_path (with the util schema before pg_catalog) to the application queries or alter the database to set the search_path:

In general, custom operators can be a powerful tool for developers and DBAs to extend the functionality of PostgreSQL and make it simpler to work with some operations. However, it's important to use them judiciously and follow best practices for naming, documentation, and testing to ensure that they are consistent and efficient. Clean up after you have completed the testing:

Option 5: Using a case-insensitive ICU collation "Custom collations" are a feature of the ICU collation provider. ICU allows extensive control over collation behavior by defining new collations with collation settings as a part of the language tag. These settings can modify the collation order to suit a variety of needs. You can go through Manage collation changes in PostgreSQL on Amazon Aurora and Amazon RDS for more information on collations. The following is a test case with custom collation to access case-insensitive data:

CREATE COLLATION public.case_insensitive (provider = icu, locale = 'und-u-ks-level2', deterministic = false);SQL

CREATE TABLE public.case_test_opt5 ( id SERIAL PRIMARY KEY, first_name TEXT NOT NULL COLLATE case_insensitive, last_name TEXT NOT NULL COLLATE case_insensitive, email TEXT NOT NULL COLLATE case_insensitive );

INSERT INTO public.case_test_opt5 (first_name, last_name, email) VALUES ('foo', 'bar', [EMAIL] ('foo', 'BAR', [EMAIL] ('foo', 'BaR', [EMAIL]

postgres=> SELECT * FROM public.case_test_opt5 WHERE last_name='bar'; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 2 | foo | BAR | [EMAIL] 3 | foo | BaR | [EMAIL] (3 rows)SQL

You can see all three rows regardless of case. Benefits Like Option 4, you don't need to change the application queries to add any built-in functions like lower or upper or add any operators like ~~* or ILIKE. Also, you don't need to set the search_path. Drawbacks The major drawback is pattern matching does not work for nondeterministic collations that is case-insensitive collations. As of PostgreSQL 15, this also does not address the ability to index nondeterministic collations. Also, the sorting of the data might change as well based on the collation that you use. You have to deal with sorting if you are changing the collation. For example, the following has the order of last_name as bar, BAR, BaR:

if we modify the column's collation and run a query we observe that the order of column changed to bar, BaR, BAR:

postgres=> ALTER TABLE public.case_test_opt5 ALTER COLUMN last_name TYPE text COLLATE "en_US"; ALTER TABLE Time: 57.316 ms postgres=> SELECT * FROM public.case_test_opt5 ORDER BY last_name; id | first_name | last_name | email ----+------------+-----------+--------------------- 1 | foo | bar | [EMAIL] 3 | foo | BaR | [EMAIL] 2 | foo | BAR | [EMAIL] (3 rows)SQL

You can see that changing the collation changed the order of the rows. Although custom collations in PostgreSQL can provide many benefits, it's important to carefully consider the potential drawbacks before implementing them in a production environment. Proper planning, testing, and ongoing maintenance can help ensure that custom collations work effectively and don't negatively impact database performance or compatibility. Clean up after you have completed the testing:

Conclusion PostgreSQL provides several options for dealing with case insensitivity, each with its own advantages and disadvantages. The best approach depends on the specific requirements of your application and the data being stored in the database. In this post, we discussed a few options with their pros and cons. Based on your requirements, you can choose the right option for your application. If you have any questions or suggestions about post, leave a comment.

About the authors Baji Shaik is a Sr. Lead Consultant with AWS ProServe, Global Competency Centre. His background spans a wide depth and breadth of expertise and experience in SQL and NoSQL database technologies. He is a Database Migration Expert and has developed many successful database solutions addressing challenging business requirements for moving databases from on premises to Amazon RDS and Amazon Aurora PostgreSQL/MySQL. He is an eminent author, having written several books on PostgreSQL. A few of his recent works include "PostgreSQL Configuration," "Beginning PostgreSQL on the Cloud," and "PostgreSQL Development Essentials." Furthermore, he has delivered several conference and workshop sessions. Scott Mead is a Database Engineer at AWS.

How Ontraport reduced data processing cost by 80% with AWS Glue

This post is written in collaboration with Elijah Ball from Ontraport. Customers are implementing data and analytics workloads in the AWS Cloud to optimize cost. When implementing data processing workloads in AWS, you have the option to use technologies like Amazon EMR or serverless technologies like AWS Glue. Both options minimize the undifferentiated heavy lifting activities like managing servers, performing upgrades, and deploying security patches and allow you to focus on what is important: meeting core business objectives. The difference between both approaches can play a critical role in enabling your organization to be more productive and innovative, while also saving money and resources. Services like Amazon EMR focus on offering you flexibility to support data processing workloads at scale using frameworks you're accustomed to. For example, with Amazon EMR, you can choose from multiple open-source data processing frameworks such as Apache Spark, Apache Hive, and Presto, and fine-tune workloads by customizing things such as cluster instance types on Amazon Elastic Compute Cloud (Amazon EC2) or use containerized environments running on Amazon Elastic Kubernetes Service (Amazon EKS). This option is best suited when migrating workloads from big data environments like Apache Hadoop or Spark, or when used by teams that are familiar with open-source frameworks supported on Amazon EMR. Serverless services like AWS Glue minimize the need to think about servers and focus on offering additional productivity and DataOps tooling for accelerating data pipeline development. AWS Glue is a serverless data integration service that helps analytics users discover, prepare, move, and integrate data from multiple sources via a low-code or no-code approach. This option is best suited when organizations are resource-constrained and need to build data processing workloads at scale with limited expertise, allowing them to expedite development and reduced Total Cost of Ownership (TCO). In this post, we show how our AWS customer Ontraport evaluated the use of AWS Glue and Amazon EMR to reduce TCO, and how they reduced their storage cost by 92% and their processing cost by 80% with only one full-time developer. Ontraport's workload and solution Ontraport is a CRM and automation service that powers businesses' marketing, sales and operations all in one place—empowering businesses to grow faster and deliver more value to their customers. Log processing and analysis is critical to Ontraport. It allows them to provide better services and insight to customers such as email campaign optimization. For example, email logs alone record 3–4 events for every one of the 15–20 million messages Ontraport sends on behalf of their clients each day. Analysis of email transactions with providers such as Google and Microsoft allow Ontraport's delivery team to optimize open rates for the campaigns of clients with big contact lists. Some of the big log contributors are web server and CDN events, email transaction records, and custom event logs within Ontraport's proprietary applications. The following is a sample breakdown of their daily log contributions:

Ontraport's solution uses Amazon Kinesis and Amazon Kinesis Data Firehose to ingest log data and write recent records into an Amazon OpenSearch Service database, from where analysts and administrators can analyze the last 3 months of data. Custom application logs record interactions with the Ontraport CRM so client accounts can be audited or recovered by the customer support team. Originally, all logs were retained back to 2018. Retention is multi-leveled by age:

Less than 1 week – OpenSearch hot storage Between 1 week and 3 months – OpenSearch cold storage More than 3 months – Extract, transform, and load (ETL) processed in Amazon Simple Storage Service (Amazon S3), available through Amazon Athena

The following diagram shows the architecture of their log processing and analytics data pipeline.

Evaluating the optimal solution In order to optimize storage and analysis of their historical records in Amazon S3, Ontraport implemented an ETL process to transform and compress TSV and JSON files into Parquet files with partitioning by the hour. The compression and transformation helped Ontraport reduce their S3 storage costs by 92%. In phase 1, Ontraport implemented an ETL workload with Amazon EMR. Given the scale of their data (hundreds of billions of rows) and only one developer, Ontraport's first attempt at the Apache Spark application required a 16-node EMR cluster with r5.12xlarge core and task nodes. The configuration allowed the developer to process 1 year of data and minimize out-of-memory issues with a rough version of the Spark ETL application. To help optimize the workload, Ontraport reached out to AWS for optimization recommendations. There were a considerable number of options to optimize the workload within Amazon EMR, such as right-sizing Amazon Elastic Compute Cloud (Amazon EC2) instance type based on workload profile, modifying Spark YARN memory configuration, and rewriting portions of the Spark code. Considering the resource constraints (only one full-time developer), the AWS team recommended exploring similar logic with AWS Glue Studio. Some of the initial benefits with using AWS Glue for this workload include the following:

AWS Glue has the concept of crawlers that provides a no-code approach to catalog data sources and identify schema from multiple data sources, in this case, Amazon S3. AWS Glue provides built-in data processing capabilities with abstract methods on top of Spark that reduce the overhead required to develop efficient data processing code. For example, AWS Glue supports a DynamicFrame class corresponding to a Spark DataFrame that provides additional flexibility when working with semi-structured datasets and can be quickly transformed into a Spark DataFrame. DynamicFrames can be generated directly from crawled tables or directly from files in Amazon S3. See the following example code:

connection_type = 's3', connection_options = {'paths': [s3:// ]}, format = 'json')Bash

It minimizes the need for Ontraport to right-size instance types and auto scaling configurations. Using AWS Glue Studio interactive sessions allows Ontraport to quickly iterate when code changes where needed when detecting historical log schema evolution.

Ontraport had to process 100 terabytes of log data. The cost of processing each terabyte with the initial configuration was approximately $500. That cost came down to approximately $100 per terabyte after using AWS Glue. By using AWS Glue and AWS Glue Studio, Ontraport's cost of processing the jobs was reduced by 80%. Diving deep into the AWS Glue workload Ontraport's first AWS Glue application was a PySpark workload that ingested data from TSV and JSON files in Amazon S3, performed basic transformations on timestamp fields, and converted the data types of a couple fields. Finally, it writes output data into a curated S3 bucket as compressed Parquet files of approximately 1 GB in size and partitioned in 1-hour intervals to optimize for queries with Athena. With an AWS Glue job configured with 10 workers of the type G.2x configuration, Ontraport was able to process approximately 500 million records in less than 60 minutes. When processing 10 billion records, they were able to increase the job configuration to a maximum of 100 workers with auto scaling enabled to complete the job within 1 hour. What's next? Ontraport has been able to process logs as early as 2018. The team is updating the processing code to allow for scenarios of schema evolution (such as new fields) and parameterized some components to fully automate the batch processing. They are also looking to fine-tune the number of provisioned AWS Glue workers to obtain optimal price-performance. Conclusion In this post, we showed you how Ontraport used AWS Glue to help reduce development overhead and simplify development efforts for their ETL workloads with only one full-time developer. Although services like Amazon EMR offer great flexibility and optimization, the ease of use and simplification in AWS Glue often offer a faster path for cost-optimization and innovation for small and medium businesses. For more information about AWS Glue, check out Getting Started with AWS Glue.

About the Authors Elijah Ball has been a Sys Admin at Ontraport for 12 years. He is currently working to move Ontraport's production workloads to AWS and develop data analysis strategies for Ontraport. Pablo Redondo is a Principal Solutions Architect at Amazon Web Services. He is a data enthusiast with over 16 years of FinTech and healthcare industry experience and is a member of the AWS Analytics Technical Field Community (TFC). Pablo has been leading the AWS Gain Insights Program to help AWS customers achieve better insights and tangible business value from their data analytics initiatives. Vikram Honmurgi is a Customer Solutions Manager at Amazon Web Services. With over 15 years of software delivery experience, Vikram is passionate about assisting customers and accelerating their cloud journey, delivering frictionless migrations, and ensuring our customers capture the full potential and sustainable business advantages of migrating to the AWS Cloud.

Natural language processing (NLP) is the field in machine learning (ML) concerned with giving computers the ability to understand text and spoken words in the same way as human beings can. Recently, state-of-the-art architectures like the transformer architecture are used to achieve near-human performance on NLP downstream tasks like text summarization, text classification, entity recognition, and more. Large language models (LLMs) are transformer-based models trained on a large amount of unlabeled text with hundreds of millions (BERT) to over a trillion parameters (MiCS), and whose size makes single-GPU training impractical. Due to their inherent complexity, training an LLM from scratch is a very challenging task that very few organizations can afford. A common practice for NLP downstream tasks is to take a pre-trained LLM and fine-tune it. For more information about fine-tuning, refer to Domain-adaptation Fine-tuning of Foundation Models in Amazon SageMaker JumpStart on Financial data and Fine-tune transformer language models for linguistic diversity with Hugging Face on Amazon SageMaker. Zero-shot learning in NLP allows a pre-trained LLM to generate responses to tasks that it hasn't been explicitly trained for (even without fine-tuning). Specifically speaking about text classification, zero-shot text classification is a task in natural language processing where an NLP model is used to classify text from unseen classes, in contrast to supervised classification, where NLP models can only classify text that belong to classes in the training data. We recently launched zero-shot classification model support in Amazon SageMaker JumpStart. SageMaker JumpStart is the ML hub of Amazon SageMaker that provides access to pre-trained foundation models (FMs), LLMs, built-in algorithms, and solution templates to help you quickly get started with ML. In this post, we show how you can perform zero-shot classification using pre-trained models in SageMaker Jumpstart. You will learn how to use the SageMaker Jumpstart UI and SageMaker Python SDK to deploy the solution and run inference using the available models. Zero-shot learning Zero-shot classification is a paradigm where a model can classify new, unseen examples that belong to classes that were not present in the training data. For example, a language model that has beed trained to understand human language can be used to classify New Year's resolutions tweets on multiple classes like career, health, and finance, without the language model being explicitly trained on the text classification task. This is in contrast to fine-tuning the model, since the latter implies re-training the model (through transfer learning) while zero-shot learning doesn't require additional training. The following diagram illustrates the differences between transfer learning (left) vs. zero-shot learning (right).

Yin et al. proposed a framework for creating zero-shot classifiers using natural language inference (NLI). The framework works by posing the sequence to be classified as an NLI premise and constructs a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class politics, we could construct a hypothesis of "This text is about politics." The probabilities for entailment and contradiction are then converted to label probabilities. As a quick review, NLI considers two sentences: a premise and a hypothesis. The task is to determine whether the hypothesis is true (entailment) or false (contradiction) given the premise. The following table provides some examples.

A man inspects the uniform of a figure in some East Asian country. Contradiction The man is sleeping.

An older and younger man smiling. Neutral Two men are smiling and laughing at the cats playing on the floor.

A soccer game with multiple males playing. entailment Some men are playing a sport.

How to deploy pre-trained zero-shot text classification models using the SageMaker JumpStart UI and run inference on the deployed model using short text data How to use the SageMaker Python SDK to access the pre-trained zero-shot text classification models in SageMaker JumpStart and use the inference script to deploy the model to a SageMaker endpoint for a real-time text classification use case How to use the SageMaker Python SDK to access pre-trained zero-shot text classification models and use SageMaker batch transform for a batch text classification use case

SageMaker JumpStart provides one-click fine-tuning and deployment for a wide variety of pre-trained models across popular ML tasks, as well as a selection of end-to-end solutions that solve common business problems. These features remove the heavy lifting from each step of the ML process, simplifying the development of high-quality models and reducing time to deployment. The JumpStart APIs allow you to programmatically deploy and fine-tune a vast selection of pre-trained models on your own datasets. The JumpStart model hub provides access to a large number of NLP models that enable transfer learning and fine-tuning on custom datasets. As of this writing, the JumpStart model hub contains over 300 text models across a variety of popular models, such as Stable Diffusion, Flan T5, Alexa TM, Bloom, and more. Note that by following the steps in this section, you will deploy infrastructure to your AWS account that may incur costs. Deploy a standalone zero-shot text classification model In this section, we demonstrate how to deploy a zero-shot classification model using SageMaker JumpStart. You can access pre-trained models through the JumpStart landing page in Amazon SageMaker Studio. Complete the following steps:

In SageMaker Studio, open the JumpStart landing page. Refer to Open and use JumpStart for more details on how to navigate to SageMaker JumpStart. In the Text Models carousel, locate the "Zero-Shot Text Classification" model card. Choose View model to access the facebook-bart-large-mnli model. Alternatively, you can search for the zero-shot classification model in the search bar and get to the model in SageMaker JumpStart. Specify a deployment configuration, SageMaker hosting instance type, endpoint name, Amazon Simple Storage Service (Amazon S3) bucket name, and other required parameters. Optionally, you can specify security configurations like AWS Identity and Access Management (IAM) role, VPC settings, and AWS Key Management Service (AWS KMS) encryption keys. Choose Deploy to create a SageMaker endpoint.

This step takes a couple of minutes to complete. When it's complete, you can run inference against the SageMaker endpoint that hosts the zero-shot classification model. In the following video, we show a walkthrough of the steps in this section.

Use JumpStart programmatically with the SageMaker SDK In the SageMaker JumpStart section of SageMaker Studio, under Quick start solutions, you can find the solution templates. SageMaker JumpStart solution templates are one-click, end-to-end solutions for many common ML use cases. As of this writing, over 20 solutions are available for multiple use cases, such as demand forecasting, fraud detection, and personalized recommendations, to name a few. The "Zero Shot Text Classification with Hugging Face" solution provides a way to classify text without the need to train a model for specific labels (zero-shot classification) by using a pre-trained text classifier. The default zero-shot classification model for this solution is the facebook-bart-large-mnli (BART) model. For this solution, we use the 2015 New Year's Resolutions dataset to classify resolutions. A subset of the original dataset containing only the Resolution_Category (ground truth label) and the text columns is included in the solution's assets.

The input data includes text strings, a list of desired categories for classification, and whether the classification is multi-label or not for synchronous (real-time) inference. For asynchronous (batch) inference, we provide a list of text strings, the list of categories for each string, and whether the classification is multi-label or not in a JSON lines formatted text file.

The result of the inference is a JSON object that looks something like the following screenshot.

We have the original text in the sequence field, the labels used for the text classification in the labels field, and the probability assigned to each label (in the same order of appearance) in the field scores. To deploy the Zero Shot Text Classification with Hugging Face solution, complete the following steps:

On the SageMaker JumpStart landing page, choose Models, notebooks, solutions in the navigation pane. In the Solutions section, choose Explore All Solutions.

On the Solutions page, choose the Zero Shot Text Classification with Hugging Face model card. Review the deployment details and if you agree, choose Launch.

The deployment will provision a SageMaker real-time endpoint for real-time inference and an S3 bucket for storing the batch transformation results. The following diagram illustrates the architecture of this method.

Perform real-time inference using a zero-shot classification model In this section, we review how to use the Python SDK to run zero-shot text classification (using any of the available models) in real time using a SageMaker endpoint.

First, we configure the inference payload request to the model. This is model dependent, but for the BART model, the input is a JSON object with the following structure:

{ "inputs": # The text to be classified "parameters": { "candidate_labels": # A list of the labels we want to use for the text classification "multi_label": True | False } }Python

Note that the BART model is not explicitly trained on the candidate_labels. We will use the zero-shot classification technique to classify the text sequence to unseen classes. The following code is an example using text from the New Year's resolutions dataset and the defined classes:

classification_categories = ['Health', 'Humor', 'Personal Growth', 'Philanthropy', 'Leisure', 'Career', 'Finance', 'Education', 'Time Management'] data_zero_shot = { "inputs": "#newyearsresolution :: read more books, no scrolling fb/checking email b4 breakfast, stay dedicated to pt/yoga to squash my achin' back!", "parameters": { "candidate_labels": classification_categories, "multi_label": False } }Python

Next, you can invoke a SageMaker endpoint with the zero-shot payload. The SageMaker endpoint is deployed as part of the SageMaker JumpStart solution.

The inference response object contains the original sequence, the labels sorted by score from max to min, and the scores per label:

{'sequence': "#newyearsresolution :: read more books, no scrolling fb/checking email b4 breakfast, stay dedicated to pt/yoga to squash my achin' back!", 'labels': ['Personal Growth', 'Health', 'Time Management', 'Leisure', 'Education', 'Humor', 'Career', 'Philanthropy', 'Finance'], 'scores': [0.4198768436908722, 0.2169460505247116, 0.16591140627861023, 0.09742163866758347, 0.031757451593875885, 0.027988269925117493, 0.015974704176187515, 0.015464971773326397, 0.008658630773425102]}Python

Run a SageMaker batch transform job using the Python SDK This section describes how to run batch transform inference with the zero-shot classification facebook-bart-large-mnli model using the SageMaker Python SDK. Complete the following steps:

Format the input data in JSON lines format and upload the file to Amazon S3. SageMaker batch transform will perform inference on the data points uploaded in the S3 file. Set up the model deployment artifacts with the following parameters:

model_id – Use huggingface-zstc-facebook-bart-large-mnli. deploy_image_uri – Use the image_uris Python SDK function to get the pre-built SageMaker Docker image for the model_id. The function returns the Amazon Elastic Container Registry (Amazon ECR) URI. deploy_source_uri – Use the script_uris utility API to retrieve the S3 URI that contains scripts to run pre-trained model inference. We specify the script_scope as inference. model_uri – Use model_uri to get the model artifacts from Amazon S3 for the specified model_id.

#set model id and version model_id, model_version, = ( "huggingface-zstc-facebook-bart-large-mnli", "*", )

# Retrieve the inference Docker container URI. This is the base Hugging Face container image for the default model above. deploy_image_uri = image_uris.retrieve( region=None, framework=None, # Automatically inferred from model_id image_scope="inference", model_id=model_id, model_version=model_version, instance_type="ml.g4dn.xlarge", )

# Retrieve the inference script URI. This includes all dependencies and scripts for model loading, inference handling, and more. deploy_source_uri = script_uris.retrieve(model_id=model_id, model_version=model_version, script_scope="inference")

# Retrieve the model URI. This includes the pre-trained model and parameters. model_uri = model_uris.retrieve(model_id=model_id, model_version=model_version, model_scope="inference") Python

Use HF_TASK to define the task for the Hugging Face transformers pipeline and HF_MODEL_ID to define the model used to classify the text:

# Hub model configuration < hub = { 'HF_MODEL_ID':'facebook/bart-large-mnli', # The model_id from the Hugging Face Hub 'HF_TASK':'zero-shot-classification' # The NLP task that you want to use for predictions }Python For a complete list of tasks, see Pipelines in the Hugging Face documentation. Create a Hugging Face model object to be deployed with the SageMaker batch transform job:

# Create HuggingFaceModel class huggingface_model_zero_shot = HuggingFaceModel( model_data=model_uri, # path to your trained sagemaker model env=hub, # configuration for loading model from Hub role=role, # IAM role with permissions to create an endpoint transformers_version="4.17", # Transformers version used pytorch_version="1.10", # PyTorch version used py_version='py38', # Python version used )Python

# Create transformer to run a batch job batch_job = huggingface_model_zero_shot.transformer( instance_count=1, instance_type='ml.m5.xlarge', strategy='SingleRecord', assemble_with='Line', output_path=s3_path_join("s3://",sagemaker_config['S3Bucket'],"zero_shot_text_clf", "results"), # we are using the same s3 path to save the output with the input )Python

Start a batch transform job and use S3 data as input:

You can monitor your batch processing job on the SageMaker console (choose Batch transform jobs under Inference in the navigation pane). When the job is complete, you can check the model prediction output in the S3 file specified in output_path. For a list of all the available pre-trained models in SageMaker JumpStart, refer to Built-in Algorithms with pre-trained Model Table. Use the keyword "zstc" (short for zero-shot text classification) in the search bar to locate all the models capable of doing zero-shot text classification. Clean up After you're done running the notebook, make sure to delete all resources created in the process to ensure that the costs incurred by the assets deployed in this guide are stopped. The code to clean up the deployed resources is provided in the notebooks associated with the zero-shot text classification solution and model. Default security configurations The SageMaker JumpStart models are deployed using the following default security configurations:

The models are deployed with a default SageMaker execution role. You can specify your own role or use an existing one. For more information, refer to SageMaker Roles. The model will not connect to a VPC and no VPC will be provisioned for your model. You can specify VPC configuration to connect to your model from within the security options. For more information, see Give SageMaker Hosted Endpoints Access to Resources in Your Amazon VPC. Default KMS keys will be used to encrypt your model's artifacts. You can specify your own KMS keys or use existing one. For more information, refer to Using server-side encryption with AWS KMS keys (SSE-KMS).

To learn more about SageMaker security-related topics, check out Configure security in Amazon SageMaker. Conclusion In this post, we showed you how to deploy a zero-shot classification model using the SageMaker JumpStart UI and perform inference using the deployed endpoint. We used the SageMaker JumpStart New Year's resolutions solution to show how you can use the SageMaker Python SDK to build an end-to-end solution and implement zero-shot classification application. SageMaker JumpStart provides access to hundreds of pre-trained models and solutions for tasks like computer vision, natural language processing, recommendation systems, and more. Try out the solution on your own and let us know your thoughts.

About the authors David Laredo is a Prototyping Architect at AWS Envision Engineering in LATAM, where he has helped develop multiple machine learning prototypes. Previously, he has worked as a Machine Learning Engineer and has been doing machine learning for over 5 years. His areas of interest are NLP, time series, and end-to-end ML. Vikram Elango is an AI/ML Specialist Solutions Architect at Amazon Web Services, based in Virginia, US. Vikram helps financial and insurance industry customers with design and thought leadership to build and deploy machine learning applications at scale. He is currently focused on natural language processing, responsible AI, inference optimization, and scaling ML across the enterprise. In his spare time, he enjoys traveling, hiking, cooking, and camping with his family. Dr. Vivek Madan is an Applied Scientist with the Amazon SageMaker JumpStart team. He got his PhD from University of Illinois at Urbana-Champaign and was a Post Doctoral Researcher at Georgia Tech. He is an active researcher in machine learning and algorithm design and has published papers in EMNLP, ICLR, COLT, FOCS, and SODA conferences.

TAGS: Hugging Face, Natural Language Processing, sagemaker, Text Classification, Zero-Shot Learning

Amazon Translate enhances its custom terminology to improve translation accuracy and fluency

Amazon Translate is a neural machine translation service that delivers fast, high-quality, affordable, and customizable language translation. When you translate from one language to another, you want your machine translation to be accurate, fluent, and most importantly contextual. Domain-specific and language-specific customizable terminology is a key requirement for many government and commercial organizations. Custom terminology enables you to customize your translation output such that your domain and organization-specific vocabulary, such as brand names, character names, model names, and other unique content (named entities), are translated exactly the way you need. To use the custom terminology feature, you should create a terminology file (CSV or TMX file format) and specify the custom terminology as a parameter in an Amazon Translate real-time translation or asynchronous batch processing request. Refer to Customize Amazon Translate output to meet your domain and organization specific vocabulary to get started on custom terminology. In this post, we explore key enhancements to custom terminology, which doesn't just do a simple match and replace but adds context-sensitive match and replace, which preserves the sentence construct. This enhancement aims to create contextually appropriate versions of matching target terms to generate translations of higher quality and fluency. Solution overview We use the following custom terminology file to explore the enhanced custom terminology features. For instructions on creating a custom terminology, refer to Customize Amazon Translate output to meet your domain and organization specific vocabulary.

Exploring the custom terminology feature Let's translate the sentence "she was a great tutor" with Amazon Translate. Complete the following steps:

On Amazon Translate console, choose Real-time translation in the navigation pane. Choose the Text tab. For Target language, choose French. Enter the text "she was a great tutor."

As shown in the following screenshot, the translation in French as "elle était une excellente tutrice."

Under Additional settings¸ select Custom terminology and choose your custom terminology file.

The translation in French is changed to "elle était une excellente éducatrice."

In the custom terminology file, we have specified the translation for "tutor" as "éducateur." "Éducateur" is masculine in French, whereas "tutor" in English is gender neutral. Custom terminology did not perform a match and replace here, instead it used the target word and applied the correct gender based on the context. Now let's test the feature with the source sentence "he has 10 sheep." The translation in French is "il a 10 agneaux." We provided custom terminology for "sheep" as "agneau." "Agneau" in French means "baby sheep" and is singular. In this case, the target word is changed to inflect plural. The source sentence "walking in the evening is precious to me" is translated to "me promener le soir est précieux pour moi." The custom terminology target word "promenant" is changed to "promener" to inflect the correct verb tense. The source sentence "I like burger" will be translated to "J'aime les sandwichs" to inflect the correct noun based on the context. Now let's test sentences with the target language as Spanish. The source sentence "any action-specific parameters are listed in the topic for that action" is translated to "odos los parámetros especificos de acción aparecen en el tema de esa acción" to inflect the correct adjective. The source sentence "in order for us to help you, please share your name" will be translated to "pour que nous puissions vous aider, veuillez partager votre nom." Some words may have entirely different meanings based on context. For example, the word "order" in English can be a sequence (as is in the source sentence) or a command or instruction (as in "I order books"). It's difficult to know which meaning is intended without explicit information. In this case, "order" should not be translated as "commande" because it means "command" or "instruct" in French. Conclusion The custom terminology feature in Amazon Translate can help you customize translations based on your domain or language constructs. Recent enhancements to the custom terminology feature create contextually appropriate versions of matching terms to generate translations of higher quality. This enhancement improves the translation accuracy and fluency. There is no change required for existing customers to use the enhanced feature. For more information about Amazon Translate, visit Amazon Translate resources to find video resources and blog posts, and refer to AWS Translate FAQs.

About the Authors Sathya Balakrishnan is a Senior Consultant in the Professional Services team at AWS, specializing in data and ML solutions. He works with US federal financial clients. He is passionate about building pragmatic solutions to solve customers' business problems. In his spare time, he enjoys watching movies and hiking with his family. Sid Padgaonkar is the Senior Product Manager for Amazon Translate, AWS's natural language processing service. On weekends, you will find him playing squash and exploring the food scene in the Pacific Northwest.

Use a reusable ETL framework in your AWS lake house architecture

Data lakes and lake house architectures have become an integral part of a data platform for any organization. However, you may face multiple challenges while developing a lake house platform and integrating with various source systems. In this blog, we will address these challenges and show how our framework can help mitigate these issues. Lake house architecture using AWS Figure 1 shows a typical lake house implementation in an Amazon Web Services (AWS) environment.

In this diagram we have five layers. The number of layers and names can vary per environmental requirements, so check recommended data layers for more details.

Landing layer. This is where all source files are dropped in their original format. Raw layer. This is where all source files are converted and stored in a common parquet format. Stage layer. This is where we maintain a history of dimensional tables as Slowly Changing Dimension Type 2 (SCD2). Apache Hudi is used for SCD2 in the Amazon Simple Storage Service (Amazon S3) bucket, and an AWS Glue job is used to write to Hudi tables. AWS Glue is used to perform any extract, transform, and load (ETL) job to move, cleanse, validate, or transform files between any two layers. For details, see using the Hudi framework in AWS Glue. Presentation layer. This is where data is being cleansed, validated, and transformed, using an AWS Glue job, in accordance with business requirements. Data warehouse layer. Amazon Redshift is being used as the data warehouse where the curated or cleansed data resides. We can either copy the data using an AWS Glue python shell job, or create a Spectrum table out of the Amazon S3 location.

The data lake house architecture shows two types of data ingestion patterns, push and pull. In the pull-based ingestion, services like AWS Glue or AWS Lambda are used to pull data from sources like databases, APIs, or flat files into the data lake. In the push-based pattern, third-party sources can directly upload files into a landing Amazon S3 bucket in the data lake. Amazon Managed Workflows for Apache Airflow (Amazon MWAA) is used to orchestrate data pipelines that move data from the source systems into a data warehouse. Amazon EventBridge is used to schedule the Airflow directed acyclic graph (DAG) data pipelines. Amazon RDS for PostgreSQL is used to store metadata for configuration of the data pipelines. A data lake architecture with these capabilities provides a scalable, reliable, and efficient solution for data pipelines. Data pipeline challenges Maintaining data pipelines in a large lake house environment can be quite challenging. There are a number of hurdles one faces regularly. Creating individual AWS Glue jobs for each task in every Airflow DAG can lead to hundreds of AWS Glue jobs to manage. Error handling and job restarting gets increasingly more complex as the number of pipelines grows. Developing a new data pipeline from scratch takes time, due to the boilerplate code involved. The production support team can find it challenging to monitor and support such a large number of data pipelines. Data platform monitoring becomes arduous at that scale. Ensuring overall maintainability, robustness, and governability of data pipelines in a lake house is a constant struggle. The benefits of a data pipeline framework Having a data pipeline framework can significantly reduce the effort required to build data pipelines. This framework should be able to create a lake house environment that is easy to maintain and manage. It should also increase the reusability of code across data pipelines. Effective error handling and recovery mechanisms in the framework should make the data pipelines robust. Support for various data ingestion patterns like batch, micro batch, and streaming should make the framework versatile. A framework with such capabilities will help you build scalable, reliable, and flexible data pipelines, with reduced time and effort. Reusable ETL framework In a metadata-driven reusable framework, we have pre-created templates for different purposes. Metadata tables are used to configure the data pipelines. Figure 2 shows the architecture of this framework:

In this framework, there are pre-created AWS Glue templates for different purposes, like copying files from SFTP to landing bucket, fetching rows from a database, converting file formats in landing to parquet in the raw layer, writing to Hudi tables, copying parquet files to Redshift tables, and more. These templates are stored in a template bucket, and details of all templates are maintained in a template config table with a template_id in Amazon Relational Database Service (Amazon RDS). Each data pipeline (Airflow DAG) is represented as a flow_id in the main job config table. Each flow_id can have one or more tasks, and each task refers to a template_id. This framework can support both the type of ingestions—pull-based (scheduled pipelines) and push-based (initiated pipelines). The following steps show the detailed flow of the pipeline in Figure 2.

To schedule a pipeline, the "Scheduled DAG Invoker Lambda" is scheduled in EventBridge, with flow_id of the pipeline as the parameter. The source drops files in a landing bucket. An event is initiated and calls the "Triggered DAG Invoker" Lambda. This Lambda function gets the file name from the event to call the Airflow API. A Lambda function queries an RDS metadata table with the parameter to get the DAG name. Both of the Lambda functions call the Airflow API to start the DAG. The Airflow webserver locates the DAG from the S3 location and passes it to the executor. The DAG is initiated. The DAG calls the functions in the common util python script with all required parameters. For any pipeline, the util script gets all the task details from the metadata table, along with the AWS Glue template name and location. For any database or API connectivity, the util function gets the secret credentials from AWS Secrets Manager based on the secret_id. The AWS Glue template file from the S3 location starts the AWS Glue job using Boto3 API by passing the required parameters. Once the AWS Glue job completes successfully, it deletes the job. If the pipeline contains any Lambda calls, the util script calls the Lambda function as per the configuration parameter. If the AWS Glue job fails due to any error in Step #11, the script captures the error message and sends an Amazon Simple Notification Service (Amazon SNS) notification.

For developing any new pipeline, the developer must identify the number of tasks that need to be created for the DAG. Identify which template can be used for which task, and insert configuration entries to the metadata tables accordingly. If there is no template available, create a new template to reuse later. Finally, create the Airflow DAG script and place it in the DAG location. Conclusion The proposed framework leverages AWS native services to provide a scalable and cost-effective solution. It allows faster development due to reusable components. You can dynamically generate and delete AWS Glue jobs as needed. This framework enables jobs tracking by configuration tables, supports error handling, and provides email notification. You can create scheduled and event-driven data pipelines to ingest data from various sources in different formats. And you can tune the performance and cost of AWS Glue jobs, by updating configuration parameters without changing any code. A reusable framework is a great practice for any development project, as it improves time to market and standardizes development patterns in a team. This framework can be used in any AWS data lake or lake house environments with any number of data layers. This makes pipeline development faster, and error handing and support easier. You can enhance and customize even further to have more features like data reconciliation, micro-batch pipelines, and more. Further reading:

Land data from databases to a data lake at scale using AWS Glue blueprints Creating a source to Lakehouse data replication pipe using Apache Hudi, AWS Glue, AWS DMS, and Amazon Redshift Temporal data lake architecture for benchmark and indices analytics

Ashutosh Dubey Ashutosh is a Global Technical leader and Solutions Architect at Amazon Web Services based out of New Jersey, USA. He has extensive experience specializing in the Data, Analytics, and Machine Learning field, and has helped Fortune 500 companies in their cloud journey to AWS.

Prantik Gachhayat Prantik is an Enterprise Architect at Infosys with 19+ years of experience in various technology fields and business domains. He has a proven track record helping large enterprises modernize digital platforms and delivering complex transformation programs. Prantik specializes in architecting modern data and analytics platforms in AWS. Prantik loves exploring new tech trends and enjoys cooking.

Build a digital asset tokenization framework for financial services use cases using Amazon Managed Blockchain – Part 1

This is the first post in a series of posts covering digital asset tokenization in financial services, a topic which is seeing tremendous interest in the sector. The series aims to be a guide for financial services customers looking to learn more about the topic, and who may be considering building a digital asset capability for their business on AWS. The series explains key concepts, offers guidance on business case alignment, highlights high level and technical design considerations, and finally presents a reference solution architecture of an illustrative business use case using Amazon Managed Blockchain and other relevant services. In this initial post we explore core concepts in digital asset tokenization for financial services and highlight applicable business use cases. Overview of digital asset tokenization in financial services Throughout this post, the terms "Distributed Ledger Technology" (DLT) and "Blockchain" are regarded as being sufficiently synonymous to simply abbreviate to "DLT" for brevity. A digital asset in the context of financial services refers to a digital representation (or "digital twin") of an underlying financial asset. Such digital assets often take the form of digital tokens typically implemented and operated on, but not exclusively, a blockchain or similar distributed ledger technology (DLT). On such a platform, the tokens can be issued, distributed, and traded as financial instruments themselves on a secondary market. The advent of DLT technology has attracted the attention of many industries, but perhaps none more so than financial services given the disruptive potential of the technology to the sector. Digital asset tokenization is a use case in the industry which aligns exceptionally well with the enabling characteristics of distributed ledger technology. Industry impact and potential of digital asset tokenization In a 2021 study conducted by Deloitte 76% of respondents saw digital asset tokenization as a significant long term opportunity, with corporate investors (comprising 31% of surveyed companies) reporting significant uptake in digital assets. In another study, 97% of institutional investors agree that "tokenization will revolutionize asset management" and a March 2023 study by Citibank estimates a Total Addressable Market Size of $4-5 trillion for distributed ledger technology (DLT)-based tokenized securities. An analysis by the World Economic Forum (Global Agenda Council) valued the total tokenized market to be 10% of global GDP by 2030:

Fig 1. Tokenization of illiquid assets to be a $16 trillion worth opportunity globally. Source: World Economic Forum – Global Agenda Council, BCG Analysis With the advent of distributed ledger technologies, a new range of capabilities addressing pain points and new business opportunities within financial services has emerged. Many of these new capabilities involve a new class of assets which are either entirely new native digital assets or digital twins of existing assets and financial instruments. Although not necessarily implemented on DLTs (or similar technology such as centralised cryptographic immutable ledgers), many digital asset providers and institutions have chosen this technology. Many large financial institutions, such as Goldman Sachs, J.P.Morgan, Santander, Societe General, and the European Investment Bank have issued digital asset financial instruments on public DLTs, or are offering new financial products which provide some form of exposure to crypto-currencies or integrate the technology into their existing portfolios. Similarly, digital asset working groups to formulate strategies around DLT technology are now commonplace within financial services institutions. The transformative potential of digital asset tokenization relates to the fact that the financial services industry has multiple layers of intermediaries and participants in distinct silos but operating on the same data which leads to significant duplication and therefore reconciliation. For example, the existing industry infrastructure supporting post-trade settlement on funds and bonds flow through intermediaries such as Central Counterparty Clearing Houses (CCPs). Such intermediaries exist to reduce counterparty risk and settlement failures by providing a trusted third party to become a counterparty to both sides of a transaction. A technology platform that can offer the same degree of settlement and counterparty risk reduction directly without multiple intermediaries offers significant cost reduction and efficiently gains. Digital asset tokens can directly exploit these benefits as all participants in the token lifecycle can transact directly with the token. In addition, the token itself executes the business logic workflow, without reliance on trusted third parties to govern and execute transactions. Some business use cases enabled by Digital Asset Tokenization

Cross-border Payments. A token, as a representation of value, is agnostic to political and economic borders and thus can be easily transferred between accounts which are located in different jurisdictions Regulated Decentralised Finance (De-Fi). By issuing a token on a platform operated by regulated entities employing full KYC/AML controls, innovations such as peer to peer lending/settlement and deposit tokens offered by decentralised finance can be leveraged in a regulated setting. The regulated entities provide the platform but do not function as an intermediatory between issuers and investors. Central Bank Digital Currency (CBDC). Tokens running on a programmable network representing a unit value held at a central bank. A unit of digital currency represented by a fungible token of currency. Delivery v Payment (DvP). During settlement of a tokenised trade transaction the smart contract ensures that the security tokens and the payment tokens are swapped simultaneously. This ensures that one party cannot receive the payment without delivering the securities, or vice versa. Bond, Repo, Euro Commercial Paper Transactions on DLT. Transactions consist with the issuance by the originating institution of a series of tokens on a DLT, where investors purchase and pay for the tokens using traditional fiat.

What is a digital asset token? A digital asset token represents a transferrable and quantifiable unit of value, or a claim on an underlying asset of value, that exists on an electronic programmable network. Digital asset tokens can either be native or non-native to the network. A native token is one that is an integral component of the network, with the network serving as its principal record-keeping system. A prominent example of a native token is Bitcoin. Here, the token (Bitcoin) is inherently bound to its originating network, and it cannot exist independently of that network. On the other hand, a non-native token typically signifies a claim on an underlying asset. A banknote, for instance, provides an apt example of a non-native token. Unlike naturally occurring entities such as gold, a banknote isn't native to our world. Instead, it embodies a claim on an asset (such as gold) held at a central bank.

Fig 2. A digital asset token comprises of codified rules, behaviours, and data. For example, how to pay a dividend, to whom, and when. What is tokenization? Tokenization is the procedure of converting a traditional asset into digital form and subsequently recording it on a programmable platform, such as one enabled by a DLT. However, the value of tokenization goes far beyond the relatively simple digitization of an asset. The fundamental differentiation of tokenization is of "portable representation of value" which refers to how a digital token, secured by cryptography and enabled by a distributed ledger or blockchain, can represent ownership in an immutable, inimitable form and be transferrable between multiple parties in a decentralized network, where trust between participants is not a prerequisite. Tokens can be generated and are linked to, or symbolize various types of assets such as securities, equity shares, participation certificates, debt instruments, shares of investment collective schemes (commonly referred to as funds), and structured products. Tokenization can rightly be perceived as the transformation of conventional securities into digital assets, commonly referred to as ledger-based securities. Why digitally tokenize a financial asset or claim? Despite the advent of electronic trading, global telecommunications and the internet, financial industry market infrastructure exist in silos with distinct tech stacks dating back many decades. Eliminating intermediaries in order to simplify operating models and lower cost, is a key benefit of tokenized representations of financial assets and their corresponding lifecycle. The decentralized nature of DLT networks that often underpin digital asset tokenization platforms enable all eligible participants direct access to the digital representation, without the involvement of intermediaries. Furthermore, tokenization has the potential to significantly improve liquidity for financial instruments which have traditionally proven to be illiquid. Fractional ownership, accessibility of trading infrastructure, execution automation and programmability (using smart contracts), transparency and interoperability across DLT networks all contribute to enhanced liquidity. This, in turn, can lead to price stabilization, settlement and counterparty risk reduction, transaction cost reduction, and reduced friction around price discovery and trading. Fractional ownership is probably the fundamental innovation tokenization brings. For example, investors in a tokenised bond would no longer be bound to the face value of a bond; for example, a $100,000 bond would become 100,000 units/tokens of $1, thus providing fractional ownership and increasing illiquidity to a broader base of potential investors. It is important to note that not all financial products are suitable for tokenization. Assets like fixed income and private equity are more constrained compared to the efficient public equities market as it stands today. It is therefore key to ensure the asset class being considered for tokenization is suitable and the benefits of tokenization are likely to be realized. What is a digital asset tokenization platform? A digital asset tokenization platform converts rights to an asset into a digital token onto a digital and programmable platform which manages the progression of the token through its business lifecycle. The state of the token is maintained on a shared multi-party ledger which is accessible to entitled participants. Participants may transact directly with each other on the platform for example, issuers transacting directly with investors, direct peer to peer trading across secondary markets, regulators monitoring transactions in real-time and settlement services executing directly on the platform, as illustrated in the following diagram:

Fig 3. The traditional method for managing financial asset lifecycles involves a "duplicated system of record." In this approach, all participants keep duplicate data in separate databases. Because of the ongoing risk of data errors, these databases need frequent reconciliation. The registry, or 'golden record' of the asset is stored by a centralised intermediatory such as a Central Security Depositary (CSD). Data is accessed by the various participants involved in the asset lifecycle, such as investment banks, corporate investors, private investors, funds, and custodians (and others service providers such as CSDs) through Application Program Interfaces (APIs). With a decentralized tokenization platform, the programmable tokens are codified on a shared platform with no single centralized entity having overall control, and all participants having direct access to the asset they are eligible to transact with thus eliminating the need for intermediaries. The state of the digital asset, for example current owner, is an attribute of the token itself – not a data record maintained by all participants. Digital asset 'smart contracts' are an intrinsic attribute of a token on the platform, codifying token behaviour and enabling automation of the business lifecycle events. The quality, reliability, consistency and auditability of a digital asset tokenization workflow implementation is therefore critically important to ensure regulatory, contractual, and compliance standards are met – perhaps more so than with conventional approaches, due to the characteristic immutability and automation of the platform. The programmability of the token's lifecycle on the platform leads to another benefit of 'composability' which refers to the ease by which smart contract of differing tokens can interoperate seamlessly to build innovative financial product. For example, if two different tokens are created using smart contracts which know how to interoperate with each other, the tokens can be easily swapped thus forming a financial swap instrument. Similarly, one token could be programmed to be equivalent in value to a number of other known tokens forming a structured hierarchy of tokens, the exact structure of which is being configured to achieve a specific business goal or end-client requirement. The lifecycle of a digital asset token, representing a claim on an underlying asset, generally follows the same lifecycle as conventional security processing; namely origination, issuance and distribution, trading, clearing, settlement, and maturity. However, a token-based platform will differ in some key regards:

The platform is more decentralized – there is no, or considerably less centralised entities, which govern the platform. Token ownership, for example, can be transferred directly between participants on the platform without passing through a central entity. With traditional centralized ledgers, the services which comprise the operation of a security; for example; custody settlement, escrow, collateral, and regulatory actions are distinct processes which are executed by various agencies running their own applications and databases. The attributes of the security, such as the owner, the custodian, the identification, the issuance value, are represented by duplicated and distributed data records across the network. A tokenization platform however, codifies these fragmented and siloed data records into a programmable unit which can exist in a distinct form on the platform. The token's lifecycle is operated in an automated manner in response to events originating externally to the platform.

The following lists the key lifecycle stages and how a tokenized asset can benefit from these stages executed by a tokenization platform:

Origination and issuance process can be significantly automated by smart contract, reducing time and cost. Trading can execute in real-time directly between counterparties on the same platform, further reducing capital requirements, counterparty risk, and settlement times. Settlement and clearing time is reduced and processes streamlined, drastically reducing errors and costly reconciliation and exception processing workflows. Servicing could be programmed via a smart contract to auto-trigger pay-outs for corporate actions. In the case of loan securitisation, borrower repayments could be pooled and distributed to investors via token smart contracts.

Fig 4. A digital asset token exists on the platform and transits various states aligned to the business workflow. Its state is visible to all eligible participants. Benefits at each stage are illustrated. Conclusion In this post, we explored the fundamental concepts underpinning digital asset tokenization within the realm of the financial services industry. Common reasons for building a platform to facilitate digital asset tokenization were highlighted, and how the traditional asset lifecycle is disrupted and enhanced using tokenization. In conclusion, digital asset tokens can take many forms, from tokenised credit cards on a cellphone, to a multi-million-dollar bond on the public Ethereum DLT. It is important to realise a DLT is not a pre-requisite for enabling tokenization, it is one facilitating technology that has the potential to fulfil many of the aspired benefits of tokenization. It is critically important to carefully consider the asset's attributes and operational mechanisms when considering tokenization, as different types of asset may yield differing degrees of benefit. Assets with little manual intervention, a clear legal framework, and clear regulation have a higher chance of capturing the various benefits of tokenization. In the next part in the series, we will look into the considerations involved in envisioning a tokenisation framework on AWS and present potential solution architectures on key aspects. In addition, the benefits of using a manged blockchain service, such as Amazon Managed Blockchain will be discussed. Newly launched capabilities such as Amazon Managed Blockchain Query and Amazon Managed Blockchain Access provide actionable real-time data across different blockchains with standardized APIs. For example, a tokenization architecture would utilize Amazon Managed Blockchain Query to extract token balances, transactions, and token events from the Ethereum blockchain in real-time, enabling rapid integration with actionable analytics and applications. Amazon Managed Blockchain Access provides public blockchain nodes for Ethereum and Bitcoin, as well as the ability to create private blockchain networks with the Hyperledger Fabric framework. These capabilities enable developers and architects to focus on differentiating innovation rather than node hosting infrastructure which offers significant architectural acceleration when planning and implementing a tokenization architecture.

Steven Bacci is a Principal Specialist Solution Architect within the Worldwide Specialist Blockchain/Web3 team at Amazon Web Services. He works closely with the product and engineering team for Amazon Managed Blockchain, advising customers globally on business case alignment, technical architectural designs, and application engineering for Blockchain and Web3 solutions with a specific focus on the financial services sector.

Measure and track cloud efficiency with sustainability proxy metrics, Part I: What are proxy metrics?

Sustainability has become an important decision-making factor for customers, employees, regulators, investors, and partners. Customers have started their journey towards a sustainable business and operations. If you're building, deploying, and maintaining IT infrastructure, reducing its environmental impact is an important step in reaching company-wide sustainability goals. Thus, sustainability has become a non-functional requirement in modern software and systems architecture, along with security, maintainability, reliability and others. When it comes to architecting workloads in the cloud, sustainability is a shared responsibility between AWS and customers. Whereby AWS optimizes for sustainability of the cloud, customers are responsible for sustainability in the cloud. Customers optimize their service usage and resource efficiency. This blog post series provides an overview on how you can establish a sustainability proxy metrics showback mechanism for teams wanting to optimize their AWS usage for sustainability. In Part I, we introduce the concept of proxy metrics and the importance of normalization. We also show examples of how customers have used this concept to reduce the environmental impact of their applications. In Part II: Establish a metrics pipeline, we discuss how you set up a proxy metrics data pipeline to establish a sustainability proxy metrics showback mechanism. Optimize your workloads with proxy metrics Every optimization should start with a goal informed with metrics or KPIs: Reducing cost, increasing performance, or reducing greenhouse gas emissions. The AWS Customer Carbon Footprint Tool (CCFT) provides the important output metric of the greenhouse gas emissions associated with a customer's AWS service usage. This emission data is used for reporting and understanding the high-level impact on a monthly basis. However, while AWS is working to increase scope and granularity of the CCFT (read this blog), a practice of continuous optimization cycles calls for fine-grained metrics. Absolute emissions don't expose the efficiency of a workload. Emissions are the outcome of multiple factors including factors which are not in the responsibility of application teams such as the usage of an application, or the carbon intensity of energy. For these purposes we complement the carbon emissions reported by the AWS Customer Carbon Footprint Tool with dependent metrics that we call sustainability proxy metrics. We have also launched the Sustainability Proxy Metrics Dashboard (you can access the dashboard from this link), as part of the Cloud Intelligence Dashboards. Good sustainability proxy metrics serve as fine-grained substitutes to carbon emissions, that provide insights into workload efficiency. Metrics that we track in near real-time and break down to application teams and resources, so they are suitable for fast optimization cycle times. They are tangible metrics that reflect resource usage, in terms of Compute, Storage and Networking (read these blogs).

As depicted in Figure 1 on the right, calculating Greenhouse Gas Emissions for AWS service usage is dependent on multiple data sources. This includes the energy required to run cloud resources (Scope 1 & 2) and the indirect emissions associated with the lifecycle of physical resources, up- and downstream in the value chain (Scope 3). Similarly, cost is a simple function of AWS service usage. But even though cost reflects usage, volume based discounts reduce cost while not reducing associated emissions. Also, the pricing structure of certain services does not reflect every aspect of resource usage – consider data transfer pricing and how there is no charge for inbound data transfer across all services in all regions or how data transfer charges don't differ depending on end customer proximity. AWS service usage in turn depends on and is used by a customer's operational processes to fulfill business needs, completing the data flow on the left. All of this comes back to efficiency and using the least amount of resources to fulfill business needs. Normalize metrics to allow for comparison We sometimes see customers counting the number of Amazon EC2 instances, or the amount of instance hours to quantify resource consumption. These metrics do not help to compare applications, identify top contributors to consumption or spot trends. Some applications run instances only for minutes before termination. Others run a single instance for a whole month. In a similar way the instance size matters. Instead of just using instance hours, you have to factor in the amount of vCPUs of an instance. We call this normalization. There are many ways to normalize:

Normalize resource usage: Use the information about the instance type and multiply the instance hours with the number of vCPUs. Alternatively, take normalization factors into account such as those used by Amazon EC2 Reserved Instances. The same applies to other services like Amazon S3 or Amazon EBS in which you take the GB hours instead. For KPIs, calculate the ratio of desired usage in relation to total usage. That's already the case with CPU utilization. If your goal is Amazon EC2 Spot adoption, then that is all spot hours divided by all vCPU hours. And if it's AWS Graviton adoption, then it is all Graviton vCPU hours divided by the total vCPU hours. You define a minimum target percentage for your application teams for this type of KPI. Use a scoring system to weight services and features differently and incentivize application teams to use resource-efficient services. For example, weight the Amazon S3 Standard storage class higher than Amazon S3 Intelligent-Tiering, by applying a multiplier, as the service description of the latter provides flexibility to AWS to optimize for using less energy and less hardware to provide the service. The goal for application teams is to drive down the weighted usage. Resource efficiency is using the least amount of resources to fulfill business needs. Your KPIs or metrics have to factor this in by normalizing the resource usage by business unit metrics. We will dive deeper into this in the next section.

Normalize by business metrics An increase in resource usage is not a cause for alarm when your business grows, but a steady consumption at dropping customer demand is. Factoring in business metrics in your KPIs helps to track and communicate efficiency over time. Business metrics are specific to the purpose of a workload. Examples include the number of monthly active users, or insurance policies managed, or successful calls to an API. You divide your resource usage by a business metric (read this user guide "Evaluate specific improvements") to calculate a sustainability KPI, like vCPU hours per transaction, as depicted in the equation below. Ideally, you want to see your sustainability KPIs go down or at least, stay on level. You will find the related concept of unit metrics for cost in the blog post "choose, create, and track your unit metrics for your applications".

In AWS re:Invent 2022 Build a cost-, energy-, and resource-efficient compute environment (CMP204) (watch the recording) Arm – a global semiconductor industry leader – present how they measured, tracked, and reduced the impact of Electronic Design Automation (EDA) jobs. They used Amazon EC2 instances' vCPU hours to calculate KPIs for Amazon EC2 Spot adoption, AWS Graviton adoption, and the resources needed per job. Similarly, Amazon Prime Video explain in AWS re:Invent 2022 Architecting sustainably and reducing your AWS carbon footprint (SUS205) (watch the recording) how they used the following sustainability proxy metrics to quantify and track the effectiveness of optimizations:

Playback experience: Infrastructure cost ($) per 1.000 concurrent streams Content delivery: Delivery bandwidth (Gbps) per stream Content discovery experience: Normalized Instance Hour (NIH) per 1000 page impressions Customer acquisition: Infrastructure cost ($) per subscription

Optimizing towards their goals, Prime Video implemented tradeoffs between sustainability goals and other non-functional requirements. To match the provisioning of resources to the spikey demand from viewers of "Thursday Night Football" they implemented automated contingency switches that turn off non-critical customer experience features if the system was under duress. Conclusion In this post we've covered the motivation for sustainability proxy metrics and KPIs. We explained the concept of usage based metrics, normalization and inclusion of business metrics, and shared examples on how customers are using these metrics to optimize for sustainability. In Part II of this blog post series, we'll dive deeper into how you set up a proxy metrics data pipeline to establish a sustainability proxy metrics showback mechanism for teams wanting to optimize their AWS usage for sustainability and for organizational adoption of efficiency best practices at scale. For detailed information on how to optimize your workloads for sustainability, please refer to the AWS Well Architected Sustainability Pillar. If you're interested to start measuring and optimizing sustainability proxy metrics for your applications, please locate the "Sustainability Proxy Metrics Dashboard" and implement today.

Katja Philipp Katja Philipp is a Senior Prototyping Architect at Amazon Web Services. She works with customers to build innovative prototypes using AI, ML, and generative AI technologies that solve real business challenges. Katja is passionate about sustainability, serverless and how technology can be leveraged to solve current challenges for a better future.

Jonas Bürkel Jonas Bürkel is a Solutions Architect at AWS based in Germany. He helps customers in the manufacturing industry build their solutions in the cloud to meet their unique business and technical requirements. Jonas is also passionate about sustainability and how technology can help us become more efficient.

Steffen Grunwald Steffen Grunwald is a Principal Sustainability Solutions Architect at Amazon Web Services. He supports customers in solving their sustainability challenges through the cloud. Having a long software engineering background, he loves to dive deep into application architectures and development processes to drive sustainability, performance, cost, operational efficiency, and increase the speed of innovation.

Introducing S3 cross-region support in the AWS SDK for Java 2.x

AWS SDK for Java team is excited to introduce the latest addition to the AWS SDK for Java 2.x: the Amazon Simple Storage Service (Amazon S3) Cross-Region Client feature. With this new feature, you can effortlessly access Amazon S3 buckets in different AWS Regions by using a single client configured for cross-region access. Join us as we explore the possibilities and convenience offered by this exciting enhancement. Motivation A common challenge when working with Amazon S3 buckets is the need to know the bucket's region to interact with it. Users might not always have explicit knowledge of the bucket's region, resulting in additional calls to determine the region. The new Amazon S3 Cross-Region client feature addresses this problem by enabling seamless access to S3 buckets across different regions. In this blog post, we explore the significance of this feature and how to use it to simplify cross-region S3 bucket access. Getting Started To begin, add the Amazon S3 dependency to your project for SDK versions 2.20.111 or higher: software.amazon.awssdk s3 2.20.111 XML By default, the cross-region feature is disabled. To enable it for an S3Client, set crossRegionAccessEnabled to true on the S3Client Builder: S3AsyncClient client = S3AsyncClient.builder() .crossRegionAccessEnabled(true) .build();Java Routing Requests to the Correct Region When you reference an existing bucket in a request, such as with the 'putObject' request, the SDK initiates a request to the region configured for the client. If the bucket does not exist in that specific region, the error response includes the actual region where the bucket resides. Subsequently, the SDK makes the request to the correct region using the region information obtained from the error response. To optimize future requests to the same bucket, the SDK caches this region in the client. Considerations for Enabling Cross-Region Bucket Access When you enable cross-region bucket access, it's important to keep in mind that the first API call may incur increased latency if the bucket is not in the client's configured location. However, subsequent calls benefit from cached region information, resulting in improved performance. Please take note of the potential latency for the initial cross-region bucket access call and plan accordingly. This feature does not affect access to the bucket; the user still needs to be authorized to access the bucket in whatever region it resides. Conclusion Cross-region bucket access simplifies the management of Amazon S3 buckets by allowing you to access a bucket without needing to know the bucket's region in advance. It is important to understand the routing process, caching capabilities, and increased latency for the first API call to buckets in different Regions. The Amazon S3 Cross-Region Client feature, available in the AWS SDK for Java 2.0, provides a valuable functionality. We hope you'll find this new feature useful. If you have any question, comments, concerns, or ideas, please open an issue or bug report on the GitHub issues page.

John Viegas John Viegas serves as a Software Development Engineer on the Java SDK team at Amazon Web Services (AWS). With a passion for enhancing the developer experience, he actively engages in projects and tools designed to elevate software development. For further insights into his contributions, feel free to explore his GitHub profile under the handle @joviegas.

Customers running Google Kubernetes Engine (GKE) clusters can now use the Amazon Elastic Kubernetes Service (Amazon EKS) to visualize GKE cluster resources. This post describes how to use Amazon EKS Connector to connect a GKE cluster to the Amazon EKS console. The EKS console provides a single pane of glass to visualize all your Kubernetes clusters. Customers that prefer a graphical user interface use the Amazon EKS console to view the Kubernetes cluster's status, configuration, and workloads. With Amazon EKS Connector, customers can also view their GKE cluster's information along with Amazon EKS clusters. In addition to GKE, Amazon EKS Connector allows you to register and connect any conformant Kubernetes cluster to Amazon EKS. Any external cluster information shown in the EKS console is view-only. Solution You can connect your GKE clusters to Amazon EKS using the AWS CLI, the AWS Management Console, or eksctl. This post uses eksctl. You can find the steps for AWS Management Console or AWS CLI here. You'll need the following tools:

You'll also need a GKE cluster on which you can install EKS Connector. With eksctl, connecting an external Kubernetes cluster to Amazon EKS is a two-step process:

Generate external cluster configuration eksctl makes it easy to register clusters by creating the required AWS resources and generating Kubernetes manifests for EKS Connector. Register your GKE cluster: eksctl register cluster \ --name \ --provider GKE \ --region Bash Please change the Region in the command above to match your environment. eksctl register cluster registers the external cluster to Amazon EKS and creates three files.

These manifests will create eks-connector statefulset in a new namespace. The manifests permit EKS connector to get and list resources in all namespaces in the applied cluster. eksctl also creates an IAM role that EKS Connector uses to invoke Systems Manager APIs. { "Version": "2012-10-17", "Statement": [ { "Sid": "SsmControlChannel", "Effect": "Allow", "Action": [ "ssmmessages:CreateControlChannel" ], "Resource": "arn:aws:eks:*:*:cluster/*" }, { "Sid": "ssmDataplaneOperations", "Effect": "Allow", "Action": [ "ssmmessages:CreateDataChannel", "ssmmessages:OpenDataChannel", "ssmmessages:OpenControlChannel" ], "Resource": "*" } ] }Bash The next step is to apply these manifests to the GKE cluster. Install EKS Connector on GKE Apply the manifests that eksctl generated in the previous step: kubectl apply -f eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yamlBash Verify that EKS Connector pods are running: kubectl get all --namespace eks-connector

NAME READY STATUS RESTARTS AGE pod/eks-connector-0 2/2 Running 0 5m49s pod/eks-connector-1 2/2 Running 0 5m31s

NAME READY AGE statefulset.apps/eks-connector 2/2 5m51sBash EKS Connector uses user impersonation to authorize against the GKE cluster's API server. Kubernetes administrators can further customize EKS Connector's permissions to limit its access to cluster resources. Please see Granting access to a user to view a cluster to configure more restrictive access. For this post, we have used a role that permits view access to all cluster resources. Once the GKE cluster is registered, you can use eksctl to list your clusters, including your GKE cluster. ~# eksctl get clusters 2021-11-11 06:32:42 [i] eksctl version 0.74.0-rc.0 2021-11-11 06:32:42 [i] using region us-east-1 NAME REGION EKSCTL CREATED eks-cluster-aws us-east-1 True gke-connected-by-eks-connector us-east-1 FalseBash You can also use the AWS CLI to list and describe registered clusters: ~# aws eks describe-cluster –name

{ "cluster": { "name": "gke-connected-by-eks-connector", "arn": "arn:aws:eks:us-east-1:831141539580:cluster/gke-connected-by-eks-connector", "createdAt": "2021-11-10T21:33:43.179000+00:00", "status": "ACTIVE", "tags": {}, "connectorConfig": { "activationId": "397acad4-4ecf-4fca-8beb-e71bc6128481", "activationExpiry": "2021-11-13T21:33:42.779000+00:00", "provider": "GKE", "roleArn": "arn:aws:iam::831141539580:role/eksctl-20211110213333349284" } } }Bash The Amazon EKS console will now show the GKE cluster along with your EKS clusters and any other registered external clusters. The user or role with which you log in to the AWS Management Console and the role used to generate manifests in the previous step should be the same.

Here's a screenshot of the EKS console that shows an overview of the GKE cluster's data plane.

You can view node level details such as its compute resources, kernel, and operating system details. The panel also displays the resource allocation and pods running on the specific node.

The Workloads tab shows workloads (pods, deployments, statefulsets, and daemonsets) running in the cluster. It also allows filtering by namespace or resource type. You can select a workload to view pod details such as events, status, labels, and annotations.

When pods contain multiple containers, you can further drill down to view container-level details such as image, mounts, ports, and environment variables.

The EKS Console also provides information on the objects in JSON format. Here's information about a node in JSON:

Similarly, you can enable the Raw view toggle to view the JSON of other Kubernetes objects in the cluster. Tags can be applied to registered clusters to help you track each cluster's owner, organization, cluster function, and so on. You can then search and filter the clusters based on the tags that you add.

Cleanup Run the following command to delete resources created in this post: kubectl delete -f eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yaml

eksctl deregister cluster Bash Conclusion Using EKS connector, you can view any conformant Kubernetes cluster information in the Amazon EKS console. You can connect any conformant Kubernetes cluster, including Amazon EKS Anywhere clusters running on-premises, self-managed clusters on Amazon Elastic Compute Cloud (Amazon EC2), and other Kubernetes clusters running outside of AWS such as GKE. Regardless of where your cluster is running, you can use the Amazon EKS console to get a centralized view of all connected clusters and the Kubernetes resources running on them. EKS Connector is open-source. Visit the EKS documentation for more details. *Google Kubernetes Engine and icon are trademarks of Google LLC. AWS is not affiliated with Google LLC or Google Kubernetes Engine.

Gokul Chandra Gokul is a Specialist Solutions Architect at Amazon Web Services. He assists customers in modernizing with containers helping them to use AWS container services to design scalable and secure applications. He is passionate about cloud native space and Kubernetes. Gokul's areas of interest include Containers, Microservices, Public & Private Cloud Platforms, Cloud Native for Telco, Edge Computing, Hybrid & Multi Cloud Architectures and NFV. You can find him in Medium @gokulchandrapr and Linkedin @gokulchandra.

Re Alvarez-Parmar In his role as Containers Specialist Solutions Architect at Amazon Web Services, Re advises engineering teams with modernizing and building distributed services in the cloud. Prior to joining AWS, he spent more than 15 years as Enterprise and Software Architect. He is based out of Seattle. Connect on LinkedIn at: linkedin.com/in/realvarez/

This post was co-written with Sparsh Agarwal, Senior Product Manager at Salesforce Introduction Amazon EKS Anywhere (Amazon EKS-A) takes the power of Amazon Elastic Kubernetes Service (Amazon EKS) beyond the AWS cloud and enables you to run Amazon EKS on your own infrastructure. It provides an installable software package for creating and operating Kubernetes clusters on-premises and automation tooling for cluster lifecycle support. Amazon EKS Anywhere can be installed on bare metal servers, VMware vSphere, CloudStack, Nutanix, AWS Snowball Edge and Docker. It allows you to leverage the benefits of EKS across hybrid environments, ensuring consistency and flexibility in your Kubernetes deployments. Amazon EKS Anywhere brings a consistent Amazon Web Services (AWS) management experience to your data center, building on the strengths of Amazon EKS Distro, an open-source distribution for Kubernetes used by Amazon EKS. MuleSoft is an AWS partner that accelerates the adoption of key cloud services while allowing customers to safely unlock the data inside legacy, on-premises, or SaaS applications. MuleSoft Anypoint Runtime Fabric is a powerful runtime environment for Mule applications, providing enhanced performance, scalability, and high availability. You typically create Mule application to perform system integrations. Mule apps are configured to run in Mule Runtime. A request to a Mule app triggers Mule to encode the request and data in a Mule Event, and to pass it to either single or multiple threads. In this post, we'll explore how to deploy Anypoint Runtime Fabric on Amazon EKS Anywhere, allowing you to run Mule applications in a Kubernetes environment across on-premises. By combining the capabilities of Anypoint Runtime Fabric and Amazon EKS Anywhere, organizations can unlock the potential of running Mule applications in Kubernetes clusters deployed on their infrastructure. This provides a unified and scalable runtime environment for Mule applications, enabling efficient resource utilization, simplified management, and seamless integration with other services in the cloud and on-premises. Throughout this post, we guide you through the process of setting up Amazon EKS Anywhere on VMware vSphere and deploying Anypoint Runtime Fabric on Amazon EKS Anywhere. By following the provided instructions and best practices, you can harness the power of MuleSoft and Amazon EKS Anywhere to build and manage resilient, high-performing Mule applications across hybrid environments. Solution overview The solution consists of setting up your VMware vSphere environment to deploy an Amazon EKS Anywhere Management cluster. This cluster will be used to deploy the workload cluster where the actual mule applications will be running. The management cluster also hosts the necessary management components for the workloads cluster such as Ingress, registry and monitoring components. In the workload cluster, the mule applications runs along with the necessary Mule runtime and runtime fabric agents.

Figure 1: MuleSoft Runtime Anytime Fabric Architecture on EKS-Anywhere Prerequisites

Administrative machine with machine requirements as listed in the EKS-A product page with the following tools installed:

AWS Command Line Interface (AWS CLI) version 2.10 or higher Helm eksctl eks-anywhere (v0.14.0 or later) kubectl rtfctl

VMware vSphere infrastructure with required resources as listed in EKS-A documentation Prepare vSphere for EKS Anywhere Amazon EKS-A Curated Packages are only available to customers with the Amazon EKS-A Enterprise Subscription. To request a free trial, use the link MuleSoft Anypoint Runtime Fabric License. To request a license, connect with your MuleSoft Account representative through this contact form Create a vSphere Cluster on VMware

Create an initial cluster on your vSphere environment that will be used as management cluster (to create and manage other clusters). Create one or more workload clusters from the management cluster.

Walkthrough Deploying MuleSoft Anypoint Runtime Fabric Execute all commands mentioned in this post on the Administrative Machine. Validate that your Kubernetes environment is ready for installation. Activation data can be obtained from the MuleSoft activation documentation page:

You can then apply your Mule license key to this installation:

kubectl CLI is used to access the Amazon EKS-A cluster and verifies MuleSoft deployment. For example, you can use the following command to verify MuleSoft rtf namespace and verify the deployed pods and their state:

kubectl get pods -A NAMESPACE NAME READY STATUS capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-7d6b5746b6-jw7bb 1/1 Running capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-574cbcd9d7-jzswd 1/1 Running capi-system capi-controller-manager-5b64bc4449-mcw5c 1/1 Running capv-system capv-controller-manager-64c4954b7c-85r25 1/1 Running cert-manager cert-manager-7568b959dc-6mtnc 1/1 Running eksa-packages cron-ecr-renew-27782220-lh8b8 0/1 Completed eksa-packages cron-ecr-renew-27782520-z84jw 0/1 Completed eksa-packages cron-ecr-renew-27782820-29g47 0/1 Completed eksa-packages eks-anywhere-packages-5d6df98848-mrmbm 1/1 Running eksa-packages eksa-auth-refresher-kxnds 0/1 Completed eksa-system eksa-controller-manager-6d5f44755c-m9bc9 1/1 Running kube-system vsphere-cloud-controller-manager-d7r5k 1/1 Running kube-system vsphere-cloud-controller-manager-fdm7m 1/1 Running kube-system vsphere-csi-node-6v497 3/3 Running rtf agent-5488fbd6db-m9zj8 2/2 Running rtf agent-upgrade-jqzs5 0/1 Completed rtf cluster-status-27783040-dlq8r 0/1 Completed rtf initial-cluster-status-64k77 0/1 Completed rtf mule-clusterip-service-66b4cfb785-jf6d7 1/1 Running rtf resource-cache-7d86dd7996-tk5ql 2/2 Running rtf rtf-install-job-m7l7z 0/1 Completed Apache Configuration

Figure 2: MuleSoft Anypoint Platform Runtime Manager console Deploy a sample Hello world Mule application and expose it with an load balancer We'll use MetalLB as the external LB in this case. Step 1: Select a range of underlay IP's that will be used for the LoadBalancer. In the setup below the underlay IP Classless Inter-Domain Routing(CIDR) is 10.2.16.0/24:

kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.2.16.14 Ready 50d v1.23.7-eks-7709a84 10.2.16.14 10.2.16.14 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket 10.2.16.15 Ready control-plane,master 11d v1.23.7-eks-7709a84 10.2.16.15 10.2.16.15 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket 10.2.32.21 Ready control-plane,master 11d v1.23.7-eks-7709a84 10.2.32.21 10.2.32.21 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket 10.2.64.29 Ready 50d v1.23.7-eks-7709a84 10.2.64.29 10.2.64.29 Bottlerocket OS 1.9.0 (vmware-k8s-1.23) 5.10.130 containerd://1.6.6+bottlerocket Apache Configuration

Step 2: Create a package configuration file for MetalLB. From Step 1, we see that the address range for four IP's 10.2.16.30-10.2.16.33 are not used by any other service. In the following code snippet, we use this IP range for the MetalLB Load Balancer. This IP range should not be used by any other service.

cat metallb.yaml apiVersion: packages.eks.amazonaws.com/v1alpha1 kind: Package metadata: creationTimestamp: null name: generated-metallb namespace: eksa-packages spec: packageName: metallb config: | IPAddressPools: - name: default addresses: - 10.2.16.30-10.2.16.33 L2Advertisements: - IPAddressPools: - default EOF Apache Configuration

Step 3: Install the EKS-A Package using EKS Anywhere CLI (don't use kubectl and always use sudo as docker.socket permissions, which needs sudo in this environment). Before the package creation create a namespace for MetalLb controllers.

sudo eksctl anywhere create packages -f metallb.yaml —kubeconfig ~/.kube/config ----------------------- The Amazon EKS Anywhere Curated Packages are only available to customers with the Amazon EKS Anywhere Enterprise Subscription ----------------------- package.packages.eks.amazonaws.com/generated-metallb created Apache Configuration

Step 4: Check if all components of MetalLB are in running state.

kubectl get all -n metallb-system NAME READY STATUS RESTARTS AGE pod/generated-metallb-controller-5dbdbdfff9-wb66j 1/1 Running 0 105s pod/generated-metallb-speaker-9bnv4 1/1 Running 0 105s pod/generated-metallb-speaker-cpstb 1/1 Running 0 105s pod/generated-metallb-speaker-kpt2m 1/1 Running 0 105s pod/generated-metallb-speaker-stjdf 1/1 Running 0 105s

NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/generated-metallb-speaker 4 4 4 4 4 kubernetes.io/os=linux 105s

NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/generated-metallb-controller 1/1 1 1 105s

NAME DESIRED CURRENT READY AGE replicaset.apps/generated-metallb-controller-5dbdbdfff9 1 1 1 105s Apache Configuration

Step 5: Install Nginx Ingress controller (there is enterprise version and community versions available – in this installing the community version).

helm upgrade --install ingress-nginx ingress-nginx --repo --namespace ingress-nginx --create-namespaceApache Configuration

Step 6: Verify that the ingress-controller service procured an external-ip (in this case its provided by MetalLB from the range provided). Once available, the curl should provide a reply as shown below.

kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.111.28.84 10.2.16.30 80:31385/TCP,443:32224/TCP 74s ingress-nginx-controller-admission ClusterIP 10.109.26.230 443/TCP 74s curl 10.2.16.30 404 Not Found 404 Not Found nginx Apache Configuration

Step 7: Create an Ingress configuration manifest. Below is the sample hello world mule application that can be accessed with hostname www.helloworld.com and path /helloWorld (www.helloworld.com/helloWorld).

cat helloworld-sample-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: helloworld-ingress namespace: rtf spec: ingressClassName: nginx rules: - host: www.helloworld.com http: paths: - pathType: Prefix backend: service: name: helloworld port: number: 8081 path: /helloWorld EOF kubectl apply -f helloworld-sample-ingress.yaml kubectl get ing -A NAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE default demo-localhost nginx demo.localdev.me 10.2.16.30 80 33h f1039f97-fc61-4966-b65c-a0b96be70e54 helloworld-ingress nginx www.helloworld.com 10.2.16.30 80 7m53s f1039f97-fc61-4966-b65c-a0b96be70e54 helloworld-rtf-ingress-template nginx rtf.example.com 10.2.16.30 80 33h Apache Configuration

Note: Copy the IP address nginx ingress for the next step. Step 8: In order to test the hello world application, add the static IP 10.2.16.30 to /etc/hosts file to resolve www.helloworld.com. Usually in general production setting, this IP will be added to customers IPAM like InfoBlox, etc. In this case, we set this static IP to the LoadBalancer service IP of the ingress-controller service.

Step 9: Now you should be able to access the service with the path configured and the specific host in the specification.

Operational support: There are no upfront commitments or fees to use Amazon EKS Anywhere. Customers can optionally purchase Amazon EKS Anywhere Enterprise Subscriptions for access to EKS-A Curated Packages as well as 24/7 support from AWS for all bundled tooling. Version support: Refer to the official Amazon EKS Anywhere and Kubernetes version support policy page. Pricing: Amazon EKS Anywhere Enterprise Subscription option available with EKS-A, which is required to get support for the EKS-A clusters and access to additional paid features such as Amazon EKS Anywhere Curated Packages.

Cleaning up To delete the resources provisioned in the blog, please execute the following commands. Kubectl delete ns metallb-system Kubectl delete ns ingress-nginx Kubectl delete ns rtfBash Please make sure to delete the EKS Anywhere cluster using the steps mentioned in the link. Conclusion In this post, we showed you how to deploy MuleSoft Anypoint Runtime Fabric on Amazon EKS Anywhere to bring the power and flexibility of MuleSoft's runtime environment to on-premises and edge locations. This combination allows organizations to build and manage scalable, resilient, and high-performing Mule applications across AWS cloud and on-premises. By following the step-by-step instructions in this blog post, you can start leveraging the capabilities of MuleSoft Anypoint Runtime Fabric on Amazon EKS Anywhere. To configure Mulesoft Anypoint Runtime Fabric on Amazon EKS, you can follow the quickstart link to deploy using CloudFormation For more information on getting started with Amazon EKS Anywhere, check out the EKS-A workshop, EKS-A documentation, or frequently asked questions. Check out the EKS-A GitHub repository and join the community Slack channel in the Kubernetes workspace to contribute.

Sparsh Agarwal, Salesforce Sparsh Agarwal is a senior product manager at Salesforce focused on transforming runtime fabric into the K8s platform used for scaling Salesforce automations. Before Salesforce, he drove data platforms at Microsoft from infancy to an industry leader and extended machine learning products to the Edge platform.

Kranthi Pullagurla Kranthi Pullagurla has over 20+ years' experience across Application Integration and Cloud Migrations across Multiple Cloud providers. He works with AWS Partners to build solutions on AWS that our joint customers can use. Prior to joining AWS, Kranthi was a strategic advisor at MuleSoft (now Salesforce). Kranthi has experience advising C-level customer executives on their digital transformation journey in the cloud.

Vikram Venkataraman Vikram Venkataraman is a Principal Solution Architect at Amazon Web Services and also a container enthusiast. He helps organization with best practices for running workloads on AWS. In his spare time, he loves to play with his two kids and follows Cricket.

Modernize Mainframe Applications for Hybrid Cloud with IBM and AWS

Visit the AWS IBM Services Partnership page Engage through the IBM Z and the Cloud Modernization Center

IBM – AWS Partner Spotlight IBM is an AWS Premier Tier Services Partner and MSP that offers consulting services and software solutions. IBM has assembled an ecosystem of highly experienced professionals dedicated to deploying customer solutions on AWS. Contact IBM | Partner Overview *Already worked with IBM? Rate the Partner *To review an AWS Partner, you must be a customer that has worked with them directly on a project.

Introduction: Signed certificates are digital passports, authenticating system identities, and ensuring secure data transmission. As organizations expand their SAP landscapes across cloud environments and integrate with third-party applications, properly implemented certificates become essential in preventing security breaches and unauthorized access, while maintaining smooth operation of business-critical systems. For many SAP administrators and security professionals, managing certificates might seem like yet another complex task in an already demanding environment. This is where AWS Certificate Manager (ACM) comes into play. AWS Certificate Manager is a service that is used to provision, manage, and deploy public and private SSL/TLS certificates. You can then use these certificates to terminate traffic on any compute workload that requires a certificate, such as those running on EC2 instances, like SAP workloads. With the announcement of publicly exportable certificates via ACM, customers can now leverage ACM to create a Personal Security Environment (PSE) for their SAP systems. For details regarding the launch of publicly exportable certificates via ACM, check out the blog post AWS Certificate Manager introduces exportable public SSL/TLS certificates to use anywhere from my colleague Channy Yun. Prerequisites: To use the certificates from ACM in SAP workloads, please see the prerequisites below:

A method to validate domain ownership for the domain in which you are requesting the certificate An exported public certificate requested and validated in ACM with the proper tags where applicable OpenSSL installed in your operating system or access to AWS Cloudshell

Note: this blog post does not cover how to request the certificate, as this has already been covered in the launch blog mentioned earlier Generating the .p12 file: Once you have exported the certificate from ACM, you will have three files:

With those 3 files saved in a folder, open your terminal of choice and execute the below OpenSSL commands from said folder First, export the private key: # openssl pkcs8 -topk8 -in private_key.txt -out private_key.pem -nocrypt Note: Ensure you have the pass phrase for the private_key.txt file generated when exporting the certificate from ACM. Then, confirm the certificate matches the exported key: # openssl x509 -noout -modulus -in certificate.txt | openssl md5 # openssl rsa -noout -modulus -in private_key.pem | openssl md5 Now let's build out the PKCS#12 file: # openssl pkcs12 -export \ # -inkey private_key.pem \ # -in certificate.txt \ # -certfile certificate_chain.txt \ # -name "CN= " \ # -out sap_ssl.p12 Note: Take note of the export password as it will be required when importing the .p12 file into SAP. Converting the .p12 into an SAP PSE: If you are working with an SAP ABAP based system, you can perform this task by logging into the system and entering transaction STRUST. From there, set to change mode and select PSE -> Import:

Provide the .p12 file and the export password you defined. Note: If you are missing a certificate from the chain (i.e. Starfield Services Root Certificate Authority" you can download the certificate from the Amazon trust repository. You can now save the PSE according to your requirements (for example, as the SAPSSLS.pse). This will replace your current PSE. If a PSE already exists in the system, ensure you have backed it up (if required).

If you are building the PSE in a non-ABAP environment (such as SAP HANA or an SAP Web Dispatcher), you can use the below steps to generate the SAP PSE In the same folder where you generated the PKCS#12 file, execute the below command: # openssl pkcs12 -export -out certificate.pfx -inkey private_key.txt -in certificate.txt -certfile certificate_chain.txt Note: You will be asked for the pass phrase for the private_key.txt file and the export password for the PKCS#12 file Now copy the certificate.pfx along with the root and certificate chain files into your SAP server and execute the following command: # sapgenpse import_p12 -r certificate_chain.txt -r root.crt -p certificate.pfx Note: You will be asked for the PKCS#12 encryption password and to set a passphrase for the SAP PSE Since the PSE is protected with credentials, you will need to generate the cred_v2 file by executing the below command with the adm user: # sapgenpse seclogin -p [ -O ] [ -x ] For more details on PSE credentials, refer to SAP Note # 1525059 – Troubleshooting problems with PSE credentials (SAP S-User required) Pricing: For details and up-to-date information on pricing for public certificates in ACM, refer to AWS Certificate Manager pricing Conclusion: AWS Certificate Manager (ACM) provides a solution for managing certificates in SAP environments. With the ability to export public certificates, organizations can now easily create Personal Security Environments (PSE) for both ABAP and non-ABAP based SAP systems. This integration simplifies certificate management, enhances security, and maintains compliance requirements for SAP landscapes. By following the steps outlined in this blog post, SAP administrators can implement secure certificate management using ACM, ensuring their SAP systems remain protected while reducing the operational overhead of certificate management. Customers can further leverage tagging to have a centralized view of their spending in certificate management. Want to know more about running your SAP landscapes on AWS? Visit our SAP on AWS page.

Migrate centralized crypto exchange workloads to AWS – Part 2

This is the English version of the original post published in the AWS Indonesia blog channel. Part 1 of our series introduced key components for running centralized cryptocurrency exchanges (CEX) on AWS. Part 2 (this post) builds upon that knowledge and guides you through migrating CEX workloads to AWS successfully. In the upcoming Part 3 we will explore how recommendations from Part 1 and Part 2 were used in real migration scenario. Our guidance comes from real-world migration projects of Indonesian cryptocurrency exchanges. We focus on practical lessons learned from actual migrations that were built upon more generic prescriptive guidance for large migrations. These insights can help both crypto exchanges and high-performance trading platforms. In this post, we discuss how to plan your AWS migration strategically, execute it securely, and validate its success through testing. We share best practices for Multi-AZ deployments, auto scaling configurations, and database management. We also show you how to perform a smooth cutover and optimize your post-migration environment. Design and plan your move to AWS Moving your crypto exchange to AWS starts with good planning. Before you begin the move, you must understand your current system and decide how to set it up in AWS. This section shows you the key steps to plan your move, from mapping your system design to setting up AWS accounts and checking service limits. Document your workload architecture design before migrating it to AWS Migration to AWS starts with knowing your current crypto exchange setup. Before moving, you must plan how your system will handle high availability, performance scaling, network speed, and multi-cloud operations. By mapping out these key areas now, you can build a strong AWS system that grows with your needs. For guidance, refer to the reference architecture in Part 1 of this series. In the following sections, we discuss the key design decisions to make on this stage. Design for high availability Your crypto exchange must maintain continuous operation. This means deploying your services across multiple Availability Zones. Set up your databases with automatic failover capabilities to prevent downtime. Distribute your application servers across different zones to support service continuity. Remember that Amazon Simple Storage Service (Amazon S3) automatically stores data across multiple zones for built-in redundancy. For more information, refer to the following resources:

Multi-AZ DB cluster deployments for Amazon RDS Tutorial: Set up a scaled and load-balanced application Amazon ECS capacity and availability Deploying Applications with High Availability on Kubernetes Amazon S3 backups

Plan your scaling for performance Crypto exchanges serve massive number of users and process large transaction volumes. Your scaling strategy should include Amazon EC2 Auto Scaling or Kubernetes Horizontal Pod Autoscaler with Amazon Elastic Kubernetes Service (Amazon EKS) for applications to handle varying loads. Consider using a combination of Amazon Elastic Compute Cloud (Amazon EC2) On-Demand and Spot Instances to manage costs effectively. For databases, solutions like Amazon Aurora Serverless can help handle unpredictable transaction patterns. For more information, refer to the following resources:

Auto Scaling launch templates Creating resources for Amazon ECS cluster auto scaling using the AWS Management Console Scale pod deployments with Horizontal Pod Autoscaler Getting Started with Karpenter

Minimize latency impact in multi-cloud migrations For crypto exchange workloads, latency is a critical performance factor that can be compromised in multi-cloud environments. When applications run on AWS while databases remain with other cloud providers, the increased latency can significantly impact performance. To minimize this impact in microservices architectures, prioritize migrating related services and their databases as complete units to AWS, limiting cross-cloud communication to only non-essential service interactions. Optimize network performance with cluster placement groups For processing hot paths in trading systems where every microsecond matters, network latency between instances can significantly impact performance. Amazon EC2 cluster placement groups help optimize these critical paths by placing EC2 instances close to each other in the same Availability Zone, reducing the physical network distance between components. By implementing cluster placement groups strategically along processing hot paths, trading systems can achieve more consistent and lower latency network performance, which is crucial for competitive market making and exchange operations. For more information, refer to Optimize tick-to-trade latency for digital assets exchanges and trading platforms on AWS. Establish foundation for your AWS accounts Before beginning your migration, it's important to establish a well-structured AWS organizational framework. AWS Organizations and AWS Control Tower serve as the foundation for managing multiple AWS accounts effectively. Your organizational structure should reflect your business needs, with separate accounts for different purposes such as business units, development environments, or specific workloads. Configure a dedicated management account and use a root user only for tasks that truly require it. AWS Control Tower plays an important role in standardizing account creation and management, making sure accounts adhere to your regulatory requirements and security standards. It's important to set up your management accounts several days before migration begins, because they require manual configuration, including payment methods to prevent operational disruptions. Security and compliance form the basis of your AWS organization. Service Control Policies (SCPs) help you maintain control by setting maximum permissions across your organization, so you can restrict actions like resource deletion or limit EC2 instance types. Backup policies provide consistent data backup across all accounts, while maintaining compliance with retention requirements and disaster recovery standards. Threat detection and security monitoring are essential components of your AWS environment. Amazon GuardDuty provides real-time threat detection using machine learning to identify potential cyber attacks and malware. AWS Security Hub offers a centralized dashboard for security monitoring, collecting security checks, and providing recommendations based on best practices. These services should be enabled organization-wide and monitored from designated administrator accounts. Activity tracking and auditing capabilities are fundamental for maintaining security and compliance. AWS CloudTrail records API activity in your AWS environment, providing audit trails for security investigations and operational troubleshooting. By centralizing these logs through a designated administration account, you maintain complete visibility across your organization. The organizational structure of your accounts on AWS creates a secure, manageable foundation for your crypto exchange operations on AWS, providing proper governance while maintaining operational efficiency. Increase service quotas AWS services come with default quotas that require careful review before you start your migration journey. These quotas represent the maximum limits for various service resources and API operations in your AWS account. You can quickly check both default and current quotas through the Service Quotas dashboard on the AWS Management Console. Quota management requires advance planning and proactive action. Request quota increases well before your migration project starts, because approval processes can take from minutes to days, with larger quota requests taking longer. Waiting until the last minute to request quota increases can cause significant project delays—even a single day's delay in quota approval can impact your entire migration timeline. This is particularly important for crypto exchanges, where service capacity directly affects trading operations.By understanding and managing your service quotas early in the planning phase, you can achieve smooth resource provisioning during migration and avoid unexpected constraints that could affect your project timeline or operational capabilities. Plan migration implementation details Good planning helps make your AWS migration successful. A clear plan helps your team understand what work needs to be done, when to do it, and how long it will take. This makes it straightforward to track progress and make sure nothing important is missed.The following are the main technical decisions to make:

Choose how to move your servers—either set them up fresh or use AWS Application Migration Service Pick a way to move your database—either export/import or use AWS Database Migration Service (AWS DMS) If changing database type, check if the AWS Schema Conversion Tool (AWS SCT) can help Choose how to move files—either download/upload or use AWS DataSync If you are using Kubernetes, check what changes your Kubernetes template files need Update your continuous integration and continuous delivery (CI/CD) pipeline system to deploy on the right AWS accounts according to the organizational structure you designed earlier Look for code that needs updates to work with AWS tools Convert infrastructure code from previous cloud providers to options compatible with AWS

Some of these activities can be done in parallel, whereas others must be done one after another. Using project management tools that your team knows well can help keep everyone informed about the progress. A clear and detailed plan can help your team work better together during the migration. Test your migration plan with proof of concept Before starting your full migration, it's helpful to run small tests to check if your plan will work. The following are the main areas to test:

Virtual machine migration – Test if AWS Application Migration Service can move your servers to AWS correctly. Even though AWS supports many operating systems, you might face issues with SSH access or server identity changes. Finding these problems early helps you fix them before the real migration starts. If your Linux-based EC2 instance becomes inaccessible, use the EC2Rescue tool to remediate common problems. Database migration – Try moving a small part of your database with AWS DMS. This helps you check if you need to make any changes in your source database before the migration. It also validates that data moves correctly between different database versions. During the proof of concept, the pre-migration assessments in AWS DMS tasks can help identify problems that might prevent a migration task from running as expected. If during migration you are changing database engines, test schema conversion with the AWS SCT. Transaction hot path testing – Check if your transaction processing path keeps latency low after moving to AWS. Test how orders flow from inbound APIs, to matching and performing trade, to delivering updated market data to the end-users. Check if transactions and market data move quickly enough. Testing early helps find slow spots before you move everything. Multi-cloud performance – Check if your applications work well when some parts are on AWS while others are still on your old infrastructure. This is important because network delays between different clouds can affect how fast your crypto exchange works. Code changes – Test how your application code works with AWS services. For example, check if your storage management code works with Amazon S3. Also test if your CI/CD pipeline and infrastructure code work properly with AWS. Consider using Amazon Q Developer to help migrate, test, and review your code. AWS Support Plan – Remember to upgrade your AWS Support Plan to at least Business level. This gives you quick access to AWS support teams if you need help solving problems during testing.

This early testing helps you find and fix problems before they affect your real migration. It also helps your team learn how AWS services work in practice. Review third-party licenses for AWS migration When moving to AWS, you must check your third-party software licenses carefully. Many software vendors tie their licenses to specific cloud providers, which means your current licenses might not work after moving to AWS. To avoid problems during migration, contact your software vendors early to understand their AWS licensing requirements and make any needed license changes or purchases before starting the migration process. Migration best practices when moving to AWS Moving a crypto exchange to AWS requires the following proven practices to achieve a successful transition. These best practices, drawn from real implementations with Indonesian crypto exchanges, help you protect sensitive data, maintain performance, and minimize disruption during migration.

Daily progress monitoring – Have short daily meetings with your team to check progress and solve any problems quickly. This helps keep your migration on track and lets you fix issues before they become bigger problems. Secure data transfer – Crypto exchanges handle sensitive data that needs protection. Use AWS Site-to-Site VPN when moving your data to AWS. This creates a secure connection whether you're using the internet or AWS Direct Connect. AWS migration tools like AWS Application Migration Service, AWS DMS, and Data Sync work well with VPNs. AWS migration tools – Make your migration faster by using AWS tools designed for this purpose. Move your virtual machines with AWS Application Migration Service, transfer databases with AWS DMS, and sync your files with Data Sync. For container workloads on Kubernetes, you can use your existing CI/CD pipeline to deploy to Amazon EKS. Database setup – AWS DMS doesn't automatically move everything. You must manually sync database items like functions, stored procedures, and indexes. This step is important for both application features and performance. Getting help – If you face technical problems you can't solve, contact AWS Support. AWS Support can help with migration issues, especially when dealing with unusual problems not covered in AWS documentation.

Following these best practices helps reduce risks and facilitates a smooth transition of your crypto exchange to AWS. By focusing on security, using the right tools, and maintaining proper oversight, you can achieve a successful migration while keeping your trading platform reliable and performant. Testing after your move to AWS After you complete the migration process, good testing helps check that your crypto exchange works well on AWS. Testing shows if your system works correctly and runs as fast as before. Your testing plan should include unit testing, functional testing, end-to-end integration testing, load testing, and performance testing. After you validate the system works correctly, run failover tests for critical components of your system, such as switching to hot standby applications and database point-in-time recovery procedures. Setting up test environments with the right data is important for good testing. AWS Application Migration Service helps by offering test instance features before the final cutover. For database testing, Amazon Aurora or Amazon Relational Database Service (Amazon RDS) blue/green deployment lets you create separate test instances while keeping production data intact and up-to-date. Container workloads on Amazon EKS can be tested using your current CI/CD pipeline in separate environments. For storage testing, you can create new S3 buckets and copy files from production. Using tools your team knows well makes testing less complicated and more consistent.Writing down test information helps track your progress. Keep records of what you test and what you find. Fix problems you find before moving to the next step. Testing with real-life data amounts and users shows if your system can handle actual work demands.This complete testing approach helps make sure your move to AWS goes well. Your crypto exchange needs to work reliably and quickly within the defined latency bounds. Testing helps make sure everything works right before going live. Final cutover after your move to AWS After testing is done and you pick a date to go live, it's time to switch your crypto exchange to AWS. This cutover requires careful planning to keep your service running smoothly. These are the key steps to follow:

Make sure your old and new databases match perfectly. Use Amazon CloudWatch to check AWS DMS metrics and verify the latest data is in place. Compare files between old and new storage systems. Use CloudWatch to check DataSync metrics and count files to make sure they match. For virtual machines using AWS Application Migration Service, monitor respective CloudWatch metrics to make sure all data is copied before switching. Point your CI/CD pipeline to AWS instead of your old provider. Update DNS records and system settings to point to AWS. This includes load balancer addresses and database connection details. Have a backup plan, and keep your old system ready in case you need to switch back quickly. Set up your applications and servers on AWS before the switch. This leaves fewer tasks for the actual cutover day.

The cutover to AWS works best when you prepare well. Check each step carefully as you go. Having a backup plan helps you react quickly if something unexpected happens. Remember that good planning makes the cutover easier and safer for your crypto exchange.This careful approach helps protect your trading service during the move to AWS. Your users should see little or no downtime during the cutover. Keep your team ready and focused until the cutover is complete. Next steps after your move to AWS Moving to AWS is just the start. Now you need to make sure your crypto exchange keeps running well. Good monitoring and post-migration cleanup help your system stay healthy and cost-effective. The following are the main tasks to handle:

Set up CloudWatch to watch your system and send alerts if problems happen. Remove old systems and accounts from your previous provider that you don't use anymore. Clean up migration tools like AWS DMS, AWS Application Migration Service, and DataSync when you're done with them. Update your system documents with new AWS information about setup and structure. Monitor your AWS costs and look for ways to save money. Start an AWS Support Business Plan if you haven't, to get help quickly if you need it.

Taking care of these tasks helps your crypto exchange run better on AWS. Thorough and continuous monitoring lets you spot and fix problems early. Removing old systems saves money and keeps things simple. Having current documents helps your team work better.Remember to check your system regularly after the move. Watch how it runs and look for ways to make it better. We discussed many AWS tools that help you keep your crypto exchange running smoothly. Your team should know how to use these tools and when to ask AWS for help.Keeping your system healthy on AWS takes ongoing work. But with good monitoring and regular updates, your crypto exchange can run well and grow over time. Stay focused on both daily operations and long-term improvements to get the most from AWS. Conclusion Moving a crypto exchange to AWS takes careful planning and work. In this post, we discussed how to do it safely and successfully. We shared what we learned from helping real exchanges move to AWS.Good planning makes a big difference in your move to AWS. Start by designing your system well and setting up AWS accounts the right way. Check AWS limits early and test your plans before making big changes. Use AWS tools to help move your data safely. Take time to test all parts of your system before going live.AWS gives crypto exchanges a strong base to grow and try new things. The tools and features AWS help you run a safe and fast trading system. As crypto trading keeps changing, AWS helps you change with it. In upcoming Part 3 of this series, we will provide real examples from Indonesian crypto exchanges. We show how they handled challenges and made AWS work for them. To learn more about running your crypto exchange on AWS, talk to your AWS team.

Agung Sidharta Agung is a Startups Solutions Architect who loves to work with customers solving their problems. In his spare time, he enjoys traveling, reading IT-related materials, and walking in the surrounding environment with his family and little dog.

Angga Pradikta Angga is a Solutions Architect at AWS. Angga helps customers navigate their cloud journey by designing and implementing secure, scalable, and cost-effective solutions.

Nikolay Vlasov Nikolay is the global lead for Distributed Ledger Technology infrastructure at AWS Worldwide Specialist Solutions Architect organization. He helps customers run workloads for decentralized web and ledger technologies on AWS.

David-Paul Dornseifer David is a Blockchain Development Architect at AWS. He focuses on helping customers design, develop and scale end-to-end blockchain solutions. He primarily focusses on confidential compute and digital asset custody related solutions as well as latency optimization for critical CEX workloads.

Migrate centralized crypto exchange workloads to AWS – Part 1

This is the English version of the original post published in the AWS Indonesia blog. Cryptocurrency exchanges are digital marketplaces where users trade various digital currencies like Bitcoin and Ethereum. The exchanges provide a secure environment where millions of people worldwide can participate in the cryptocurrency economy.The cryptocurrency trading landscape features two distinct types of exchanges: centralized (CEXs) and decentralized (DEXs). Centralized exchanges operate like traditional financial institutions, with companies managing user transactions and maintaining control over digital assets. In contrast, decentralized exchanges function through blockchain networks, without intermediaries and giving traders direct control over their funds. Indonesia has emerged as a significant player in the global cryptocurrency market, ranking third in worldwide crypto adoption in 2024. The country hosts 30 officially registered centralized cryptocurrency exchanges, demonstrating the robust growth of digital asset trading in the region. In 2024, AWS has gained recognition as one of two cloud providers recommended by Indonesia's financial authority, PT Central Finansial X (CFX), for cryptocurrency exchange operations. This three-part series explores the world of centralized cryptocurrency exchanges and their implementation on AWS Cloud infrastructure. We share practical insights and technical guidance for building reliable, secure, and scalable cryptocurrency trading platforms. Part 1 (this post) examines the business model of centralized exchanges and presents reference architectures based on AWS services. Part 2 focuses on migration strategies and lessons learned, and upcoming Part 3 showcases real-world implementations from Indonesian cryptocurrency exchanges. The series aims to help both new and established exchanges understand the benefits of cloud infrastructure in the digital asset trading ecosystem. Crypto exchange business blueprint Let's first review key components of the CEX business blueprint, as illustrated in the following diagram.

1. Trade engine At the center of cryptocurrency exchanges lies the trade engine. These engines work as stateless microservices that process and match trading orders. They communicate through specialized messaging systems for order management and trade matching. When users place orders, the trading engine processes each request and matches buyers with sellers at the best available prices. The system stores all trading information in specialized services called trade feeds and order books. Crypto exchanges also use blockchain nodes and wallets to settle and store users' crypto assets, acting as custodians. They can build their own blockchain infrastructure or use specialized services. Modern trading engines integrate with blockchain technology to make asset trading faster and more efficient. This integration allows for quick order processing while maintaining secure asset management on the blockchain network. 2. Trade feeds and order books The trade engine maintains the incoming stream of trading intentions in the order books service. These books maintain all open buy orders (bids) and sell orders (asks), along with their respective prices and quantities. It's similar to a marketplace bulletin board where people actively post their intentions to buy or sell at various price points, before any actual transactions occur.Trade feeds, on the other hand, function as historical records of completed transactions, essentially documenting what has already occurred in the market. They record the final execution price of trades, the volume traded, the exact timestamp of execution, and whether it was a buy or sell transaction. You can think of trade feeds like a receipt book or transaction history—they provide a complete record of all finalized deals that have taken place on the exchange. 3. Market data distribution Real-time market data plays a crucial role in keeping trading systems accurate and efficient. External providers supply current price information to trading engines, making sure transactions occur at fair market rates. This continuous flow of data helps maintain market integrity and supports informed trading decisions. 4. Analytics and user experience The trading system collects data that helps detect unusual trading patterns and supports detailed business analysis and regulatory reporting. The collected data also enables exchanges to provide better services to users, including personalized portfolio management and improved trading experiences. 5. Liquidity providers and market makers Professional market makers and liquidity providers play essential roles in maintaining healthy trading environments. These partners help create stable markets by consistently offering trading pairs and managing price fluctuations. The entire system operates under strict security measures, protecting user assets during all trading stages and maintaining careful monitoring of all transactions. 6. Security and monitoring Security remains the top priority in cryptocurrency exchange operations. Exchanges implement comprehensive security measures throughout their systems to protect users' assets through all key stages: onboarding, trading, and off-boarding. Regular monitoring occurs at both technical and business levels, with special attention to transaction safety and trading activity oversight. Implementing the CEX blueprint on AWS The following diagram illustrates how this business blueprint can be designed with AWS services.

1. At trade The trading platform consists of four main modules: UI, API layer with core services, blockchain layer, and wallet layer. The frontend interface uses AWS Amplify for development and operation, with Amazon Route 53 handling domain names and Amazon CloudFront distributing content globally for fast access. Trading users need stable real-time connections, so we propose using WebSocket connections managed by a Network Load Balancer and Amazon EC2 Auto Scaling to maintain service during usage spikes. For core trading services, we suggest using AWS Fargate with either Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS) to run and scale containerized applications. The blockchain nodes will operate on Amazon Elastic Compute Cloud (Amazon EC2) instances using both regular x86 and cost-efficient Amazon Graviton processors. For wallet security, we recommend AWS Key Management Service (AWS KMS) and AWS Nitro Enclaves to protect blockchain keys and sensitive operations. 2. Persistent data storage We propose different storage services for different types of data:

Amazon Aurora or Amazon Relational Database Service (Amazon RDS) for accurate trading records Amazon ElastiCache for storing frequently accessed pricing data Amazon DynamoDB for handling user information with quick access Amazon OpenSearch Service for advanced data searching Amazon Simple Storage Service (Amazon S3) for storing files and documents

3. Market data distribution The platform needs to share market information in real time with many users. We propose using Amazon Kinesis or Amazon Managed Streaming for Apache Kafka (Amazon MSK) to handle continuous data streams. Amazon EventBridge will connect different parts of the system by routing events between services. This design lets services work independently while maintaining communication. EventBridge includes security features like filters, access controls, and data protection to keep events safe and trackable. 4. External provider connections For connecting with outside partners like market makers and payment services, we recommend the following network solutions:

AWS Transit Gateway for connecting multiple networks centrally NAT gateways for safe internet access AWS Site-to-Site VPN for secure connections with local networks AWS Direct Connect for fast private connections to AWS

5. Pre-trade (data analysis and machine learning) For market analysis and trading support, we suggest the following services:

Amazon S3, AWS Glue, and AWS Lake Formation to organize data Amazon Redshift and Amazon Athena to analyze large amounts of data Amazon Quick Sight to create visual reports Amazon SageMaker to build and run machine learning models Amazon Bedrock to manage AI-powered applications AWS Lambda to connect these services automatically

6. Security and system monitoring The platform needs strong security and monitoring systems:

Amazon CloudWatch to track system performance through metrics and logs AWS X-Ray to help find problems in applications AWS Identity and Access Management (IAM) to control who can access different services AWS Shield to protect against denial of service (DoS) attacks AWS CloudTrail to record all system actions Amazon GuardDuty to look for security threats AWS WAF to protect web applications from attacks AWS Security Hub to help manage all security information in one place

Conclusion In Part 1 of our series, we detailed the fundamental components of centralized cryptocurrency exchanges and their implementation using AWS services. We explored how critical elements like trade engines, order books, and market data distribution can be built using AWS Cloud services. The architecture we've presented combines security measures, scalable infrastructure, and real-time data processing capabilities essential for modern crypto trading platforms. Part 2 will focus on practical migration strategies for existing exchanges moving to AWS, including technical challenges encountered during actual implementations in Indonesia's crypto market.

Enhanced search with match highlights and explanations in Amazon SageMaker

Amazon SageMaker now enhances search results in Amazon SageMaker Unified Studio with additional context that improves transparency and interpretability. Users can see which metadata fields matched their query and understand why each result appears, increasing clarity and trust in data discovery. The capability introduces inline highlighting for matched terms and an explanation panel that details where and how each match occurred across metadata fields such as name, description, glossary, and schema. Enhanced search results reduces time spent evaluating irrelevant assets by presenting match evidence directly in search results. Users can quickly validate relevance without analyzing individual assets. In this post, we demonstrate how to use enhanced search in Amazon SageMaker. Search results with context Text matches include keyword match, begins with, synonyms, and semantically related text. Enhanced search displays search result text matches in these locations:

Search result: Text matches in each search result's name, description, and glossary terms are highlighted. About this result panel: A new About this result panel is displayed to the right of the highlighted search result. The panel displays the text matches for the result item's searchable content including name, description, glossary terms, metadata, business names, and table schema. The list of unique text match values is displayed at the top of the panel for quick reference.

Data catalogs contain thousands of datasets, models, and projects. Without transparency, users can't tell why certain results appear or trust the ordering. Users need evidence for search relevance and understandability. Enhanced search with match explanations improves catalog search in four key ways: 1) transparency is increased because users can see why a result appeared and gain trust, 2) efficiency improves since highlights and explanations reduce time spent opening irrelevant assets, 3) governance is supported by showing where and how terms matched, aiding audit and compliance processes, and 4) consistency is reinforced by revealing glossary and semantic relationships, which reduces misunderstanding and improves collaboration across teams. How enhanced search works When a user enters a query, the system searches across multiple fields like name, description, glossary terms, metadata, business names and table schema. With enhanced search transparency, each search result includes the list of text matches that were the basis for including the result, including the field that contained the text match, and a portion of the field's text value before and after the text match, to provide context. The UI uses this information to display the returned text with the text match highlighted. For example, a steward searches for "revenue forecasting," and an asset is returned with the name "Sales Forecasting Dataset Q2" and a description that contains "projected sales figures." The word sales is highlighted in the name and description, in both the search result and the text matches panel, because sales is a synonym for revenue. The About this result panel also shows that forecast was matched in the schema field name sales_forecast_q2. Solution overview In this section we demonstrate how to use the enhanced search features. In this example, we will be demonstrating the use in a marketing campaign where we need user preference data. While we have multiple datasets on users, we will demonstrate how enhanced search simplifies the discovery experience. Prerequisites To test this solution you should have an Amazon SageMaker Unified Studio domain set up with a domain owner or domain unit owner privileges. You should also have an existing project to publish assets and catalog assets. For instructions to create these assets, see the Getting started guide. In this example we created a project named Data_publish and loaded data from the Amazon Redshift sample database. To ingest the sample data to SageMaker Catalog and generate business metadata, see Create an Amazon SageMaker Unified Studio data source for Amazon Redshift in the project catalog. Asset discovery with explainable search To find assets with explainable search:

Enter the search text user-data. While we get the search results in this view, we want to get further details on each of these datasets. Press enter to go to full search.

In full search, search results are returned when there are text matches based on keyword search, starts with, synonym, and semantic search. Text matches are highlighted within the searchable content that is shown for each result: in the name, description, and glossary terms.

To further enhance the discovery experience and find the right asset, you can look at the About this result panel on the right and see the other text matches, for example, in the summary, table name, data source database name, or column business name, to better understand why the result was included.

After examining the search results and text match explanations, we identified the asset named Media Audience Preferences and Engagement as the right asset for the campaign and selected it for analysis.

Conclusion Enhanced search transparency in Amazon SageMaker Unified Studio transforms data discovery by providing clear visibility into why assets appear in search results. The inline highlighting and detailed match explanations help users quickly identify relevant datasets while building trust in the data catalog. By showing exactly which metadata fields matched their queries, users spend less time evaluating irrelevant assets and more time analyzing the right data for their projects. Enhanced search is now available in AWS Regions where Amazon SageMaker is supported. To learn more about Amazon SageMaker, see the Amazon SageMaker documentation.

Ramesh H Singh Ramesh is a Senior Product Manager Technical (External Services) at AWS in Seattle, Washington, currently with the Amazon DataZone team. He is passionate about building high-performance ML/AI and analytics products that enable enterprise customers to achieve their critical goals using cutting-edge technology.

Pradeep Misra Pradeep is a Principal Analytics and Applied AI Solutions Architect at AWS. He is passionate about solving customer challenges using data, analytics, and AI/ML. Outside of work, Pradeep likes exploring new places, trying new cuisines, and playing board games with his family. He also likes doing science experiments, building LEGOs and watching anime with his daughters.

Ron Kyker Ron is a Principal Engineer with Amazon DataZone at AWS, where he helps drive innovation, solve complex problems, and set the bar for engineering excellence for his team. Outside of work, he enjoys board gaming with friends and family, movies, and wine tasting.

Rajat Mathur Rajat is a Software Development Manager at AWS, leading the Amazon DataZone and SageMaker Unified Studio engineering teams. His team designs, builds, and operates services which make it faster and straightforward for customers to catalog, discover, share, and govern data. With deep expertise in building distributed data systems at scale, Rajat plays a key role in advancing the data analytics and AI/ML capabilities of AWS.

Kyle Wong Kyle is a Software Engineer at AWS based in San Francisco, where he works on the Amazon DataZone and SageMaker Unified Studio team. His work has been primarily at the intersection of data, analytics, and artificial intelligence, and he is passionate about developing AI-powered solutions that address real-world customer challenges.

How Zepto scales to millions of orders per day using Amazon DynamoDB

Named after a microscopic unit of time representing 10⁻²¹, Zepto is an Indian quick-commerce venture launched in 2021. The company specializes in lightning-fast delivery services, bringing groceries and essential items to urban consumers in just minutes through its technology platform and optimized logistics network. Operating in multiple cities across India, Zepto has a comprehensive catalog, including fresh produce, electronics, beauty products, clothing, and toys. Since its launch in 2021, Zepto has undergone significant growth, evolving from initial operations with a single micro-warehouse to now operating more than 1,000 stores and processing millions of orders daily. Amazon Web Services (AWS) has been a partner in Zepto's growth journey, providing reliable and scalable services and infrastructure that has helped it grow. To learn more, watch the video Zepto and AWS. In this post, we describe how Zepto transformed its data infrastructure from a centralized relational database to a distributed system for select use cases. We discuss the challenges encountered with Zepto's original architecture to support the business scale, the shift towards using key-value storage for cases where eventual consistency was acceptable, and Zepto's adoption of Amazon DynamoDB—a fully managed, serverless, key-value NoSQL database that delivers single digit milli second latency at any scale. We also discuss how Zepto overcame various challenges, including roll out to production, and the lessons learned. Previous architecture at Zepto Born in the cloud, Zepto initially had architected with a simple monolithic application serving its mobile and web applications requests called Zepto Storefront that had its transactional data stored and served from Amazon Aurora PostgreSQL-Compatible Edition, a fully managed, PostgreSQL-compatible, and ACID-compliant relational database engine that combines the speed, reliability, and manageability of Amazon Aurora with the simplicity and cost-effectiveness of open source databases. It had multiple modules, like Order Management, Inventory Management, Payments Service, and more, packaged together as a single web application and deployed on a set of Amazon Elastic Compute Cloud (Amazon EC2) instances, an AWS service that provides sizable compute capacity in the cloud. We found the monolithic architecture wasn't supporting Zepto's growing scale and business demands. To address this, Zepto started decomposing the Zepto Storefront application with a set of simplified, scalable, and robust microservices that have their own purpose-built data stores, deployed on Amazon Elastic Kubernetes Service (Amazon EKS). Amazon EKS is a fully managed Kubernetes service that you can use to run Kubernetes in both the AWS Cloud and on-premises data centers. With the journey of breaking up the monolithic application halfway through, Zepto application requests were served from both the Zepto Storefront (the monolith) application and the microservices, as shown in the following architectural diagram.

The following are some of the key data stores backing Zepto's microservices and the Zepto Storefront application:

Aurora PostgreSQL-Compatible – The microservices—User Management, Payment Services, and more—and the Zepto Storefront application where the Order Management Service (OMS) and other modules exist have their transactional data stored in their own databases set up in Aurora PostgreSQL clusters. Amazon OpenSearch Service – Amazon OpenSearch Service is an AWS managed service that you can use to run and scale OpenSearch clusters without having to worry about managing, monitoring, and maintaining your infrastructure, is used by the Search Service that serves Zepto's products search and discovery use cases. MongoDB – The microservices – Catalog, Product Service and more uses MongoDB as the primary data store for storing product, and category related information.

Zepto's data lake is set up on Amazon Simple Storage Service (Amazon S3), an object storage service that offers industry-leading scalability, data availability, security, and performance. It is kept synchronized with Zepto's transactional data stores—Aurora PostgreSQL Compatible and MongoDB—with the help of streaming change data capture (CDC) through a Kafka cluster set up with Amazon Managed Streaming for Apache Kafka (Amazon MSK). Amazon MSK is a streaming data service that manages Apache Kafka infrastructure and operations, making it simple for developers and DevOps managers to run Kafka applications and Kafka Connect connectors on AWS. Multiple MSK clusters support Zepto's requirements, such as the events cluster that facilitates communication between the applications (Zepto Storefront and the microservices), the search cluster that receive batch inference data after data analytics and processing, the CDC cluster, and more. Challenges In this section, we discuss the challenges Zepto faced with its Storefront monolithic application, specifically around OMS, one of the core modules, which managed the end-to-end lifecycle of an order. Performance challenges When Zepto received orders, each order was created as a new entry in the Aurora Customer_Orders table with an initial status of ORDER_RECEIVED. Due to the high volume of daily order processing over multiple years, the table started accumulating billions of records. This resulted in the following challenges:

Expensive reads – Although most orders process successfully, some transactions can be delayed or disrupted by various factors, such as payment failures and gateway delays. To manage these cases, Zepto's automated monitoring jobs regularly scan the Customer_Orders table to detect problematic transactions and trigger appropriate responses, including listening to payment service events, triggering orders cancellation for payment failures and more. However, as the table size grew substantially, the frequent execution of these monitoring queries began to experience performance degradation. Expensive writes – When orders move through different stages from creation to fulfillment, their status needs to be updated in the Customer_Orders table, for example ORDER_RECEIVED, PAYMENT_SUCCESSFUL and, ORDER_CONFIRMED. These status updates, triggered by events from MSK topics, required locking the corresponding order records in the table for updating. As a result, multiple concurrent updates often compete for these locks, which led to increased write latency. Autovacuum during peak hours – Although autovacuum helps improve the performance by invoking VACUUM and ANALYZE commands for removing obsolete data or tuples from the tables and updating query statistics, several parameters can affect when the process gets triggered. Zepto witnessed autovacuum triggered on the Customer_Orders table during peak hours, which competed with regular database operations for system resources and eventually impacted read and write operations on the table. Zepto also witnessed autovacuum not keeping up with very frequent data modifications that caused the query planner to use outdated statistics, resulting in inefficient query plans and slower query runtimes.

Operational challenges To support Zepto's micro-warehouses operating around the clock, the team faced several operational challenges:

Autovacuum configuration complexity – The team frequently invested time optimizing various autovacuum parameters for critical large tables, like the Customer_Orders table. However, this optimization process grew more complex over time, because team had to constantly adjust settings to handle tables with high data modification rates and evolving usage patterns driven by frequent feature additions to meet business needs. Monitoring challenges – Beyond autovacuum configuration, the team needed to continuously monitor and adjust numerous database parameters to optimize the Customer_Orders table's performance. This extensive parameter tuning and maintenance work consumed significant time that could have been better spent on core business activities. Off-peak hours activities – Scheduling resource-intensive operations on the Customer_Orders table, such as creating and dropping indexes, had to be performed during off-peak hours.

Data re-architecture Zepto began exploring methods to restructure its OMS to resolve the challenges related to performance, scalability, and operations through the microservices architecture. Because the OMS can't be functionally broken down into microservices all in one stage due to other legacy system dependencies, we came up with a hybrid approach: maintaining strongly relational use cases in Aurora PostgreSQL-Compatible while moving non-relational use cases to key-value stores like DynamoDB, which provides consistent performance regardless of scale. This initiative resulted in the creation of a new microservice called Draft-Order service, which handles orders (both successful and unsuccessful) before passing the confirmed orders for which the payment went through to the OMS. This lightweight Draft-Order service is aimed to accept orders regardless of their payment status and provide logical separation between payment-related order status updates from the order fulfillment process. The following diagram represents the order management workflow.

Zepto's teams have extensive experience with Aurora PostgreSQL-Compatible and its broad application versatility within their applications, but we chose DynamoDB for two key reasons:

DynamoDB delivers consistent single-digit millisecond performance at any scale DynamoDB's serverless nature brings in operational excellence like no provisioning servers, no version upgrades, no maintenance windows, no patching, no downtime maintenance and more Cost analysis based on Zepto's throughput requirements indicated DynamoDB to be more cost effective.

With those insights, we started designing the Draft-Order service with a DynamoDB table to manage the orders through the following workflow:

When a new order is received, an entry is added to the DynamoDB table The service monitors order-related events from payment services from the events cluster and updates the order status accordingly, such as PAYMENT_SUCCESSFUL or PAYMENT_UNSUCCESSFUL When the order reaches the successful payment completion status, the service

calls an idempotent OMS Create Order API that creates an entry in the Aurora PostgreSQL Customer_Orders table for fulfillment and other processes marks the order status as ORDER_CONFIRMED publishes the status to the events cluster

This way, payment-related update and read processes that previously happened on the Customer_Orders table through OMS are offloaded to the DynamoDB table through Draft-Order service. Query access patterns DynamoDB differs from relational databases because it requires upfront knowledge of query patterns to provide single-digit query performance at scale and efficient use of read/write capacity units; see First steps for modeling relational data in DynamoDB for more details. Therefore, our first step was to identify specific query access patterns:

Get order – Get the order using ORDER_ID, a unique identifier of an order used throughout as the referential key in other Aurora PostgreSQL tables Update order – Update the order item attributes like order status using ORDER_ID Get orders where payment was unsuccessful – Get orders that have remained in a specific status longer than the given time. This is particularly useful for identifying orders with extended payment processing issues or incomplete transactions beyond a timeframe, allowing for appropriate follow-up actions

DynamoDB schema modeling After we had identified the access patterns, we moved forward with the schema design in collaboration with the AWS team. The Draft_Orders table is the base table that stores the required details of the orders placed by Zepto's customers, with ORDER_PK (unique identifier of an order) as the partition key, and other attributes:

ORDER_PK – The partition key that can have one of two types of order identifiers:

ORDER_ID: A unique identifier used for internal service-to-service communication ORDER_CODE: A human-readable unique identifier

The prefix "ORDER_ID" or "ORDER_CODE" is added to the value for identification.

STATE – Order status like ORDER_RECEIVED, PAYMENT_SUCCESSFUL, ORDER_CONFIRMED and more. EXPIRE_AT – Time to live (TTL) DATA – Snapshot of the attributes of the order that will be stored in the Aurora PostgreSQL Customer_Orders table upon successful payment completion POS_PK– Pending Order Scan Partition Key, an attribute that helps in keeping orders in the global secondary index (GSI) uniformly across partitions. See Using Global Secondary Indexes in DynamoDB for more details. POS_SK – A time ordered key CREATED_ON and UPDATED_ON – Creation and update timestamp

Most of the values of keys and the attributes size are limited in size, except for the values of the DATA attribute, which vary in the range of tens of KBs, which required careful consideration while designing the schema; see Best practices for storing large items and attributes in DynamoDB for additional details. Because of this, we choose Snappy, a fast and efficient data compression and decompression technique for the DATA attribute values. See Example: Handling binary type attributes using the AWS SDK for Java document API for details on how to implement compression logic for binary type attributes. DynamoDB's Time To Live (TTL) feature helps in automatically removing items within a few days after their specified expiration timestamp. The Draft_Orders table uses the TTL feature where each order is configured with an EXPIRE_AT attribute. So, the order entries get deleted within few days of their expiration, leaving the table with the latest orders. During this window, scheduled jobs periodically check for orders that have remained beyond a typical timeframe, and they trigger appropriate actions to ensure all the orders reach either ORDER_CONFIRMED or ORDER_TERMINATED status. While the first three access patterns are well served with pointed queries on Draft_Orders table, the fourth access pattern requires retrieving orders that remained in ORDER_RECEIVED status longer than the given time. While it can be achieved by querying the base table using attributes that are not part of the primary key, it may not be efficient; hence, we created a GSI with POS_PK as the partition key, and POS_SK as the sort key. In addition, we selected only the CREATED_ON attribute in the GSI that is required for fetching the orders basis their creation time, rather than including all the base table attributes. This targeted approach helps minimize storage and write costs, since a smaller index with fewer projected attributes is more cost-efficient; see GSI Attribute projections for additional details. The following screenshot illustrates the GSI keys and projected attributes' details.

To prevent hot partition issues and for distributing the orders evenly across partitions, we use POS_PK; see Designing partition keys to distribute your workload in DynamoDB for more details. Additionally, this POS_PK helps filter and maintain only those orders that still need payment completion in the GSI. It is achieved by nullifying the POS_PK value in the base table upon both successful and unsuccessful payment by the Draft-Order service. When the POS_PK value is nullified in the base table, the corresponding entry eventually gets removed from the GSI, leaving only the orders requiring payment to be completed in the GSI. To take action on payment incomplete orders, the Draft-Order service runs scheduled jobs for every 5 minutes that query the GSI for payment pending orders. Some of them might get through with orders marked ORDER_CONFIRMED because their payments succeed with alternate payments during the process, whereas some might not, which eventually get marked as ORDER_TERMINATED after the configured number of retries. Current architecture Zepto continued to refactor its Storefront into purpose-built microservices with their own data stores. Bringing in the Draft-Order service backed by DynamoDB became a natural fit with the existing system of microservices, as shown in the following architectural diagram.

By enabling Amazon DynamoDB Streams, which captures a time-ordered sequence of item-level modifications in the Draft_Orders table, Zepto processes the CDC logs using the Apache Flink connector for DynamoDB that writes the stream data into the data lake for further processing. See DynamoDB Streams and Apache Flink for additional details. Rollout to production The new architecture with the DynamoDB table is designed to hold the last few days of orders, and hence data migration and back-filling from the Customer_Orders table have been completely skipped. With this new architecture in place, initially 10% of read and write operations were offloaded from the Aurora PostgreSQL Customer_Orders table to a DynamoDB Draft_Orders table. To maintain the uniqueness in the Draft_Orders table for both ORDER_CODE and ORDER_ID for any order, the team had combined two operations within a single transaction using TransactWriteItems API, as shown in the following code (see Managing complex workflows with DynamoDB transactions for additional details):

@transactional { Create Order(unique_order_code) without other order related details; Condition: if no order exists with the given unique_order_code; Create Order (unique_order_id) with order code and other order related details; Condition: if no order exists with the given unique_order_id; }Scala

The team's initial tests showed that the above statements in the DynamoDB Transaction had an average latency of approximately 18 milliseconds. After investigating, we discovered that including non-essential synchronous operations within a transaction was causing performance issues. To improve performance, we removed these operations from the transaction and instead used two separate PutItem API calls – one for creating the order with ORDER_CODE without any order related details followed by creating the order with the ORDER_ID, ORDER_CODE and other order related details in sequence. This change delivered two main benefits.

Response times became consistently fast, staying under 10 milliseconds per DynamoDB API call We cut DynamoDB's Write Capacity Unit consumption in half since we eliminated the transaction overhead. Note: Transactions in DynamoDB require two read/write operations per item, as explained in Capacity management for transactions.

The key lesson from this experience was to avoid bundling operations in transactions unless absolutely necessary. See Best practices for transactions for additional details. Following these improvements, Zepto implemented the changes gradually: first to 30% of production traffic and then expanding to full production deployment. Benefits of adopting DynamoDB The new architecture offered several improvements over the previous monolithic architecture using Aurora PostgreSQL:

The Create Order API's performance improved by 60% on average, and 40% for p99 Zepto experienced consistent, single-digit millisecond performance for operations on the DynamoDB table, which wasn't the case with the previous architecture

Reduced operational overhead: Zepto engineers were freed from the undifferentiated heavy lifting operational tasks like cluster maintenance, including patching, upgrading, and parameter fine-tuning based on multiple metrics Improved scalability: DynamoDB's serverless nature enables Zepto to scale from quiet late-night deliveries to high-demand festival rushes and sports event peaks – all without managing servers.

Conclusion In this post, we shared the challenges that Zepto experienced with its monolithic data management architecture, and how it adopted DynamoDB with a microservices-based architecture. With this migration, Zepto now handles millions of orders daily while cutting costs, achieving a 60% faster Create Order API and enhanced operational efficiency and scalability. Given these positive outcomes, Zepto is now looking to implement DynamoDB in other areas of their business. It is common to see developer teams trying to find the one-size-fits all database, which doesn't exist. We recommend developers to find a database that matches their specific application needs. AWS database services are specialized for handling specific data models and access patterns, offering optimized performance, scalability, and cost-effectiveness compared to general-purpose databases. To explore AWS' purpose-built database offerings, refer to AWS Cloud Databases.

Rahul Bennapalan Rahul is a Principal Engineer at Zepto with deep expertise in designing scalable application architectures, working with NoSQL databases, and managing cloud infrastructure. He has led the design and scaling of large-scale systems across critical domains like order management, customer support, and more.

Nitin Jain Nitin is a Principal Engineer with 7 years of experience in software development, system design, and architecture in the e-commerce industry. He has been instrumental in scaling engineering systems to handle rapid business growth, focusing on building high-performance, resilient, and scalable platforms that support expansion and innovation.

Kayalvizhi Kandasamy Kayalvizhi is a Principal Solutions Architect (APAC) at AWS. She uses her expertise to transform concepts into reality, with emphasis on AWS Cloud-based solutions, serverless architectures, and microservices. When she's not working, Kayalvizhi enjoys chess as a FIDE-rated player and shares her passion for the game by coaching her daughters the art of playing chess.

Orchestrating big data processing with AWS Step Functions Distributed Map

Developers seek to process and enrich semi-structured big data datasets with durably orchestrated network-based workflows. For example, during quarterly earnings season, finance organizations run thousands of market simulations simultaneously to provide timely insights for scenario planning or risk management—these workloads require coordination between raw datasets and on-premise servers to provide the latest market information. AWS Step Functions is a visual workflow service capable of orchestrating over 14,000 API actions from over 220 AWS services to build distributed applications. Now, Step Functions Distributed Map streamlines big data dataset transformation by processing Amazon Athena data manifest and Parquet files directly. Using its Distributed Map feature, you can process large scale datasets by running concurrent iterations across data entries in parallel. In Distributed mode, the Map state processes the items in the dataset in iterations called child workflow executions. You can specify the number of child workflow executions that can run in parallel. Each child workflow execution has its own, separate execution history from that of the parent workflow. By default, Step Functions runs 10,000 parallel child workflow executions in parallel. Distributed Map can process AWS Athena data manifest and Parquet files directly, eliminating the need for custom pre-processing. You also now have visibility into your Distributed Map usage with new Amazon CloudWatch metrics: Approximate Open Map Runs Count, Open Map Run Limit, and Approximate Map Runs Backlog Size. In this post, you'll learn how to use AWS Step Functions Distributed Map to process Athena data manifest and Parquet files through a step-by-step demonstration.

This post is part of a series of post about AWS Step Functions Distributed Map:

Processing Amazon S3 objects at scale with AWS Step Functions Distributed Map S3 prefix Optimizing nested JSON array processing using AWS Step Functions Distributed Map Orchestrating big data processing with AWS Step Functions Distributed Map

Use case: IoT sensor data processing You'll build a sample application that demonstrates processing IoT sensor data in Parquet format using Step Functions Distributed Map. These Parquet data files and a manifest file containing the list of the data files are exported from Athena. The data temperature, humidity, and lbattery level from different devices. The following table shows sample of sensor data:

Your objective is to use the Athena data manifest file, get the list of Parquet files, and iterate over the data in the files to detect anomalies and also stream the processed data through Amazon Kinesis Data Firehose to an Amazon S3 bucket for further analytics using Athena queries. Following is the criteria to detect anomaly:

Low battery conditions: less than 20% Humidity anomalies: more than 95% or less than 5% Temperature spikes: more than 35°C or less than -10°C

The following diagram represents the AWS Step Functions state machine:

The Distributed Map runs an Athena query which generates Parquet data files and an Athena manifest file (csv). The manifest file contains the list of Parquet data files. Distributed Map processes these Parquet data files in parallel using child workflow executions. You can control the number of child workflow executions that can run in parallel using MaxConcurrency parameter. See Step Functions service quotas to learn more about concurrency limits. Each child workflow execution invokes an AWS Lambda function to process the respective Parquet file. The Lambda function processes individual sensor readings and detects anomalies according to the preceeding logic and returns a processed sensor data summary response. The child workflow sends the summary response record to Amazon Kinesis firehose stream which stores the results in a specified Amazon S3 results bucket.

The following Athena Start QueryExecution state runs an UNLOAD query to generate data files in Parquet format and a manifest file in CSV. The output will be stored in the S3 bucket specified in the UNLOAD query and the manifest file will be stored in the S3 bucket configured for the Athena workgroup.

{ "QueryLanguage": "JSONata", "States": { "Athena StartQueryExecution": { "Type": "Task", "Resource": "arn:aws:states:::athena:startQueryExecution.sync", "Arguments": { "QueryString": "UNLOAD (WRITE_YOUR_SELECT_QUERY_HERE) TO 'S3_URI_FOR_STORING_DATA_OBJECT' WITH (format = 'JSON')", "WorkGroup": "primary" }, "Output": { "ManifestObjectKey": "{% $join([$states.result.QueryExecution.ResultConfiguration.OutputLocation, '-manifest.csv']) %}" }, "Next": "Next State" … }Code

The following ItemReader is configured to use a manifest type of "ATHENA_DATA" with "PARQUET" data input.

{ "QueryLanguage": "JSONata", "States": { ... "Map": { ... "ItemReader": { "Resource": "arn:aws:states:::s3:getObject", "ReaderConfig": { "ManifestType": "ATHENA_DATA", "InputType": "PARQUET" }, "Arguments": { "Bucket":"Bucket": "{% $split($substringAfter($states.input.ManifestObjectKey, 's3://'), '/')[0] %}",, "Key": "{% $substringAfter($substringAfter($states.input.ManifestObjectKey, 's3://'), '/') %}" } }, ... } }Code

Additional supported InputType options are CSV and JSONL. All objects referenced in a single manifest file must have the same InputType format. You specify the Amazon S3 bucket location of Athena manifest CSV file under Arguments. The context object contains information in a JSON structure about your state machine and execution. Your workflows can reference the context object in a JSONata expression with $states.context. Within a Map state, the Context object includes the following data:

"Map": { "Item": { "Index" : Number, "Key" : "String", // Only valid for JSON objects "Value" : "String", "Source": "String" } }Code

For each Map state iteration, Index contains the index number for the array item that is being currently processed, Key is available only when iterating over JSON objects, Value contains the array item being processed, and Source contains one of the following:

For state input, the value will be : STATE_DATA For Amazon S3 LIST_OBJECTS_V2 with Transformation=NONE, the value will show the S3 URI for the bucket. For example: S3://amzn-s3-demo-bucket. For all the other input types, the value will be the Amazon S3 URI. For example: S3://amzn-s3-demo-bucket/object-key.

Using this newly introduced Source field in the context object, you can connect the child executions with the source object. Prerequisites

Access to an AWS account through the AWS Management Console and the AWS Command Line Interface (AWS CLI). The AWS Identity and Access Management (IAM) user that you use must have permissions to make the necessary AWS service calls and manage AWS resources mentioned in this post. While providing permissions to the IAM user, follow the principle of least-privilege. AWS CLI installed and configured. If you are using long-term credentials like access keys, follow manage access keys for IAM users and secure access keys for best practices. Git Installed AWS Serverless Application Model (AWS SAM) installed Python 3.13+ installed

Set up the state machine and sample data Run the following steps to deploy the Step Functions state machine.

Clone the GitHub repository in a new folder and navigate to the project root folder.

Run the following command to install required Python dependencies for the Lambda function.

python3 -m venv .venv source .venv/bin/activate python3 -m pip install -r requirements.txtCode

Stack name: The CloudFormation stack name (for example, sfn-parquet-file-processor) AWS Region: A supported AWS Region (for example, us-east-1) Keep rest of the components to default values. Note the outputs from the AWS SAM deploy. You will use them in the subsequent steps. Run the following command to generate sample data in csv format and upload it to an S3 bucket. Replace with the value from sam deploy ouptut.

Create the Athena database and tables Before you can run queries, you must set up an Athena database and table for your data.

From Amazon Athena console, navigate to workgoups, select the workgroup named "primary". Select Edit from Actions. In the query result configuration section, select the options as follows:

Management of query results – select customer managed Location of query results – enter s3:// . Replace with the value from sam deploy output. Choose Save to save the changes to the workgroup

Select Query editor tab and run the following commands to create database and tables

Create an Athena table in database iotsensordata that references the S3 bucket containing the raw sensor data. In this case it will be . Replace with the value from sam deploy output.

CREATE EXTERNAL TABLE IF NOT EXISTS `iotsensordata`.`iotsensordata` (`deviceid` string, `timestamp` string, `temperature` double, `humidity` double, `batterylevel` double, `latitude` double, `longitude` double ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ('field.delim' = ',') STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3:// /daily-data/' TBLPROPERTIES ( 'classification' = 'csv', 'skip.header.line.count' = '1' );SQL

Create an Athena table in database iotsensordata that references the S3 bucket having the analytics results streamed from Kinesis Data Firehose. Replace with value from sam deploy output. And replace with the current year (e.g 2025).

CREATE EXTERNAL TABLE IF NOT EXISTS iotsensordata.iotsensordataanalytics (deviceid string, analysisDate string, readingTimestamp string, readingsCount int, metrics struct , anomalies array , anomalyCount int, healthStatus string, timestamp string ) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES ( 'ignore.malformed.json' = 'FALSE', 'dots.in.keys' = 'FALSE', 'case.insensitive' = 'TRUE' ) STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 's3:// / /' TBLPROPERTIES ('classification' = 'json', 'typeOfData'='file');SQL

Start your state machine Now that you have data ready and Athena set up for queries, start your state machine to retrieve and process the data.

Run the following command to start execution of the Step Functions. Replace the and with the value from sam deploy output..

aws stepfunctions start-execution \ --state-machine-arn \ --input '{ "IoTDataBucketName": " "}'Code The Step Functions state machine has the Athena StartQueryExecution state which has an UNLOAD query that generates the sensor data files in a parquet format and a manifest file in CSV format. The manifest will have 5 rows referencing the 5 parquet files. The state machine will process these 5 parquet files in one map run. Run the following command to get the details of the execution. Replace the executionArn from the previous command.

After you see the status SUCCEEDED, run the following command from Athena query editor to check the processed output from Kinesis Data Firehose that was streamed to S3 bucket referenced by the Athena table created in step 4 of the preceding section.

If any of the sensor data exceeds the thresholds, the healthstatus attribute will be set to "anomalies_detected". The workflow produced a summary table of metadata which you can now query for reporting.

Review workflow performance Using the following observability metrics, you can review key performance behavior of your data processing workflow. The AWS/States namespace includes the following new metrics for all Step Functions Map Runs.

OpenMapRunLimit: This is the maximum number of open Map Runs allowed in the AWS account. The default value is 1,000 runs and is a hard limit. For more information, see Quotas related to accounts. ApproximateOpenMapRunCount: This metric tracks the approximate number of Map Runs currently in progress within an account. Configuring an alarm on this metric using the Maximum statistic with a threshold of 900 or higher can help you take proactive action before reaching the OpenMapRunLimit of 1,000. This metric enables operational teams to implement preventive measures, such as staggering new executions or optimizing workflow concurrency, to maintain system stability and prevent backlog accumulation. ApproximateMapRunBacklogSize: This metric shows up when the ApproximateOpenMapRunCount has reached 1,000 and there are backlogged Map Runs waiting to be executed. Backlogged Map Runs wait at the MapRunStarted event until the total number of open Map Runs is less than the quota.

The following graph shows an example of these new metrics. Use the maximum statistic to visualize these metrics. ApproximateMapRunBacklogSize metrics appear after accounts start getting throttled on the OpenMapRunLimit limit. The OpenMapRun (orange line) is the account hard limit of 1,000 shown as a static line. The ApproximateOpenMapRunCount (violet line) is the current number of active OpenMap runs. The ApproximateMapRunBacklogSize (green line) indicates the map runs waiting in backlog to be processed. When the ApproximateOpenMapRunCount is lower than 1000 (OpenMapRun limit) there are no map runs in backlog. However, when the count reaches the OpenMapRun limit, the backlog of map runs starts to build up. After the active runs complete, the backlog will start to drain out and new runs will begin execution.

Clean up To avoid costs, remove all resources created for this post once you're done. From the Athena query editor, run the following commands:

DROP TABLE `iotsensordata`.`iotsensordata`; DROP TABLE `iotsensordata`.`iotsensordataanalytics`; DROP DATABASE `iotsensordata`; SQL

Run the following commands from the AWS CLI after replacing the variable to delete the resources you deployed for this post's solution:

aws s3 rm s3:// --recursive aws s3 rm s3:// --recursive sam deleteSQL

Conclusion With this update, Distributed Map now supports additional data inputs, so you can orchestrate large-scale analytics and ETL workflows. You can now process Amazon Athena data manifest and Parquet files directly, eliminating the need for custom pre-processing. You also now have visibility into your Distributed Map usage with the following metrics: Approximate Open Map Runs Count, Open Map Run Limit, and Approximate Map Runs Backlog Size. New input sources for Distributed Map are available in all commercial AWS Regions where AWS Step Functions is available. For a complete list of AWS Regions where Step Functions is available, see the AWS Region Table. The improved observability of your Distributed Map usage with new metrics is available in all AWS Regions. To get started, you can use the Distributed Map mode today in the AWS Step Functions console. To learn more, visit the Step Functions developer guide. For more serverless learning resources, visit Serverless Land.

Optimizing nested JSON array processing using AWS Step Functions Distributed Map

When you're working with large datasets, you've likely encountered the challenge of processing complex JSON structures in your automated workflows. You need to preprocess arrays within nested JSON objects before you can run parallel processing on them. Extracting data used to require custom code and extra processing steps, delaying you from building your core application logic. With AWS Step Functions Distributed Map, you can process large datasets with concurrent iterations of workflow steps across data entries. Using the enhanced ItemsPointer feature of Distributed Maps, you can extract array data directly from JSON objects stored in Amazon S3. Alternatively, for JSON object as state input, you can use Items (JSONata) or ItemsPath (JSONPath). With this enhancement you can point directly to arrays nested within JSON structures, eliminating the need for custom preprocessing of your data. With ItemsPointer, Items, and ItemsPath you can select the nested array data and simplify your workflows. In this post, we explore how to optimize processing array data embedded within complex JSON structures using AWS Step Functions Distributed Map. You'll learn how to use ItemsPointer to reduce the complexity of your state machine definitions, create more flexible workflow designs, and streamline your data processing pipelines—all without writing additional transformation code or AWS Lambda functions.

Use case: e-commerce product data enrichment In this e-commerce use case example, you'll build a sample application that demonstrates processing of product inventory data for an e-commerce application using AWS Step Functions Distributed Map. The application receives a JSON file from an upstream application containing an array of product information. The Step Functions workflow reads the JSON file containing product data from an S3 bucket and iterates over the array to enrich each product data in the array. The following diagram presents the AWS Step Functions state machine.

The state machine reads the product-updates.json file from an input S3 bucket. The file contains a JSON array of products. The Distributed Map state in the state machine, selects the JSON array node using ItemsPointer and iterates over the JSON array. For each of the items within the array, the state machine invokes a Lambda function for data enrichment. The Lambda function adds product stock and price information to the product data. The state machine saves the updated product data in an Amazon DynamoDB table. Finally, the state machine uploads the execution metadata into an output S3 bucket. See limits related to state machine executions and task executions.

MaxConcurrency can be configured to specify the number of child workflow executions in a Distributed Map that can run in parallel. If not specified, then Step Functions doesn't limit concurrency and runs 10,000 parallel child workflow executions. You can read a JSON file from a S3 bucket using ItemReader and its sub-fields. If the JSON file, from the S3 bucket, contains a nested object structure, you can select the specific node with your data set with an ItemsPointer. For example, the following input JSON file:

{ "version": "2024.1", "timestamp": "2025-09-26T10:49:36.646197", "productUpdates": { "items": [ { "productId": "PROD-001", "name": "Wireless Headphones", "price": 79.99, "stock": 150, "category": "Electronics" }, { "productId": "PROD-002", "name": "Smart Watch", "category": "Electronics" }, … ] } }Code

The following JSONata-based workflow configuration extracts a nested list of products from productUpdates/items:

"ItemReader": { "Resource": "arn:aws:states:::s3:getObject", "ReaderConfig": { "InputType": "JSON", "ItemsPointer": "/productUpdates/items" }, "Arguments": { "Bucket": "amzn-s3-demo-bucket", "Key": "updates/product-updates.json" } }Code

For JSONPath-based workflow note that Arguments is replaced with Parameters:

The ItemReader field is not needed when your dataset is JSON data from a previous step. ItemsPointer is only applicable when the input JSON objects read from an S3 bucket. If you are using JSON as state input to a Distributed Map, then you can use the ItemsPath (for JSONPath) or Items (for JSONata) field to specify a location in the input that points to JSON array or object used for iterations. Prerequisite To use Step Functions Distributed Map, verify you have:

Set up and run the workflow Run the following steps to deploy the Step Functions state machine.

Clone the GitHub repository in a new folder and navigate to the project folder.

Stack name: Stack name for CloudFormation (for example, stepfunctions-json-array-processor) AWS Region: A supported AWS Region (for example, us-east-1) Accept all other default values. The outputs from the sam deploy will be used in the subsequent steps. Run the following command to generate product-updates.json file containing a nested JSON array of sample products and upload the product-updates.json file to the input S3 bucket. Replace InputBucketName with the value from sam deploy output.

Run the following command to start execution of the Step Functions workflow. Replace the StateMachineArn with the value from sam deploy output.

aws stepfunctions start-execution \ --state-machine-arn \ --input '{}'Code The state machine reads the input product-updates.json file and invokes a Lambda function to update the database for every product in the array after adding price and stock information. The execution metadata is also uploaded into the results bucket.

Monitor and verify results Run the following steps to monitor and verify the test results.

Run the following command to get the details of the execution. Replace executionArn with your state machine ARN.

aws stepfunctions describe-execution --execution-arn Code Wait until the status shows SUCCEEDED. Run the following commands to validate the processed output from ProductCatalogTableName DynamoDB table. Replace the value ProductCatalogTableName with the value from sam deploy output.

Check that the DynamoDB table contains the enriched product data including price and stock attributes. Example output:

{ "Items": [ { "ProductId": { "S": "PROD-005" }, "lastUpdated": { "S": "2025-10-07T20:33:34.507Z" }, "stock": { "N": "129" }, "price": { "N": "139.25" } }, { "ProductId": { "S": "PROD-003" }, "lastUpdated": { "S": "2025-10-07T20:33:34.576Z" }, "stock": { "N": "471" }, "price": { "N": "40.92" } }, … ], "Count": 5, "ScannedCount": 5, "ConsumedCapacity": null }Code

Clean up To avoid costs, remove all resources you've created while following along with this post. Run the following command after replacing the variable to delete the resources you deployed for this post's solution:

aws s3 rm s3:// --recursive aws s3 rm s3:// --recursive sam deleteCode

Conclusion In this post, you learned how to use Step Functions Distributed Map for extracting array data natively from JSON objects stored in a S3 bucket. By removing custom data extraction code, you can simplify the processing of your large-scale parallel workloads. With ItemsPointer you can extract array data within JSON files stored in a S3 bucket , and with Items(JSONata) or ItemsPath (JSONPath), you can extract arrays from complex JSON state input, adding flexibility to your workflow designs. New input sources for Distributed Map are available in all commercial AWS Regions where AWS Step Functions is available. For a complete list of AWS Regions where Step Functions is available, see the AWS Region Table. To get started, you can use the Distributed Map mode today in the AWS Step Functions console. To learn more, visit the Step Functions developer guide. For more serverless learning resources, visit Serverless Land.

By: Yaniv Ehrlich, Partner Strategy Leader, Generative Innovation and Delivery – AWS By: Cher Simon, Principal Deep Learning Architect, Generative Innovation and Delivery – AWS The AWS Generative AI Innovation Center (GenAIIC) proudly launches the Partner Agent Factory (PAF), a pioneering co-innovation program designed to accelerate the development of enterprise-grade, production-ready AI Agents and Tools with our Partner Innovation Alliance (PIA) partners. This initiative offers a unified go-to-market strategy, enabling customers to access transformative AI agent solutions on AWS Marketplace. PAF provides immersive expert guidance and support, empowering PIA partners to build end-to-end AI solutions that meet dynamic business requirements. Customers benefit from the combined expertise of GenAIIC's proven methodologies and PIA partners' industry knowledge, available through AI Agents and Tools on AWS Marketplace. This strategic collaboration fosters innovation while driving security, governance, and simplified procurement. With PAF, enterprises can rapidly adopt scalable AI Agent solutions, driving tangible business results.

"The Partner Agent Factory is a unique program to advance our mission to democratize enterprise AI adoption. By combining GenAIIC's proven methodologies with our partners' deep industry expertise, we are building AI agents that help customers create transformative business solutions that deliver measurable impact. PAF embodies our commitment to collaborative innovation, ensuring that cutting-edge AI capabilities reach companies across all sectors through trusted partnerships and rigorous quality standards." – Taimur Rashid, Director and Head of Generative Innovation and Delivery, AWS.

Accelerating AI Innovation through Expanded Strategic Partnerships The GenAIIC Partner Agent Factory (PAF) aims to amplify the success and strategic partnerships with our high-performing PIA partners, converting AI potential into tangible business value. Since its launch in 2023, GenAIIC has guided thousands of customers from AI experimentation to full-scale deployment, achieving significant productivity gains and enhanced customer experiences. Building on this momentum, GenAIIC connects customers with a global network of PIA partners to scale support for both proof-of-concept and production implementations. Through PAF, GenAIIC co-innovates enterprise-grade AI Agents and Tools alongside PIA members, combining AWS's proven AI methodologies with partners' deep industry expertise to deliver production-ready solutions that address real-world business challenges faster and more effectively than either organization could achieve alone. Unlock the Full Potential of AI with PAF PAF is designed to deliver unparalleled benefits to AWS customers and partners by fostering a dynamic environment of innovation and collaboration:

Collaborative Excellence: PAF combines the deep technical expertise of GenAIIC with the specialized domain knowledge of our PIA partners. This synergy ensures a holistic approach to solving complex business challenges with Agentic AI, leading to more effective and tailored solutions. Accelerated Innovation Pace: Integrating marketplace-ready agentic solutions and accelerators developed through the GenAIIC PAF reduces time-to-market for Agentic AI innovations and enhances organizational agility in response to dynamic business needs. Scalable Solutions: PAF is committed to supporting the expansion and adaptability of Agentic AI solutions. Our framework ensures these solutions can evolve to meet the growing and changing demands of enterprises, providing long-term value and flexibility.

PAF Pathway: Advancing from GenAIIC Bootcamp to AWS Marketplace Success To advance to the PAF, PIA partners must first complete a hands-on, immersive GenAIIC bootcamp, which provides an in-depth exploration of GenAIIC methodology. This advanced training is followed by structured PAF Academy sessions that refine the solution candidate through business strategy discovery, a science delivery deep dive, and a path-to-production demonstration showcasing tangible customer results. These components are critical for the comprehensive board review assessment, ensuring the solution meets high standards of quality, effectiveness, and industry alignment. Upon successful evaluation, solutions are published to the AWS Marketplace, offering cutting-edge Agentic AI solutions to a wide range of customers.

Figure 1: GenAIIC Partner Agent Factory Introducing the PAF AI Agents and Tools We are excited to announce the first four PAF launch partners, who will spearhead this initiative. These collaborations span various industries and AI applications, highlighting the extensive capabilities and potential of generative AI in transforming enterprise solutions. OneByZero – ETL Crew: Generative AI for ETL Modernization ETL Crew, developed by OneByZero in collaboration with GenAIIC PAF, is an Agentic AI platform that automates legacy ETL modernization for cloud migrations, delivering up to 50% faster transitions with reduced costs. The solution eliminates manual coding by intelligently transforming legacy pipelines into cloud-native architectures while ensuring consistency and compliance. Learn more.

"As part of AWS's GenAIIC Partner Innovation Alliance, we at OneByZero are proud to introduce our ETL Crew solution on the AWS Marketplace – having been rigorously supported, vetted and co-innovated through the GenAIIC Partner Agent Factory. This collaboration ensures that customers gain an enterprise-grade capability, leveraging not just the latest from AWS but also their GenAI assets, and guidance for secure and scalable architecture." – Vibhore Kumar, CTO, OneByZero. "Through our collaboration with OnebyZero (OBZ) and AWS, Krungsri has been able to accelerate our experimentation with Generative AI for code conversion, accelerating Cloud migration by more than 50%, leveraging Kumo (Krungsri's AWS based data platform) to drive efficiency by using data analytics and AI across multiple business domains and workstreams. We are excited about OBZ's ETL Crew solution, built with AWS GenAIIC Partner Agent Factory, to enhance our continued success." – K. Tul Roteseree, Head of Enterprise Data and Analytics Group, Bank of Ayudhya PCL (Krungsri).

CI&T Conversational Banking powered by Agentic Intelligence CI&T Conversational Banking, powered by Agentic Intelligence and co-innovated with GenAIIC PAF, enables customers to perform everyday banking tasks such as checking balances and loan simulations through WhatsApp with human-like conversations. This multilingual Agentic AI solution delivers 24/7 personalized service with enterprise-grade security while reducing operational costs for financial institutions. Learn more.

"We are very proud to be pioneering, together with AWS, a new generation of user experiences for financial services—more human, intuitive, and intelligent. This transformation has been accelerated through our collaboration with the AWS Generative AI Innovation Center's Partner Innovation Alliance and its Partner Agent Factory program, which have enabled us to design and scale agentic solutions that deliver tangible business outcomes for our clients." – Bruno Guicardi, President & Co-founder, CI&T. "At banQi, we believe that technology brings people closer together. That's why, together with CI&T and AWS GenAIIC Partner Agent Factory, we're building conversational AI solutions that make banking simpler, more human, and more accessible. Through this collaboration, we're delivering practical, intelligent experiences that strengthen relationships and make everyday financial life easier for our customers." – Bruno Giovani da Silva Maria, Financial Services Coordinator, banQi.

Quantiphi QAIssist – Agentic Contact Center QAIssist is an AI-powered contact center solution co-innovated by Quantiphi and GenAIIC PAF that uses autonomous agents to handle complex customer requests across voice and chat while maintaining enterprise security for regulated industries. The solution intelligently processes multi-step workflows, accesses real-time data, and escalates to human agents when needed, delivering faster and context-aware customer experiences. Learn more.

"Our collaboration with AWS through the GenAl Innovation Center (GenAIIC) Partner Innovation Alliance (PIA) has been a catalyst in accelerating hands-on generative Al innovation. The PIA Bootcamp experience paved the way for our participation in the Partner Agent Factory (PAF), enabling us to co-innovate with AWS GenAIIC experts and transform GenAl concepts into scalable, agentic solutions. This progression reflects our shared commitment to infusing GenAl into every layer of enterprise transformation-driving efficiency, innovation, and measurable business outcomes across industries." – Jim Keller, Global AWS Leader, Quantiphi. "With AWS and Quantiphi's Partner Agent Factory (PAF) solution, we're moving to a platform that facilitates a much simpler integration with our Unclaimed Property Claims-processing system. By transforming our contact center into a smarter, more efficient, and citizen-focused operation, we will be better able to serve a very fast-growing need. The modernization potentially optimizes costs while improving service accessibility, but also lays the foundation for future innovation with Agentic AI—enabling our team to deliver faster, more personalized support to the Citizens of Utah." – Dennis L. Johnston, Utah State Treasurer's Office Administrator, Unclaimed Property Division.

Crayon refiNER – An Agentic NER Solution Crayon refiNER is an AI-powered named entity recognition framework built on LLMs that automatically detects, validates, and refines entities across multiple languages and domains using intelligent tool-driven workflows. Co-innovated with AWS GenAIIC, Crayon refiNER continuously learns and improves extraction accuracy through self-refining feedback loops while providing transparent, auditable results with minimal manual intervention. Learn more.

"Partnering with AWS GenAIIC on the Partner Agent Factory Launch Program enables us to bring together innovation, expertise, and reusable GenAI assets to rapidly turn ideas into impactful solutions. As a member of GenAIIC Partner Innovation Alliance, this collaboration strengthens our ability to help customers realize tangible business value through generative AI." – Vikas Bhonsle, Executive Vice President – APAC, India and Middle East, Crayon.

Embark on Your AI Journey with PAF PAF is dedicated to delivering tangible business results through innovation, collaboration, and excellence in Agentic AI. Explore GenAIIC's PAF solutions on the AI Agents and Tools on AWS Marketplace today. Connect with us at [EMAIL] to harness the power of Agentic AI for your business success.

From Business Logic to Working Code: How Kiro Changes Who Can Build

Supply chain managers understand inventory reconciliation. Compliance officers know audit requirements. Marketing teams grasp campaign workflows. What if they could build their own enterprise applications directly from that expertise? This isn't theoretical. Citizen development tools like Kiro replace traditional coding with natural language specification. Business users describe what they need in plain English, and Kiro builds working applications that those users can iterate and refine themselves. "When inventory falls below 100 units, email the supplier and create a purchase order" becomes a working application in days. You can iterate to production in weeks instead of months. The approach succeeds because business users no longer translate their needs through analysts and developers (a process that loses critical context). Domain expertise becomes working software. The potential is massive. By 2025, 70% of new applications developed by enterprises will use low-code or no-code technologies, up from less than 25% in 2020, according to Gartner. This shift unlocks business users as an enormous workforce for application development. The driver behind this trend: IDC forecasts that by 2026, more than 90% of organizations worldwide will feel the pain of the IT skills crisis, amounting to some $5.5 trillion in losses caused by product delays, impaired competitiveness, and loss of business. How Specification-Driven Development Changes the Game Three technical capabilities make this possible. Amazon Bedrock AgentCore provides enterprise-grade runtime with 8-hour session support. (Previous solutions timed out after minutes, forcing users to restart complex workflows.) Integrated tools eliminate the need for expert developers to assemble complex toolchains. And iterative refinement lets users test with real data immediately instead of waiting for complete implementations. These capabilities solve the core failure pattern of earlier citizen development attempts. When business users have to predefine complete workflows upfront, they can't discover edge cases until deployment. As AWS VP Deepak Singh explained in an interview with SiliconANGLE, Kiro lets developers work side by side with an agent to define requirements and application structure. "You collaborate with the agent on figuring out what your requirements should be and how your application should be structured," said Singh. "As the application goes from your little fun prototype to a full application, it grows with you." Testing helps improve the speed advantage. Traditional no-code platforms require users to map complete business logic into visual workflows before seeing results. Specification-driven development lets users describe outcomes, test with real data immediately, and refine based on actual results—cutting discovery cycles from weeks to days. From Four Hours to Four Minutes: A Real Implementation Consider Sarah, a supply chain manager at a medical device manufacturer who spent four hours every Monday reconciling inventory data across three systems. IT said it would take six months to build a solution. Sarah described her reconciliation process to Kiro: "Pull data from systems A, B, and C. Flag discrepancies over $1,000. Email alerts to procurement when stock falls below safety levels. Generate weekly variance reports for finance." The first version caught 80% of her use cases. Through three iterations over two weeks—discovering edge cases, handling exceptions for returned items, and adding finance team notifications—she built a solution that handles 95% of her Monday reconciliation automatically. Two weeks of development turned four hours of manual work into a fifteen-minute automated process—months ahead of IT's six-month timeline. Building Your Citizen Development Framework When launching citizen development initiatives, three critical foundations help determine success or failure. 1. Governance without Strangulation Your governance framework should prevent problems through design rather than approval workflows. Use pre-approved connectors to enterprise systems that enforce your existing security model and prohibit direct database access. Automated scanning catches sensitive data exposure. Require approval only for external connections, not internal data sources. When Sarah's inventory tool scales to handle $2M in monthly orders, that triggers a review for enterprise scaling — recognition that the tool has outgrown its pilot, not that it's done something wrong. 2. Distributed Support Model Identify power users in each business unit: the Excel experts who already build complex macros, the process optimizers who create workarounds, and the people others ask for help. Train them as citizen development mentors who provide domain expertise while IT maintains platform infrastructure. This approach frees professional developers to focus on platform capabilities and complex integrations. When Sarah built her inventory tool, she didn't need IT to hold her hand. She asked Mike (the procurement Excel wizard) for business logic help. 3. Metrics That Matter Track development cycle time, hours saved, and process improvements. Compare citizen-built solution timelines to typical IT project durations. Monitor usage patterns—when tools scale beyond their original team, you can replicate that success enterprise-wide. Addressing Resistance Department managers often worry about losing control when teams self-serve their technology needs. Their concern stems from the decades when "IT oversight" meant project approval and resource allocation. Give these managers new responsibilities that leverage their oversight skills: identifying patterns across team-built solutions, connecting use cases between departments, and escalating complex requirements to IT. They become innovation process managers rather than request gatekeepers. The framing matters as much as the responsibilities. Present citizen development as capability building ("Build tools that make you more effective than your competitors") rather than cost reduction. Cost reduction implies more work for the same pay and triggers defensive reactions. Capability building positions managers as competitive advantage creators who develop their teams' skills. A procurement manager who helps her team build three automation tools in a quarter has accomplished a feat worth recognizing in performance reviews. Watch for resistance disguised as governance concerns. "We need more controls before expanding" often means "I'm uncomfortable with the pace of change." Address this by showing controlled expansion: Start with one additional team that has a clear use case and strong power user, succeed there, then expand. Incremental proof points overcome fear better than comprehensive governance documents. Your 30-Day Pilot Plan Start small, measure everything, and scale what works. Week 1-2: Selection and Setup Have your team download Kiro and identify three power users who spend 4+ hours weekly on repetitive tasks. Choose processes with clear rules and measurable outcomes, such as inventory reconciliation, compliance reporting, and customer onboarding workflows. Avoid anything that requires complex judgment calls or creative decisions. Week 3-4: Build and Iterate Have users describe their processes in plain English, test with real data immediately, and refine based on results. Expect iteration—first versions typically capture 80% of use cases. Document what requires IT intervention versus what users solve independently. Measurement and Scaling Track hours saved, errors caught, and processes accelerated through platform usage data and quarterly user surveys. Compare citizen development timelines to typical IT estimates for projects of a similar scope. Look for patterns in successful use cases that indicate which approaches work best and merit scaling enterprise-wide. Use these outcomes to justify a broader rollout. The New Role of Technology Leaders Technology leaders who win in this environment recognize that their strength comes not from controlling who can build but from controlling how building happens safely. Your role evolves from building every application to building the infrastructure that lets your team members build within governance frameworks you design. This requires new skills: designing governance frameworks that enable rather than restrict, identifying patterns across distributed solutions, and knowing when citizen-built applications need enterprise scaling versus remaining team-specific. Citizen-built applications that can handle millions of transactions indicate platform success, not governance failure.

TAGS: Agility, AI, Business Value, Change Leadership, governance, Leadership, Organizational Agility

Tom Godden Tom Godden is an Enterprise Strategist and Evangelist at Amazon Web Services (AWS). Prior to AWS, Tom was the Chief Information Officer for Foundation Medicine where he helped build the world's leading, FDA regulated, cancer genomics diagnostic, research, and patient outcomes platform to improve outcomes and inform next-generation precision medicine. Previously, Tom held multiple senior technology leadership roles at Wolters Kluwer in Alphen aan den Rijn Netherlands and has over 17 years in the healthcare and life sciences industry. Tom has a Bachelor's degree from Arizona State University.

Unlock Seamless iSCSI Storage Integration: A Guide to FSxN on ROSA Clusters for iSCSI

In a previous blog, we introduced an exciting feature in the Trident 25.02 release that simplifies preparing worker nodes of an OpenShift Cluster for iSCSI workloads. This enhancement eliminates the need for manual preparation, streamlining the process for Kubernetes cluster worker nodes and benefiting users of Red Hat OpenShift Service on AWS (ROSA). With this feature, provisioning persistent volume for various workloads, including virtual machines on OpenShift virtualization on bare metal nodes within a ROSA cluster, becomes effortless. In this blog, we will provide a comprehensive guide on installing Amazon FSx for NetApp ONTAP (FSxN) on AWS and utilizing it to provision storage for containers and virtual machines running on ROSA clusters. Join us as we walk you through the installation and configuration of Trident 25.02, showcasing how to create container applications and virtual machines on ROSA clusters using iSCSI volumes. Additionally, we will demonstrate that Trident supports Read Write Many (RWX) access modes for iSCSI volumes in Block mode, enabling live migrations of VMs created with iSCSI storage. Get ready to unlock seamless storage integration and enhance your ROSA cluster deployments! ROSA clusters with FSxN storage ROSA integrates seamlessly with Amazon FSx for NetApp ONTAP (FSxN), a fully managed, scalable shared storage service built on NetApp's renowned ONTAP file system. With FSxN, customers can leverage key features such as snapshots, FlexClones, cross-region replication with SnapMirror, and a highly available file server that supports seamless failover. The integration with NetApp Trident driver—a dynamic Container Storage Interface (CSI)—facilitates the management of Kubernetes Persistent Volume Claims (PVCs) on storage disks. This driver automates the on-demand provisioning of storage volumes across diverse deployment environments, making it simpler to scale and protect data for your applications. One key benefit of FSxN is that it is a true first party AWS offering just like EBS, meaning customers can retire their committed spend with AWS and get support directly from them as well. Solution overview This diagram shows the ROSA cluster deployed in multiple availability zones (AZs). ROSA cluster's master nodes, infrastructure nodes are in Red Hat's VPC, while the worker nodes are in a VPC in the customer's account. We'll create an FSxN file system within the same VPC and install the Trident provisioner in the ROSA cluster, allowing all the subnets of this VPC to connect to the file system.

Prerequisites ● AWS account ● A Red Hat account ● IAM user with appropriate permissions to create and access ROSA cluster ● AWS CLI ● ROSA CLI ● OpenShift command-line interface (oc) ● Helm 3 documentation ● A HCP ROSA cluster ● Access to Red Hat OpenShift web console Step 1: Provision FSx for NetApp ONTAP Create a multi-AZ FSx for NetApp ONTAP in the same VPC as the ROSA cluster. There are several ways to do this. We are showing the creation of FSxN using a CloudFormation (CFN) Stack

Run the command below by replacing the parameter values with your own values:

# cd rosa-fsx-netapp-ontap/fsx aws cloudformation create-stack \ --stack-name ROSA-FSXONTAP \ --template-body file://./FSxONTAP.yaml \ --region \ --parameters \ ParameterKey=Subnet1ID,ParameterValue=[subnet1_ID] \ ParameterKey=Subnet2ID,ParameterValue=[subnet2_ID] \ ParameterKey=myVpc,ParameterValue=[VPC_ID] \ ParameterKey=FSxONTAPRouteTable,ParameterValue=[routetable1_ID,routetable2_ID] \ ParameterKey=FileSystemName,ParameterValue=ROSA-myFSxONTAP \ ParameterKey=ThroughputCapacity,ParameterValue=1024 \ ParameterKey=FSxAllowedCIDR,ParameterValue=[your_allowed_CIDR] \ ParameterKey=FsxAdminPassword,ParameterValue=[Define Admin password] \ ParameterKey=SvmAdminPassword,ParameterValue=[Define SVM password] \ --capabilities CAPABILITY_NAMED_IAM Where : region-name: same as the region where the ROSA cluster is deployed subnet1_ID : id of the Preferred subnet for FSxN subnet2_ID: id of the Standby subnet for FSxN VPC_ID: id of the VPC where the ROSA cluster is deployed routetable1_ID, routetable2_ID: ids of the route tables associated with the subnets chosen above your_allowed_CIDR: allowed CIDR range for the FSx for ONTAP security groups ingress rules to control access. You can use 0.0.0.0/0 or any appropriate CIDR to allow all traffic to access the specific ports of FSx for ONTAP. Define Admin password: A password to login to FSxN Define SVM password: A password to login to SVM that will be created. Verify that your file system and storage virtual machine (SVM) have been created using the Amazon FSx console.

3. Install Trident CSI driver for the ROSA cluster a. Install Trident using the Trident certified Operator in the operator hub. For additional methods of installing Trident, refer to the Trident documentation. Ensure that all Trident pods are running after the installation is successful.

Install the operator in the trident namespace. Once the operator is installed, click on view operator. Now install the trident orchestrator by clicking on create instance.

Go to the yaml view and update the nodePrep parameter to include iscsi.

Once the orchestrator status changes to installed, ensure that all trident pods are in the running state.

Now you can log back into the ROSA worker nodes and verify that iscsid and multipathd are running and that the multipath.conf file has the required entries. [root@localhost fsx]# oc debug node/ip-10-0-0-196.us-west-2.compute.internal Starting pod/ip-10-0-0-196us-west-2computeinternal-debug-tv7vw … To use host binaries, run `chroot /host` Pod IP: 10.0.0.196 If you don't see a command prompt, try pressing enter. sh-5.1# sh-5.1# sh-5.1# chroot /host sh-5.1# sh-5.1# systemctl status iscsid ● iscsid.service – Open-iSCSI Loaded: loaded (/usr/lib/systemd/system/iscsid.service; enabled; preset: disabled) Active: active (running) since Wed 2025-08-06 20:31:37 UTC; 5min ago TriggeredBy: ● iscsid.socket Docs: man:iscsid(8) man:iscsiuio(8) man:iscsiadm(8) Main PID: 621624 (iscsid) Status: "Ready to process requests" Tasks: 1 (limit: 99844) Memory: 4.1M CPU: 5ms CGroup: /system.slice/iscsid.service └─621624 /usr/sbin/iscsid -f Aug 06 20:31:37 ip-10-0-0-196 systemd[1]: Starting Open-iSCSI… Aug 06 20:31:37 ip-10-0-0-196 systemd[1]: Started Open-iSCSI. sh-5.1# systemctl status multipathd ● multipathd.service – Device-Mapper Multipath Device Controller Loaded: loaded (/usr/lib/systemd/system/multipathd.service; enabled; preset: enabled) Active: active (running) since Wed 2025-08-06 20:31:38 UTC; 5min ago TriggeredBy: ○ multipathd.socket Process: 621707 ExecStartPre=/sbin/modprobe -a scsi_dh_alua scsi_dh_emc scsi_dh_rdac dm-multipath (code=exited, stat> Process: 621708 ExecStartPre=/sbin/multipath -A (code=exited, status=0/SUCCESS) Main PID: 621709 (multipathd) Status: "up" Tasks: 7 Memory: 18.9M CPU: 37ms CGroup: /system.slice/multipathd.service └─621709 /sbin/multipathd -d -s Aug 06 20:31:38 ip-10-0-0-196 systemd[1]: Starting Device-Mapper Multipath Device Controller… Aug 06 20:31:38 ip-10-0-0-196 multipathd[621709]: ——–start up——– Aug 06 20:31:38 ip-10-0-0-196 multipathd[621709]: read /etc/multipath.conf Aug 06 20:31:38 ip-10-0-0-196 multipathd[621709]: path checkers start up Aug 06 20:31:38 ip-10-0-0-196 systemd[1]: Started Device-Mapper Multipath Device Controller. sh-5.1# sh-5.1# sh-5.1# cat /etc/multipath.conf defaults { find_multipaths no } blacklist { device { vendor .* product .* } } blacklist_exceptions { device { vendor NETAPP product LUN } }

Configure the Trident CSI backend to use FSx for NetApp ONTAP (ONTAP SAN for iSCSI)

The Trident back-end configuration tells Trident how to communicate with the storage system (in this case, FSxN). For creating the backend, we will provide the credentials of the Storage Virtual machine to connect to, along with the cluster management lif and the SVM to use for storage provisioning. We will use the ontap-san driver to provision storage volumes in FSxN file system.

Create the backend object using the command shown and the following yaml. #cat tbc-fsx-san.yaml

apiVersion: v1 kind: Secret metadata: name: tbc-fsx-san-secret type: Opaque stringData: username: fsxadmin password: &lt;value provided for Define SVM password as a parameter to the Cloud Formation Stack&gt; --- apiVersion: trident.netapp.io/v1 kind: TridentBackendConfig metadata: name: tbc-fsx-san spec: version: 1 storageDriverName: ontap-san managementLIF: &lt;management lif of the file system in AWS&gt; backendName: tbc-fsx-san svm: &lt;SVM name that is created in the file system&gt; defaults: storagePrefix: demo nameTemplate: "{{ .config.StoragePrefix }}_{{ .volume.Namespace }}_{{ .volume.RequestName }}" credentials: name: tbc-fsx-san-secret YAML

# oc apply -f tbc-fsx-san.yaml -n trident 2. Verify backend object has been created and Phase is showing Bound and Status is Success [ec2-user@ip-10-0-128-119 ~]$ oc create -f tbc-fsx-san.yaml -n trident secret/tbc-fsx-san-secret created tridentbackendconfig.trident.netapp.io/tbc-fsx-san created [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ oc get tbc -n trident NAME BACKEND NAME BACKEND UUID PHASE STATUS tbc-fsx-san tbc-fsx-san 10d013ab-b291-4a0c-91fa-6c76eddf554e Bound Success [ec2-user@ip-10-0-128-119 ~]$ 3. Create Storage Class for iSCSI Now that the Trident backend is configured, you can create a Kubernetes storage class to use the backend. Storage class is a resource object made available to the cluster. It describes and classifies the type of storage that you can request for an application. # cat sc-fsx-san.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: sc-fsx-san provisioner: csi.trident.netapp.io parameters: backendType: "ontap-san" media: "ssd" provisioningType: "thin" fsType: ext4 snapshots: "true" storagePools: "tbc-fsx-san:.*" allowVolumeExpansion: true #oc create -f sc-fsx-san.yaml 4. Verify storage class is created [ec2-user@ip-10-0-128-119 ~]$ oc create -f sc-fsx-san.yaml storageclass.storage.k8s.io/sc-fsx-san created [ec2-user@ip-10-0-128-119 ~]$

[ec2-user@ip-10-0-128-119 ~]$ oc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE gp2-csi ebs.csi.aws.com Delete WaitForFirstConsumer true 21h gp3-csi (default) ebs.csi.aws.com Delete WaitForFirstConsumer true 21h sc-fsx-san csi.trident.netapp.io Delete Immediate true 13s [ec2-user@ip-10-0-128-119 ~]$ 5. Create a Snapshot class in Trident so that CSI snapshots can be taken # cat snapshotclass.yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: trident-snapshotclass driver: csi.trident.netapp.io deletionPolicy: Retain # oc create -f snapshotclass.yaml [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ oc create -f snapshotclass.yaml volumesnapshotclass.snapshot.storage.k8s.io/trident-snapshotclass created [ec2-user@ip-10-0-128-119 ~]$ [ec2-user@ip-10-0-128-119 ~]$ oc get volumeSnapshotClass NAME DRIVER DELETIONPOLICY AGE csi-aws-vsc ebs.csi.aws.com Delete 22h trident-snapshotclass csi.trident.netapp.io Retain 18s [ec2-user@ip-10-0-128-119 ~]$ This completes the installation of Trident CSI driver and its connectivity to FSxN file system using iSCSI. Using ISCSI storage for container apps on ROSA

Deploying a Postgresql application using iSCSI storage class Use the following yaml file to deploy postgresql app

#cat postgres-san.yaml apiVersion: apps/v1 kind: Deployment metadata: name: postgres spec: replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:14 env: - name: POSTGRES_USER #value: "myuser" value: "admin" - name: POSTGRES_PASSWORD #value: "mypassword" value: "adminpass" - name: POSTGRES_DB value: "mydb" - name: PGDATA value: "/var/lib/postgresql/data/pgdata" ports: - containerPort: 5432 volumeMounts: - name: postgres-storage mountPath: /var/lib/postgresql/data volumes: - name: postgres-storage persistentVolumeClaim: claimName: postgres-pvc --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgres-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: sc-fsx-san --- apiVersion: v1 kind: Service metadata: name: postgres spec: selector: app: postgres ports: - protocol: TCP port: 5432 targetPort: 5432 type: ClusterIP #oc create namespace postgres-san root@localhost HAFSX]# oc create -f postgres-san.yaml -n postgres-san deployment.apps/postgres created persistentvolumeclaim/postgres-pvc created service/postgres createdYAML b. Verify that the application pod is running. Verify and a PVC and PV are created for the application. Note that the storage class for the PVC uses the san storage class previously created using iSCSI. c. Verify that iSCSi sessions are created in the node where the pod runs. [ec2-user@ip-10-0-128-119 ~]$ oc debug node/ip-10-0-1-192.us-west-2.compute.internal Starting pod/ip-10-0-1-192us-west-2computeinternal-debug-4j8gg … To use host binaries, run `chroot /host` Pod IP: 10.0.1.192 If you don't see a command prompt, try pressing enter. sh-5.1# chroot /host sh-5.1# iscsiadm -m session tcp: [1] 10.0.0.107:3260,1028 iqn.1992-08.com.netapp:sn.5cdf7ad172f811f0883cc908a40ebab0:vs.3 (non-flash) tcp: [2] 10.0.2.18:3260,1029 iqn.1992-08.com.netapp:sn.5cdf7ad172f811f0883cc908a40ebab0:vs.3 (non-flash) sh-5.1#

d. Verify that a lun is created Verify that a lun is created on the volume in FSxN for this application and the lun is mapped. You can log into the FSxN CLI using fsxadmin and the password you previously created. FsxId0d3560ad60fbad076::> volume show -vserver fsx Vserver Volume Aggregate State Type Size Available Used% ——— ———— ———— ———- —- ———- ———- —– fsx fsx_root aggr1 online RW 1GB 972.2MB 0% fsx trident_postgres_san_postgres_pvc_a56f2 aggr1 online RW 5.50GB 5.45GB 0% fsx vol1 aggr1 online RW 1TB 860.2GB 0% 3 entries were displayed. FsxId0d3560ad60fbad076::> lun show -volume trident_postgres_san_postgres_pvc_a56f2 -vserver fsx Vserver Path State Mapped Type Size ——— ——————————- ——- ——– ——– ——– fsx /vol/trident_postgres_san_postgres_pvc_a56f2/lun0 online mapped linux 5GB FsxId0d3560ad60fbad076::> igroup show -vserver fsx Vserver Igroup Protocol OS Type Initiators ——— ———— ——– ——– ———————————— fsx ip-10-0-1-192.us-west-2.compute.internal-239f6c7b-d5da-4c33-94a5-65d13c0caaeb iscsi linux iqn.1994-05.com.redhat:cae290244627 Using ISCSI storage for VMs on OpenShift Virtualization in ROSA 1. Verify you have baremetal nodes as worker nodes in the cluster. To be able to create VMs, you need to have bare metal nodes on the ROSA cluster. 2. Install OpenShift Virtualization using the Operator You can install OpenShift Virtualization using the OpenShift Virtualization Operator in the Operator hub. Once it is installed and configured, Virtualization will be populated in the UI of the OpenShift Console.

3. Deploy a VM using iSCSI storage class Click on Create VirtualMachine and select From template. Select Fedora VM. You can choose any OS that has source available. 4. Customize the VM Customize the VM to provide the storage class for the boot disk and create additional disks with the selected storage class. Click on Customize VirtualMachine.

5. Click on the Disks tab and click on Edit for the root disk

6. Ensure you have selected sc-fsx-san for storage class. Select Shared Access (RWX) for Access mode and Select Block for Volume Mode. Trident Supports RWX Access mode for iSCSI storage in Volume Mode Block. This setting is a requirement for the PVC of the disks so that you can perform live migration of VMs. Live migration is migration of a VM from one worker node to another for which RWX access mode is required and Trident supports it for iSCSI in Block Volume Mode.

Note: If sc-fsx-san was set as the default storage in the cluster, then this storage class will automatically be picked. 8. Add another disk Click Add disk and select empty disk (since this is just an example) and ensure that sc-fsx-san disk is chosen, Volume Mode Block and Access Mode RWX are chosen. Click Save and the Click Create VirtualMachine. The VM comes to a running state.

9. Check the VM pods, PVCs. Verify that PVCs are created using iSCSI storage class and RWX Access modes. [root@localhost fsx]# oc get pods,pvc NAME READY STATUS RESTARTS AGE pod/virt-launcher-fedora-demo-vm-lr2rj 1/1 Running 0 111s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE persistentvolumeclaim/dv-fedora-demo-vm-disk-green-impala-85-cls8e4 Bound pvc-cfceae37-d159-46d9-8925-4e3bc35ebdc6 30Gi RWX sc-fsx-san 2m22s persistentvolumeclaim/dv-fedora-demo-vm-rootdisk-2uz76f Bound pvc-98fb0938-7530-4ec8-bf36-78e7101468f5 30Gi RWX sc-fsx-san 2m22s [root@localhost fsx]#

10. Verify that a LUN is created in each volume corresponding to the disk PVCs by logging into the FSxN CLI.

FsxId0d3560ad60fbad076::> volume show -vserver fsx Vserver Volume Aggregate State Type Size Available Used% ——— ———— ———— ———- —- ———- ———- —– fsx fsx_root aggr1 online RW 1GB 971.0MB 0% fsx trident_default_prime_cc24a6a2_86ca_4e0b_a467_e9fac9e391a9_cfcea aggr1 online RW 33GB 33.00GB 0% fsx trident_openshift_virtualization_os_images_prime_2a2f581f_87a3_4e7a_8d3a_3a3d54253796_8a3b8 aggr1 online RW 33GB 31.59GB 4% fsx trident_openshift_virtualization_os_images_prime_54ef0fcf_f60e_4561_b8e2_571d1cdebc06_eaf2e aggr1 online RW 33GB 31.33GB 5% fsx trident_openshift_virtualization_os_images_prime_7e5eb814_68a6_4836_8321_b2d14d838ffc_91f5f aggr1 online RW 33GB 30.86GB 6% fsx trident_openshift_virtualization_os_images_prime_9cf9f23d_1ed8_4100_a05e_c73c938a2d78_bebc7 aggr1 online RW 33GB 30.68GB 7% fsx trident_openshift_virtualization_os_images_prime_a4d7925b_d11b_40d6_a052_1c9932e9c0c6_2e76b aggr1 online RW 33GB 31.07GB 5% fsx trident_openshift_virtualization_os_images_prime_e77ccf75_7798_43cf_85d6_f9f69f9eb200_5c85c aggr1 online RW 33GB 32.36GB 1% fsx trident_openshift_virtualization_os_images_tmp_pvc_50094ec5_587a_4de2_86a4_7f200f8e89b8_98fb0 aggr1 online RW 33GB 32.33GB 2% fsx trident_postgres_san_postgres_pvc_a56f2 aggr1 online RW 5.50GB 5.39GB 2% fsx vol1 aggr1 online RW 1TB 849.1GB 0% 11 entries were displayed. FsxId0d3560ad60fbad076::> lun show -volume trident_default_prime_cc24a6a2_86ca_4e0b_a467_e9fac9e391a9_cfcea Vserver Path State Mapped Type Size ——— ——————————- ——- ——– ——– ——– fsx /vol/trident_default_prime_cc24a6a2_86ca_4e0b_a467_e9fac9e391a9_cfcea/lun0 online mapped linux 30GB FsxId0d3560ad60fbad076::> lun show -volume trident_openshift_virtualization_os_images_tmp_pvc_50094ec5_587a_4de2_86a4_7f200f8e89b8_98fb0 Vserver Path State Mapped Type Size ——— ——————————- ——- ——– ——– ——– fsx /vol/trident_openshift_virtualization_os_images_tmp_pvc_50094ec5_587a_4de2_86a4_7f200f8e89b8_98fb0/lun0 online mapped linux 30GB FsxId0d3560ad60fbad076::> FsxId0d3560ad60fbad076::> igroup show Vserver Igroup Protocol OS Type Initiators ——— ———— ——– ——– ———————————— fsx ip-10-0-1-192.us-west-2.compute.internal-239f6c7b-d5da-4c33-94a5-65d13c0caaeb iscsi linux iqn.1994-05.com.redhat:cae290244627 fsx ip-10-0-1-227.us-west-2.compute.internal-239f6c7b-d5da-4c33-94a5-65d13c0caaeb iscsi linux iqn.1994-05.com.redhat:45b335a3f4b 2 entries were displayed.

Conclusion In this blog, we successfully demonstrated how to integrate FSx for NetApp ONTAP as a shared file system with a ROSA cluster using a Hosted Control Plane, leveraging the NetApp Trident CSI driver for iSCSI storage. We illustrated how Trident release 25.02 streamlines the preparation of worker nodes by configuring iSCSI and multipathing for iSCSI on ONTAP storage. Our comprehensive, step-by-step guide detailed the configuration of the Trident backend and storage class for iSCSI, and how to utilize them to create containers and VMs. We emphasized that the ontap-san driver supports the RWX access mode in Block Volume Mode for iSCSI, making it ideal for VM disks in OpenShift virtualization and enabling live migration of VMs. For further information on Trident, please refer to the NetApp Trident documentation. Additionally, you can find more resources, including detailed guides and videos, in the Red Hat OpenShift with NetApp section under Containers in the NetApp Solutions documentation. To clean up the setup from this post, follow the instructions provided in the GitHub repository.

Ryan Niksch Ryan Niksch is a Partner Solutions Architect focusing on application platforms, hybrid application solutions, and modernization. Ryan has worn many hats in his life and has a passion for tinkering and a desire to leave everything he touches a little better than when he found it.

Banumathy Sundhar In my current role as a Technical Marketing Engineer and in my past role as a Technology Enablement Professional, I have carried out my responsibilities in various ways. I evangelize platforms and products, deep dive into technical areas (Kubernetes, OpenShift, AWS, Azure and Google Clouds), provide live or recorded demos, share information, educate and up-skill via my blogs, live and virtual multi-day sessions. I have provided technical validation of solutions for our customers with NetApp products integrated with OpenShift clusters in a hybrid cloud environment. In my previous role, I have developed and delivered a wide variety of training on technical topics with hands-on lab-intensive content, technical presentations, certification prep sessions and lab sessions at Conferences and Instructor Summits.

Mayur Shetty Mayur Shetty is a Senior Solution Architect within Red Hat's Global Partners and Alliances organization. He has been with Red Hat for four years, where he was also part of the OpenStack Tiger Team. He previously worked as a Senior Solutions Architect at Seagate Technology driving solutions with OpenStack Swift, Ceph, and other Object Storage software. Mayur also led ISV Engineering at IBM creating solutions around Oracle database, and IBM Systems and Storage. He has been in the industry for almost 20 years, and has worked on Sun Cluster software, and the ISV engineering teams at Sun Microsystems.

Decoupling the compute and data layers with AWS MGN, AWS DataSync and Amazon FSx

As more organizations take their first steps in the cloud migration journey, we're seeing them facing a common on-premises challenge: applications that are tightly coupled with their datasets. This means that while their applications (running on physical or virtual machines) are ready to be migrated to the cloud using the rehost ("lift-and-shift") strategy, there may be massive, multi-terabyte block storage volumes attached to them that create migration bottlenecks. This traditional "lift-and-shift" of both compute and storage at once can be slow, risky, and complex. This challenge is especially acute in data-intensive industries. Consider a financial services firm that is running an on-premises data analytics application on a powerful server. Imagine a case where the C: drive hosts the operating system, database engine (e.g. SQL Server) or an application, while the business-critical data is spread across several large volumes: a 4TB D: drive holds active financial transaction data, a 6TB E: drive archives historical records, and an 8TB F: drive contains shared data marts for business intelligence reporting. While functional, this setup creates significant challenges for scaling capacity, collaboration, or ensuring business continuity—all because the data is locally stored on that single on-premises server. To overcome this, in this blog post we'll share with you our recommended strategy for decoupling these layers by first migrating your on-premises data volumes, those large D:, E:, and F: drives, to a fully managed shared storage service in the cloud, such as Amazon FSx for NetApp ONTAP, using AWS DataSync. AWS DataSync is an online data transfer service that simplifies data migration and helps you quickly, easily, and securely, transfer your file or object data to, from, and between AWS storage services. This strategy preserves file permissions and metadata while transforming your rigid, hardware-dependent storage into a flexible, enterprise-grade managed service. Additionally, by relocating your data to a managed file system, you gain access to features such as automated backups and disaster recovery, all with minimal disruption to the running application. In this blog post, we'll walk you through executing this strategy in the form of a scenario, providing you with a practical blueprint for accelerating your own cloud journey. We'll demonstrate how to prepare the on-premises server for data migration, configure a DataSync task, and perform the cutover from using the local storage drives to a remote mounted file system. We'll also show you how to streamline the compute migration by using AWS Application Migration Service (AWS MGN), and selectively replicating only the OS and application volumes. By following this decoupling strategy, when you're ready to complete the server migration using AWS MGN, your new Amazon EC2 instance can mount the exact same cloud-based shared file system that stores your decoupled datasets. Prerequisites While familiarity with these AWS services and concepts is beneficial, this post focuses on the strategic approach and best practices for a successful migration, rather than detailed technical setup. Before proceeding, ensure you have the following prerequisites in place:

An AWS account with the necessary AWS Identity and Access Management permissions required for accessing these services AWS MGN initialized in your target AWS Region. An Amazon FSx file system (e.g. for Windows File Server or for NetApp ONTAP) provisioned in your target VPC. This will serve as the destination for your data. Established network connectivity between your on-premises environment and your AWS VPC, and firewall rules in place to allow connectivity with AWS services

Important Note: This migration pattern is designed for file-level access scenarios only. If your application requires block-level disk access (such as for Windows Server Failover Clustering Quorum), this decoupling strategy is not suitable, and you should maintain the traditional approach of migrating compute and storage together. Walkthrough: The Decoupling process Step 1: Expose On-Premises Data Volumes as Shares Now that we understand the big picture, let's look at the first critical step on-premises: making the data available for migration. Remember those massive D:, E:, and F: drives that are attached directly to your single server? The beauty of this storage-first approach is that we don't need to physically move them. Instead, we'll simply "liberate" the data from the local server's direct control by exposing it to the network.

Figure 1: List the physical volumes attached to your on-premises server. Think of it this way: the data is already stored in a file system, just like a standard network drive. By enabling simple Windows File Sharing, you can turn those local volumes into network shares. It's a quick process—you'll create a share for each volume (e.g. \your-server\data-D, \your-server\data-E, \your-server\data-F) and grant a specific service account read-only permissions. This read-only access is a best practice that ensures the migration tool can't accidentally modify your source data.

Figure 2: Create SMB shares for the non-bootable drives. Once those shares are active, they become the "source" for your migration. The DataSync agent, which we'll cover in the next section, can now connect to these shares and begin its work, treating them just like any other network-accessible file system. With this one simple step, you've taken the first stride toward decoupling your storage and compute, opening the door for a smooth, non-disruptive migration. Step 2: Provision your cloud storage with Amazon FSx With your on-premises data now exposed and ready to be migrated, the next step is to create its new home in the cloud. This is where Amazon FSx for NetApp ONTAP comes in. You'll create a file system that is not only highly available and durable, but also fully managed. This new storage will have all the enterprise-grade features you're accustomed to, like data deduplication and compression to save costs, and snapshots for quick, point-in-time restores. Most importantly, it will provide the same familiar user experience you're used to on-premises. You will create a new SMB share (e.g. \fsx-ontap-endpoint\app-A) that will be the destination for our application's on-premises volumes. FSx for NetApp ONTAP seamlessly integrates with your existing Microsoft Active Directory infrastructure, allowing you to maintain security for your data at the file and folder levels. The beauty of this step is that you're not just replicating data; you're modernizing your entire storage layer. This new cloud-native storage is fully decoupled from any single compute instance, which means it can be accessed by your existing on-premises team during the migration and will be ready for your new cloud-based Amazon EC2 instances when you're ready for the cutover. Step 3: Deploy the AWS DataSync Agent With our data now accessible via SMB shares, the next step is to introduce the key player in our migration: the AWS DataSync agent. Think of the DataSync agent as a smart, automated delivery driver. The agent connects to your on-premises shares, grabs the data, and securely and efficiently delivers it to its new home in the cloud. It's the essential bridge between your legacy on-premises environment and your new AWS infrastructure. Make sure you architect and plan accordingly, so that the agent is deployed to a network that has connectivity to the servers where data is located, as well as the DataSync backend service. DataSync agents may be deployed as a Virtual Machine on supported platforms, as well as an Amazon EC2 Instance. Step 4: Create and Run the DataSync Task With all the pieces in place—your on-premises data exposed as network shares and your new FSx for NetApp ONTAP file system ready in the cloud—the final step is to orchestrate the migration itself. In the AWS console, you'll configure the transfer locations – source SMB (or NFS, if you choose to follow this strategy with a Linux server) and target FSx for ONTAP, then create a DataSync task. This task is a data transfer job that brings the source and destination locations together and sets task options for moving file or object data between AWS storage services and on-premises systems, or other cloud providers. DataSync can be tuned to meet your needs by offering a number of powerful options here, including scheduling the task for a convenient time, filtering which files and folders to include, and configuring how it handles things like symbolic links and user permissions.

Figure 3: Monitoring the DataSync data copy task When you run the task for the first time, DataSync performs a full initial transfer of all your data. After that, you can schedule the task to run on a regular basis for periodic incremental loads. DataSync will identify and transfer only the files that have changed since the last run, ensuring your cloud storage stays up to date with minimal bandwidth usage. DataSync handles the heavy lifting, ensuring a fast, secure, and reliable migration with minimal impact on your running application. Step 5: Prepare for Cutover Before the final cutover, it's crucial to prepare your on-premises environment. Start by running a final incremental DataSync task to ensure all the latest changes have been synced to your FSx file system. Once that's complete, you'll schedule a maintenance window to take the on-premises application or database offline and unmount the original D:, E:, and F: drives from your local server. With those local volumes disconnected, the final step is to remount the new cloud file system. You'll now mount your Amazon FSx share at the same mount location or drive letter, so the data is available for your on-premises server the same way it was with a local drive. This seamless transition allows your application to start using the cloud-based storage without any changes to its configuration or file paths. Step 6: Validate and Resume the Application Once the file system is remounted, the final steps are to validate and resume the application. First, perform a quick check to ensure the application can correctly read and write data to the newly mounted cloud volumes. Once you validate and confirm everything is working as expected, you can bring the application back online, pointing it to the new, cloud-based storage. Step 7: Migrate or Modernize? After a successful data migration, the next logical step is to address the compute layer. Now that the data is decoupled and residing on a cloud-native service like Amazon FSx, you can follow an efficient approach for migrating your application server. AWS MGN is the ideal service for this. When installing the MGN replication agent on your on-premises server, you can choose to selectively replicate only the OS and application volumes. With the large data volumes that were previously attached locally already migrated, this is a crucial step—you'll use a flag in the command-line installer to configure the agent to only migrate local volumes that contain the operating system and the application. In the below screenshot, the –drives command instructs MGN to only replicate the C: drive, significantly reducing the time it takes for replicating your server's data to AWS.

Figure 4: Use the –drives flag on Windows or –devices on Linux to specify the bootable drive for replication by MGN This storage-first approach gives you a powerful opportunity to modernize your compute architecture. By decoupling your application from its on-premises storage, you've paved the way to paths beyond a simple lift-and-shift migration to an EC2 instance, opening the door to explore modern, containerized platforms. For example, you can start working on modernizing your Windows-based application using AWS Transform for .NET workloads and deploy your application on Amazon EKS (Elastic Kubernetes Service), leveraging the persistent storage with native support for containerized workloads provided by FSx for ONTAP. This allows you to scale and manage your application with far greater flexibility than a traditional server. Conclusion In this post, we walked you through a strategic pattern that takes you beyond a simple lift-and-shift migration and helps you take your first step in modernizing your monolithic applications. By using AWS DataSync and AWS Application Migration Service together, you can decouple large data volumes from your application servers during a rehost migration. Consider adapting this approach to meet your migration timelines while simultaneously unlocking the benefits of cloud: improved agility, independent scalability, and enhanced resilience for your architecture.

Dor Shiloni Dor is an AWS Senior Specialist Solution Architect, a member of the EMEA Migration and Modernization team. He is passionate about working with customers to understand their needs, leveraging the cloud to architect innovative solutions and driving business value for them.

David Stenson David is a Migration and Modernization Solution Architect at AWS. In his role, he provides guidance to customers on their migration and modernization journey. He lives in Dublin, Ireland and in his spare time, likes to read and spend time with his family.

Improve Cost Visibility and Observability with AWS Cost Categories – Part 1: Fundamentals and Basic Grouping Techniques

As cloud environments continue to expand and evolve, deriving value from their cloud investments becomes paramount for businesses operating on AWS. The ability to group and analyze costs across resources and accounts is crucial for gaining visibility, identifying optimization opportunities, and making data-driven decisions. Organizations aim to accurately allocate and track cloud costs across different business units, projects, and environments to improve budgeting, enable effective chargeback processes, and make informed optimization decisions. With AWS Cost Categories, a free feature, you can create rules to flexibly group and visualize cost using various dimensions such as account, charge type, service and even other Cost Categories. In this first part of our two-part series, we will explore the fundamentals of AWS Cost Categories and demonstrate how they can transform your cost management approach. We'll cover:

Cost Categories overview and key benefits How to flexibly group and visualize costs using 'ANY' or 'ALL' operators Practical implementation examples including:

Cost grouping by region Cost grouping across multiple dimensions Splitting charges within AWS Cost Categories

Through these examples, you will learn how to implement AWS Cost Categories to gain better control and visibility over your cloud spending, enabling more accurate cost allocation, improved financial planning, and strategic decision-making. AWS Cost Categories Overview AWS Cost Categories, a feature within the AWS Cloud Financial Management product suite, allows you to create customized categories to map and organize your cloud spending based on your organizational structure and business needs. This empowers organizations to effectively manage their cloud finances, enabling accurate cost allocation, streamlined chargeback processes, and data-driven optimization decisions. This feature addresses key business needs such as tracking expenses across complex organizational structures, attributing costs to specific projects or departments, and gaining granular insights for budget planning and cost control. Using Cost Categories, you can map your cost and usage information based on defined rules using various dimensions, such as Linked Account and Cost allocation Tags. Once Cost Categories are set up and enabled, you will be able to view your cost and usage information by these Cost Categories starting at the beginning of the month or retroactively for up to 12 months in AWS Cost Explorer, AWS Budgets, Data Exports, AWS Billing Conductor, and AWS Cost Anomaly Detection. Prerequisites To follow along with this post, make sure you have the following prerequisites in place:

An AWS account. If you don't have one, follow the instructions here to create and activate it. A fundamental grasp of creating Cost Categories. Activated Cost allocation tags. Once activated, it typically takes up to 24 hours for these tags to become available for use. Appropriate IAM permissions to access and manage AWS Cost Categories (ce:CreateCostCategoryDefinition, ce:DeleteCostCategoryDefinition, ce:DescribeCostCategoryDefinition, ce:ListCostCategoryDefinitions, ce:UpdateCostCategoryDefinition).

How to flexibly group and visualize cost using Cost Categories Before diving into specific examples, let us explore various ways to group and analyze your AWS costs using Cost Categories. Throughout this guide, we will use regional cost tracking as our primary example to demonstrate these grouping capabilities. Regional cost tracking is a critical requirement for organizations operating global cloud infrastructures. Many enterprises need to monitor and analyze their AWS spending across different geographical locations to support regional business units, comply with local regulations, and optimize resource allocation. By selecting the 'Region' dimension, you can create Cost Categories that align with your global AWS footprint. This enables granular visibility into spending patterns across your operational regions, such as US East (Virginia) (us-east-1), US East (Ohio) (us-east-2), etc. With this level of regional cost breakdown, you can identify opportunities for optimization, such as resource allocation, and disaster recovery planning. Tracking costs by regions, as shown in Figure 1, helps identify where your highest expenses occur and enables strategic decisions about resource placement and regional pricing advantages.

Figure 1. Cost Explorer view of cost breakdown with group by via dimension 'Region'

Beyond granular region-level visibility, you can consolidate and categorize your cloud costs based on geographic regions that align with your organization's structure and reporting needs. For example, if your business operates across North America, Europe, Asia-Pacific, and Latin America, you can create custom Cost Categories to group your AWS spending accordingly. Instead of viewing costs broken down by individual AWS Regions like us-east-1 or eu-central-1, you can define broader categories such as 'Cost in North America', 'Cost in EMEA', 'Cost in APAC', and 'Cost in LATAM'. This allows you to gain a higher-level, regional perspective on your cloud expenditure, which may be more meaningful for your finance, operations, and executive teams. To accomplish this, you can leverage the 'Region' dimension in the AWS Cost Categories feature. Simply create a new Cost Category rule, and under the 'Region' dimension, specify the appropriate values that map to your preferred geographic groupings. Cost Grouping by Region To create a Cost Category with Region dimension:

Navigate to AWS Cost Categories by searching 'Cost Categories' in the AWS Console search bar, or access it through the Billing and Cost Management Console's left navigation menu. Click 'Create Cost Category' on the AWS Cost Categories console. Group your costs

Name your 'Cost Category' as 'Cost By AWS Regions' and click 'add rule' to group your cost. Select your rule type as 'Manually define how to group costs'. Select the dimension as 'Region' to group the cost by, select the matching operator as 'Is' and select the value for the dimension. Before creating the rule by clicking the 'Create rule' button, provide a name such as 'Cost in NAMER' for the cost group.

Figure 2 below illustrates how you can define these regional Cost Categories:

Note: 24 hours after the creation of your new Cost Categories, the status of your newly created Cost Categories will be updated from 'processing' to 'applied' on the home page of 'Cost Categories.' When you select the 'AWS Regions' Cost Category, you will see a detailed cost breakdown for each region, as shown in Figure 3. You may notice an 'uncategorized' segment in the report, which represents costs that are not tied to any specific AWS Region. By clicking on this 'uncategorized' section, you will be taken to a filtered view in Cost Explorer. This view helps you identify which services are generating these uncategorized costs, and you can apply additional filters, such as linked accounts, to determine who owns these expenses. It is important to note that not all AWS costs can be categorized by region due to the nature of certain global services or specialized pricing components. Understanding these uncategorized costs through Cost Explorer helps you identify any potential gaps in your cost allocation strategy and maintain comprehensive cost visibility across your AWS environment.

'ANY' or 'ALL' Operators: The rule creation process includes two powerful operators: 'ANY' and 'ALL.' The 'ANY' operator enables you to create more inclusive rules by allowing costs to be grouped based on multiple conditions within a single dimension. Conversely, the 'ALL' operator enables you to create more specific rules by requiring costs to match all the specified conditions. For instance, you want to consolidate all costs related to your 'North America' operations. You can use the 'Region' dimension and apply the 'ANY' operator to include multiple region values such as 'us-east-1', 'us-east-2', 'us-west-1', and 'us-west-2'. This approach ensures that your 'North America' Cost Category encompasses expenses from all these regions, offering a comprehensive overview of your spending in this geographic area. On the other hand, the 'ALL' operator allows you to create more specific rules by requiring costs to match all the specified conditions. The flexibility offered by the 'ANY' and 'ALL' operators empowers you to define Cost Categories that align precisely with your organization's needs, whether you are looking for broader, more inclusive groupings or narrowly targeted, specific cost tracking. Grouping cost with Multiple Dimensions We can also use multiple dimensions inside the Cost Category definition to group and visualize cost. In the example below, you can visualize the cost of each of their accounts by region by adding both dimensions in a single rule and choosing the appropriate cost values. To accomplish this, you can leverage the existing Cost Category 'Cost by Region' illustrated above as one of the dimensions in the AWS Cost Categories feature. To create a new multiple dimensions Cost Category 'Account spend by region' to group cost of each of their accounts by region, follow these steps:

Click 'Create Cost Category' on the AWS Cost Categories console. Grouping your costs Name your 'Cost Category' as 'Account spend by region' and click 'add rule' to group your cost. Select your rule type as Regular rule to Manually define how to group costs Add the 'Account' dimension and select the appropriate values. Click 'Add condition' to add another dimension to the rule, Select the existing Cost Category 'Cost by Region'. Provide a name such as "AccountA in NAMER" for the cost group, and then click the "Create rule" button.

By using multiple dimensions in a single Cost Category rule, as shown in Figure 4, you can effectively visualize the cost of each account by region.

Configure the Cost Category by adding individual rules that map specific AWS account and region combinations as shown in Figure 5, ensuring accurate cost allocation across geographical segments like NAMER and APAC based on AWS regional spend patterns

Splitting charges within AWS Cost Categories We can use split charges to allocate costs across multiple Cost Category values. This is useful when a cost cannot be directly attributed to a single Cost Category bucket, definition, or owner. Common examples include data transfer costs, API metrics with aggregated costs, and support costs. You can define split charge when you create or edit your Cost Categories and allocate these charges using the following allocation methods – Proportional, Fixed or Even Split. To split charges, follow these steps:

For existing Cost Categories, click 'Split charges' tab, and then 'Create Split charge'. Select the 'Source value' that you want to split and name your uncategorized costs. Chose the 'Target value' – Choose one or more target Cost Category values to allocate the source Cost Category value to. Chose any of the below appropriate 'Charge Allocation Method'

Proportional – Allocates costs using proportionally weighted target values, ideal for organizations that want cost distribution to reflect actual resource consumption. For example, if Project A uses 60% of the compute resources and Project B uses 40%, the AWS costs would be split accordingly using this method, ensuring that projects consuming more resources bear a larger portion of the costs. Fixed – To Allocate your costs by defining a percentage for each of your target values. For Organizations with strict budgetary agreements or cost-sharing arrangements. For example, if a consulting firm has three clients with predefined agreements, Client X could be allocated 50%, Client Y 30%, and Client Z 20% of the total AWS costs, regardless of actual usage. This method ensures compliance with contractual obligations and simplifies billing, even when actual resource consumption varies. Even Split – Allocate your costs evenly across your target values. For Organizations where resources are shared equally or when detailed usage tracking is not necessary. For example, if a university research department has five teams sharing AWS resources, each team would be responsible for 20% of the total AWS costs, regardless of their individual usage patterns. This method promotes collaboration and simplifies budgeting by ensuring equal cost distribution among all participating groups.

Figure 6, Cost Category – Split Charges and Figure 7, Cost Category – Create Split Charges below illustrate how you can create split charges on an existing Cost Category and allocate the uncategorized spend proportionally (using the allocation method) to existing Cost Category values:

Figure 8 below shows the Uncategorized spend 'Split-Charge' is Proportionally split across the other values in the Cost Category. In this example, there was an uncategorized cost of $7.74 that has been proportionally split across the four regions. The 'Total Allocated Cost' metric includes both the raw cost for each region and its portion of the split uncategorized cost. This provides a more comprehensive view of costs, ensuring that all expenses, including those initially uncategorized, are appropriately allocated to each region.

Clean Up While AWS Cost Categories do not incur any charges, you may want to clean up test configurations to maintain organizational clarity in your cost reporting. To Delete Test Cost Categories: If you created 'Cost Categories' for testing or learning purposes, you can remove them to keep your cost reporting organized

Navigate to the AWS Cost Categories console. Select the test 'Cost Category' you want to remove. Choose 'Delete' from the Actions menu. Confirm deletion by typing the Cost Category name.

Cost Categories are a free feature and do not generate any AWS charges. Cost Category deletion cannot be undone. Historical cost data associated with deleted 'Cost Categories' will no longer be available for reporting. It may take up to 24 hours for Cost Category deletions to fully process. Deleted Cost Categories will still appear in historical reports for the time period they were active.

Conclusion In this first part of our series, we've explored how AWS Cost Categories can transform your approach to cloud financial management. We've covered the fundamentals of creating and configuring Cost Categories, using flexible operators to define grouping rules, and implementing practical examples for regional cost tracking, multi-dimensional analysis, and charge splitting. By implementing these foundational AWS Cost Categories techniques, organizations can achieve:

Enhanced visibility into spending patterns across different regions and accounts. More accurate cost allocation aligned with business structures. Flexible grouping capabilities that adapt to your unique organizational needs. Improved financial reporting with consistent categorization across all AWS cost management tools. These capabilities enable more informed decision-making, better budget planning, and clearer cost accountability across your organization. The ability to group and analyze costs in meaningful ways provides the foundation for a mature cloud financial management practice.

In part two of this series, we will build on these fundamentals by exploring advanced techniques for hierarchical ost categorization and programmatic management. We'll demonstrate how to create multi-level organizational structures within your Cost Categories and how to automate 'Cost Category' creation and management at scale using AWS APIs and JSON templates.

Karthikeyan KM KM is a Senior Technical Account Manager supporting Enterprise Users at AWS. With over 20 years of IT experience, he focuses on designing secure, reliable, and scalable solutions while ensuring operational excellence. He is passionate about helping customers accelerate their digital transformation journeys through efficient cloud architectures that align with their business objectives.

Bhaskar Mazumdar Bhaskar is a Senior Technical Account Manager with extensive experience helping organizations optimize their cloud infrastructure and accelerate their digital transformation journeys. He focuses on designing scalable, secure, and cost-effective architectures that align with business objectives.

Vivek Somi Vivek is a seasoned Technical Account Manager focused on security and cloud financial management at Amazon Web Services (AWS). He plays a pivotal role in supporting State and Local Government customers. He acts as a trusted advisor, guiding AWS customers through the complexities of AWS services to achieve their strategic outcomes.

A guide to sending international SMS with US toll-free numbers and AWS End User Messaging

AWS End User Messaging now supports international SMS capabilities for US toll-free numbers (TFNs). This new feature allows businesses to use a single US TFN to send SMS messages to over 150 countries, simplifying global outreach. It primarily benefits customers who need to send one-way transactional alerts—like one-time passwords (OTPs) or shipping notifications—and businesses that want to rapidly prototype and test their messaging strategy in new international markets without the overhead of procuring country-specific numbers. This guide will walk you through the pros and cons of this feature and show you how to enable it and when to use it versus traditional, country-specific sending methods. What are international US toll-free numbers? An International US toll-free Number is a standard US TFN that has been enabled with the capability to send SMS messages to destinations outside of the United States. This feature is backward compatible, meaning you can enable it on any new or existing US TFNs in your account. How to enable international sending There are three primary ways to enable this feature for your US toll-free numbers:

Enable international sending when registering a new number in the console. Enable international sending for an existing number in the console. Enable international sending for an existing number via the AWS CLI.

Let's discuss these in detail. 1. Enable international sending when registering a new number in the console To enable international sending, when registering a new number in the console, do the following steps.

From the AWS End User Messaging console, choose Manage SMS.

Under Configurations in the left navigation menu, choose Phone numbers and choose Request originator.

On the Select country screen, choose United States (US) as your destination country.

On the Define use case screen, configure the various options listed for your intended Messaging use case, and select Yes to enable International sending. Choose Next.

Under the Select originator type screen, choose Toll-free. Validate your Resource policy choices and choose Next.

On the Review and request screen, verify that the information you entered is correct and choose Request. Note that the US toll-free number registration requests can take approximately 15 business days to be approved.

For more information, see Request a phone number in AWS End User Messaging SMS. 2. Enable international sending for an existing number in the AWS console If you have already acquired a TFN, you can enable the international sending feature at any time.

From the AWS End User Messaging console, under Configurations in the left navigation menu, choose Phone numbers and choose an existing toll-free number.

3. Enable international sending for an existing number in the AWS CLI The update-phone-number command allows you to modify a phone number's capabilities, while the describe-phone-numbers command allows you to verify its status. To enable International Sending, use the --international-sending-enabled flag: aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --international-sending-enabled \ --region us-east-1 Bash Note: Replace "phone-a1b2c3d4e5f67890" with your actual phone number's ID To disable International Sending, use the --no-international-sending-enabled flag: aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --no-international-sending-enabled \ --region us-east-1Bash A successful command (for update-phone-number) returns the full JSON object for the phone number. Confirm the change by checking that the InternationalSendingEnabled value is true. { "PhoneNumberArn": "arn:aws:sms-voice:us-east-1:111122223333:phone-number/phone-a1b2c3d4e5f67890", "PhoneNumberId": "phone-a1b2c3d4e5f67890", "PhoneNumber": "+18005550199", "Status": "ACTIVE", "IsoCountryCode": "US", "MessageType": "TRANSACTIONAL", "NumberCapabilities": [ "SMS" ], "NumberType": "TOLL_FREE", "MonthlyLeasingPrice": "2.00", "TwoWayEnabled": true, "InternationalSendingEnabled": true, "CreatedTimestamp": "2025-08-15T10:30:00.123Z" } JSON Use the describe-phone-numbers command with your Phone Number ID to check its current configuration at any time. aws pinpoint-sms-voice-v2 describe-phone-numbers \ --phone-number-ids "phone-a1b2c3d4e5f67890" \ --region us-east-1Bash Benefits and Limitations This feature offers a powerful new way to reach a global audience, but it's important to understand where it shines and what its limitations are. Benefits (Advantages)

Global Reach with a Single Number: Send SMS to over 150 countries using a single, existing US TFN. Simplified Management: Avoid the operational overhead and cost of purchasing and managing a fleet of country-specific phone numbers. Rapid Prototyping and Testing: Quickly test messaging campaigns in new international markets before committing to the best practice approach of acquiring dedicated in-country numbers. Cost Optimization for One-Way Alerts: Provides a cost-effective method for sending high-volume, one-way transactional messages like OTPs, appointment reminders, and shipping notifications globally. Speed to Implement: toll-free number registration is currently the fastest way to get an originator in the US and begin sending SMS. Check here for current timelines.

Two-Way SMS is Limited to the US and Canada: Reliable, two-way SMS conversations are only supported for recipients in the United States and Canada. One-Way Only for All Other Countries: For all other destinations, this is a one-way only. Best-Effort Deliverability: Sending outside of the US and Canada is on a "best-effort" basis. The phone number that appears on the recipient's device may be replaced with a local number or Sender ID, which is why two-way messaging will not work for these destinations. For more details on maximizing delivery, please read A Guide to Optimizing SMS Delivery and Best Practices. Managed Opt-Out is Not Guaranteed Internationally: The automatic STOP reply functionality does not work for destinations outside of the US and Canada. For international recipients, you must provide an alternative opt-out method. Standard Throughput (3 MPS): International TFNs have a default throughput of 3 Message Parts Per Second (MPS). For high-volume, high-throughput campaigns, dedicated country-specific numbers (like short codes) are the recommended best practice.

Understanding the Cost The pricing for this feature is straightforward:

No Additional Monthly Fees: There is no extra charge to enable the international sending capability on your US TFN. You only pay the standard monthly lease for the number itself. Pay-Per-Use Messaging: You are billed for each outbound SMS message at the standard, per-message rate for the destination country.

For a complete and up-to-date list of prices by country, please visit the AWS End User Messaging Pricing page. When to use this vs. country-specific numbers Choosing the right tool depends on your use case. Here's a simple comparison:

Use Case Use International TFN? Use Country-Specific Number (Best Practice)?

Rapidly test a new market Yes. It's the fastest way to get started. No, this approach takes more time to set up.

Send one-way alerts (OTPs, notifications) Yes. It is a simple, cost-effective solution. Yes, but it's more complex if you need to send to many countries

Require two-way conversations Only for the US and Canada. Yes. This is the required approach for reliable two-way SMS in a specific country.

Guarantee your brand/number appears consistently No, plan for Sender ID to not be preserved, as this is not guaranteed internationally. Yes. This is the primary reason to use a dedicated in-country number.

Maximize deliverability for critical campaigns No, delivery is "best-effort." Yes. A local number provides the highest chance of successful delivery.

Considerations and next steps Once you have enabled your international sending over US toll-free numbers, you can enhance your messaging strategy by considering resilience, monitoring, and scalability. The following resources provide best practices for enhancing your sending.

Monitoring Delivery: To monitor delivery rates and patterns by country, you can use Configuration Sets to create event destinations. This allows you to stream SMS events (like DELIVERED or FAILED) to services like Amazon CloudWatch or Amazon Data Firehose for analysis. Building Resilience: For implementing robust delivery, including automatic retry strategies for failed messages, we recommend reading our guide: How to build resilient SMS delivery with AWS End User Messaging. Broader Global Strategy: For a deeper look at the strategic elements of a global SMS program, our post on How to Manage Global Sending of SMS with AWS End User Messaging provides valuable insights and includes a template for organizing use cases and selecting originators.

Conclusion International SMS for US toll-free numbers is a powerful strategic tool for businesses looking to simplify their global messaging. It excels at enabling rapid testing in new markets and efficiently delivering one-way transactional alerts across the globe from a single number. However, it is not a replacement for the best practice of using dedicated, in-country phone numbers when reliable two-way conversations and guaranteed branding are critical to your campaign's success. By understanding its benefits and limitations, you can strategically use this feature to get going quickly while planning a long-term move towards country-specific codes for your most important markets.

Brett Ezell Brett Ezell is a Solutions Architect for AWS End User Messaging and Amazon Simple Email Service. Outside of work, Brett spends his time collecting vinyl, comic books, catching live shows, and training at the gym.

Bruno Giorgini Bruno Giorgini is a Sr Solutions Architect specialized in AWS Communication Developer Services. When he is not crafting innovative solutions for clients, Bruno enjoys spending quality time with his family, exploring the scenic hiking trails. His passion for technology and its potential to drive business transformation keeps him motivated to deliver impactful solutions for organizations around the world.

Know Before You Go: An AWS Partner's Guide to re:Invent 2025

We're looking forward to bringing you AWS re:Invent 2025! The conference is December 1 – 5 in Las Vegas, with virtual opportunities to watch keynotes and Innovation Talks live. All of us at Amazon Web Services (AWS) are excited to connect with AWS Partners and AWS Marketplace Sellers from around the globe. Whether you're joining us in Las Vegas or virtually, you'll have access to groundbreaking announcements, technical sessions, and unparalleled networking opportunities that will help transform your business. To maximize your re:Invent experience, we compiled essential information for our AWS Partner and AWS Marketplace communities. Reserved seating is available for registered attendees to secure seats in our most popular sessions. Sign in to the attendee portal and visit the event catalog to choose your sessions. To help plan your re:Invent experience, download the AWS Events mobile app, available for both iOS and Android. Can't make it to Las Vegas? Register now for free to watch live keynotes and Innovation Talks. Attending re:Invent for the first time? Here are some tips to make the most of your experience:

Wear comfortable shoes. You may be walking an average of 10+ miles a day. Use the complimentary shuttle service between venues to save energy for networking and sessions. Check the re:Invent mobile app for real-time shuttle tracking and plan extra time during peak hours. Get 50% off voucher for an AWS Certification exam. All in-person re:Invent attendees will receive a 50% discount voucher valid for any AWS Certification exam. The voucher can be used for online-proctored or test center-based exams. Earn your AWS Certification prior to attending re:Invent and enjoy special recognition during the event, including access to the AWS Certified Lounge to recharge and network. Stay connected—bring chargers for your devices. Visit the AWS Partner Booth in the main Expo for charging stations and a comfortable space to connect with other Partners and AWS experts while your devices power up. Stay hydrated. Visit the Hydrate & Help stations throughout the Venetian campus. Each time you refill your water bottle, you're supporting a charity in their effort to build clean water systems around the world. Have fun! re:Invent isn't just about learning—it's about community, with multiple ways to join the action. Participate in the 5K, attend Bingo After Dark, get a commemorative tattoo, test your skills at the Sports Forum, challenge fellow developers to BuilderCards, and wrap up an incredible week at re:Play, the ultimate re:Invent party.

AWS Partner Experience at re:Invent This year's AWS Partner experience delivers targeted content and activities to help you build innovative solutions, drive successful business outcomes, and create new possibilities with AWS. You'll gain practical insights on how to apply AWS technologies to better serve your customers, while learning how the AWS Partner Network (APN) and AWS Marketplace can help you scale and transform your business. Don't miss our comprehensive lineup of Partner-focused sessions and technical tracks throughout the week, anchored by the must-attend Keynote with Ruba Borno on Wednesday, December 3. Visit us at the main Expo in the Venetian. At the APN Booth, discover strategies to build, market, sell, and grow your business with AWS. Stop by the AWS Marketplace Booth to explore software applications to help you innovate and the Partner Specialization Booth where you can connect with AWS-vetted experts across 12 critical use cases, from security to AI. Stop by for personalized AWS Partner Central support to make sure you're prepared to leverage new features. Take advantage of numerous networking opportunities throughout the week to build meaningful connections with cloud innovators from around the globe. For important details about health and safety protocols, registration information, and accessibility accommodations, review the re:Invent FAQs.

You're invited to join Ruba Borno, Vice President of AWS Worldwide Channels and Alliances, for the Partnership Advantage Keynote on Wednesday, December 3, from 3 PM to 4:30 PM PT at the Venetian. Join Dr. Ruba Borno and special guests as they unveil how customers are achieving extraordinary breakthroughs with the AWS Partner Network and the AWS Marketplace. As we enter a new agentic era, listen to first-hand experience of customers and partners on how to harness agents and generative AI to accelerate innovation, drive impact, and redefine what's possible. Hear how they're laying the foundational elements to make it all a reality—modernizing securely, scaling faster, and unlocking entirely new business outcomes.

AWS CEO Keynote with Matt Garman Join Matt Garman on Tuesday, December 2, from 8 AM to 10:30 AM PT as he shares his perspective on cloud transformation. Matt will highlight how AWS is shaping the future of cloud technology through continuous innovation. Through behind-the-scenes insights and customer success stories, he'll illustrate how AWS is reinventing what's possible in cloud computing to empower customers and partners with what they need to build a better future.

Innovation Talks Dive deep with AWS thought leaders in our Innovation Talks and learn about the latest cloud technologies. Discover how advancements in AI, analytics, industries, and other key areas can help push your business forward.

Join us for AWS-led breakout sessions, a cornerstone of the AWS Partner experience (PEX) at re:Invent. These lecture-style sessions, delivered by AWS experts, customers, and Partners, provide comprehensive insights to accelerate your AWS journey. If you're attending virtually, breakouts will be available on-demand after the conference. Within the PEX track, you'll also find AWS Marketplace sessions that share the latest features and techniques to sell your products and services. To further enrich your learning experience, we offer additional session formats:

Chalk Talks: Interactive sessions with small audiences featuring a short lecture delivered by an AWS expert, followed by in-depth Q&A discussion. Workshops: Two-hour hands-on sessions where you'll work in teams or individually to solve real-world problems using AWS services. Bring your laptop to build alongside AWS Partner experts. Builder Labs: Get real-time guidance from AWS experts while you work within your own AWS sandbox environment.

➤ Explore AWS Partner-focused (PEX) content in the event catalog. See PEX sessions listed by day below and reserve your seats! Monday, December 1

PEX308-R – Architecting Agentic AI for Supply Chain: Data-to-Decision Patterns PEX404-R – Building Production-Ready AI Applications with Amazon Bedrock PEX303 – Accelerating Development and DevSecOps with Amazon Bedrock & Amazon Q PEX202 – Agentic AI's new generation of Industry and Line of Business solutions PEX314 – AWS Marketplace seller workshop: Publish, update, and manage products PEX203 – Accelerating workload modernization using next generation agentic AI PEX318 – Building Customer Trust with AI Agents: Transform Your MSP Operations PEX305 – Automotive Supply Chain Optimization using AI PEX401-R1 – Deploying Intelligent Agent Systems with MCP PEX316-R1 – Build agents & tools for Bedrock AgentCore and sell in AWS Marketplace

PEX401-R – Deploying Intelligent Agent Systems with MCP PEX319 – Architecting Compliant GenAI Systems for Healthcare Workflows PEX205 – From automation to autonomy: How AWS Partners drive SMB AI evolution at scale PEX209 – AWS Marketplace growth through product-led growth and international expansion PEX309 – Breakdown to Breakthrough: Crafting AI-Powered Smart Machine Support PEX106 – Maximizing customer outcomes: AWS Partner Led Customer Success PEX308-R1 – Architecting Agentic AI for Supply Chain: Data-to-Decision Patterns PEX208 – Transform digital experience: Accelerate AWS Partner & customer success PEX405-R – AI That Remembers: Building Memory Systems for AI Agents PEX204 – Go-to-market in today's political climate: a policy and AWS Marketplace overview PEX206 – Greenfield unlocked: Best practices for expanding new customer engagement PEX304 – Migrate and Modernize Legacy Workflows to Intelligent Agents PEX207 – AI-powered co-selling: How AWS and partners accelerate growth together

PEX316-R –Build agents & tools for Bedrock AgentCore and sell in AWS Marketplace PEX108 – AWS Generative AI Innovation Center driving enterprise success with AWS Partners PEX301 – Agentic AI Solutions for Financial Services & Insurance with AWS PEX201 – Expanding agentic AI: Accelerate, optimize and scale generative AI PEX402 – Engineering Intelligence: Multi-Agent AI Systems for Industrials PEX109 – Migration & Modernization: How AWS Partners help customers transform PEX302 – Building enterprise-grade Agentic AI factory PEX110 – Solution-based selling strategies in AWS Marketplace PEX315 – Intelligent Security Operations with AWS: AI-Powered Incident Response PEX310 – From Motion to Meaning: AI-Powered Human Movement Analysis on AWS PEX102 – Next-gen MSPs: From service providers to strategic partners. PEX307-R1 – Accelerating Cloud Migration with Agentic AI: Hands-on AWS Transform PEX320 – Agentic Code Generation for Industrial Analytics and Predictive Maintenance PEX405-R1 – AI That Remembers: Building Memory Systems for AI Agents PEX306 – Integration to impact: Accelerate AWS Partner revenue measurement PEX112 – Monetizing agentic AI solutions: A Partner's guide for AI agents & tools PEX312 – Trust and Transparency: Implementing Scalable Data Lineage for AI/ML PEX311 – Multimodal Video Understanding AI at Scale with Amazon Bedrock

PEX104 – Generative AI excellence: How AWS Partners help customers from strategy to scale PEX403 – Metadata Intelligence: The Key to Trusted and Smart Agentic AI Systems PEX103 – Accelerate your growth: AWS Partner Programs for Software Partners PEX105 – AWS Global Startup Program – how to accelerate your GTM success PEX107 – Fast Track to Growth: Leveraging Partner Programs & AWS Marketplace for success PEX404-R1 – Building Production-Ready AI Applications with Amazon Bedrock PEX113 – AI-powered co-sell: Unlock partner success with AWS Partner Central PEX313 – Customize Amazon Nova models for Agentic AI with SageMaker & Bedrock PEX101 – Accelerate new customer engagement and revenue growth

ISV Track for Software Partners New this year, the ISV track is designed for independent software vendors building, scaling, and optimizing their applications on AWS. Led by AWS experts, these sessions draw from real-world implementations to share technical patterns, implementation strategies, and architectural best practices. Whether you're migrating existing applications, building new solutions, or looking to integrate AWS services into your software, the ISV track provides targeted content to accelerate your journey, including:

ISV302 – Scaling SaaS applications with cell-based architectures ISV308 – Accelerating SaaS Implementations with Amazon Bedrock and Amazon Nova ISV314 – From Code to Market: Build and Launch AI Agents on AWS Marketplace ISV310 – Global Growth: A Framework for Expanding into New Regions ISV401 – Elevate Multi-Tenant SaaS Operations: DevOps Strategy with Kubernetes

➤ Explore all ISV track sessions in the event catalog.

Spotlight: Fast Track to Growth: Leveraging Partner Programs & AWS Marketplace for Success (PEX107) Don't miss this breakout session on Thursday, December 4 with Julia Chen, VP, AWS Partner Core, and Matt Yanchyshyn, VP, AWS Marketplace and Partner Services. Explore how AWS is transforming Partner success through profitable programs and product innovation from AWS Marketplace and Partner Services. Discover how AWS can fast-track your business growth through enhanced incentives, AI-driven automation, and global reach. Learn about new features enabling faster time-to-value, including automated AWS Specialization validation, simplified contract management, and enhanced AWS Marketplace capabilities. Whether you're focused on migration acceleration, generative AI adoption, or expanding globally, you'll leave with a clear plan to scale efficiently while increasing profitability. This session is intended for AWS Partners. ➤ Register for the PEX107 session >

AWS Marketplace Buyer Sessions In addition to PEX sessions for selling in AWS Marketplace, we offer sessions designed for buyers at all levels. Whether you're new to cloud marketplaces or an existing AWS Marketplace buyer, you'll hear from experts and customers, and get hands-on experience with AWS Marketplace features. ➤ Here are the AWS Marketplace buyer-focused sessions happening at re:Invent: Monday, December 1

AIM328: Accelerate agent solution development with third-party agents & tools AIM304-R: Build agentic observability workflows with products from AWS Marketplace AIM3318: From idea to impact: Harness AI agents and tools in AWS Marketplace

AIM304-R – Build agentic observability workflows with products from AWS Marketplace AIM3311 – Supercharge agentic workflows on AWS with third-party agents and tools

COP212 – Optimize software and AI Agent procurement with AWS Marketplace

MAM107 – Simplify discovery and deployment of software with AWS Marketplace NTA101 – Unlock Innovation: A Guide to AWS Partner Solutions in AWS Marketplace .

Visit Our Booths at the Main Expo The main Expo at the Partner Experience Pavilion features three booths: the AWS Partner Network booth for Partner growth strategies, the AWS Marketplace booth for selling and procurement solutions, and the Specialized Partner Solutions Booth for technical expertise and validation. AWS Partner Network Booth Visit the AWS Partner Network booth to learn how to build and grow your business with AWS. Connect with AWS experts to explore Partner programs, co-selling opportunities, marketing resources, and solutions across data and AI, migration and modernization, industries, public sector, SMBs, and more. Learn how to showcase your technical expertise and differentiate your business. Meet our AWS Partner Ambassadors—successful leaders passionate about sharing their AWS expertise and success stories. Visit for a chance to win prizes while discovering strategies for building solutions with AWS. New this year: Get hands-on AWS Partner Central support to ensure you're ready to take advantage of new features and maximize your partnership success. AWS Marketplace Booth Join us at the AWS Marketplace booth to connect with experts who can help you grow your business. Whether you're a software seller or buyer, our team is ready to answer technical questions, provide personalized guidance, and help you explore the latest AWS Marketplace innovations. New to AWS Marketplace? Our experts will show you how to get started. Already using AWS Marketplace? Discover new ways to optimize your experience and maximize results. Stop by for technical support and participate in our new discovery game—collect stamps as you explore the pavilion and connect with experts to win prizes. PeerSpot will be on site helping sellers capture verified customer feedback that strengthens their AWS Marketplace presence. Invite your customers at re:Invent to leave a verified review for your AWS Marketplace listings, or request your scheduling link for online reviews. Specialized Partner Solutions Booth The Specialized Partner Solutions booth highlights the value of AWS Specialization Partners, showcasing Partner solutions across 12 use cases: CloudOps, Security and Managed Security Service Providers (MSSP), Analytics, Resilience, Migration and Modernization, AI and Agentic AI, End User Computing and Connect, FinOps, Security Incident Response and Cyber Insurance, AWS Built-In, AI Security and Governance, and Database Specializations. Visit the booth to learn how AWS validates Partners' technical expertise and discover complementary Partner solutions that can expand your business opportunities. As an AWS Specialization Partner, your solutions may be featured to customers seeking your specific capabilities. New this year: Explore three expanded Partner domains—Security Incident Response & Cyber Insurance to help you prepare for and respond to security events, FinOps to optimize your cloud spend and financial operations, and AI Security & Governance to ensure responsible and secure AI implementations. Stop by for demos, one-on-one consultations, and exclusive Partner insights while entering for a chance to win prizes. AWS Partners LIVE! Join us for AWS Partners LIVE!, streaming directly from the Partner Experience Pavilion at re:Invent (Booth #1190). Our rotating crew of hosts leads dynamic conversations with AWS experts, diving deep into AI, Security, and Industries through interactive sessions. Experience unique insights, breaking news, and customer success stories—complete with games, hot takes, and more. Tuesday

AI LIVE! 10 AM – 1 PM Security LIVE! 1 PM – 3 PM; 4:30 PM – 6 PM

Security LIVE! 10 AM – 11:30 AM AI LIVE! 11:30 AM – 1:30 PM Industries LIVE! 1:30 PM – 4 PM

Can't make it to Las Vegas? Watch live on YouTube!

Lightning Theater Sessions for AWS Partners Want to learn more about partnering with AWS? Visit our Lightning Theater sessions at the main Expo in the Venetian for 20-minute presentations from the AWS Partner team covering key benefits, programs, and opportunities for AWS Partners. ➤ Here's the lineup of Lightning Theater sessions: Tuesday, December 2

PEX205 – From automation to autonomy: How AWS Partners drive SMB AI evolution at scale PEX209 – AWS Marketplace growth through product-led growth and international expansion PEX106 – Maximizing customer outcomes: AWS Partner Led Customer Success PEX208 – Transform digital experience: Accelerate AWS Partner & customer success PEX206 – Greenfield unlocked: Best practices for expanding new customer engagement PEX207 – AI-powered co-selling: How AWS and partners accelerate growth together

PEX108 – AWS Generative AI Innovation Center driving enterprise success with AWS Partners PEX109 – Migration & Modernization: How AWS Partners help customers transform PEX110 – Solution-based selling strategies in AWS Marketplace PEX102 – Next-gen MSPs: From service providers to strategic partners PEX112 – Monetizing agentic AI solutions: A Partner's guide for AI agents & tools

PEX104 – Generative AI excellence: How AWS Partners help customers from strategy to scale PEX103 – Accelerate your growth: AWS Partner Programs for Software Partners PEX105 –AWS Global Startup Program – how to accelerate your GTM success PEX113 – AI-powered co-sell: Unlock partner success with AWS Partner Central .

Attendee Guides for AWS Partner Personas Make the most of your re:Invent experience with expert guidance from AWS Ambassadors. These experienced community leaders have created tailored attendee guides packed with insider tips, strategic recommendations, and proven best practices for different Partner roles and interests. Whether you're a solutions architect, business leader, or technical professional, you'll find a curated path that matches your goals. Each guide offers session recommendations, networking opportunities, must-attend events for your role, and expert tips to maximize your experience. Explore our collection of Attendee Guides to build your optimal re:Invent experience:

AWS Marketplace Sellers by Alan Blockley, Thoughtworks Cloud administrators and operational professionals by Justin Cook, Eviden Cloud alliance leads by Ruslan Kusov, SoftServe Product and technical by Mark Ross, Eviden Sales and marketing leaders by Dorian Sezen, kloia Senior executives by Todd Bernson, BSC analytics

AWS Public Sector Partners Kick off an exciting week at the Public Sector Partner Forum on Monday, December 1, from 11:30 AM to 2 PM PT at the Encore. Join Rishi Bhaskar, Director, Worldwide Public Sector Partner Sales, AWS, for an engaging and interactive session where you'll have an opportunity to hear from guest speakers Julia Chen, VP AWS Partner Core, and Peter Amirkhan, SVP, Public Sector at Zscaler, and participate in roundtable conversations with AWS leaders and their peers. This is an invite-only event. Interested in attending? Reach out to [EMAIL] Round out your re:Invent experience at Partners in the Cloud, a Partner Appreciation Reception on Wednesday, December 3, 5:30 PM – 7 PM at the iconic Ghostbar. Located on the 55th floor of the Palms Resort & Casino's Ivory Tower, this networking event offers the perfect setting to connect with the AWS Public Sector team and fellow Partners while enjoying spectacular views of the Las Vegas Strip. Register to attend > .

Want to participate in a gamified learning event? Interested in testing your AWS knowledge at re:Invent? AWS GameDay challenges participants to use AWS solutions to solve real-world technical problems in a team-based setting. Unlike traditional workshops, GameDays are open-ended and non-prescriptive to give participants the freedom to explore and think outside the box. ➤ Here is the schedule for AWS GameDay events and sessions:

GHJ305 – Agentic AI (sponsored by Dynatrace) GHJ306 – AI-Assisted Developer Experience (sponsored by New Relic) GHJ307 – Cloud Architecture (sponsored by Datadog) GHJ308 – Security (sponsored by SentinelOne) ARC301 – AI-powered troubleshooting: From chaos to Well-Architected ARC309 – The Frugal Architect: Building cost-aware architectures CMP326 – Java Modernization and Performance Optimization SEC403 – Winning the DDos Game

No need to find a team, we'll match you with other attendees at the event. Bring your laptop, your A-game, and get ready for an unforgettable experience that will level up your AWS expertise!

To help you prepare for your re:Invent experience, AWS Partners have enhanced training and certification options available in AWS Skill Builder. Advance your professional goals with access to 900+ free courses and learning plans, gain hands-on experience in an AWS console sandbox, explore game-based learning including AWS Cloud Quest and AWS Card Clash, take AWS Certification Official Practice Exams, and discover new generative AI training content. Start your AWS Skill Builder subscription to unlock exclusive training content for your team, including AWS Builder Labs, AWS Jam Journeys, and Digital Classroom courses. Once you arrive in Las Vegas, there are plenty of opportunities to develop your skills and propel your AWS practice forward. Visit the Developers Community Pavilion at the main Expo (Booth #1502), which brings together AWS Training & Certification, Open Source, and DevEx teams to present an integrated developer journey featuring interactive learning stations focused on hands-on learning and community engagement. In the Arcade Arena, you'll find two interactive experiences: the AWS Skill Builder Meeting Simulator, where you'll practice customer conversations through AI-powered simulations, and AWS Card Clash, a strategic card game where players deploy AWS services to build architectures and score points. Connect with AWS T&C experts at the AWS Certification Lounge for networking and community building. Partner Bootcamps Join us for exclusive, hands-on technical workshops led by AWS experts (Mandalay Bay Convention Center). These advanced (300-level) to expert (400-level) bootcamps are designed specifically for AWS Partners. Bring your laptop and get ready for guided, intensive learning experiences. Reserved seating is available and space is limited. ➤ View all Partner Bootcamp sessions in the event catalog. December 1

TNC305 – Transform Business Tasks into AI Actions TNC303 – Building Secure agents with Q Index

TNC302 – Build AI Agents Ready for Enterprise TNC402 – Gen AI Security Testing & Protection Strategies

TNC403 – Migrate the Well-Architected way TNC401 – Build generative AI security runbooks with Amazon Bedrock

TNC304 – Instrument Your Serverless Apps for Full Visibility TNC301 – Accelerate VMware Migration with AWS Transform

AWS Certification Connect with fellow AWS certified professionals in our dedicated lounge space at the Venetian (open Monday – Thursday 7:30 AM to 5 PM), offering refreshments, comfortable seating, and a space to watch keynotes, charge your device, and participate in special activations for the AWS certified community. re:Invent in-person attendees receive a 50% discount on any AWS Certification exam. Vouchers are valid for online-proctored or test center exams. Join these certification exam prep sessions onsite to accelerate your AWS Certification readiness:

TNC103 – AWS Escape Room: Exam Prep for Cloud Practitioner – Monday, Dec 1 at 2:30 PM – 5:30 PM TNC105 – Prepare for your next AWS Certification exam Tuesday, Dec 2 at 11 AM – 12 PM TNC108 – The value of AWS Certifications in your enterprise strategy Tuesday, Dec 2 at 11:30 AM-12:30 PM TNC204 – AWS Escape Room: Exam Prep for AWS Certified Solutions Architect – Associate – Tuesday, Dec 2 at 12:30 PM – 3:30 PM Thursday, Dec 4 at 2:30 PM – 5:30 PM TNC109 – Exam Prep: Transform Your Team's Cloud Training with Cohort-Based Training – Wednesday, Dec 3 at 11 AM – 12 PM TNC102 – AWS Escape Room: Exam Prep for AWS Certified AI Practitioner – Wednesday, Dec 3 at 12:30 PM – 3:30 PM; Thursday, Dec 4 at 8:30 AM – 11:30 AM TNC207 – Developing AI solutions: what every developer should know Thursday, Dec 4 at 2:30 PM – 3 PM TNC106 – The Evolution of AWS Skills Validation Thursday, Dec 4 at 3 PM – 3:30 PM

Connect with AWS Ambassadors With thousands of AWS enthusiasts gathering at re:Invent each year, attendees have often requested new ways to network, discover, and connect. Again this year, we have AWS Ambassadors engaged in PeerTalk, re:Invent's networking program designed to help facilitate meaningful connections. AWS Ambassadors are presenting at re:Invent breakouts, chalk talks and workshops, and supporting Partners and customers at the APN Booth. We're proud to recognize the 2025 top AWS Ambassadors from across the world. Congratulations to all of our AWS Ambassador award winners!

Get Inspired by Our Sponsoring Partners Hundreds of AWS Partners are sponsoring re:Invent 2025 and will showcase their solutions in the main Expo. Visit their booths and Lightning Theater sessions to connect. Meet our 2025 AWS re:Invent sponsors >

Stay Connected Keep up with the latest re:Invent 2025 updates and use the hashtag #reInvent to join the conversation:

Read the APN Blog for the latest updates from re:Invent Follow the AWS Partners page on LinkedIn Follow @AWS_Partners and @awsmarketplace on X/Twitter Subscribe to the APN YouTube channel Join the AWS Partner email list for APN news and events

TAGS: APN Programs, AWS Ambassadors, AWS GameDay, AWS re:Invent, AWS re:Invent 2025

Optimizing undersea cables: how Orsted and AWS modeled seabed thermal properties

This post was contributed by Ross Pivovar, Rafał Ołdziejewski, Cindy Xin Qi Lee Offshore wind farms play a critical role in the global transition to renewable energy and clean power generation. But generating electricity is only half the battle—safely and efficiently transporting that power to the grid through undersea cables is equally important. Today, we'll explore how Ørsted, a global leader in offshore wind energy, partnered with AWS to develop a proof-of-concept for a complex thermal modeling challenge for their submarine cables, enabling more efficient operations and potentially extending cable lifespan. In this post, we describe a proof-of-concept project. The solutions, methodologies, and products mentioned haven't yet been vetted for general use in Ørsted, so they represent prototype work rather than production systems. Background: why cable thermal modeling matters Undersea cables (Figure 1) are the unsung heroes of offshore wind energy. These complex structures carry electricity from offshore wind turbines to onshore substations, ultimately connecting to the power grid. For companies like Ørsted, optimizing cable performance is critical for several reasons:

High stakes failures: Undersea cables are extremely expensive and difficult to repair when they fail. Their immense size and weight make transportation challenging, and installation requires specialized vessels, specific weather windows, and highly trained personnel. Cost-revenue balance: If cables aren't utilized to their full capacity, valuable revenue is lost. Conversely, if pushed beyond safe limits, they risk premature failure. Project viability: For newer, larger wind farms, cable capacity can be the difference between a project being financially viable or not.

The goal with thermal cable models is to optimize the use of cables—from early development to operation.

Figure 1 The diagram shows offshore wind turbines connected to an offshore substation via orange "Array Cables" (66 kV), with electricity then transmitted to shore through a green "Export Cable" (220 kV) to a transformer station. This illustration demonstrates how wind-generated electricity is collected, voltage-converted at sea, and efficiently transmitted to land for distribution.

The TEEM model: predicting cable temperature To manage their cable infrastructure, Ørsted developed a thermal model using a technique called TEEM (Thermal Electrical Equivalent Model [1]). This method transforms the complex 3D heat transfer problem of an undersea cable (Figure 2) into a simplified but accurate electrical equivalent model (Figure 3). TEEM treats the cable as a series of cylindrical layers, each with specific thermal properties.

Heat generation is represented by current sources Heat transfer resistance is represented by electrical resistors Heat storage capacity is represented by capacitors

This approach allows Ørsted to predict cable temperatures based on electrical current, cable design specifications, and installation conditions. The model's key task is to determine if, for a given set of thermal conditions, the cable's maximum safe operating temperature of 90°C is exceeded. If this threshold is exceeded the insulation begins to degrade, potentially leading to failure.

Figure 2 2D cross-sectional view of a subsea power cable showing its layered construction with multiple protective layers and three cylindrical cores visible in the cross section.

Figure 3 The image displays a detailed electrical circuit diagram representing the thermal modeling of a cable system, with resistors (R1-R4) arranged in series along the top and multiple capacitors (C1-C12) connected below. Color-coded sections indicate different components of the cable structure including conductor (blue), dielectric (green), lead sheath (red), oversheath (black), filler & armour bedding (light green), armour (blue), and serving (orange), demonstrating how heat transfer in cables can be analyzed using electrical circuit analogies.

The challenge: unknown seabed properties While Ørsted had extensive knowledge about their cable designs and materials, they faced an important challenge regarding the thermal properties of the seabed materials surrounding their cables. These properties, such as thermal resistivity and heat capacity, directly impact how efficiently heat dissipates from the cable into the surrounding environment. The uncertainty stemmed from limited and coarse-grained measurements taken during the design phase, combined with the fact that these properties may change over time. This created an input-data uncertainty constraint in their thermal models. Without more accurate and localized values for seabed thermal properties, temperature predictions contained margins of uncertainty, potentially leading to either:

Overly conservative operation—sacrificing power capacity and revenue Excessive risk of overheating—jeopardizing expensive infrastructure

The challenge was clear: how could Ørsted improve the accuracy of thermal property measurements for seabed materials at specific locations along their vast submarine cable network, accounting for both spatial variation and temporal changes? The solution: combining IoT data with physics-based simulation To solve their undersea cable thermal modeling problem, Ørsted partnered with AWS to develop a solution combining real-world measurements with computational optimization. Here's how they approached it:

Ørsted compiled a comprehensive dataset by combining distributed optical sensing along the cables with additional survey information. The dataset includes:

Cable temperatures at strategic points Precise cable burial depths from geophysical surveys Geographical coordinates (organized using location identifiers called "TAZIDs" that segment the cable route into manageable sections) Ambient seawater temperatures from sensors and models

This integrated dataset was used to calibrate the thermal models.

With this data in hand, the team implemented an iterative optimization approach:

Initial estimates for thermal properties (e.g., thermal resistivity) at each location are provided from field investigations during the design process and existing soil models. Run the Thermal Electrical Equivalent Model (TEEM) to simulate expected temperatures Compare simulated results with actual measurements on the operational cable Adjust thermal properties to minimize the discrepancy Repeat until convergence

Mathematically, this process minimizes an objective function representing the error between measured and simulated temperatures:

Where represents the thermal parameters being optimized at each location, i is index for individual temperature measurements, and n is the total number of temperature measurements or data points used in the optimization process.

Beyond optimization, the team conducted rigorous statistical analysis to validate their results:

Grouped measurements by TAZID location identifiers Applied non-parametric statistical techniques to handle non-Gaussian distributions Implemented bootstrap sampling (5000 samples per location) to calculate robust confidence intervals Performed variogram analysis to assess spatial correlation between sampling points Used temporal models such as Autoregressive Integrated Moving Average (ARIMA) to perform hypothesis testing and confirm the existence of material properties that are changing over time

Figure 5 demonstrates how thermal resistivity varies significantly along the cable route, with error bars representing 1st/99th percentiles from bootstrap analysis of the median values. The analysis reveals that current thermal resistivity assessments are overly conservative, as they assume homogeneous soil properties within each TAZID. Statistical distribution fitting shows many locations exhibit multi-modal patterns consistent with soil property mixtures rather than uniform composition, indicating the current approach fails to capture the full complexity of soil characteristics within individual TAZIDs.

Figure 5 Demonstration of change in the median of each TAZID group vs distance. Error bars are based on 1/99 quantiles of 5000 sample bootstraps for the median. Medians are at the average distance for a group. The value of this optimization process extends beyond simply generating new thermal property values. By systematically comparing the optimized thermal resistivity values against the initial estimates derived from pre-construction field investigations and standard soil models, Ørsted gained insights into the accuracy of their existing modeling approaches. In many locations, the optimization revealed significant discrepancies between assumed and actual thermal properties—up to 30% difference in some seabed environments. These findings enabled Ørsted to recalibrate their baseline assumptions for future projects, refine their cable design safety margins with greater precision, and develop more sophisticated regional thermal property maps. Additionally, by identifying patterns in the discrepancies, engineering teams could investigate specific environmental factors that might explain why conventional models underestimated or overestimated thermal resistivity in particular conditions, ultimately improving the guidance provided by their existing predictive models.

Building on this foundation, the team extended their approach to simultaneously optimize multiple thermal properties—specifically thermal resistivity and volumetric heat capacity. This required implementing Simultaneous Perturbation Stochastic Approximation (SPSA), which efficiently estimates gradients in high-dimensional spaces:

is a random perturbation vector is a small positive number that decreases with iterations

The key advantage of SPSA is that it requires only two function evaluations to estimate gradients regardless of parameter dimensionality—vastly more efficient than the n+1 evaluations needed by standard finite difference methods. Since spatial statistical analysis demonstrated sufficient independence between sensor locations, the team was able to parallelize the gradient calculations across AWS infrastructure, dramatically accelerating the optimization process. Optimization results and operational impact The thermal property optimization delivered several significant analytical insights for Ørsted's undersea cable modeling:

Refined Thermal Property Understanding: The analysis revealed significant variations in thermal resistivity across different seabed locations (TAZID groups), with non-Gaussian distributions and notable outliers. This provided a more nuanced understanding of the thermal environment surrounding the cables than previously assumed values. Improved Model Accuracy: By using optimized thermal resistivity values rather than assumed constants, the TEEM simulations achieved substantially reduced error rates at specific cable distances, enabling more accurate temperature predictions. Temporal Variation Insights: ARIMA model analysis of the TEEM residuals suggested that thermal resistance may vary over time, indicating that the constant thermal resistance assumption in the current model may be limiting long-term simulation accuracy. Material Heterogeneity Confirmation: Statistical analysis revealed that observed uncertainty in thermal resistivity is primarily due to process variability rather than measurement or model error, confirming heterogeneity in seabed sediments. Computational Framework Development: A scalable computational approach was established using AWS resources, with optimal EC2 instance selection providing cost-effective execution of complex thermal simulations.

From static model to living digital twin With our established methodology now validated, the next evolution is to transform this historical-data physics model into a living digital twin. Having confirmed the presence of temporal components in seabed properties, we can now leverage the combination of sensors, TEEM modeling, and AWS's event-driven infrastructure to create an adaptive thermal model. This dynamic system continuously reassesses how soil thermal properties change over time, enabling Orsted to detect thermal anomalies before the cable temperature exceeds its limit and inform design decisions for more robust cable systems in future deployments. How AWS enabled the prototype solution The computational demands of this project were substantial. Each TEEM simulation processed millions of temperature data points, and thousands of simulations were needed for the optimization process. AWS provided the scalable computing infrastructure that made this analysis feasible.

To determine the most appropriate computing resources, the team used the CloudInstanceOptimizer tool to evaluate 114 different EC2 instance types, conducting systematic performance testing with replications to identify the optimal configuration. Figure 5 shows an example output from the optimizer. The analysis revealed that memory-optimized instances were most suitable for this workload, with each TEEM execution requiring approximately 8GB of RAM per vCPU.

Figure 6 : runtime (s) vs cost ($) for each EC2 type reviewed with the TEEM model. Each color is a different instance type. This figure can be filtered and analyzed in the streamlit app.

Each TEEM execution used Ray on AWS Batch, a distributed computing framework. Ray allows for precise control over RAM usage for each TEEM execution, which is crucial for proper utilization of the Ray cluster when scaling up calculations to review all available data as fast as possible.

For univariate analysis: 9 nodes, 1600 vCPUs, and approximately 13 terabytes of RAM For multivariate analysis: 17 nodes, 3264 vCPUs, and approximately 21 terabytes of RAM

Despite the large computational requirements, AWS's scalable infrastructure delivered results quickly:

The univariate optimization completed in less than 2 hours The more complex multivariate analysis finished in less than 3 hours

This rapid turnaround enabled the team to iterate on their approach, refine their methodologies, and conduct comprehensive statistical analysis within a reasonable timeframe. Conclusion This proof-of-concept collaboration between Ørsted and AWS demonstrates the power of combining domain expertise with cloud computing capabilities to solve complex engineering challenges. By successfully estimating the thermal properties of seabed materials, Ørsted gained valuable insights that will help them optimize their undersea cable operations, potentially extending cable lifespan and maximizing the economic value of their offshore wind projects. The project's success relied on several key elements:

Sophisticated physics-based modeling with TEEM Advanced optimization algorithms including gradient descent and SPSA Rigorous statistical analysis to quantify uncertainty Massive parallel processing enabled by AWS's scalable infrastructure

As renewable energy continues to grow, particularly offshore wind, the insights and methodologies developed in this project will help ensure that the critical undersea cable infrastructure connecting these resources to the grid operates safely, efficiently, and reliably—bringing us one step closer to a sustainable energy future. Note: The figures referenced in this blog are illustrative examples from the original project documentation. Contact Orsted for access to the complete analysis and results. References [1] Frank P. Incropera, David P. DeWitt, Theodore L. Bergman, and Adrienne S. Lavine. Fundamentals of Heat and Mass Transfer. 7th Edition, Wiley, 2011.

Ross Pivovar Ross has over 15 years of experience in a combination of numerical and statistical method development for both physics simulations and machine learning. Ross is a Senior Solutions Architect at AWS focusing on development of self-learning digital twins, multi-agent simulations, and physics ML surrogate modeling.

Cindy Xin Qi Cindy Xin Qi Lee: is a Digital Innovation Specialist at Ørsted with over 6 years of experience in strategic transformation, computational modelling, and automation across banking and renewable energy sectors. At Ørsted, she develops in-house tools for offshore wind projects, focusing on array cable layout optimization, harmonic analysis, cable thermal modelling, and structural load simulations.

Rafał Ołdziejewski Rafał Ołdziejewski: For over 11 years, Rafał has leveraged advanced mathematical and computational methods for quantum simulations in condensed matter physics and stochastic processes in the renewables sector. At Orsted, Rafał specializes in stochastic cable rating, cable thermal models, and stochastic financial planning and scheduling risk management.

Powering serverless multi-account analytics with AWS Lambda, Amazon Aurora, and Amazon Quick Suite

As a large Amazon Web Services (AWS) public sector customer, you might need information about which accounts contributed to this month's billing surge, which workloads generated the most security findings, or which services experienced the highest adoption rates. Although straightforward in single-account environments, finding such information becomes complex when organizations manage hundreds or thousands of AWS accounts. Without comprehensive visibility across multiple accounts, public sector organizations lack the information needed to make sound decisions about the costs, security, and performance of their cloud workloads. As a result, they experience adverse impacts on decision-making speed and quality. In this post, we will discuss a comprehensive analytics solution we designed, which provides public sector decision makers with the insights necessary to accelerate data-driven decision-making through a serverless, event-driven architecture. Understanding the multi-account analytics challenge Organizations rely on multi-account AWS strategies for security boundaries, cost management, and compliance across business units. However, many organizations lack full access to their payer account or centralized AWS Organizations account due to organizational constraints or partner management arrangements. This distributed approach creates visibility gaps, making comprehensive cloud infrastructure oversight difficult. The need for centralized analytics becomes critical in public sector environments where compliance and security standards demand continuous monitoring. This implementation addresses these challenges through a hub-and-spoke architecture that aggregates multi-account data into a centralized analytics solution, enabling unified visibility while maintaining the security benefits of account separation. The solution implements a serverless architecture built on proven AWS services. At its foundation, the solution uses AWS Lambda for data processing, Amazon Aurora PostgreSQL-Compatible Edition for data storage, and Amazon Quick Suite for visualization and analytics. This serverless approach is cost-effective and provides the scalability needed to handle data from hundreds of AWS accounts. The architecture consists of two primary account types. The Analytics Account serves as the central hub that receives, processes, and aggregates data from all sender accounts. The Sender Accounts act as spokes that collect and transmit their infrastructure data to the analytics account. Each sender account runs a lightweight Lambda function that gathers comprehensive data from over 20 AWS services and securely uploads it to the central Amazon Simple Storage Service (Amazon S3) bucket. The data flow follows a six-step process that begins with Amazon EventBridge triggering sender Lambda functions on a daily schedule. These functions retrieve data collection scripts from the analytics account, gather comprehensive infrastructure data, and upload daily and monthly JSON files to Amazon S3. Amazon S3 events then trigger receiver Lambda functions that process the JSON data and load it into Aurora PostgreSQL-Compatible, where it becomes available for analysis through Amazon Quick Suite dashboards. The following diagram illustrates this architecture.

Data collection capabilities This solution collects data from over 20 AWS services, delivering comprehensive cloud infrastructure visibility to organizations. The solution aggregates security and compliance data from AWS Security Hub, AWS Config, Amazon GuardDuty, AWS Identity and Access Management (IAM), and AWS CloudTrail. These services track user activity, evaluate resource configurations, and provide threat detection across AWS accounts. This solution leverages AWS Cost Explorer, AWS Systems Manager, and AWS Trusted Advisor for comprehensive cost and resource management—providing detailed analysis, usage metrics, and optimization guidance. For security posture assessment, it gathers security inventory data from AWS Key Management Service (AWS KMS), AWS WAF, AWS Secrets Manager, and AWS Certificate Manager. This data collection creates unified dashboards that deliver technical and business insights for decision-making. The solution aggregates data from multiple sources into a single view, addressing multi-account AWS environment challenges through integrated monitoring capabilities. Security architecture Multilayered security protection maintains data integrity and access control. The Amazon Aurora database operates in private subnets without internet access, following AWS best practices where database instances in private subnets can't access the internet directly. Virtual private cloud (VPC) endpoints deliver secure connectivity without internet gateways by creating private connections that don't traverse the public internet. The solution enforces least privilege access through service-specific IAM roles and automatic database credential rotation through AWS Secrets Manager. IAM policies assign permissions that determine who can manage Amazon Relational Database Service (Amazon RDS) resources, and database engine security features control database login access. Cross-account access operates through controlled S3 bucket policies that restrict data uploads to authorized sender accounts only. Security groups manage VPC and Amazon Quick Suite access, functioning as instance-level virtual firewalls alongside network access control lists (ACLs) that provide subnet-level protection. Network-level security works with application-level access controls to create defense-in-depth protection. This security model maintains strict security standards while delivering centralized analytics capabilities through multiple security control layers across all infrastructure components. The technical specifications use AWS serverless technologies, including Lambda functions running Python 3.13 with 10 GB memory allocation and 15-minute timeout configurations. The Amazon Aurora Serverless v2 database delivers Multi-AZ deployment with Data API enabled for performance and availability. The VPC architecture uses a 10.0.0.0/16 network with three private subnets distributed across Availability Zones for high availability and fault tolerance. Analytics and visualization through Amazon Quick Suite The solution uses the powerful analytics capabilities of Amazon Quick Suite to transform raw infrastructure data into actionable insights. The solution establishes secure VPC connectivity to Aurora PostgreSQL through VPC endpoints and therefore sensitive data never traverses the public internet. Amazon Quick Suite support for multiple data sources—including Amazon S3, Amazon Aurora, and traditional databases—provides flexibility for organizations with diverse data requirements. The integration with Amazon Quick Suite enables natural language querying capabilities. Users can ask questions about their infrastructure in plain English and receive immediate, data-driven answers. This democratization of analytics access means that both technical and business stakeholders can benefit from the solution's insights without requiring deep technical expertise. The solution features comprehensive dashboards that present critical insights across multiple domains of AWS infrastructure management. These dashboards visualize key metrics for account status, cost management, security assessments, configuration details, operational metrics, application health, and system resilience. With these analytics capabilities, teams can maintain continuous visibility of their compliance requirements, analyze cost trends, detect potential security issues, and fine-tune resource allocation throughout their AWS environment.

Implementation and deployment considerations Successful deployment requires careful planning and adherence to best practices. Organizations should begin with a phased rollout, starting with a pilot group of accounts before expanding to full deployment. By following this approach, teams can validate the data collection process, test dashboard functionality, and refine access controls before scaling to production environments. Sender account configuration remains minimal, requiring only the deployment of lightweight Amazon CloudFormation templates and the configuration of cross-account permissions. This streamlined approach makes it possible to add new accounts to the analytics solution with minimal technical expertise. For implementation details, refer to this GitHub repository. Best practices and operational excellence Organizations deploying this implementation must establish data governance policies that include data retention schedules and access control procedures. Continuous security monitoring and alerting protects the solution while monitoring the connected account security posture. Performance optimization through AWS Lambda monitoring and Amazon Aurora utilization tracking maintains the solution's performance when data volumes increase. Cost management requires implementing Amazon S3 lifecycle policies to control storage costs and optimizing resource usage based on utilization patterns. Organizations must establish backup strategies, implement CloudWatch alarms for critical components, and develop data validation processes for data quality and reliability. The architecture handles hundreds of sender accounts and processes large data volumes efficiently. The serverless architecture scales automatically with organizational growth, and multi-Region deployment capabilities support geographically distributed organizations. API integration plans deliver RESTful access to analytics data for programmatic integration with existing enterprise systems. Future enhancements include machine learning (ML) integration for predictive analytics and anomaly detection, extended data source support for additional AWS services and third-party integrations, and advanced visualization capabilities with custom widgets and dashboard functionality. The architecture follows AWS best practices by taking a data-driven approach to building high-performance systems. Conclusion In this post, we introduced a serverless multi-account analytics solution that provides public sector organizations with a single view across their AWS accounts, using AWS Lambda, Amazon Aurora PostgreSQL, and Amazon Quick Suite for comprehensive infrastructure insights. To learn more about implementing similar solutions in your organization, explore AWS best practices for multi-account strategies and consider how centralized analytics can enhance your cloud governance and compliance efforts. Ready to implement your own solution? Access the deployment template hosted in AWS Samples GitHub repositories today.

Arun PC Arun is a senior solutions architect at AWS based in Singapore, covering the public sector. He has 20 years of experience in pre-sales engineering, consulting, and solution architecture across ASEAN. He holds a bachelor's degree in engineering and is the holder of four US patents.

Akshat Sawhney Akshat is a senior technical account manager at AWS. He brings deep expertise in cloud infrastructure and serverless architecture, focusing on helping organizations implement comprehensive analytics solutions. Akshat is dedicated to enabling customers to make data-driven decisions through innovative AWS solutions.

Vignesh Pillai Vignesh, customer solutions manager at AWS, has over 14 years of product development and software engineering expertise in data and AI. He is passionate about helping customers build and implement scalable, resilient, and cost-effectives solutions that drive business value.

How Amazon Search increased ML training twofold using AWS Batch for Amazon SageMaker Training jobs

In this post, we show you how Amazon Search optimized GPU instance utilization by leveraging AWS Batch for SageMaker Training jobs. This managed solution enabled us to orchestrate machine learning (ML) training workloads on GPU-accelerated instance families like P5, P4, and others. We will also provide a step-by-step walkthrough of the use case implementation. Machine learning at Amazon Search At Amazon Search, we use hundreds of GPU-accelerated instances to train and evaluate ML models that help our customers discover products they love. Scientists typically train more than one model at a time to find the optimal set of features, model architecture, and hyperparameter settings that optimize the model's performance. We previously leveraged a first-in-first-out (FIFO) queue to coordinate model training and evaluation jobs. However, we needed to employ a more nuanced criteria to prioritize which jobs should run in what order. Production models needed to run with high priority, exploratory research as medium priority, and hyperparameter sweeps and batch inference as low priority. We also needed a system that could handle interruptions. Should a job fail, or a given instance type become saturated, we needed the job to run on other available compatible instance types while respecting the overall prioritization criteria. Finally, we wanted a managed solution so we could focus more on model development instead of managing infrastructure. After evaluating multiple options, we chose AWS Batch for Amazon SageMaker Training jobs because it best met our requirements. This solution seamlessly integrated AWS Batch with Amazon SageMaker and allowed us to run jobs per our prioritization criteria. This allows applied scientists to submit multiple concurrent jobs without manual resource management. By leveraging AWS Batch features such as advanced prioritization through fair-share scheduling, we increased peak utilization of GPU-accelerated instances from 40% to over 80%. Amazon Search: AWS Batch for SageMaker Training Job implementation We leveraged three AWS technologies to set up our job queue. We used Service Environments to configure the SageMaker AI parameters that AWS Batch uses to submit and manage SageMaker Training jobs. We used Share Identifiers to prioritize our workloads. Finally, we used Amazon CloudWatch to monitor and the provision of alerting capability for critical events or deviations from expected behavior. Let's dive deep into these constructs. Service environments. We set up service environments to represent the total GPU capacity available for each instance family, such as P5s and P4s. Each service environment was configured with fixed limits based on our team's reserved capacity in AWS Batch. Note that for teams using SageMaker Training Plans, these limits can be set to the number of reserved instances, making capacity planning more straightforward. By defining these boundaries, we established how the total GPU instance capacity within a service environment was distributed across different production jobs. Each production experiment was allocated a portion of this capacity through Share Identifiers. Figure 1 provides a real-world example of how we used AWS Batch's fair-share scheduling to divide 100 GPU instance between ShareIDs. We allocated 60 instances to ProdExp1, and 40 to ProdExp2. When ProdExp2 used only 25 GPU instances, the remaining 15 could be borrowed by ProdExp1, allowing it to scale up to 75 GPU instances. When ProdExp2 later needed its full 40 GPU instances, the scheduler preempted jobs from ProdExp1 to restore balance. This example used the P4 instance family, but the same approach could apply to any SageMaker-supported EC2 instance family. This ensured that production workloads have guaranteed access to their assigned capacity, while exploratory or ad-hoc experiments could still make use of any idle GPU instances. This design safeguarded critical workloads and improved overall instance utilization by ensuring that no reserved capacity went unused.

Share Identifiers. We used Share Identifiers to allocate fractions of a service environment's capacity to production experiments. Share Identifiers are string tags applied at job submission time. AWS Batch used these tags to track usage and enforce fair-share scheduling. For initiatives that required dedicated capacity, we defined preset Share Identifiers with quotas in AWS Batch. This reserved capacity for production tracks. These quotas acted as fairness targets rather than hard limits. Idle capacity could still be borrowed, but under contention, AWS Batch enforced fairness by preempting resources from overused identifiers and reassigned them to underused ones. Within each Share Identifier, job priorities ranging from 0 to 99 determined execution order, but priority-based preemption only triggered when the ShareIdentifier reached its allocated capacity limit. Figure 2 illustrates how we setup and used our share identifiers. ProdExp1 had 60 p4d instances and ran jobs at various priorities. Job A had a priority of 80, Job B was set to 50, Job C was set to at 30, and Job D had a priority 10. When all 60 instances were occupied and a new high-priority job (priority 90) requiring 15 instances was submitted, the system preempted the lowest priority running job (Job D) to make room, while maintaining the total of 60 instances for that Share Identifier.

Amazon CloudWatch. We used Amazon CloudWatch to instrument our SageMaker training jobs. SageMaker automatically publishes metrics on job progress and resource utilization, while AWS Batch provides detailed information on job scheduling and execution. With AWS Batch, we queried the status of each job through the AWS Batch APIs. This made it possible to track jobs as they transitioned through states such as SUBMITTED, PENDING, RUNNABLE, STARTING, RUNNING, SUCCEEDED, and FAILED. We published these metrics and job states to CloudWatch and configured dashboards and alarms to alert anytime we encountered extended wait times, unexpected failures, or underutilized resources. This built-in integration provided both real-time visibility and historical trend analysis, which helped our team maintain operational efficiency across GPU clusters without building custom monitoring systems. Operational impact on team performance By adopting AWS Batch for SageMaker Training jobs, we enabled experiments to run without concerns about resource availability or contention. Researchers could submit jobs without waiting for manual scheduling, which increased the number of experiments that could be run in parallel. This led to shorter queue times, higher GPU utilization, and faster turnaround of training results, directly improving both research throughput and delivery timelines. How to set up AWS Batch for SageMaker Training jobs To set up a similar environment, you can follow this tutorial, which shows you how to orchestrate multiple GPU large language model (LLM) fine-tuning jobs using multiple GPU-powered instances. The solution is also available on GitHub. Prerequisites To orchestrate multiple SageMaker Training jobs with AWS Batch, first you need to complete the following prerequisites: Clone the GitHub repository with the assets for this deployment. This repository consists of notebooks that reference assets:

Create AWS Batch resources To create the necessary resources to manage SageMaker Training job queues with AWS Batch, we provide utility functions in the example to automate the creation of the Service Environment, Scheduling Policy, and Job Queue. The service environment represents the Amazon SageMaker AI capacity limits available to schedule, expressed by maximum number of instances. The scheduling policy indicates how resource computes are allocated in a job queue between users or workloads. The job queue is the scheduler interface that researchers interact with to submit jobs and interrogate job status. AWS Batch provides two different queues we can operate with:

FIFO queues – Queues in which no scheduling policies are required Fair-share queues – Queues in which a scheduling policy Amazon Resource Name (ARN) is required to orchestrate the submitted jobs

We recommend creating dedicated service environments for each job queue in a 1:1 ratio. FIFO queues provide basic message delivery, while fair-share scheduling (FSS) queues provide more sophisticated scheduling, balancing utilization within a Share Identifier, share weights, and job priority. For customers who don't need multiple shares but would like the ability to assign a priority on job submission, we recommend creating an FSS queue and using a single share within it for all submissions.To create the resources, execute the following commands:

You can navigate the AWS Batch Dashboard, shown in the following screenshot, to explore the created resources.

ml-c5-xlarge-queue – A FIFO queue with priority 2 used for CPU workloads ml-g6-12xlarge-queue – A fair-share queue with priority 1 used for GPU workloads

The associated scheduling policy for the queue ml-g6-12xlarge-queue is with share attributes such as High priority (HIGHPRI), Medium priority (MIDPRI) and Low priority (LOWPRI) along with the queue weights. Users can submit jobs and assign them to one of three shares: HIGHPRI, MIDPRI, or LOWPRI and assign weights such as 1 for high priority and 3 for medium and 5 for low priority. Below is the screenshot showing the scheduling policy details:

For instructions on how to set up the service environment and a job queue, refer to the Getting started section in Introducing AWS Batch support for SageMaker Training Jobs blog. Run LLM fine-tuning jobs on SageMaker AI We run the notebook notebook.ipynb to start submitting SageMaker Training jobs with AWS Batch. The notebook contains the code to prepare the data used for the workload, upload on Amazon Simple Storage Service (Amazon S3), and define the hyperparameters required by the job to be executed. To run the fine-tuning workload using SageMaker Training jobs, this example uses the ModelTrainer class. The ModelTrainer class is a newer and more intuitive approach to model training that significantly enhances user experience. It supports distributed training, build your own container (BYOC), and recipes. For additional information about ModelTrainer, you can refer to Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK – Part 1: ModelTrainer. To set up the fine-tuning workload, complete the following steps:

Select the instance type, the container image for the training job, and define the checkpoint path where the model will be stored:

Create the ModelTrainer function to encapsulate the training setup. The ModelTrainer class simplifies the experience by encapsulating code and training setup. In this example:

SourceCode – The source code configuration. This is used to configure the source code for running the training job by using your local python scripts. Compute – The compute configuration. This is used to specify the compute resources for the training job.

from sagemaker.modules.configs import Compute, OutputDataConfig, SourceCode, StoppingCondition from sagemaker.modules.distributed import Torchrun from sagemaker.modules.train import ModelTrainer

# Define the script to be run source_code = SourceCode( source_dir="./scripts", requirements="requirements.txt", entry_script="train.py", )

# Define the compute compute_configs = Compute( instance_type=instance_type, instance_count=instance_count, keep_alive_period_in_seconds=0 )

# Define the ModelTrainer model_trainer = ModelTrainer( training_image=image_uri, source_code=source_code, base_job_name=job_name, compute=compute_configs, distributed=Torchrun(), stopping_condition=StoppingCondition(max_runtime_in_seconds=7200), hyperparameters={ "config": "/opt/ml/input/data/config/args.yaml" }, output_data_config=OutputDataConfig(s3_output_path=output_path), role=role, )Python

Set up the input channels for ModelTrainer by creating InputData objects from the provided S3 bucket paths for the training and validation datasets:

train_input = InputData( channel_name="train", data_source=train_dataset_s3_path, ) val_input = InputData( channel_name="val", data_source=val_dataset_s3_path, ) config_input = InputData( channel_name="config", data_source=train_config_s3_path, )

Queue SageMaker Training jobs This section and the following are intended to be used interactively so that you can explore how to use the Amazon SageMaker Python SDK to submit jobs to your Batch queues. Follow these steps:

In the next cell, submit two training jobs in the queue:

job_name_1 = job_name + "-low-pri" queued_job_1 = queue.submit( model_trainer, TRAINING_INPUTS, job_name_1, priority=5, share_identifier="LOWPRI" ) job_name_2 = job_name + "-mid-pri" queued_job_2 = queue.submit( model_trainer, TRAINING_INPUTS, job_name_2, priority=3, share_identifier="MIDPRI" )Code

Display the status of running and in queue jobs We can use the job queue list and job queue snapshot APIs to programmatically view a snapshot of the jobs that the queue will run next. For fair-share queues, this ordering is dynamic and occasionally needs to be refreshed because new jobs are submitted to the queue or as share usage changes over time.

The following screenshot shows the jobs submitted with low priority and medium priority in the Runnable State and in the queue.

You can also refer to the AWS Batch Dashboard, shown in the following screenshot, to analyze the status of the jobs.

As shown in the following screenshot, the first job executed with the SageMaker Training job is the MEDIUM PRIORITY one, by respecting the scheduling policy rules defined previously.

You can explore the running training job in the SageMaker AI console, as shown in the following screenshot.

Submit an additional job You can now submit an additional SageMaker Training job with HIGH PRIORITY to the queue:

job_name_3 = job_name + "-high-pri" queued_job_3 = queue.submit( model_trainer, TRAINING_INPUTS, job_name_3, priority=1, share_identifier="HIGHPRI" )Code

You can explore the status from the dashboard, as shown in the following screenshot.

The HIGH PRIORITY job, despite being submitted later in the queue, will be executed before the other runnable jobs by respecting the scheduling policy rules, as shown in the following screenshot.

As the scheduling policy in the screenshot shows, the LOWPRI share has a higher weight factor (5) than the MIDPRI share (3). Since a lower weight signifies higher priority, a LOWPRI job will be executed after a MIDPRI job, even if they are submitted at the same time.

Clean up To clean up your resources to avoid incurring future charges, follow these steps:

Verify that your training job isn't running anymore. To do so, on your SageMaker console, choose Training and check Training jobs. Delete AWS Batch resources by using the command python create_resources.py --clean from the GitHub example or by manually deleting them from the AWS Management Console.

Conclusion In this post, we demonstrated how Amazon Search used AWS Batch for SageMaker Training Jobs to optimize GPU resource utilization and training job management. The solution transformed their training infrastructure by implementing sophisticated queue management and fair share scheduling, increasing peak GPU utilization from 40% to over 80%.We recommend that organizations facing similar ML training infrastructure challenges explore AWS Batch integration with SageMaker, which provides built-in queue management capabilities and priority-based scheduling. The solution eliminates manual resource coordination while providing workloads with appropriate prioritization through configurable scheduling policies. To begin implementing AWS Batch with SageMaker Training jobs, you can access our sample code and implementation guide in the amazon-sagemaker-examples repository on GitHub. The example demonstrates how to set up AWS Identity and Access Management (IAM) permissions, create AWS Batch resources, and orchestrate multiple GPU-powered training jobs using ModelTrainer class.

The authors would like to thank Charles Thompson and Kanwaljit Khurmi for their collaboration. About the authors

Mona Mona Mona is a generative AI Specialist Solutions Architect at Amazon focusing. She is a published author of two books – Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide.

Mayank Jha Mayank is a Senior Machine Learning Engineer at Amazon Search working on the model training optimization. He is passionate about finding practical applications for complex problems at hand and aims to develop solutions that have a deep impact on how businesses and people thrive.

Bruno Pistone Bruno is a Senior generative AI and ML Specialist Solutions Architect for AWS based in Milan. He works with large customers helping them to deeply understand their technical needs and design AI and Machine Learning solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He enjoys spending time with his friends and exploring new places, as well as travelling to new destinations.

James Park James is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In his spare time he enjoys seeking out new cultures, new experiences, and staying up to date with the latest technology trends.

In today's security-conscious world, organizations are implementing strong authentication methods for applications running inside virtual desktop environments. A common challenge is enabling users to leverage their local security keys and biometric devices with applications running inside a remote desktop session. Amazon WorkSpaces now addresses this challenge with WebAuthn redirection, allowing users to seamlessly use their local FIDO2-compatible authentication devices with applications running inside their Windows and Linux WorkSpaces. This redirection capability bridges the gap between physical and virtual environments, enhancing security without compromising productivity. What is WebAuthn and why does it matter in virtual desktops? Web Authentication (WebAuthn) is an open standard that enables strong, passwordless authentication using hardware security keys, biometrics, and platform authenticators like Windows Hello. By using WebAuthn redirection in Amazon WorkSpaces, users can leverage the same authentication methods in their virtual desktop that they use on their local devices, providing a consistent experience across environments while enhancing security. WebAuthn redirection bridges the gap between your local authentication devices and remote applications. When an application inside your WorkSpace requests WebAuthn authentication, that request is securely redirected to your local device, allowing you to authenticate with your security key or biometric authenticator. Implementation Options Amazon WorkSpaces offers two approaches to WebAuthn integration: Standard WebAuthn (Windows and Linux WorkSpaces) uses browser extensions to facilitate WebAuthn redirection, ensuring compatibility across different platforms. This implementation works with popular browsers including Microsoft Edge, Google Chrome, and Chromium. Enhanced WebAuthn (Windows WorkSpaces) eliminates the need for browser extensions, providing a streamlined experience. Key benefits include:

No browser extension required Support for WebAuthn in both web browsers and native Windows applications Improved authentication performance Seamless experience across different application types.

Both approaches enable users to authenticate with a variety of FIDO2-compliant devices including YubiKeys, Windows Hello, and other biometric authenticators. Business Benefits WebAuthn Redirection for Amazon WorkSpaces helps organizations:

No USB redirection required – Use security keys and biometric authenticators without the complexity of USB device management Seamless authentication experience – Users authenticate to applications in their WorkSpace using their local devices without additional configuration Consistent security implementation – Apply the same authentication policies across physical workstations and virtual desktops Zero additional hardware cost – Leverage existing authentication devices without purchasing specialized equipment for virtual environments

Availability WebAuthn support is available today in all AWS Regions where Amazon WorkSpaces is offered. It works with:

Windows WorkSpaces bundles with DCV (both Enhanced and Standard WebAuthn) Linux WorkSpaces bundles (Standard WebAuthn) Amazon WorkSpaces clients for Windows, macOS, and Linux

There is no additional charge for using WebAuthn redirection with Amazon WorkSpaces. Getting Started To enable WebAuthn redirection in your WorkSpaces environment:

Ensure your WorkSpaces are running the latest version of the WSP host agent (version 2.0.0.1425 or higher on Windows and version 2.1.0.1923 or higher on Linux WorkSpaces) Verify your WorkSpaces client is updated to version 5.29.0 or higher Ensure WebAuthn is enabled through GPO on the WorkSpace. The feature is enabled by default.

Conclusion WebAuthn redirection in Amazon WorkSpaces eliminates the traditional barriers between local authentication devices and virtual desktop applications. By enabling your users to authenticate with their familiar FIDO2-compatible devices directly within their WorkSpaces, you can strengthen security while maintaining the productivity your organization depends on. Get started today with WebAuthn redirection and transform how your users authenticate in virtual desktop environments. For detailed implementation instructions, refer to the WebAuthn Redirection section of the WorkSpaces administrator guide.

. . Chirag is a Senior Product Manager for Amazon DCV, where he works with enterprise customers on remote work and virtualization solutions. He is passionate about building products that solve real business challenges while delivering intuitive user experiences.

Application authorization is a critical component of modern software systems, determining what actions users can perform on specific resources. Many organizations have adopted Open Policy Agent (OPA) with its Rego policy language to implement fine-grained authorization controls across their applications and infrastructure. While OPA has proven effective for policy-as-code implementations, organizations are increasingly looking for more performant and managed services that reduce operational overhead while maintaining the flexibility and power of policy-based authorization. Amazon Verified Permissions is a fully managed authorization service that uses the Cedar policy language to help you implement fine-grained permissions for your applications. Cedar is an open source policy language developed by AWS that provides many of the same capabilities as Rego while offering improved performance (42–60 times faster than Rego), straightforward policy authoring, and formal verification capabilities. By migrating from OPA to Verified Permissions, organizations can reduce the operational burden of managing authorization infrastructure while gaining access to a service designed specifically for scalable, secure authorization. This migration offers several key benefits: reduced infrastructure management overhead, improved policy performance and validation, enhanced security through the AWS managed service model, and seamless integration with other AWS services. Additionally, Cedar's syntax is designed to be more intuitive than Rego, reducing the effort needed to write, read, and maintain policies. In this post, we explore the process of migrating from OPA and Rego to Verified Permissions and Cedar, including policy translation strategies, software development and testing approaches, and deployment considerations. We walk through practical examples that demonstrate how to convert common Rego policies to Cedar policies and integrate Verified Permissions into your existing applications.

The migration from OPA to Verified Permissions represents a shift from self-managed authorization infrastructure to a fully managed service. In a typical OPA setup, customers have OPA servers running either as sidecars, standalone services, or embedded libraries that evaluate Rego policies against incoming authorization requests. These servers pull policy bundles from storage systems and maintain their own performance and availability. With Verified Permissions, AWS manages the entire authorization infrastructure. Applications make API calls to the Verified Permissions service which evaluates Cedar policies stored in managed policy stores. This removes the need to operate and maintain OPA servers, manage policy distribution, or handle service scaling and availability. This shift means that your team can concentrate on authorization logic rather than infrastructure management while gaining the benefits of the scale and reliability provided by AWS.

It's important to understand the fundamental differences between the Rego and Cedar policy languages before beginning your migration. These differences will shape how you approach translating your existing policies.

Rego policies are built around rules that can be evaluated to produce sets of results. Rego uses a logic programming approach where you define conditions that must be satisfied for a rule to be true. Policies often involve complex queries, loops, and comprehensions to examine data structures. Example Rego policy

# Rule 1: Allow users with the viewer role to read documents allow { input.action == "read" input.resource.type == "document" input.user.role == "viewer" } # Rule 2: Allow users with the editor role to write documents allow { input.action == "write" input.resource.type == "document" input.user.role == "editor" }

Cedar takes a more declarative approach with explicit permit and forbid statements. Each Cedar policy is a standalone authorization decision that clearly states what is being allowed or denied. Cedar policies are designed to be human-readable and straightforward to audit. Equivalent Cedar policies

// Policy 1: Allow principals with the viewer role to read documents permit ( principal in UserRole::"viewer", action == Action::"read", resource in ResourceType::"document" ); // Policy 2: Allow principals with the editor role to write documents permit ( principal in UserRole::"editor", action == Action::"write", resource in ResourceType::"document" );

One of the most significant differences between the two evaluation engines is how they handle data. Rego works with arbitrary JSON input data, giving users complete flexibility in how they structure authorization requests. Users can access any field in your input data using Rego's path notation. Cedar allows for the creation of a defined schema with typed entities. This means that users need to model authorization data as entities with specific types, attributes, and relationships. While this requires more upfront planning, it provides superior validation, runtime performance, and tooling support.

Rego and Cedar differ fundamentally in their approaches to policy evaluation. Rego uses a logic programming model and, as a result, policy evaluation functions much like a logic puzzle solver. It starts with a question and searches backward through linked rules to find an answer. This approach allows for flexible policy composition but can often be slower, less predictable, and more difficult to audit. Cedar, on the other hand, uses a simpler functional evaluation approach. It uses a straightforward evaluation model where each policy is checked independently against the authorization request. Policies use basic conditional logic to produce fast, deterministic allow or deny decisions. A policy either fully matches the authorization request (principal, action, resource, and all conditions), or it doesn't apply. This is essential for high-performance authorization scenarios where predictable evaluation time and clear audit trails are essential. Cedar policy evaluation follows four core principles:

Default deny for access not explicitly granted Forbid overrides permit for handling policy conflicts Order-independent evaluation to prevent bugs Deterministic outcomes for reliable results

Before you can begin migrating your authorization policies, you need to establish the foundational infrastructure in Verified Permissions.

To illustrate the migration process, you will use a fictional document management application that uses OPA and Rego for authorization. The first step in migrating to Verified Permissions is creating a policy store. A policy store is a container for your Cedar policies and schema. You can create multiple policy stores for different applications or environments. When creating a policy store, you choose between two validation modes:

STRICT mode: Requires a schema against which policies are validated OFF mode: Allows policies without a schema (useful for initial testing)

For production migrations, STRICT mode is recommended because it provides better validation compared to OFF mode and can enable optimizations that reduce the entity data needed for authorization requests. You can create a policy store through the AWS Management Console, AWS Command Line Interface (AWS CLI), or programmatically using AWS SDKs. The following example uses the AWS CLI:

aws verifiedpermissions create-policy-store \ --region us-east-1 \ --validation-settings mode=STRICT \ --description "Migration from OPA to Amazon Verified Permissions"

If the request is successful, you should see a JSON encoded response that looks like the following:

{ "policyStoreId": "PSEXAMPLEabcdefg012345", "arn": "arn:aws:verifiedpermissions:us-east-1:123456789012:policy-store/PSEXAMPLEabcdefg012345", "createdDate": "2025-09-15T10:30:45.123456+00:00", "lastUpdatedDate": "2025-09-15T10:30:45.123456+00:00" }

Make note of the policyStoreId from the response—you will need it for subsequent operations.

In STRICT mode, Verified Permissions requires a Cedar schema that defines the types of entities in an authorization system. This schema serves several important purposes, including validating policies at creation time, enabling entity slicing performance optimizations, enabling better tooling and IDE support, and documenting your authorization model. The schema should define:

Entity types: The kinds of objects in your system (for example, users, roles, documents, and so on.) Attributes: Properties that entities can have (for example, department, classification, and createdDate) Actions: Operations that can be performed (for example, read, write, and delete) Relationships: How entities relate to each other (for example, user belongs to role, document owned by user)

When designing a schema, you should consider how your current OPA input data maps to Cedar entities. For example, if your Rego policies access input.user.department, you will need a User entity type with a department attribute. The following is an example Cedar schema for your document management application:

{ "MyApp": { "entityTypes": { "User": { "shape": { "type": "Record", "attributes": { "department": {"type": "String"}, "jobLevel": {"type": "Long"}, "email": {"type": "String"} } } }, "Role": { "shape": { "type": "Record", "attributes": {"name": {"type": "String"}} } }, "Document": { "shape": { "type": "Record", "attributes": { "owner": {"type": "Entity", "name": "User"}, "classification": {"type": "String"}, "createdDate": {"type": "String"} } } } }, "actions": { "read": {"appliesTo": {"principalTypes": ["User"], "resourceTypes": ["Document"]}}, "write": {"appliesTo": {"principalTypes": ["User"], "resourceTypes": ["Document"]}}, "delete": {"appliesTo": {"principalTypes": ["User"], "resourceTypes": ["Document"]}} } } }

To apply this schema to the policy store you created earlier using the AWS CLI, you can run the following command:

aws verifiedpermissions put-schema \ --region us-east-1 \ --policy-store-id YOUR_POLICY_STORE_ID \ --definition file://schema.json

Ensure that you replace YOUR_POLICY_STORE_ID with the policyStoreId that was returned when you created your policy store. You can view the visualized policy schema (shown in Figure 1) in the Verified Permissions console by going to Policy Store and choosing Schema.

With your policy store and schema in place, you can now begin translating your Rego policies into Cedar policies, following common authorization patterns.

Role-based access control (RBAC) is one of the most used authorization patterns. In RBAC systems, users are assigned roles, and roles are granted permissions to perform actions on resources. In your current Rego implementation, you might check if a user has a specific role in their roles array, then allow certain actions based on that role. Your Rego policy might look something like the following:

allow if { input.user.roles[_] == "editor" input.action in ["read", "write"] }

allow if { input.user.roles[_] == "viewer" input.action == "read" }

When migrating to Cedar, you will model this using entity relationships where users belong to role entities.

// Admin users can perform any action on any resource permit ( principal in MyApp::Role::"admin", action, resource );

// Editor users can read and write on every resource permit ( principal in MyApp::Role::"editor", action in [MyApp::Action::"read", MyApp::Action::"write"], resource );

// Viewer users can only read on every resource permit ( principal in MyApp::Role::"viewer", action == MyApp::Action::"read", resource );

Migration approach To successfully migrate your RBAC policies from Rego to Cedar, follow these steps:

Define User and Role entity types in your schema Create permit policies for each role-action combination Use the Cedar in operator to check role membership Consider creating role hierarchies if you have nested roles

Key differences Understanding the fundamental differences between Rego and Cedar's approach to RBAC will help you design more effective policies:

Cedar uses entity relationships instead of checking array membership Each permission becomes a separate, explicit policy Role hierarchies are modeled through entity parent-child relationships

Attribute-based access control (ABAC) makes authorization decisions based on attributes of the user, resource, action, and environment. This is often more flexible than RBAC but can be more complex to implement. In Rego, you would access various attributes from the input data and use them in policy conditions:

default allow := false # Anyone can read public documents allow if { input.action == "read" input.resource.classification == "public" }

# Users can read internal documents from their department allow if { input.action == "read" input.resource.classification == "internal" input.user.department == input.resource.department }

# Users can write to documents they own allow if { input.action == "write" input.resource.owner == input.user.id }

Cedar handles this through entity attributes and policy conditions using the when and unless clauses.

// Anyone can read public documents. Blank 'principal' and 'resource' entities are wildcards that match everything permit ( principal, action == MyApp::Action::"read", resource ) when { resource.classification == "public" };

// Users can read internal documents from their department permit ( principal, action == MyApp::Action::"read", resource ) when { resource.classification == "internal" && principal.department == resource.department };

// Users can write to documents they own permit ( principal, action == MyApp::Action::"write", resource ) when { resource.owner == principal };

Migration approach Migrating ABAC policies requires careful mapping of attributes from your Rego input structure to Cedar's entity model:

Identify the attributes used in your current policies Map these attributes to entity attributes in your Cedar schema Use when clauses in Cedar policies to implement attribute-based conditions Consider using context for environment-specific attributes (time, IP address, and so on)

Key differences Cedar's schema-driven approach to attributes provides several advantages over Rego's dynamic attribute access:

Cedar requires attributes to be defined in the schema Cedar schema validation helps catch attribute access errors at policy creation time Complex attribute logic might need to be split across multiple policies

Relationship-based access control (ReBAC) grants permissions based on properties of the resource being accessed or relationships between the user and the resource (such as ownership). In Rego, this might be expressed as follows:

# Allow document owners to perform any action allow if { input.resource.type == "document" input.resource.owner_id == input.user.id }

# Alternative: checking ownership through a separate ownership data structure allow if { input.resource.type == "document" ownership := data.ownerships[input.resource.id] ownership.owner_id == input.user.id }

In the preceding example, ownership is checked by comparing the owner_id attribute on the resource with the user's ID. You might access this from the input data directly or from a separate data source. In Cedar, relationships are first-class concepts. The resource.owner == principal syntax directly checks if the principal is the owner entity referenced by the resource. This is more natural and type-safe than string comparisons:

permit ( principal, action, resource is MyApp::Document ) when { resource.owner == principal };

Migration approach Converting relationship-based policies requires modeling your data relationships as Cedar entity references:

Model resources as Cedar entities with relevant attributes Use resource attributes in policy conditions Model ownership and other relationships through entity references Use Cedar's attribute access syntax for resource properties

Many authorization systems need to consider contextual information such as time of day, user location, or request characteristics (IP address, user-agent, and so on). Expressing this in Rego would look like the following example:

default allow := false # Allow read access during business hours (9 AM to 5 PM UTC) allow if { input.action == "read" current_hour := time.clock([time.now_ns(), "UTC"])[0] current_hour >= 9 current_hour <= 17 }

In Cedar, the same policy logic can be expressed like the following:

// Allow read access during business hours (9 AM to 5 PM UTC) permit ( principal, action == MyApp::Action::"read", resource ) when { context.currentTime.hour >= 9 && context.currentTime.hour <= 17 };

Migration approach Context-based policies in Cedar use the context parameter passed with each authorization request:

Use Cedar's context feature for environment information Pass time-based information in the authorization request context Create policies with time-based conditions using context attributes Consider caching implications for time-sensitive policies

After migrating your policies to Cedar, you need to update your application code to integrate with Verified Permissions.

The most significant change in your application code will be replacing OPA API calls with Verified Permissions API calls. Understanding the differences between these systems will help you plan your integration work effectively. The sample code in this section is written in Python.

When calling OPA, you typically send a single JSON payload containing the authorization data. For example, your current OPA request might look like the following:

opa_request = { "input": { "user": { "id": "user123", "department": "engineering", "role": "editor" }, "resource": { "id": "doc456", "type": "document", "owner": "user123" }, "action": "read" } }

response = requests.post( " ", json=opa_request ) authorized = response.json()["result"]

Verified Permissions requires a more structured approach where principals, resources, and actions are explicitly typed entities.

import boto3 import json from typing import Dict, Any, List

class AuthorizationService: def __init__(self, policy_store_id: str, region: str = 'us-east-1'): self.client = boto3.client('verifiedpermissions', region_name=region) self.policy_store_id = policy_store_id

#Check if a principal is authorized to perform an action on a resource. def is_authorized(self, principal: Dict[str, Any], action: str, resource: Dict[str, Any], context: Dict[str, Any] = None) -> bool: try: # Convert to Cedar entity format principal_entity = self._to_cedar_entity(principal, "User") resource_entity = self._to_cedar_entity(resource, "Document") action_entity = {"actionType": "MyApp::Action", "actionId": action}

request = { 'policyStoreId': self.policy_store_id, 'principal': principal_entity, 'action': action_entity, 'resource': resource_entity }

response = self.client.is_authorized(**request) return response['decision'] == 'ALLOW' except Exception as e: print(f"Authorization error: {e}") return False

def _to_cedar_entity(self, entity_data: Dict[str, Any], entity_type: str) -> Dict[str, Any]: # Convert application data to Cedar entity format return { 'entityType': f'MyApp::{entity_type}', 'entityId': str(entity_data.get('id', '')), 'attributes': entity_data }

Entity type declarations: Each entity (principal, resource) must include an entityType that matches your Cedar schema Entity IDs: Every entity requires a unique entityId for identification Action format: Actions are specified with an actionType and actionId rather than as simple strings Separate context: Environmental information like time, IP address, or user agent is passed in a separate context parameter

OPA returns whatever your Rego policy outputs, which could be a Boolean, a set of allowed actions, or complex nested data structures. Regardless of the policy outputs, Verified Permissions returns a consistent authorization decision structure:

# Amazon Verified Permissions response structure { 'decision': 'ALLOW',# or 'DENY' 'determiningPolicies': [...],# Which policies determined the decision 'errors': [...]# Errors that occurred during evaluation }

Your application logic becomes simpler because you need to check for only ALLOW or DENY:

# Example principal (user) user = { 'id': 'user123', 'department': 'engineering', 'jobLevel': 5, 'email': [EMAIL] }

# Example resource (document) document = { 'id': 'doc456', 'owner': 'user123', 'classification': 'internal', 'department': 'engineering' }

# Example context context = { 'currentHour': 14,# 2 PM 'userAgent': 'MyApp/1.0' }

# Check authorization can_read = auth_service.is_authorized(user, 'read', document, context) can_write = auth_service.is_authorized(user, 'write', document, context)

print(f"User can read document: {can_read}") print(f"User can write document: {can_write}")

OPA errors typically relate to policy evaluation issues or server connectivity problems. With Verified Permissions, you'll encounter AWS-specific error types, as shown in the following example:

def is_authorized_with_error_handling(self, principal, action, resource, context=None): try: principal_entity = self._to_cedar_entity(principal, "User") resource_entity = self._to_cedar_entity(resource, "Document") action_entity = {"actionType": "MyApp::Action", "actionId": action}

response = self.client.is_authorized(**request) return response['decision'] == 'ALLOW' except ClientError as e: error_code = e.response['Error']['Code']

if error_code == 'ResourceNotFoundException': print(f"Policy store not found: {self.policy_store_id}") elif error_code == 'ValidationException': print(f"Invalid request: {e.response['Error']['Message']}") elif error_code == 'ThrottlingException': print("Request throttled - consider implementing exponential backoff") else: print(f"AWS error: {error_code}")

It's important to note that the AWS SDK provides built-in retry logic for transient failures. The following is an example of how you can enable this feature:

# Configure retry behavior config = Config( retries={ 'max_attempts': 3, 'mode': 'adaptive'# Automatically adjusts retry behavior }, connect_timeout=5, read_timeout=10 )

Your current authorization data needs to be transformed into Cedar's entity format. This transformation happens in the _to_cedar_entity method shown in the error handling changes example, but let's break down what's involved. Extracting entity information Identify which parts of your current OPA input represent the principal, resource, and action. In most OPA implementations, this mapping is straightforward:

# Current OPA structure opa_input = { "user": {...},# This becomes the principal "resource": {...},# This becomes the resource "action": "read"# This becomes the action }

# Map to Cedar structure principal = opa_input["user"] resource = opa_input["resource"] action = opa_input["action"]

Adding type information Cedar requires explicit type declarations for all entities. You'll need to determine the appropriate entity type based on your schema:

def _determine_entity_type(self, entity_data: Dict[str, Any]) -> str: # Determine the Cedar entity type based on entity data. This logic will be specific to your application. # Example: determine type based on entity structure or type field if 'role' in entity_data: return 'User' elif 'document_type' in entity_data: return 'Document' elif 'name' in entity_data and 'member_count' in entity_data: return 'Team' else: raise ValueError(f"Cannot determine entity type for: {entity_data}")

def _to_cedar_entity(self, entity_data: Dict[str, Any], entity_type: str = None) -> Dict[str, Any]: # Convert application data to Cedar entity format. if entity_type is None: entity_type = self._determine_entity_type(entity_data)

return { 'entityType': f'MyApp::{entity_type}', 'entityId': str(entity_data.get('id', '')), 'attributes': entity_data }

Structuring attributes Cedar attributes must match your schema definition, so you might need to transform attribute names or values. This is also a chance to iterate and improve on naming. The following example demonstrates a code pattern to convert attribute names and values in code.

def _prepare_attributes(self, entity_data: Dict[str, Any], entity_type: str) -> Dict[str, Any]: #Prepare entity attributes according to Cedar schema requirements. attributes = {}

if entity_type == 'User': # Map OPA field names to Cedar schema field names attributes = { 'department': entity_data.get('dept', entity_data.get('department')), 'jobLevel': int(entity_data.get('job_level', entity_data.get('jobLevel', 0))), 'email': entity_data.get('email', entity_data.get('email_address')) } elif entity_type == 'Document': attributes = { 'classification': entity_data.get('classification','internal'), 'department': entity_data.get('department'), 'owner': entity_data.get('owner', entity_data.get('owner_id')) }

# Remove None values return {k: v for k, v in attributes.items() if v is not None}

Handling context Separate environmental information from entity data. Context information should not be part of entity attributes.

# Entity data only includes intrinsic properties principal = { 'id': user_data['id'], 'department': user_data['department'], 'jobLevel': user_data['job_level'] }

resource = { 'id': resource_data['id'], 'classification': resource_data['classification'], 'owner': resource_data['owner'] }

# Context includes environmental and request-specific data context = {} if request_metadata: context = { 'currentHour': request_metadata.get('hour'), 'ipAddress': request_metadata.get('ip_address'), 'userAgent': request_metadata.get('user_agent'), 'requestTime': request_metadata.get('timestamp') } return self.is_authorized(principal, action, resource, context)

The most critical aspect of migration testing is verifying that you have correctly migrated your authorization logic from Rego to Cedar. This requires systematic testing with comprehensive test cases.

Inventory current policies: Document your current Rego policies, including their decision logic, input data requirements, and expected outcomes for key test scenarios Create test scenarios: Develop test cases covering all policy branches and edge cases Capture current behavior: Run your test cases against OPA to establish baseline results Test Cedar policies: Run the same test cases against your Cedar policies Analyze differences: Investigate mismatches and adjust policies accordingly

When testing your policies, start with basic, straightforward policies before tackling complex ones. Test both positive cases (should be allowed) and negative cases (should be denied) and include edge cases and boundary conditions. Additionally, test with real production data (anonymized if necessary) to verify that your policies will work effectively when implemented in production. It's also important to compare the performance characteristics of your OPA setup with Verified Permissions across several key metrics. These metrics should include average response time for authorization requests, throughput (requests per second), and error rates under normal and stress conditions. During testing, test from the actual deployment environment used by your application and account for network latency to AWS services. Finally, you should test the complete integration between your application and Verified Permissions across several critical areas. Your integration testing should cover authentication and AWS credential handling, request/response data transformation, error handling and fallback scenarios, connection pooling and resource management, and logging and monitoring integration to help ensure that the components work together seamlessly.

A successful migration from OPA to Verified Permissions requires careful planning and a risk-managed deployment approach that minimizes disruption to your production systems.

Rather than switching entirely to Verified Permissions in a single step, implement a phased migration to reduce risk.

Parallel deployment: Deploy Verified Permissions alongside your existing OPA infrastructure and route a small percentage of authorization requests to the new system. Log and compare results between both systems, focusing on non-critical operations initially to minimize risk during the transition process. Gradual traffic shift: Gradually increase the percentage of requests routed to Verified Permissions while monitoring system performance, error rates, and authorization accuracy. Implement circuit breaker patterns to fall back to OPA if needed and expand to more critical operations as your confidence grows in the reliability and performance of the new system. Full migration: Route all traffic to Verified Permissions but keep OPA infrastructure running temporarily. Monitor system behavior under full production load and decommission OPA infrastructure after stability is confirmed and you are confident in the performance of the new system.

Use feature flags to control the migration process through various flag types. These include percentage-based rollout to route a specific percentage of requests to the new system, user-based rollout to route specific users or user groups to the new system, operation-based rollout to route specific types of operations to the new system, and environment-based rollout to use different systems in different environments. Feature flags provide several benefits, including instant rollback capability if issues arise, granular control over migration scope, A/B testing of authorization decisions, and safe experimentation with new policies.

When migrating from Rego to Cedar, you might encounter several common issues. In this section, you'll find a troubleshooting guide.

Some Rego policies use complex logic that doesn't directly translate to Cedar. For example:

# Complex Rego policy with loops and comprehensions allow { some i # The i variable is used to iterate over the items in the input.user.permissions array input.user.permissions[i].resource == input.resource.id input.user.permissions[i].actions[_] == input.action # The wildcard _ is used to iterate over the items in the actions array }

In these scenarios, you should restructure your data model to work better with Cedar's entity-based approach. For example, Cedar provides the in operator for improved performance and readability, as shown in the following example:

permit ( principal, action, resource ) when { principal has permission && resource in principal.permission.resources && action in principal.permission.actions };

You can use the schema validation tools provided by Verified Permissions to triage these issues.

Adhering to the following recommendations and best practices will help you build a maintainable, secure, and performant authorization system with Verified Permissions.

Well-designed policies are the foundation of a reliable authorization system and directly impact maintainability and security:

Schema-first design: Start with a comprehensive schema design before writing policies. A well-designed schema makes policy authoring more maintainable. Basic, explicit policies: Favor multiple basic policies over complex monolithic ones. Cedar's explicit permit/forbid model works best with clear, straightforward policy statements. Meaningful naming: Use descriptive names for entity types, attributes, and policy descriptions. This improves understandability and maintainability of polices. Documentation: Document your authorization model, including entity relationships, policy intentions, and business rules.

Successfully migrating your authorization system requires balancing speed with safety through deliberate, incremental steps:

Incremental approach Don't attempt to migrate everything at once. Start with basic, low-risk policies and gradually move to more complex scenarios. Start in audit mode: Calculate and log the policy decisions for both systems. This will help you to compare results without impacting runtime authorization. Comprehensive testing: Invest heavily in testing during migration. The cost of thorough testing is much less than the cost of authorization failures in production. Parallel operations: Run both systems in parallel during migration to validate policy behavior and build confidence in the new system. Team training: Ensure your team understands Cedar's policy model and syntax. The conceptual differences from Rego require a learning investment.

Maintaining a production authorization system requires ongoing attention to operational concerns beyond the initial migration:

Version control: Treat policies as code with proper version control, code review, and deployment processes. Monitoring and alerting: Implement comprehensive monitoring from day one. Authorization issues can have significant business impact. Regular audits: Periodically review and audit policies to verify that they still meet business requirements and security standards. Performance optimization: Continuously monitor and optimize performance, particularly around caching strategies and policy efficiency.

Migrating from Open Policy Agent to Amazon Verified Permissions represents a significant step toward reducing operational overhead, improving runtime authorization performance and enhancing governance while maintaining robust authorization capabilities. The migration journey from OPA to Verified Permissions isn't only about changing technologies, it's an opportunity to improve your authorization architecture, enhance security practices, and build a more scalable foundation for your application's access control needs. Thank you for reading this post. If you have comments or questions about migrating from OPA to Verified Permissions, leave them in the comments section below.

The following links provide resources for further reading on the topics covered in this blog post:

Amazon Verified Permissions User Guide Cedar Policy Language Specification Simplify How You Manage Authorization in Your Applications with Amazon Verified Permissions Cedar Policy Language on GitHub Amazon Verified Permissions API Reference

If you have feedback about this post, submit comments in the Comments section below. If you have questions about this post, contact AWS Support.

Samuel Folkes Samuel is a Senior Security Solutions Architect at Amazon Web Services with more than 18 years of experience in software architecture, networking, and cybersecurity. Prior to AWS, he worked as a software engineer and led engineering teams across multiple industries. Samuel specializes in identity and access management and is passionate about using emerging technologies to drive business value.

Jupyter Deploy: Create a JupyterLab application with real-time collaboration in the cloud in minutes

Jupyter notebooks have become a popular tool for data scientists, researchers, educators and analysts who need to experiment with code, visualize data, and document their findings. Many users run Jupyter on their laptops. This creates limitations to collaborate with a distributed team because users cannot securely provide direct access to their local JupyterLab application over the internet. Similarly, users are limited by the compute power of their own device. If their workload requires more compute, for example GPU accelerators to fine-tune deep learning models, it requires a different setup. Large enterprises can afford teams of engineers to set up and maintain deployment frameworks or managed services that support distributed compute with secure and fast remote connections, but that is beyond the resources of small organizations such as startups or research teams. Introducing Jupyter Deploy Today, the AI/ML Open Source team at AWS is excited to share the availability of Jupyter Deploy: a new open source command line interface (CLI) to deploy Jupyter to the cloud in a few minutes. Jupyter Deploy gives you a dedicated and sharable application running on your own domain with encrypted HTTP (TLS), GitHub OAuth integration, real-time-collaboration, and high performance UV-based development environment for applying updates and further configure your application. The CLI leverages Infrastructure-as-code (IaC) to let you define and manage resources using configuration files instead of manual setup through web consoles. Like the rest of Jupyter, the Jupyter Deploy CLI is vendor neutral, although this blog will focus on deploying to AWS. GitHub OAuth enables your collaborators to log in with their existing GitHub accounts instead of creating and managing new credentials. With a few commands from the Jupyter Deploy CLI, you can set up an infrastructure-as-code project, configure it, and use it to create and manage the cloud resources. The first step consists of selecting a Jupyter Deploy template. Templates are pre-built infrastructure-as-code projects that handle the technical set up for you. Think of them as recipes that tell the system exactly what cloud resources to create and how to configure them. These templates are available as Python libraries and are automatically discovered by the Jupyter Deploy CLI after you install them in your Python environment. The Jupyter Deploy CLI ships with a base template with terraform infrastructure-as-code engine, AWS cloud provider, Amazon Elastic Compute Cloud (Amazon EC2) instance, and GitHub authentication. How Jupyter Deploy simplifies collaboration Once you have deployed your JupyterLab application to your new Amazon EC2 instance, you can access it from your web browser as you would if Jupyter was running locally on your laptop. But unlike a locally-run JupyterLab, you can also share its URL with others. When your collaborators paste the URL in their own web browser, they will be prompted to authenticate via GitHub. If their identity matches your allowlist, they will get access to your JupyterLab application, and you can work simultaneously thanks to the real-time-collaboration feature of Jupyter with the latest improvements provided by jupyter-server-documents. Need more compute? Run a couple commands from the directory of your Jupyter Deploy project to swap out your instance. Amazon EC2 offers a broad range of instance types: you can use compute optimized instances for CPU-intensive data science projects or GPU instances for deep-learning, for example. Another collaborator needs access your application? Another simple command takes care of it! Same goes for adding more storage; you can mount new Amazon Elastic Block Stores (Amazon EBS), scale the existing ones, or mount a new or existing Amazon Elastic File System (Amazon EFS) with a couple jupyter-deploy commands. How to use Jupyter Deploy The first-time set up process involves five main steps: configuring AWS, getting a domain, setting up GitHub authentication, installing the tool, and deploying your environment. You can reuse your domain, AWS account, GitHub apps for future deployments. Set up an AWS account and AWS credentials locally If you do not have an AWS account, you can take advantage of the AWS Free Tier program to set up your Jupyter Deploy project. Next, install the AWS Command Line Interface (AWS CLI) in your environment and configure the AWS CLI to access your own AWS account. Confirm your setup by running the following command that verifies that your AWS credentials are working correctly: aws sts get-caller-identity # you should see a response referencing your AWS account. Buy a domain on Amazon Route 53 Next, open the AWS console, log in with your email and password, and navigate to Amazon Route 53. Amazon Route 53 is the AWS service that manages Domain Name System (DNS) operations. Amazon Route 53 will allow you to associate DNS records – a global address such as amazon.com – to the IP address of the Amazon EC2 instance the base template will set up for you. To follow the steps in this post, you need to own a domain and have the ability to create DNS records with Amazon Route 53. If you do not own a domain, you can buy one from Amazon Route 53 console for less than $20 per year. Navigate to Amazon Route53 console. In the navigation pane, choose Domains, then Registered domains and choose Register domains.

We went ahead and purchased the domain. It automatically creates a Route 53 hosted zone in your AWS account. Per Amazon Route 53 documentation, a hosted zone is a container for records, which includes information about how you want to route traffic for a domain (such as example.com) and all of its subdomains (such as www.example.com, retail.example.com, and seattle.accounting.example.com). A hosted zone has the same name as the corresponding domain.

Set up your GitHub OAuth app The next step is to set up a GitHub OAuth app. Jupyter Deploy will configure your Amazon EC2 instance to redirect users to GitHub to authenticate by referencing this GitHub app in your GitHub account. If you do not have a GitHub OAuth app yet, follow this link to create one in your GitHub account. For more details, refer to the GitHub documentation. · Application name: type JupyterLab application (or any name) · Homepage URL: type your {your-domain}, in our case example.com · Application description: type jupyter.{your-domain} or leave empty (useful to remember the domain that this OAuth app refers to. · Authorization callback URL: type {your-domain}/oauth2/callback, in our case GitHub will use this callback URL to communicate the result of the authentication to your application. This URL MUST match exactly your domain or the authentication flow will fail. · Enable Device Flow: leave unset. Now that your GitHub OAuth app is configured, you should be able to see it on the developer settings page. When you open it, you'll see it has a Client ID and a Client Secret. Write down the Client ID, then generate the secret and save its value, we'll need both of them later. Create your Python environment and install Jupyter Deploy Next, let's start the jupyter-deploy project. We will create a Python virtual environment to separate the dependencies of your jupyter-deploy project from the rest of your system. We will use UV to do so. · Open a terminal, and navigate to your home directory cd ~ · Create a UV project: uv init jupyter-deploy-projects --bare · Install the Jupyter Deploy CLI: uv add jupyter-deploy . This command creates a virtual environment in your current directory, then downloads and install the CLI, the base template, and its dependencies. · Now activate this environment: source .venv/bin/activate · Then run jupyter-deploy --help You're all set! Time to create our first Jupyter Deploy project. Initialize your project Similarly to uv, the Jupyter Deploy CLI sets up a project for you: · In your terminal, create a new directory for your project: mkdir jupyterlab-on-ec2 · Change directory: cd jupyterlab-on-ec2 · Then run jupyter-deploy init . This generates the following project scaffolding: ~/jupyter-deploy-projects/jupyterlab-on-ec2 |_ engine |_ manifest.yaml |_ services |_ variables.yaml Configure your project with the interactive experience of Jupyter Deploy It's now time to configure your project. You'll need at hand: · your domain · your GitHub OAuth app Client ID · your GitHub OAuth app Client secret · your GitHub username Run jupyter-deploy config -s to verify your installation and prompt you to install required tools such as terraform, AWS CLI, jq and possibly other tools your specific template needs. Follow these instructions to install the dependencies, then run the command again: jupyter-deploy config -s The template uses Lets Encrypt to generate the TLS certificates so that the traffic to your JupyterLab application is encrypted end-to-end. You will now enter the interactive experience. The CLI will prompt you to select values for each of the required variables for this template. · var.domain, type your domain (just the domain, without in our case example.com · var.letsencrypt email, type your email. Letsencrypt uses it to communicate with your upcoming TLS certificates expiry. You do not have to confirm this email, in which case you will not receive any communication from letsencrypt but your JupyterLab application will work fine. · var.oauth_allowed_org leave blank · var.oauth_allowed_teams type: [] · var.oauth_allowed_usernames type ["your-user-name"] · var.oauth_app_client_id type the Client ID · var.oauth_app_client_secret type the Client secret · var.subdomains type jupyter If you made a mistake, run jupyter-deploy config -- VAR_VALUE For example jupyter-deploy config --domain example.com will change the domain variable while keeping the rest of your configuration in place. Create the cloud resources Now it's time to actually create your cloud resources. Run the following command jupyter-deploy up Here terraform takes over, and prepares the deployment for you. Infrastructure-as-code is a technology that enables you to create and manage multiple inter-dependent cloud resources based on declarative files. Terraform is a popular software tool for infrastructure as code. It is not affiliated with AWS, and can be used to deploy other cloud provider resources. Terraform prepares the plan, and displays the resources it will create in your AWS account using your AWS credentials. Type yes when prompted. Open your application Now, your infrastructure is ready, and is serving your application at the sub-domain.domain that we specified. Access it with jupyter-deploy open Your terminal might display a pop up asking you to confirm whether you trust this domain. Since you own it, you can trust it so answer yes. Now you will be redirected to the authentication page

You will then be redirected to the familiar JupyterLab launcher page.

From here, you can open and run notebooks, install new packages, or interact with the terminal. Add a collaborator and share your notebook URL Next, let's start collaborating on this application. Ask the person you want to collaborate with for their GitHub username. Then run the following command from your jupyter-deploy project directory: jupyter-deploy users list You should see only your own GitHub username. Now add your collaborator's username with: jupyter-deploy users add Verify the application was updated by running again jupyter-deploy users list Then provide your application URL to your collaborator (by email, text message, whichever way your prefer). Only your collaborator can access your application so you do not need to take steps to secure this URL. If you do not remember the URL, run jupyter-deploy open again. Now you can both access the JupyterLab application from your respective web browser, and you will see each other's changes in real time. Stop your instance to save on Cloud bill At the end of your workday, you may want to turn off your Amazon EC2 instance and save on your AWS bill or free-tier allowance. You can do so with a simple jupyter-deploy command for your jupyter-deploy project. jupyter-deploy host stop That's it, your Amazon EC2 instance state and file system will be preserved and you can resume your work where you left it when you reconnect. Things to know Upgrade your instance The base template uses t3.medium as the default Amazon EC2 instance type. That is the recommended instance for a simple workload, however you may run into capacity limits if your workload needs more CPU or more memory. This is where infrastructure-as-code really helps; by updating one string in the declaration files, we can re-apply the project and mutate the existing infrastructure. jupyter-deploy makes this even easier, you can just run: jupyter-deploy config --instance-type t3.2xlarge then: jupyter-deploy up Share access to your JupyterLab application with a GitHub organization and GitHub teams If you have a small team, you can also manage access to your JupyterLab application at the GitHub organization and GitHub team levels. The jupyter-deploy CLI makes this easy by providing jupyter-deploy teams and jupyter-deploy organization commands. The base template only supports GitHub, but there are many other identity providers that support the OAuth2 protocol. Future work includes extending support to other such identities in the Amazon EC2 base template, or with new templates. Mount additional Amazon EBS volumes or Amazon EFS file systems You can easily create or associate existing storage volumes or file systems to the Amazon EC2 instance serving your JupyterLab application. Use the —additional-ebs-mounts of the config command to create or reuse Amazon EBS volume, and mount them under the home directory of your JupyterLab application. You can find additional configuration options by running jupyter-deploy config --help. Call to collaborate and contribute This blog walked you through an end-to-end deployment of a JupyterLab application to Amazon EC2, using GitHub as OAuth provider. There are many other ways to deploy Jupyter, and the jupyter-deploy CLI provides an extensible framework to do so. Our team will be working on adding another template targeting Kubernetes on Amazon Elastic Kubernetes Service (Amazon EKS). It will support running multiple applications on the cluster, and allow users to dynamically create, share and delete their own applications. Running an application on remote compute, including on Kubernetes, is not limited to AWS. The project welcomes contributions that add new functionalities such as other OAuth providers, other TLS certificate providers, and other Cloud providers. Users would install the templates from pypi , GitHub or local using the package manager of their choice (pip, or uv as in this blog), then use the options of the init command to select it. We encourage open source contributions to update the AWS EC2 base template, or to improve the CLI itself. Right now, the CLI and the base template are source-controlled from the same GitHub repository. To learn more and get involved, here you can find: · Project repository: · PyPI page of the CLI: · PyPI page of the base template:

Jonathan Guinegagne Jonathan Guinegagne is a Senior Software Engineer with Amazon SageMaker JumpStart at AWS. He got his master's degree from Columbia University. His interests span machine learning, distributed systems, and cloud computing, as well as democratizing the use of AI. Jonathan is originally from France and now lives in New York City.

Danilo Poccia Danilo works with startups and companies of any size to support their innovation. In his role as Chief Evangelist (EMEA) at Amazon Web Services, he leverages his experience to help people bring their ideas to life, focusing on serverless architectures and event-driven programming, and on the technical and business impact of machine learning and edge computing. He is the author of AWS Lambda in Action from Manning.

Amazon Connect is an AI-powered customer experience solution that enables superior outcomes at a lower cost. Since its 2017 public launch, Amazon Connect has become an AI leader, transforming how organizations of all types interact with their customers. In the Q3 2025 earnings report last week, Amazon announced a significant milestone: Amazon Connect achieved a $1 billion annualized revenue run rate, with AI optimizing over 12 billion minutes of customer interactions in the previous year. Even with these signals of success, Amazon Connect remains mission-driven, measuring results through satisfied end-customers, fulfilled agents, and delighted business leaders – just as it has since its inception. Let's explore the Amazon Connect story – from internal solution to AI pioneer. The origin story Like Amazon Web Services (AWS), Amazon Connect exemplifies Amazon's ability to transform internal solutions into industry-leading services. The journey began in 2007 when the internal customer service team decided to propose building a new unified solution from scratch to replace three contact center vendors. The vendors were requiring a $3M up-front investment for a hardware upgrade, plus recurring license and maintenance fees. The proposed internal solution was not only less expensive but also supported Amazon's stated mission of being the Earth's most customer-centric company. "We looked around at the time for all of the other products that could work. Nothing was very compelling," said Jon Jay, who founded the original customer service team and remains one of Connect's most senior engineering leaders. "None of them had high volume capacity, so we would have needed to manage over a dozen instances of these contact center solutions. They were also extremely expensive and didn't solve any of the problems we hoped to address in delighting customers." After receiving approval from Jeff Wilke, former CEO of Amazon Worldwide Consumer, the team successfully deployed their first pilot in 2007, followed by full deployment in 2008. What started as an internal efficiency project would go on to serve Amazon's various divisions for years – from customer service to HR and transportation – generating estimated annual savings of $60 million compared to competitive solutions. New acquisitions such as Audible and Zappos also enthusiastically embraced this solution and proved that there was appetite in the market for a uniquely Amazon approach to customer experience. "When we showed other Amazon teams what we built, it spread like wildfire. Zappos, Audible – they all had the same headaches with traditional contact centers. We'd show them our solution and they'd say 'Wait – you solved all of our biggest problems,'" said Jay. Product launch and early success The decision to make Amazon Connect available to the public came in Q3 2015, with approval from then-AWS CEO Andy Jassy and led by Pasquale DeMaio. "Talking to potential external customers, it was clear that they faced the same challenges as Amazon," added Jay. "We saw an opportunity to disrupt a legacy industry with a cloud-based solution that was easy to implement, offered unrivaled velocity of innovation, and could scale with the reliability and security that the largest enterprises require. That's when we knew we weren't fixing just Amazon's problems – we were solving issues the whole industry had been stuck with for decades." After just over a year in development as an external service, Amazon Connect made its public debut at Enterprise Connect 2017, quickly gaining traction among large enterprises facing significant scaling challenges. The deep integration with AWS services enabled seamless scaling and rapid feature development, while early adoption of Alexa AI technology through Lex delivered natural language interactive voice response (IVR) capabilities that set Amazon Connect apart. Early adopters like Capital One, Hilton, and GE were drawn to Connect's unique value proposition: a cloud-native architecture that eliminated the need for traditional telephony infrastructure. This innovative approach transformed what was traditionally a year-long buildout process into a weekend project for many organizations, dramatically reducing both time-to-market and operational complexity. "From the very beginning, organizations that wanted our speed of innovation found success partnering with Amazon Connect," said VP of Amazon Connect, Pasquale DeMaio. "Organizations know that we understand customer experience in a very personal way. It's one thing to claim to be customer-obsessed. At Amazon, we live it everyday." When the COVID-19 pandemic struck, Amazon Connect's cloud-native design proved prescient. Its self-service setup and native work-from-home agent support became crucial advantages as organizations scrambled to maintain customer service operations with remote workforces. The ability to function with just standard internet connections and headsets, eliminating the need for specialized phone equipment, made it an ideal solution for the sudden shift to remote work. By the end of the pandemic, Amazon Connect had tens of thousands of customers. Leading the AI revolution in customer experience Amazon Connect's evolution took a significant leap forward in 2019 with the launch of AI-powered conversational analytics, sentiment analysis, and more, without the complex technical requirements typical of competitors. Customers only needed to select a checkbox to enable these AI features, versus weeks of deployment with other solutions. In 2023, Amazon Connect achieved a leadership position for the first time in two key industry reports: the Forrester Wave for Contact Center as a Service and Gartner's Contact Center as a Service Magic Quadrant. Amazon Connect has maintained these leadership positions in subsequent reports through present day. Additional AI features were rapidly delivered across the customer journey. With the advent of generative AI, the team pivoted their roadmap to embrace Large Language Model (LLM) technologies, enabling features like automated agent wrap-up, call summarization, and LLM-based self-service experiences. In December 2024, Amazon Connect reported that 6 billion minutes of customer interactions were optimized by AI, showing the scale at which customers were putting AI to work in real scenarios. When Amazon Connect unveiled its "next generation" in March 2025, featuring AI enabled throughout the customer journey, customer response was overwhelmingly positive. Now, Amazon Connect is optimizing 12 billion minutes of customer interactions with AI—double what was announced less than a year ago. Amazon Connect is delivering innovative and integrated AI solutions at enterprise scale. Recently, multiple global brands have selected Amazon Connect over other providers and emerging AI-native players. In these evaluations, Amazon Connect demonstrated superior performance in critical areas such as intent detection accuracy, AI agent safety, and human-AI collaboration capabilities. These wins demonstrate Connect's ability to deliver best-in-class AI solutions beyond just a great demo, in a way that enables mission-critical use cases based on technical merit and reliability. Looking ahead Amazon Connect's achievement of a $1 billion revenue run rate milestone positions it as the only usage-based customer experience solution to reach this scale. This pay-as-you-go approach also uniquely positions Amazon Connect for the AI and agentic future. "We've had no shortage of inflection points: first, being a key driver of making cloud-based contact centers the new normal, then enabling businesses to navigate remote work while managing massive swings in demand during COVID, and finally delivering real-world results with Gen AI," said DeMaio. "Now we face two more: delivering safe and ethical agentic AI at enterprise scale and evolving from reactive to proactive customer engagement—all while expanding into new areas of opportunity to solve customer problems." From an internal project to solve Amazon's own customer service challenges, Amazon Connect evolved into a global service trusted by tens of thousands of customers in 8 years. And, it's still Day 1. With the same curiosity that sparked its creation and a mission to transform experiences, the team continues to pioneer AI-powered solutions aimed at solving problems and delighting customers. To learn more about Amazon Connect, visit the Amazon Connect page. Ready to transform your customer service experience with Amazon Connect? Contact us. About the Author Heidi Elmore is a serial technologist passionate about enterprise communications. She has launched UCaaS, CPaaS, and CCaaS products and led teams across multiple functions. When she's not obsessing over enterprise software, she rides and shows dressage horses.

Tonic AI: Synthetic data solutions for software and AI development Enterprises use the Tonic.ai solution to transform sensitive data into secure, high-fidelity synthetic datasets for AI and analytics workloads. Through a new notebook-based workflow, AWS customers can now use Tonic Textual on data stored in their lakehouse architecture of Amazon SageMaker to transform high-risk, unstructured data into safe, high-quality assets. This de-identified data is essential for a wide range of generative AI use cases, including fine-tuning large language models (LLMs), building knowledge bases for Retrieval Augmented Generation (RAG) workflows, creating evaluation datasets, and testing generative AI applications. To see the full workflow in action, readers can find a detailed, step-by-step guide on the Tonic.ai blog. "The SageMaker next generation incubator program put our mission on the fast track: bringing Tonic Textual's advanced technology directly to enterprises building on SageMaker. It has been pivotal in helping them replace sensitive information with high-quality synthetic data, dramatically shortening the path from raw data to impactful, production-ready AI models." — Tomer Benami, VP of Business Development Activeloop: Unlocking AI Data Analysis Activeloop, a pioneering AI infrastructure company, specializes in developing solutions that optimize AI data management and processing workflows. Their flagship product, Deep Lake, serves as a database specifically designed for AI applications, enabling efficient handling of multimodal data, including text, images, signals, and literature, by delivering impressive speeds in Apache Spark workloads of up to 3.9 times faster, compared to open source alternatives. Enhanced by their integration with the next generation of Amazon SageMaker and its lakehouse architecture approach, Activeloop combines Deep Lake's ultra-fast, multimodal AI data retrieval capabilities with the enterprise-grade scalability and versatility of SageMaker. This integration enables direct access to complex AI data from object storage, streamlining analytics and artificial intelligence and machine learning (AI/ML) model development. By unifying multimodal data with Deep Lake's advanced indexing capabilities, organizations can seamlessly connect textual, numerical, and visual information, accelerating insights and innovation across various domains, from healthcare to drug discovery, transforming their data assets into competitive advantages while maintaining efficient, scalable, and cost-effective AI operations. Read more about the solution on the Activeloop blog. "Through the incubator, we've gained invaluable technical enablement and marketing support, allowing us to scale our Deep Lake service capabilities to meet enterprise demand." — Davit Buniatyan, CEO Weaviate: For AI engineers who think big Weaviate is a cloud-centered open source vector database that simplifies development and deployment of AI applications. It offers vector search, keyword, and hybrid search capabilities through a scalable, flexible platform with pluggable architecture to connect with ML models. Production deployments benefit from built-in multi-tenancy, replication, role-based access control (RBAC) authorization, zero-downtime backups, advanced filtering, vector compression, and out-of-the-box RAG capabilities. Weaviate is available as a hosted service, self-managed instance, or as fully managed Weaviate Cloud. By handling the infrastructure and operational details, Weaviate Cloud frees developers to focus on innovation while providing enterprise-ready solutions through various hosting options including serverless cloud, enterprise cloud, and bring your own cloud. Weaviate has developed a comprehensive SageMaker Unified Studio notebook solution that bridges the gap between traditional data storage and AI-powered applications by using existing lakehouse data. This solution seamlessly transforms your structured or unstructured data stored in your lakehouse into a vectorized format within Weaviate's vector database, enabling sophisticated semantic search capabilities and RAG queries for enhanced data utilization. Read about the solution on Weaviate blog. "The SageMaker Incubator provided us the guidance and support we needed to bridge the gap between open source innovation and enterprise readiness, making it easier for our customers to deploy Weaviate at scale." — Tony Le, Sr. Director of Partners Snowplow: Turn customer behavior into competitive advantage Snowplow provides a customer data infrastructure that organizations can use to collect and stream clickstream data in real time. Organizations can capture complete, high-fidelity event data across every digital touchpoint—governed, modeled, and ready for downstream analytics and AI-driven use cases. Through the SageMaker Startup Partner Incubator program, Snowplow has built a solution to collect your data in real time and ingest it into your lakehouse on AWS. Once ingested, teams can use this lakehouse data to drive analytics use cases. You can run extract, transform, and load (ETL) capabilities available within SageMaker Unified Studio or use the SageMaker integration with Amazon Quick Sight to generate dashboards for enterprise reporting. You can further power ML use cases like recommendations and next best action. The joint solution provides a seamless path from a trusted behavioral data foundation to real-time AI-driven optimization at scale. Learn more about our partnership and joint solution at Snowplow for Amazon SageMaker. "The SageMaker Incubator program has accelerated our ability to deliver real-time, AI-ready clickstream data directly into customers' SageMaker environments. With Snowplow's solution deployable in the customer's tenant, and Snowplow Signals enabling easy real-time feature computation and serving, teams can securely build and operationalize ML and agentic models with rich behavioral data without sacrificing control or compliance." — Yali Sasson, Snowplow Co-founder and CTO SuperAnnotate: Streamlining AI development SuperAnnotate helps efficiently curate, label, and validate multimodal data for AI training and evaluation. Its robust annotation environment and connected data pipelines enable enterprises to accelerate time-to-production by up to 72%, powered by a seamless human and AI feedback loop that ensures consistent, high-quality data. SuperAnnotate's integration with Amazon SageMaker streamlines the entire machine learning workflow from raw data to AI deployment. Annotated datasets from SuperAnnotate are stored in standardized formats and are directly accessible in Amazon Simple Storage Service (Amazon S3) within the SageMaker Unified Studio environment. This gives teams simple, consistent access to labeled data in SageMaker Unified Studio to use in any stage of model development – from training, fine-tuning, and evaluation – without complex data handling. Together, SuperAnnotate and SageMaker create human-in-the-loop active learning cycles to continuously retrain and improve your models. "The SageMaker Incubator gave us exactly what we needed to move fast – deep technical support and tight collaboration, helping us deliver a truly integrated solution at scale. It's been an incredible experience building alongside AWS to deliver a unified workflow that brings greater value and speed to our mutual customers." – Vahan Petrosyan, Cofounder and CEO Conclusion In this post, we showed how innovative startups such as Tonic.ai, Activeloop, Weaviate, Snowplow, and SuperAnnotate are harnessing Amazon SageMaker to deliver enterprise-grade data and AI solutions. Partners in the program benefit from the robust capabilities of Amazon SageMaker. The unified environment enables comprehensive data management across data lakes and warehouses and provides AI-assisted development support and built-in governance and security controls. Partners can use seamless data sharing and ETL capabilities so they can focus on innovation rather than infrastructure management. For startups that want to scale their data and AI solutions for enterprises, the Amazon SageMaker Startup Partner Incubator provides a proven path to success. The program offers comprehensive support, including technical enablement, marketing assistance, and accelerated go-to-market strategies. Through this support, partners can rapidly develop and deploy enterprise-grade solutions that solve real business challenges while maintaining the highest standards of security and compliance. Ready to transform your startup's potential into enterprise value? Join the Amazon SageMaker Startup Partner Incubator today by visiting our application page at Amazon SageMaker Startup Partner Incubator.

TAGS: Amazon SageMaker, Amazon SageMaker Studio, Announcements, Lakehouse, Partner solutions

How Indeed scaled Governance across 1,000+ AWS accounts with AWS Trusted Advisor

Indeed is the #1 job site¹ in the world. With 615 million Job Seeker Profiles², people in more than 60 countries across 28 languages come to Indeed to search for jobs, post resumes, and research companies. Over 3.3 million employers use Indeed to find and hire new employees. Supporting this massive scale requires resilient, well-architected cloud infrastructure. Indeed prioritizes operational excellence by following AWS Well-Architected framework recommendations, but they also embrace AWS with a decentralized approach to foster velocity, accuracy, and accountability. To balance these concerns, they developed an approach to score maturity and adapt to ongoing challenges. The challenges of their decentralized approach included teams encountering service limits as they rapidly adopted AWS services. This led to reactive management that threatened the availability of scaling-dependent services. Additionally, they discovered inefficient resource utilization, such as underutilized Amazon Relational Database Service (RDS) instances consuming significant budgets across multiple AWS accounts without centralized oversight. These issues, among others, highlighted the need for a more strategic approach to organization-wide visibility and governance as they continued to grow and scale their operations. They recognized the need for a unified framework to enable meaningful inter-team comparisons and prioritize business outcomes. To achieve this, they leveraged AWS Trusted Advisor data through a data lake with robust reporting and visualization capabilities. This approach significantly improved their cloud governance, fostering continuous enhancement and optimization across they organization. The Dual Impact of Decentralization: Accelerated Progress and Dispersed Insights Indeed's engineering culture prioritizes product team autonomy and data-driven results. This philosophy has served well and enabled rapid innovation and the ability to respond quickly to market demands in the competitive job search landscape. To meet demands, they scaled for growth within AWS to encompass over 1,000 accounts across product teams and inevitably encountered a fundamental tension: decentralized progress limiting overall organizational observability. They had limited visibility into our overall cloud health posture. Cost optimization opportunities were distributed across accounts without a systematic way to identify the highest-impact improvements. Additionally, they needed mechanisms to benchmark team performance or share best practices organization-wide. With over 1,000 AWS accounts generating thousands of Trusted Advisor recommendations across six key areas—cost optimization, performance, security, fault tolerance, service limits, and operational excellence—manual approaches couldn't provide the comprehensive oversight and accountability they needed to meet for resilience with organization-wide initiatives such as improving high availability and having a disaster recovery plan. Championing Trust: Education and Enablement as Pillars of Progress Indeed began by creating cross-account IAM roles to enable centralized data collection. This enabled a centralized service account and a daily collection process to assume "trusted-advisor-data-collection" roles across all AWS accounts. The daily collection process, executed in Python, systematically collected Trusted Advisor data from all accounts, normalized the results, and subsequently uploaded them to a central Amazon S3 bucket. This bucket was partitioned by day to facilitate long-term retention. The collected data was then integrated into Indeed's existing data warehouse, where it could be correlated with organizational ownership information to ensure comprehensive visibility. This crucial step linked each finding directly to the responsible team, ensuring comprehensive visibility across the organization.

Figure 1: Account architecture for centralized aggregation of Trusted Advisor findings

Once they addressed implementation, they drove adoption across teams by educating them to review their scores across the six pillars through self-service in their own environments pertaining to their AWS account. And addressed common resistance points by maintaining: ● Transparency First: Rather than implementing top-down mandates, they made all data openly accessible, enabling teams to derive their own insights. This approach fostered trust and demonstrated value prior to requesting behavioral modifications. ● Early Win Showcases: They identified immediate, high-impact improvements that teams could implement, thereby creating positive associations with the new system. They present two specific examples where this implementation helped product teams. 1. Use Trusted Advisor data to identify idle RDS instances by platform teams A company-wide report detailing teams with the highest incidence of idle databases enabled platform teams to focus optimization efforts where they would achieve the most significant cost reductions. With 44% of RDS instances identified as idle, this singular initiative yielded immediate returns on investment and illustrated to teams how centralized data could augment existing optimization endeavors.

2. Proactive service limit management Previously, teams discovered service limits reactively when they were impacted —such as throttled Amazon DynamoDB operations. With centralized Trusted Advisor monitoring, teams began receiving early warnings when service limits moved from "OK" to "Warning" status across their AWS Accounts. In one instance, the early alert system identified that one of 17 DynamoDB tables was approaching its read capacity service limit in a production account. This early warning allowed the team to proactively request a quota increase before customers were affected. Building on this momentum, Indeed focused on scaling these benefits across the organization by democratizing cloud health data. The integrated data warehouse combines Trusted Advisor metrics with Indeed's ownership information, powering dashboards that allow teams to visualize cloud health, benchmark performance, and identify optimizations. This approach not only improved cloud stewardship across the organization but also enhanced developer productivity by providing clear visibility into how resource allocation choices affect cost optimization. Excellence as Competitive Advantage At Indeed, the evolution of cloud governance, achieved through centralized Trusted Advisor data, transcends mere operational enhancement; it has emerged as a strategic differentiator directly influencing their capacity to effectively serve job seekers and employers. By correlating cloud health metrics with business outcomes, they established links between technical excellence and competitive advantage. Teams that proactively manage their service limits encounter fewer outages, optimized resource utilization translates into cost reductions suitable for reinvestment, and an elevated security posture fosters trust. Their vision extends beyond current capabilities as they develop integration between Trusted Advisor data and existing service availability metrics within their data warehouse. Then they can answer critical questions: Which recommendations provide the greatest availability improvements? How do optimizations impact customer experience? Which teams' cloud health improvements correlate most strongly with business outcomes? Conclusion Not all organizations have access to data lakes with ownership information or service health metrics. For those customers, several excellent starting points are available: AWS Trusted Advisor Organizational View provides centralized visibility into all Trusted Advisor findings across AWS accounts, while the Trusted Advisor dashboard from Cloud Intelligence Dashboards on AWS collection offers rich visualization and analytics capabilities built on Amazon QuickSight. These Cloud Intelligence Dashboards provide pre-built, customizable reporting that can jumpstart your Trusted Advisor optimization program without requiring extensive data engineering resources. Success comes from combining the right tooling—whether a custom data lake solution, Organizational View, or Cloud Intelligence Dashboards—with creating a culture that actively adopts and acts on Trusted Advisor recommendations. Organizations need comprehensive visibility across complex environments, democratized access that empowers every team member, and cultural alignment that treats cloud stewardship as core to organizational mission rather than overhead to be minimized. ¹ Comscore, Total Visits, March 2025 ² Indeed data (worldwide), job seeker accounts that have a unique, verified email address About the authors

Susan Calland Susan Calland, a Senior Cloud Engineer at Indeed, finds satisfaction in both the business and technical aspects of her role. She enjoys coding solutions, budgeting, collaborating effectively with partners, creating visualizations to effectively communicate insights, and leveraging AI to optimize cloud infrastructure. Susan finds it particularly gratifying to demonstrate how enhanced security can lead to business savings.

Gayathri Veale Gayathri Veale, a Site Reliability Engineer at Indeed, boasts over 20 years in platform engineering and infrastructure. As an individual contributor on the Engineering Platform team, she ensures Indeed's data-driven culture is supported by a resilient, reliable, and scalable backbone enabling developers and internal users to address business initiatives.

Brett Seib Brett Seib is a Senior Solutions Architect based in Austin, Texas, with over a decade of professional experience architecting products and guiding AWS customers. He is passionate about innovating and leveraging technology to solve complex business challenges. Brett specializes in enterprise solutions, Artificial Intelligence (AI), and data analytics, helping organizations accelerate their business outcomes and drive digital transformation.

Alak Eswaradass Alak Eswaradass is a Principal Solutions Architect at AWS based in Chicago, IL. She focuses on empowering AWS customers to design innovative cloud architectures that solve complex business challenges. She is passionate about using artificial intelligence to advance accessibility, particularly in helping customers develop generative AI-powered assistive technologies that make digital experiences more inclusive. When she's not working, Alak enjoys spending time with her daughters and exploring the outdoors with her dogs.

Raj Athavale Raj Athavale is a Technical Account Manager at AWS, where he partners with enterprise customers to implement scalable cloud governance frameworks and multi-account optimization strategies. Passionate about containerization with Amazon EKS and AI solutions architecture, Raj specializes in solving complex technical challenges that enable organizations to maintain innovation velocity while achieving operational excellence at scale.

Wasif Abdul Rauf Wasif is a Senior Technical Account Manager based out of Houston TX. He loves to dive deep into his customer's technical and operational challenges, working closely to uncover solutions. With a strong interest in Generative AI, machine learning, and data analytics, Wasif is driven by a passion for innovation.

How Omnissa saved millions by migrating to Amazon RDS and Amazon EC2

Omnissa, formerly the VMware End-User Computing business, is a digital workspace technology leader that delivers smart, seamless, and secure digital work experiences for organizations worldwide. It serves 26,000 customers, including the top seven of the Fortune 500 companies. The company's mission is to simplify and secure the digital work environment, empowering employees to work productively from anywhere, on any device, without sacrificing IT control or user experience. Omnissa Workspace ONE®, Unified Endpoint Management (UEM) , helps its customers secure and manage all devices that connect to a network, both on premises and in the cloud. Omnissa Horizon® provides solutions for virtual desktops and apps, digital employee experience, and robust security—all underpinned by AI-driven automation. With AWS, Omnissa was able to successfully migrate its mission-critical workloads from VMware Cloud on AWS (VMC-A) to native AWS services, including Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) for SQL Server. This migration exemplifies the strategic impact of Omnissa's cloud transformation journey. Using these services, it not only achieved a substantial cost reduction of 39%, saving millions, but also significantly enhanced operational efficiency. Omnissa used this migration as an opportunity to start a transformation initiative, which included refactoring its applications and databases as part of a multi-year modernization strategy. This approach aims to further optimize costs and transfer the resulting savings to its customers. Josh Broch, VP of Engineering at Omnissa, commented on the success of the migration:

"The transition from VMC-A to Amazon RDS for SQL Server and Amazon EC2 was a critical strategic move for us. The AWS team's expertise, proactive approach, and unwavering support were instrumental in making this complex migration a success. Not only have we achieved significant cost savings, but we've also gained a more flexible, scalable infrastructure that allows us to better serve our customers and drive innovation."

Anita Singh Anita is a Principal Specialist Solutions Architect with over 20 years of experience architecting solutions focused on database technologies. She guides customers in designing and building highly scalable, available, and performant applications, while specializing in database migrations and modernization initiatives to AWS Cloud. Her expertise extends to implementing resilient database architectures and integrating operational data with analytics and AI services, helping organizations maximize their cloud infrastructure investments.

Satya Pattanaik Satya is a Senior Solutions Architect at AWS, where he helps Independent Software Vendors (ISVs) build scalable and resilient applications on the AWS Cloud. Before joining AWS, he played significant role in driving growth and success for many organizations. Outside of work, he pursues his passion for BBQ, continuously learning new techniques to create flavorful dishes and experimenting with various recipes.

Scott Webber Scott is a Principal Customer Solutions Manager at AWS, leading digital transformation and Gen AI initiatives for large ISV customers. He has 25+ years of technology industry experience, including leadership roles at AWS, Informatica, Infor, and NTT DATA, Scott brings extensive expertise in cloud adoption, digital innovation, and business value optimization.

Neha Sharma Neha is a Senior Technical Account Manager at AWS and based in Bay Area, California. Neha provides strategic technical guidance for Independent Software Vendors (ISVs) to innovate and operate their workloads efficiently on AWS cloud.

Cees "Case" Molenaar Cees is a Sr. Databases Account Manager with over 3 ½ years covering managed database services at AWS. His customers are primarily large, international ISVs. Cees specializes in creating data strategies and helping his customers with database migrations and modernization initiatives. By aligning the right technical resources and programs, he simplifies and accelerates the process and within budget. Outside work, he enjoys family, friends, sports, travel, history and comedy.

Camilo Leon Camilo is a Principal Solutions Architect at AWS specialized in databases and based in San Francisco, California. He works with AWS customers, providing architectural guidance and technical support for the design, deployment, and management of their AWS relational database workloads and business applications. In his spare time, he enjoys mountain biking, photography, and movies.

Introducing cross-account support for Amazon CloudFront Virtual Private Cloud (VPC) origins

In November 2024, Amazon CloudFront introduced CloudFront Virtual Private Cloud (VPC) origins, a security feature that allowed customers to deliver content from applications hosted in private subnets. In addition, we are now introducing cross-account support for Amazon CloudFront VPC origins, enabling network traffic flow between Amazon CloudFront and Application Load Balancers (ALBs), Network Load Balancers (NLBs), or Amazon Elastic Compute Cloud (Amazon EC2) instances deployed within private subnets across different Amazon Web Services (AWS) accounts. This enhancement allows you to keep your Amazon VPC origins and CloudFront distributions in separate AWS accounts, enabling organizations with multi-account strategies to use VPC origins while maintaining their desired account structure. Many AWS customers set up multiple AWS accounts to isolate ownership and development processes across different business functions. This approach, following modern cloud security architecture principles, typically involves creating multiple AWS Organizational Units (OUs) with dedicated network and security accounts. While CloudFront distributions, serving as global entry points for customer applications, are typically deployed in network accounts, customers have been able to link public origins from multiple accounts to a single distribution. However, organizations wanting to enhance their security posture with VPC origins have faced limitations, as VPC origins and CloudFront distributions had to reside in the same account. This meant customers who had their origins in multiple AWS accounts, had to keep their accounts in public subnets to get the scale and performance benefits of CloudFront. Customers then had to maintain additional security controls, such as access control lists (ACL), at both the edge and within regions, rather than benefiting from the inherent security of VPC origins. Until now, these customers were unable to take advantage of the security benefits of VPC origins while maintaining their preferred multi-account architecture. Architecture diagram In this section we examine the key components of sharing cross-account VPC origins for CloudFront distributions, as shown in the following diagram.

Figure 1: VPC origins shared using AWS Resource Access Manager (AWS RAM) with a separate Resource Consumer account where the CloudFront distribution is located The cross-account sharing process involves two main entities: (1) the Resource Owner (the AWS account containing the VPC origin resource and the underlying infrastructure) and (2) the Resource Consumer (the AWS account hosting the CloudFront distribution). To share a VPC origin across two accounts, the Resource Owner first creates the (3) VPC origin in their own account. For HTTPS only, the Resource Owner may configure a custom domain with a valid (4) certificate. This is an AWS Certificate Manager (ACM) certificate for ALB/NLB. For HTTP only deployments, ACM certificates are not necessary. Then, using (5) AWS Resource Access Manager (AWS RAM), they can share the VPC origin with the desired Resource Consumer account. (6) At this stage, Resource Consumers are able to use the shared VPC origin in their CloudFront distribution, specifying the custom domain for HTTPS communication. Prerequisites The following instructions are for HTTPS only VPC origins setup. Before proceeding with this walkthrough, ensure you have the following resources already deployed in your Resource Owner Account: EC2 instances running your application (in our example, Apache web servers), an ALB fronting these instances, proper VPC and subnet configuration with instances deployed, a public-facing custom domain (for example api.example.com), and a certificate registered in ACM for that domain. This allows the VPC Origin to have HTTPS communication between CloudFront and the origin. These resources are necessary in order to proceed with the VPC origin setup, shown below.

Figure 2: Workflow for VPC origins sharing using AWS RAM, enabling setup with a cross-account CloudFront distribution Getting started This section explains the process, starting with how to set up VPC origin in the Resource Owner Account containing an ALB fronting Apache web servers on EC2 instances. Once completed, we'll detail how to share this VPC origin with the Resource Consumer Account using AWS RAM. Next, you create a CloudFront distribution in the Resource Consumer Account using the shared origin. Finally, you will learn how to verify the setup by accessing the web content through CloudFront's domain name. Navigate to the Resource Owner Account.

Create a VPC origin exclusively in the Resource Owner Account. For our example, we're using an ALB, so we input the ALB's ARN as demonstrated in the following image. Under Protocol, we choose HTTPS only to make sure of a secure communication between CloudFront and the origin.

When the VPC origin status shows Deployed, choose the VPC origin and choose the Share VPC origin button.

On the next screen, you can either choose an existing Resource share or choose the Create resource share button. For this example, create a new Resource share.

On the next screen, specify the Principal type and corresponding details (either account number, Organization ARN, or Organization unit ARN). For this example, we use AWS account as the Principal type and enter an AWS account ID. For this example, the AWS account ID is for Resource Consumer Account.

After completion, return to the VPC origin screen where a green banner confirms the successful creation of the resource share.

Although we created the resource share for VPC origin through the CloudFront console, you can also create it from the AWS RAM console. If you want to observe the share that was created, then you can navigate to the AWS RAM console.

If you are using an already existing resource share, then you can skip this step. Otherwise, when you are in the Resource Consumer Account, in the AWS RAM console, go to Shared with me, choose Resource shares, choose the name of the resource share, and choose Accept resource share.

On the next screen, choose VPC origin under Origin type. In the Origin section, choose Browse VPC origins to open a popup window. Choose the origin and select Choose. Under VPC origin endpoint, enter your custom domain (api.example.com is used as an example, but this should be your own public domain name that you own) because we are using HTTPS only and need the domain that matches your ACM certificate. Then, choose Next.

On the next screen, select Do not enable security protections and choose Next. Although we have decided not to use AWS WAF for this specific example, we recommend implementing it to secure your distribution.

On the next screen, review the details, scroll down, and choose Create distribution. You are taken to a screen where you should see the distribution in the Deploying state, which takes a few minutes to complete.

After the distribution is created, copy its Distribution domain name.

Access the copied domain name using a web browser. You should see that you can successfully reach the shared VPC origin from the Resource Owner Account through the CloudFront distribution in the Resource Consumer Account.

Figure 14: Access the URL for the CloudFront distribution created using shared VPC origin This example demonstrates how you can maintain a secure and organized infrastructure within your organization by keeping the VPC origin in one account (Resource Owner Account) while hosting the CloudFront distribution in another account (Resource Consumer Account). This setup allows for centralized management of VPC resources while enabling other accounts to securely access and distribute content through CloudFront. Considerations

If the Resource Owner Account stops sharing the VPC origin with the Resource Consumer Account, then the existing traffic continues to function. Shared VPC origin deletion needs a specific sequence:

Disable CloudFront distribution in Resource Consumer Account Unshare Shared VPC origin in Resource Owner Account Delete Shared VPC origin in Resource Owner Account

Directly from the CloudFront console Through the AWS RAM console in the use-east-1 (N. Virginia) AWS Region Both methods achieve the same result, providing flexibility in management.

To view and accept the resource share for the VPC origin in the Resource Consumer account, set your AWS Region to us-east-1 (N. Virginia), because global resources such as VPC origins are visible only in this Region.

VPC origins are marked as "(shared)" in Resource Consumer Account Resource Consumer Account cannot re-share received VPC origins One-way sharing: Resource Owner Account → Resource Consumer Account only

AWS RAM uses the managed permission: AWSRAMDefaultPermissionCloudfrontVpcOrigin This permission specifically allows sharing of the Resource type: cloudfront:Vpcorigin No additional permission configurations needed for basic sharing functionality

The Resource Owner Account can configure a custom domain (for example api.example.com) with a valid ACM certificate for their origin. The Resource Consumer Account must use this custom domain as the VPC origin endpoint in their CloudFront distribution configuration. This makes sure of proper SSL/TLS certificate validation between CloudFront and the origin.

Conclusion Cross-account support for Amazon CloudFront VPC origins eliminates the previous constraints that required origins and distributions reside in the same account. Using AWS RAM, organizations can now share VPC origins across account boundaries while maintaining their multi-account structures. With this innovation, development teams are now able to manage applications in private VPC subnets within their own accounts, while content delivery teams can reference these origins from CloudFront distributions in separate accounts. Get started today by using CloudFront VPC origins with cross account support. About the Authors

Salman Ahmed Salman is a Senior Technical Account Manager in AWS Enterprise Support. He specializes in guiding customers through the design, implementation, and support of AWS solutions. Combining his networking expertise with a drive to explore new technologies, he helps organizations successfully navigate their cloud journey. Outside of work, he enjoys photography, traveling, and watching his favorite sports teams.

Ankush Goyal Ankush is a Senior Technical Account Manager at AWS Enterprise Support, specializing in helping customers in the travel and hospitality industries optimize their cloud infrastructure. With over 20 years of IT experience, he focuses on leveraging AWS networking services to drive operational efficiency and cloud adoption. Ankush is passionate about delivering impactful solutions and enabling clients to streamline their cloud operations.

Kunj Thacker Kunj is a Technical Account Manager at AWS and is based out of Vancouver, Canada. He has an extensive background in Network and Infrastructure engineering prior to this role. He is passionate about new technologies and enjoys helping customers build, implement, and optimize their cloud infrastructure on AWS.

Configuring the AWS WAF Anti-DDoS managed rule group for your resources and clients

Users wanting to protect themselves from Layer 7 (HTTP) DDoS threats can use the AWS WAF L7 Anti-DDoS managed rule group to detect and mitigate DDoS events in single digit seconds. The Anti-DDoS managed rule group has a default configuration that is appropriate for many applications and clients. However, there are clients that need special attention. Browser-based Single Page Applications (SPAs) making fetch requests, and native mobile applications making API requests, do not support all mitigations and may have their requests inadvertently stopped during a detected event. In this post we provide guidance on how to tune your web access control list (web ACL) for different kinds of clients and resources to optimize your DDoS protection and user experience. How AWS WAF Anti-DDoS managed rule group trades off mitigation effectiveness with end-user impact When the Anti-DDoS managed rule group is added to your AWS WAF web ACL configuration, it quickly learns your traffic patterns and establishes baselines for each protected resource. It identifies anomalies by comparing current traffic to these baselines, assigning suspicion scores to requests for use in mitigations. By default, requests designated low and medium suspicion have a soft mitigation in the form of a silent JavaScript-based browser challenge. This challenge is imperceptible to legitimate users but significantly increasing resource costs for threat actors. Requests designated as high suspicion of contributing to DDoS attacks have a hard mitigation in the form of being blocked even if they have completed a challenge. The Anti-DDoS managed rule group defines challengeable requests as those using the GET method for URIs that do not match the Exempt URI regular expression (URIs that can't handle a silent JavaScript challenge). These requests are assigned the label awswaf:managed:aws:anti-ddos:challengeable-request. All other requests are non-challengeable. The AWS WAF Challenge action is the soft mitigation for challengeable requests. By default, challengeable requests with the header Accept: text/html receive a JavaScript payload that completes a task and issues a token/cookie. Then, the user is redirected back to their original page. Figure 1 shows this as a sequence diagram, and subsequent requests automatically include this cookie. Challengeable requests for other content types receive an HTTP 202 Request Accepted status and cannot proceed because the challenge is not completed. Further details on how the challenge works can be found in this AWS Networking and Content Delivery post.

Figure 1: Sequence diagram of the interstitial challenge interaction between a user, their browser, and AWS WAF protecting a resource

The use of soft and hard mitigations helps balance the effectiveness of DDoS mitigation with the impact to legitimate users during DDoS events. You may have considered minimizing the impact to legitimate users using clients that do not support this sequence by disabling the challenge or reducing the sensitivity of the block action as shown in Figure 2. This introduces the risk that your resource may be inadequately protected against DDoS attacks, or legitimate users are finding their falsely identified requests blocked due to increased sensitivity.

Figure 2: AWS WAF Anti-DDoS managed rule group configuration to disable the challenge

There are a number of rules and configuration options in the Anti-DDoS managed rule group that enable you to find the right trade-off between resource availability during a DDoS attack and the experience of your legitimate users who may or may not support the challenge soft mitigation. The flow chart in Figure 3 shows how the Anti-DDoS managed rule group works based on the incoming request and rule group configuration. The following section details how you can change the rule group configuration in a way that best makes the trade-off between your user experience and resource sensitivity during a DDoS attack.

Figure 3: Flow diagram for how the AWS WAF Anti-DDoS managed rule group evaluates each rule

Scenario 1: your clients or request types don't support the challenge The default Anti-DDoS managed rule group configuration sets the ChallengeAllDuringEvent rule to the challenge. Legitimate users experience disruption if they are making challengeable requests from unsupported clients during a DDoS event. The solution that offers the best experience for your users is to allow their clients to complete the challenge. The AWS WAF client integrations proactively complete a challenge and acquire a token as part of loading your resource outside of DDoS events. These SDKs need the AWS WAF Targeted Bot Control AMR to be deployed in the web ACL. All requests can now be treated as challengeable by the Anti-DDoS managed rule group. There are two options: JavaScript SDK: This SDK is suited for browser-based applications. This is achieved by embedding JavaScript in the HTML head of your application so that the challenge completes during page load. The cookie is automatically sent in subsequent requests. Mobile SDK: This SDK is available for both Android and iOS. The underlying workflow is similar to the JavaScript SDK as described in the AWS Developer Guide. If you have other types of clients that cannot use these SDKs (for example, machine-to-machine communication), then you cannot treat all requests as challengeable and must review the next scenarios for handling non-challengeable requests. If you operate across domains (for example an SPA www.company.com makes fetch requests to api.company.com), then you also need to configure the token domain correctly. This post covers different scenarios for cross domain configurations. The solution that gives your resource the most protection from DDoS requests is to make all requests challengeable. Figure 4 shows relaxing the Exempt URI regular expressions to \x00 to match no requests. The challenge no longer breaks API requests and static assets such as images, because all requests include the token when using SDK.

Figure 4: AWS WAF Anti-DDoS managed rule group configuration to challenge all requests

Non-GET (for example POST/PUT/DELETE) requests continue to be defined as non-challengeable by the Anti-DDoS managed rule group. However, the client integrations now include the token, which means they pass a challenge. You can make sure that clients making these requests have completed a challenge by adding the following rule after the Anti-DDoS managed rule group to challenge requests matching the DDoS event label (awswaf:managed:aws:anti-ddos:event-detected) for non-GET HTTP methods. { "Name": "ChallengeNonGetDuringDDoSEvent", "Priority": 100, "Action": { "Challenge": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "ChallengeNonGetDuringDDoSEvent" }, "Statement": { "AndStatement": { "Statements": [ { "NotStatement": { "Statement": { "ByteMatchStatement": { "FieldToMatch": { "Method": {} }, "PositionalConstraint": "EXACTLY", "SearchString": "GET", "TextTransformations": [ { "Type": "NONE", "Priority": 0 } ] } } } }, { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:event-detected" } } ] } } } JSON Scenario 2: Our clients cannot use the AWS WAF client integrations In this scenario, you with the trade-off of either impacting legitimate users using clients that cannot support the challenge or stopping DDoS requests that may impact the availability of your resource. Depending on the extent to which you are willing to impact legitimate users, you can start by changing the default rule group configuration. The following configuration options are ordered from highest to lowest in terms of providing your application with the most protection against DDoS attacks by relaxing the challenge soft mitigation as little as possible:

Permitting non-suspicious requests without challenge during an event: Changing the ChallengeAllDuringEvent rule to Count means that your legitimate users' non-suspicious requests are no longer challenged during a DDoS event. All suspicious requests are still challenged by the ChallengeDDoSRequests rule. Legitimate users making non-suspicious requests from native mobile applications and SPA fetch requests are no longer impacted. This trades off inconvenience for legitimate users falsely identified as making suspicious requests in exchange for stronger protection for your protected resource. Permitting more suspicious requests without challenge: If you are still concerned about legitimate users being falsely identified as sending low suspicion requests, then you can change the Challenge sensitivity to Medium. Requests from users with low suspicion are permitted so that your legitimate users are less likely to be impacted. Requests with medium suspicion are still challenged. This may be an acceptable trade-off for mobile clients that cannot support challenge and if you are concerned about low suspicion requests impacting legitimate users. Not challenging more endpoints: If you are unable to support the challenge, then add the minimal number of endpoints to this expression to permit your legitimate users through, while still using a soft mitigation for other endpoints that support the challenge. Consequently, you may find that a large volume of non-challengeable requests overwhelms your sensitive protected resource.

These options improve the user experience because users are less likely to be impacted by the challenge. At a certain scale this may mean either too few DDoS requests are blocked, or too many legitimate users are impacted by a challenge that they cannot complete. The following scenario offers a solution for this. Scenario 3: Non-challengeable requests overwhelm your resource before they're blocked You may still have a certain volume of non-challengeable requests, especially if the challenge mitigation is disabled or when you have limited ability to use the AWS WAF client integrations. Non-challengeable requests are always permitted until they reach the DDoS block sensitivity (high by default). This means that sensitive endpoints may be overwhelmed with low or medium suspicion requests because the soft mitigation isn't applied. The following common use-cases need special consideration:

POST requests for form submissions (such as from HTML pages) Single Page Apps (for example React) making fetch requests to API endpoints, particularly for endpoints that put load on your server Native mobile applications making requests to API endpoints Machine-to-Machine requests to API endpoints Requests for images or other static assets that overwhelm your resource

GET requests are made non-challengeable by the Exempt URI regular expressions than by default excludes /api and static assets. We recommend that you reduce the scope of this expression to make sure that as many requests can be challenged as possible. You have two options if this isn't possible:

Update the DDoS block sensitivity from low to medium (or high): Medium and high suspicion requests are blocked, ideally before impacting your resource negatively. Setting the block sensitivity to high also blocks low suspicion requests. This limits accessibility for legitimate users incorrectly identified with medium (or low) suspicion DDoS requests but maintains broader availability. Create an alternative soft limit by limiting the volume of non-challengeable requests during an event: Rather than negatively impacting your potentially legitimate users by blocking low/medium suspicion DDoS requests, you can create a rate-based rule that limits the number of non-challengeable suspicious requests allowed during a DDoS event. You can use an aggregation key such as IP, users (for example using the Authorization header), or a specific sensitive endpoint URI (for example an endpoint that completes an expensive database query).The following rule added after the Anti-DDoS managed rule group limits suspicious non-challengeable requests from a user (based on the Authorization header) to 10 requests per minute. Legitimate users with low/medium suspicion requests can still interact with your resource, while still limiting the volume of suspicious requests reaching your protected resource. { "Name": "RateLimitSuspiciousUserRequests", "Priority": 100, "Action": { "Block": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "RateLimitSuspiciousUserRequests" }, "Statement": { "RateBasedStatement": { "Limit": "10", "AggregateKeyType": "CUSTOM_KEYS", "EvaluationWindowSec": 60, "CustomKeys": [ { "Header": { "Name": "Authorization", "TextTransformations": [ { "Type": "NONE", "Priority": 0 } ] } } ], "ScopeDownStatement": { "AndStatement": { "Statements": [ { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:ddos-request" } }, { "NotStatement": { "Statement": { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:challengeable-request" } } } } ] } } } } } JSON Scenario 4: Challengeable requests overwhelm your resource before DDoS requests are blocked A threat actor may be able to overcome challenges, leading to low and medium suspicion requests overwhelming your server before they can blocked as high suspicion requests. This may be an issue if a GET request for HTML pages puts sufficient load on your server (for example a listing page that undertakes an expensive database query). You have three options:

Update the DDoS block sensitivity from low to medium (or high): Medium and high suspicion requests are blocked, ideally before it negatively impacts your resource. Setting block sensitivity to high will also block low suspicion requests. This limits accessibility for legitimate users incorrectly identified with medium (or low) suspicion DDoS requests but maintains broader availability. Create an alternative soft mitigation by blocking suspicious DDoS requests to sensitive endpoints: Rather than blanket blocking challengeable low/medium DDoS requests, you can create a rule that matches the awswaf:managed:aws:anti-ddos:ddos-request label and blocks these known requests from targeting specific sensitive endpoints. The following rule applied after the Anti-DDoS managed rule group blocks identified DDoS requests that are for the sensitive URI endpoint /products/list. { "Name": "BlockSensitiveEndpointsDDoS", "Priority": 100, "Action": { "Block": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "BlockSensitiveEndpointsDDoS" }, "Statement": { "AndStatement": { "Statements": [ { "ByteMatchStatement": { "FieldToMatch": { "UriPath": {} }, "PositionalConstraint": "EXACTLY", "SearchString": "/products/list", "TextTransformations": [ { "Type": "NONE", "Priority": 0 } ] } }, { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:ddos-request" } } ] } } } JSON Require CAPTCHA completion during an event: If your resource is particularly sensitive to DDoS attacks, or you have no tolerance for negatively impacting legitimate users, then you can demand a CAPTCHA be completed. This CAPTCHA significantly increases the resource costs for a threat actor. The following rule added after the Anti-DDoS managed rule group forces users to complete a CAPTCHA for challengeable requests that match the event detected label (awswaf:managed:aws:anti-ddos:event-detected). Legitimate users can still engage with your resource. CAPTCHA attempts are charged at the standard rate. Further details on CAPTCHA and how to integrate can be found in this AWS Networking and Content Delivery post. { "Name": "CAPTCHADDoSEvent", "Priority": 8, "Action": { "Captcha": {} }, "VisibilityConfig": { "SampledRequestsEnabled": true, "CloudWatchMetricsEnabled": true, "MetricName": "CAPTCHADDoSEvent" }, "Statement": { "AndStatement": { "Statements": [ { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:event-detected" } }, { "LabelMatchStatement": { "Scope": "LABEL", "Key": "awswaf:managed:aws:anti-ddos:challengeable-request" } } ] } } }

JSON Conclusion In this post we described how the new AWS WAF Anti-DDoS managed rule group uses soft and hard mitigations to achieve a balance between the availability of a resource and the user experience of legitimate users. We described situations commonly encountered when using these mitigations with different clients, and explained how to effectively trade off user experience with mitigation effectiveness even if clients cannot support challenges. If you still have concerns about your ability to respond to sophisticated DDoS threats, then consider AWS Shield Advanced. Shield Advanced provides 24/7 access to the Shield Response Team (SRT) working with you to deploy custom mitigations to specific DDoS threats. About the authors

David MacDonald David is a Senior Solutions Architect focused on helping New Zealand startups build secure and scalable solutions. He has spent most of his career building and operating SaaS products that serve a variety of industries.

Joanna Knox Joanna is a Senior Cloud Support Engineer at AWS in Sydney Australia. With a background in network engineering, application delivery and security, she focuses on helping customers architect themselves to be resilient against DDoS and other security threats, as well as helping customers actively under DDoS. Outside of work she likes to DJ and cook.

Introducing the Overview of the AWS European Sovereign Cloud whitepaper

Amazon Web Services (AWS) recently released a new whitepaper, Overview of the AWS European Sovereign Cloud, available in English, German, and French, detailing the planned design and goals of this new infrastructure. The AWS European Sovereign Cloud is a new, independent cloud for Europe, designed to help public sector organizations and customers in highly regulated industries meet their evolving sovereignty and compliance needs. This effort, backed by a €7.8 billion investment in infrastructure, jobs creation, and skills development, will launch its first AWS Region in the State of Brandenburg, Germany by the end of 2025. This whitepaper provides a broad overview of the AWS European Sovereign Cloud highlighting how AWS is helping customers achieve their sovereignty requirements while benefitting from access to the full power of AWS. Key aspects covered in the whitepaper include:

Infrastructure – Dedicated physical infrastructure with multiple Availability Zones, following the established AWS Regional model approach Logical isolation – Logical separation from existing AWS Regions, with independent billing, account, and identity systems Operational control – Measures to help assure independent operation of the AWS European Sovereign Cloud, including staffing requirements Data sovereignty – Design that helps make sure customer content and customer-created metadata remain within EU boundaries unless customers choose otherwise Corporate governance – A distinct corporate structure under EU law, with EU nationals serving as managing directors and an independent advisory board Approach to law enforcement requests – The technical, operational, and legal measures implemented to help protect customer data and manage law enforcement requests

The whitepaper describes how these elements work together to deliver sovereign control and operational autonomy of our expansive service portfolio to meet Europe's digital sovereignty needs. The AWS European Sovereign Cloud will be the only fully featured, independently operated sovereign cloud backed by strong technical controls, sovereign assurances, and legal protections designed to meet the needs of European governments and enterprises. Customers and partners using the AWS European Sovereign Cloud will benefit from the full power of AWS including the same service portfolio, security, availability, performance, architecture, APIs, and innovations such as the AWS Nitro System. We have already made—and will continue to make—new investments in the design, development, and operation of the AWS European Sovereign Cloud. We are building on the strong foundation that has underpinned AWS services for years, including our long standing commitment to customer control over data residency, our design principal of strong regional isolation, our deep European engineering roots, and our more than a decade of experience operating multiple independent clouds for the most critical and restricted workloads. For more information about the AWS European Sovereign Cloud visit AWS European Sovereign Cloud.

J.D. Bean J.D. is Principal Architect of the AWS European Sovereign Cloud. His interests include security, privacy, and compliance. He is passionate about his work enabling AWS customers' successful cloud journeys. J.D. holds a Bachelor of Arts from The George Washington University and a Juris Doctor from New York University School of Law.

TAGS: data sovereignty, Digital Sovereignty, EU Data Protection, European Union, Public Sector, Security Blog

Cómo Enviar SMS Internacionales con Números Gratuitos de EE.UU. Usando AWS End User Messaging

AWS End User Messaging ahora admite capacidades de SMS internacional para Números Gratuitos de EE.UU. (TFN). Esta nueva función permite a las empresas usar un solo TFN de EE.UU. para enviar mensajes SMS a más de 150 países, simplificando el alcance global. Beneficia principalmente a clientes que necesitan enviar alertas transaccionales unidireccionales—como contraseñas de un solo uso (OTP) o notificaciones de envío—y empresas que quieren crear prototipos rápidamente y probar su estrategia de mensajería en nuevos mercados internacionales sin la complejidad de adquirir números específicos por país. Esta guía te mostrará los pros y contras de esta función y cómo habilitarla y cuándo usarla versus métodos tradicionales de envío específicos por país. ¿Qué Son los Números Gratuitos Internacionales de EE.UU.? Un número gratuito internacional de EE.UU. es un TFN (toll-free number) estándar de EE.UU. que ha sido habilitado con la capacidad de enviar mensajes SMS a destinos fuera de Estados Unidos. Esta función es compatible con versiones anteriores, lo que significa que puedes habilitarla en cualquier TFN de EE.UU. nuevo o existente en tu cuenta. Cómo Habilitar el Envío Internacional Hay tres formas principales de habilitar esta función para tus Números Gratuitos de EE.UU.:

Habilitar el envío internacional al registrar un nuevo número en la consola. Habilitar el envío internacional para un número existente en la consola. Habilitar el envío internacional para un número existente a través del AWS CLI.

1. Habilitar Al Registrar un Nuevo Número Gratuito de EE.UU. (Consola)

Desde la consola de AWS End User Messaging, navega a Administrar SMS

Desde la consola de AWS End User Messaging, navega a Configuraciones > Números de teléfono > y selecciona Originador de la solicitud

Paso 1: Seleccione un país, selecciona Estados Unidos (US) como tu país de destino

En Paso 2: Defina el caso de uso, configura las diversas opciones listadas para tu Caso de uso de mensajería previsto, y selecciona Sí para habilitar el envío Internacional, antes de hacer clic en Siguiente

Para Paso 3: Seleccionar tipo de originador, selecciona Gratuito, valida tus opciones de Política de recursos, selecciona Siguiente

En Paso 4: Revisar y solicitar: Verifica que la información que ingresaste sea correcta y selecciona Solicitar. Nota: Las solicitudes de registro de números gratuitos de EE.UU. pueden tomar aproximadamente 15 días hábiles para ser aprobadas.

Para más información, consulta Solicitar un número de teléfono en AWS End User Messaging SMS 2. Habilitar para un Número Gratuito de EE.UU. Existente (Consola o CLI) Si ya has adquirido un TFN, puedes habilitar la función de envío internacional en cualquier momento. Usando la Consola de Administración de AWS:

Navega a Configuraciones > Números de teléfono > y selecciona un número Gratuito existente

Marca la casilla Habilitar envío internacional en los detalles de tu número de teléfono

Usando el AWS CLI El comando update-phone-number te permite modificar las capacidades de un número de teléfono, mientras que el comando describe-phone-numbers te permite verificar su estado. 1. Para Habilitar el Envío Internacional: Usa el parámetro --international-sending-enabled aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --international-sending-enabled \ --region us-east-1 Bash Nota: Reemplaza "phone-a1b2c3d4e5f67890" con el ID real de tu número de teléfono 2. Para Deshabilitar el Envío Internacional: Usa el parámetro --no-international-sending-enabled aws pinpoint-sms-voice-v2 update-phone-number \ --phone-number-id "phone-a1b2c3d4e5f67890" \ --no-international-sending-enabled \ --region us-east-1Bash Respuesta Esperada (para update-phone-number): Un comando exitoso devuelve el objeto JSON completo para el número de teléfono. Confirma el cambio verificando que el valor InternationalSendingEnabled sea true { "PhoneNumberArn": "arn:aws:sms-voice:us-east-1:111122223333:phone-number/phone-a1b2c3d4e5f67890", "PhoneNumberId": "phone-a1b2c3d4e5f67890", "PhoneNumber": "+18005550199", "Status": "ACTIVE", "IsoCountryCode": "US", "MessageType": "TRANSACTIONAL", "NumberCapabilities": [ "SMS" ], "NumberType": "TOLL_FREE", "MonthlyLeasingPrice": "2.00", "TwoWayEnabled": true, "InternationalSendingEnabled": true, "CreatedTimestamp": "2025-08-15T10:30:00.123Z" } JSON 3. Para Verificar el Estado Actual: Usa el comando describe-phone-numbers con tu ID de Número de Teléfono para verificar su configuración actual en cualquier momento. aws pinpoint-sms-voice-v2 describe-phone-numbers \ --phone-number-ids "phone-a1b2c3d4e5f67890" \ --region us-east-1Bash Beneficios y Limitaciones Esta función ofrece una nueva forma poderosa de llegar a una audiencia global, pero es importante entender dónde destaca y cuáles son sus limitaciones. Beneficios (Ventajas)

Alcance Global con un Solo Número: Envía SMS a más de 150 países usando un solo TFN de EE.UU. existente. Gestión Simplificada: Evita la complejidad operacional y el costo de comprar y gestionar una flota de números de teléfono específicos por país. Prototipado y Pruebas Rápidas: Prueba rápidamente campañas de mensajería en nuevos mercados internacionales antes de comprometerte con el enfoque de mejores prácticas de adquirir números dedicados en el país. Optimización de Costos para Alertas Unidireccionales: Proporciona un método rentable para enviar mensajes transaccionales unidireccionales de alto volumen como OTP, recordatorios de citas y notificaciones de envío globalmente.

SMS Bidireccional Limitado a EE.UU. y Canadá: Las conversaciones SMS bidireccionales confiables solo son compatibles para destinatarios en Estados Unidos y Canadá. Solo Unidireccional para Todos los Otros Países: Para todos los otros destinos, esto es solo unidireccional. Entrega con máximo esfuerzo (no garantizado): El envío fuera de EE.UU. y Canadá es con máximo esfuerzo (no garantizado). El número de teléfono que aparece en el dispositivo del destinatario puede ser reemplazado con un número local o ID de Remitente, por lo que la mensajería bidireccional no funcionará para estos destinos. Para más detalles sobre maximizar la entrega, lee Una Guía para Optimizar la Entrega de SMS y Mejores Prácticas (Inglés). La Exclusión Gestionada No Está Garantizada Internacionalmente: La funcionalidad automática de respuesta STOP no funciona para destinos fuera de EE.UU. y Canadá. Para destinatarios internacionales, debes proporcionar un método alternativo de exclusión. Rendimiento Estándar (3 MPS): Los TFN internacionales tienen un rendimiento predeterminado de 3 Partes de Mensaje Por Segundo (MPS). Para campañas de alto volumen y alto rendimiento, los números específicos por país dedicados (como códigos cortos) son la mejor práctica recomendada.

Entendiendo el Costo El precio para esta función es directo:

Sin Tarifas Mensuales Adicionales: No hay cargo extra por habilitar la capacidad de envío internacional en tu TFN de EE.UU. Solo pagas el arrendamiento mensual estándar por el número mismo. Mensajería de Pago por Uso: Se te factura por cada mensaje SMS saliente a la tarifa estándar por mensaje para el país de destino.

Para una lista completa y actualizada de precios por país, visita la página de Precios de AWS End User Messaging. Cuándo Usar TFN Internacional vs. Números Específicos por País Elegir la herramienta correcta depende de tu caso de uso. Aquí hay una comparación simple:

Caso de Uso ¿Usar TFN Internacional? ¿Usar Número Específico por País (Mejor Práctica)?

Probar rápidamente un nuevo mercado Sí. Es la forma más rápida de comenzar. No, este enfoque toma más tiempo para configurar.

Enviar alertas unidireccionales (OTP, notificaciones) Sí. Es una solución simple y rentable. Sí, pero es más complejo si necesitas enviar a muchos países.

Requerir conversaciones bidireccionales Solo para EE.UU. y Canadá. Sí. Este es el enfoque requerido para SMS bidireccional confiable en un país específico.

Garantizar que tu marca/número aparezca consistentemente No, planifica que el ID del Remitente no se preserve, ya que esto no está garantizado internacionalmente. Sí. Esta es la razón principal para usar un número dedicado en el país.

Maximizar la entregabilidad para campañas críticas No, la entrega es "con máximo esfuerzo". Sí. Un número local proporciona la mayor probabilidad de entrega exitosa.

Consideraciones y Próximos Pasos Una vez que hayas habilitado tu envío internacional sobre Números Gratuitos de EE.UU., puedes mejorar tu estrategia de mensajería considerando resistencia, monitoreo y escalabilidad. Los siguientes recursos proporcionan mejores prácticas para mejorar tu envío.

Monitoreo de Entrega: Para monitorear tasas de entrega y patrones por país, puedes usar Conjuntos de Configuración para crear destinos de eventos. Esto te permite transmitir eventos SMS (como DELIVERED o FAILED) a servicios como Amazon CloudWatch o Amazon Data Firehose para análisis. Construyendo Resistencia: Para implementar entrega robusta, incluyendo estrategias de reintento automático para mensajes fallidos, recomendamos leer nuestra guía: Cómo construir entrega de SMS resistente con AWS End User Messaging (Inglés). Estrategia Global Más Amplia: Para una mirada más profunda a los elementos estratégicos de un programa SMS global, nuestro post sobre Cómo Gestionar el Envío Global de SMS con AWS End User Messaging (Inglés) proporciona información valiosa e incluye una plantilla para organizar casos de uso y seleccionar originadores.

Conclusión El SMS Internacional para Números Gratuitos de EE.UU. es una herramienta estratégica poderosa para empresas que buscan simplificar su mensajería global. Destaca en permitir pruebas rápidas en nuevos mercados y entregar eficientemente alertas transaccionales unidireccionales en todo el mundo desde un solo número. Sin embargo, no es un reemplazo para la mejor práctica de usar números de teléfono dedicados en el país cuando conversaciones bidireccionales confiables y marca garantizada son críticas para el éxito de tu campaña. Al entender sus beneficios y limitaciones, puedes usar estratégicamente esta función para comenzar rápidamente mientras planificas un movimiento a largo plazo hacia códigos específicos por país para tus mercados más importantes.

Taking a comprehensive perspective to mainframe application modernization with a disposition strategy

Introduction Mainframe customers face a myriad of options to modernize. Organizations now face urgent imperatives to modernize due to talent shortages, high and often rising costs, and restrictions on business agility imposed by the legacy environment. Customers also find themselves navigating a multitude of modernization patterns, tools, and strategies. The disposition strategy includes guiding principles that help you address complex mainframe monolithic applications and large code bases. Customers decompose legacy applications into manageable chunks, develop a target migration pattern, sequence the migrations, and build integrations to remaining mainframe workloads. This is iterated on until the application is fully decoupled. A disposition strategy is a multi-pattern approach to mainframe modernization. The right pattern is selected for the right workload, based on prioritization of business and IT objectives and workloads characteristics. Rather than view mainframe modernization as the collection of individual projects and disjointed migration phases, the disposition strategy advocates for the creation of a comprehensive perspective. This includes a roadmap that defines the entire journey from mainframe to cloud native from the beginning. This approach serves to accelerate migrations, reduce risk, and help customers achieve business and technology objectives in acceptable timeframes. Define a North Star Mainframe estates are varied across geographies, industries, and customers, with no two mainframe applications truly being alike. There is great diversity in legacy technologies, business objectives, future state requirements, and risk appetites across customers. Many large enterprises have multiple lines of business supported by applications running on the mainframe. These business lines also have a wide number of business leaders, application owners, and stakeholders across the organization dependent on mainframe processing. This dynamic often results in a scenario where organizations lack a clear strategy for the future state of the mainframe. We often see customers early in their modernization journey who have a mix of strategies actively being implemented or planned by different mainframe stakeholders. This results in a disjointed approach to modernization. The modernization may not have a north star vision articulated that guides the organization on their modernization journey. The first step for any successful mainframe modernization program is to define a north star. This north star is shared by the C-Suite and often the Board of Directors of an organization. Customers recognize the growing risks, costs, and competitive disadvantages associated with remaining on the mainframe. With executive leadership guidance for modernizing legacy applications, we see customers operate with greater speed, urgency, and successful delivery. Without a clear mission, we see that customers engage in a series of disjointed, tactical modernization programs. Disjointed programs may succeed in moving workloads of the mainframe platform yet struggle to deliver the full benefits from the modernization. In some cases, this may even result in MIPS usage increasing due to constraints put on the mainframe. To avoid this situation, we advise customers to define a north star by answering three primary questions:

Why are we modernizing? Where are we going? When will it happen?

Answering these questions sets the foundation for a successful mainframe migration program and helps the organization define common business and technical goals shared by the organization. Business Criteria for Modernization: Balancing business requirements with technical realities Business objectives can vary dramatically across divisions within organizations. Some business units have urgent requirements to change the functionality of their legacy applications, while others resist modification to established processes or user experiences. Broadly, we see priorities from the business falling into two categories: Category 1: Business functionality remains the same, but technology modernization is essential for business agility. No desire for functional change. In this scenario, the business users are quite happy with the existing business functions supported by the legacy applications. The business functions and workflows that exist today do not need to be altered, and the business users are opposed to the idea of functional changes. This can even be true for customers who have users interacting with greenscreens. Users that have been using greenscreens may be operating at such a high efficiency level that replacing this with a modern UX could result in productivity declines. Customers should generally expect that the majority of their mainframe workloads will fall into this category. Mainframe applications have persisted in organizations for decades. These applications may be well-suited to the business with years of custom business logic. They could be tailor-made for large enterprises and their differentiated way of doing business in their respective industries. Not every application requires functional transformation. For systems where stability and predictability are paramount, it is important to consider:

Refactor options that convert the legacy applications to a modern programming language and relational database. For example, using AWS Transform for mainframe, customers can refactor COBOL applications to Java on AWS using specialized AI agents from AWS. Re-platforming options that maintain existing functionality and preserve legacy technology stacks while moving to more modern infrastructure. For example, AWS Mainframe Modernization capabilities provide options to re-platform to mainframe compatible runtimes in the cloud.

For these applications, focus on communicating the non-functional benefits of modernization: resiliency, reduced impact radius, availability, agility. Category 2: Business functionality requires changes to remove technical debt, add new features, and decompose monoliths for product alignment. Requirements for functional enhancements. This is where the business requires functional enhancements to be made to the applications. Generally, customers expect only a portion of their mainframe workloads to fall into this category. In this situation, the business may have desires for modern UIs, real time capabilities, or faster batch processing. Customers may also have goals to break legacy monoliths into product aligned business functions. This approach creates a microservices architecture for loose coupling, which facilitates agility and innovation. Customers are increasingly met with growing end-customer expectations for near real-time capabilities. These are challenging to introduce to mainframe monoliths. Additionally, we meet with customers who have ambitions to expand into new markets, industries, or otherwise grow their customer base. Often, customers with aggressive growth objectives find that maintaining mainframe applications hinders their ability to grow and win new business.

Growth enablement: Which applications, if modernized, would unlock new revenue streams or support business expansion? Customer experience impact: Which applications directly affect customer interactions and satisfaction? Market responsiveness: Which systems currently limit your ability to respond to market changes? Innovation potential: Which applications would benefit most from modern development practices and integration with cutting-edge technologies?

Business units with clearly articulated functional requirements for enhancement should typically receive higher prioritization. Their specific needs, whether for new features, improved user experiences, or integration capabilities, provide concrete objectives that can drive modernization efforts and demonstrate tangible value.

Reimagine pattern is defined as the rewriting and rearchitecting of a mainframe application to a modern technology stack. The goal of this modernization is to introduce functional changes to the application. When the business requires new functionality, the reimagine pattern is the preferred approach.

Technical Criteria for Modernization The disposition strategy for mainframe modernization should also incorporate technical criteria evaluated at both the organizational and application levels. Evaluating Technical considerations at the organization level: Data center exit strategy or urgent migration deadlines Organizations facing mandates for a data center exit have urgent migration deadlines that must prioritize speed and risk mitigation. The rehost pattern, meaning moving mainframe workloads to a partner-managed data center, with no code changes, might be a pragmatic first step. The rehost approach can often be completed in months, bypassing the lengthy cycles of alternative migration patterns. Rehosting can provide business continuity. It also lays the foundation for future modernization and helps organizations incrementally adopt more advanced patterns. These patterns can address modernization needs like refactor or reimagine within time and resource constraints. Niche Mainframe technologies: programming language, transactional monitor, database Specific programming languages, transactional monitors, and database technologies used by existing mainframe applications can have a significant impact on the feasibility and complexity of modernization efforts. This is an important technical consideration when evaluating a mainframe modernization disposition strategy. Certain mainframe technologies, like Natural/Adabas, IDMS, and others may not be directly supported or fully supported by some modernization patterns like re-platforming or refactoring. The availability and maintainability of skills for these legacy mainframe technologies is also a factor. This can limit the modernization options available, and patterns like replace or reimagine might be the only viable choices. Organizations must carefully assess the mainframe technology stack being used and how that aligns with the feasibility and complexity of different modernization approaches. This technical assessment is a key input into determining the right disposition strategy. Vendor renewal timelines Vendor renewal timelines can be an important technical consideration when evaluating a mainframe modernization disposition strategy. Organizations often have various mainframe vendors with different software licensing contracts. Renewal timelines raise risks when commercial terms are being evaluated. In this case, some customers determine the best value outcome includes an exit from those vendor technologies as quickly as possible. This timeline velocity can also influence the modernization approach to choose. For example, if there is an urgent deadline to exit a license agreement, a refactoring pattern may be more suitable than a replacement or reimagine approach. Refactoring helps the organization to modernize the application while preserving the core functionality, which can often be done more quickly than a full rewrite or reimplementation. However, it's important to note that not all refactoring solutions support all mainframe technologies. An assessment of the right refactoring solution for the specific technologies in use must be completed. In some cases, there may not be an obvious or proven refactoring solution available. A reimagination or replacement approach may be the only viable option to exit the vendor technology by the required deadline. It is important to evaluate mainframe vendor contracts and renewal timelines as part of the overall technical assessment when determining the best modernization strategy. This helps align the selected approach with the urgency to exit particular vendor technologies. Available mainframe skillsets When companies face a resource talent risk due to constrained mainframe skillsets, it is important to choose mainframe modernization options that have less reliance on those skills. In such cases, strategies like refactoring and reimagining the mainframe applications can be effective approaches. Conversely, when companies have a capable mainframe talent pool within their organization, a re-platforming approach can be a suitable strategy for modernization. They can use existing expertise to migrate the workloads to a more modern platform. Evaluating Technical Complexity and Dependencies of the application/workload The selection of an appropriate pattern should be based on both business considerations and technical requirements. It should consider the specific characteristics of each workload or application. It is critical to conduct a thorough technical assessment of the various applications and workloads to determine the best modernization approach for each one. During this assessment phase, consider the following factors:

Source technologies: Evaluate the programming languages and the volume of the existing source code. Some languages and frameworks are more amenable to automated transformation and modernization than others. This can impact the feasibility and complexity of certain modernization patterns. Data considerations: Assess the data store technologies used on the mainframe (Db2, IMS DB, VSAM, etc.). Evaluate the volumes of data, the complexity of the data structures, and the relationships between data entities. The nature and complexity of the data can influence the appropriate modernization approach. Degree of coupling: Identify the level of coupling between the different applications and workloads. For example, an application that includes transactional context propagation likely has tight coupling. In this case, it would pose more modernization challenges than those with loose coupling or clear service boundaries. This is because the interdependencies of the tightly coupled functions must be addressed in order and be specifically managed in the modernization journey. Integration complexity and dependencies: Assess the various integration points between the applications and workloads. Identify shared resources, data dependencies, and the overall complexity of the integration landscape. This can help determine the appropriate modernization pattern that can preserve the existing integrations or help to provide a transition with less risk. External interfaces: Depending on the chosen modernization pattern, some client applications running outside of the mainframe but accessing it through external interfaces may also be changed. Verify that the selected pattern supports the required interfaces for all external connection points, API operations, and data exchange mechanisms with external systems.

Grouping applications that access the same data in read/write mode together and choosing the same patterns for those groups. Selecting the same pattern for workloads with a high degree of coupling Considering the impact of the source programming languages on the feasibility of different patterns Selecting a pattern that will minimize changes to external interfaces and integrations where possible.

An application and workload analysis is a key input into the overall disposition strategy. It can add the right modernization patterns and solutions for workloads based on their unique technical characteristics and dependencies. Strategy Development Building a Business-Outcome-Driven Program Rather than treating modernization as a purely technical exercise, develop a program that:

Works backwards from the organizations north star: As mentioned at the start, customers need an organizational strategy and approach for their mainframe estate. Successful mainframe migration projects operate within the parameters set by the company's leadership. Why are they modernizing, where are they going, and when will they achieve it. Aligns with strategic business objectives: Modernization should support specific business outcomes such as increased agility, improved customer experience, or new capabilities. Considers the entire portfolio from the start: Even if modernization is defined as a phased approach, planning should cover the complete application landscape to avoid creating new technology silos. Balances tactical wins with strategic goals: Design the program to deliver incremental value while working toward comprehensive modernization. Establishes clear metrics for success: Define how you'll measure progress in both business and technical dimensions.

Without a strategy set at the C-suite level, individual teams may adopt divergent approaches or fall into a "wait and see" mindset. This could delay modernization and create additional complexity. Avoid the "rebuild everything" pitfall Our experience shows that the 80/20 principle generally applies to mainframe estates: roughly ~80% of the mainframe applications do not require functional changes, and ~20% of the applications must be reimagined. We advise customers to consider modernization approaches that include substantial mainframe exits. Customers like Transamerica and Goldman Sachs have successfully used refactor and re-platform patterns to move mission-critical mainframe workloads to AWS. Taking individual application-by-application approaches may be too slow to meet business imperatives. Consider incorporating multiple modernization patterns, based on business and technological objectives.

Refactoring at scale: AWS Transform for mainframe provides refactor capabilities that can help you to modernize the legacy application to modern, Java frameworks. This pattern can be used when you want to reduce dependency on legacy technologies while benefiting from the accelerated migration timeline offered by deterministic tooling. Re-platforming: Re-platforming uses emulation technology to achieve a like-for-like migration of mainframe applications. This is often referred to as a "COBOL to COBOL migration". In this case, the re-platforming pattern can address situations with COBOL talent shortages, and accelerate mainframe exits.

Mass modernization approaches combined with strategic reimagining provide customers the opportunity to align technical and business outcomes while still driving towards a platform exit. Customers who consider multiple patterns in their strategy are able to address more varied objectives within their organization. This is done while delivering operational cost reductions goals within the same time period. Conclusion: The Time for Action is Now Today, the imperative for mainframe application modernization is strong. Beyond the commonly cited challenges of talent shortages, increasing software costs, and organizational inefficiencies, a new driver has emerged: the growing impact of generative AI on software development. As generative AI coding assistants revolutionize productivity for modern languages, the development speed and productivity gap between modern and mainframe technologies will compound. Organizations with applications in COBOL, Assembler, or PL/1 languages face a growing competitive disadvantage regarding the speed to value. Their peers may operate core systems written in modern technologies that can move at ever increasing development speeds. There is no "silver bullet" for mainframe modernization. Success requires a business-driven, multi-pattern approach that aligns IT and business objectives around concrete outcomes. By using automation and iterating incrementally, you can focus on the value beyond cost savings. The disposition strategy provides a framework for this journey, one that recognizes the nuances of each application portfolio. By modernizing mainframe applications with this approach, organizations can preserve the valuable business logic built over decades while positioning themselves for future innovation demands.

Tim Gray Tim is the worldwide go-to-market lead for AWS Transform for mainframe. Tim focuses on the go-to-market strategy to help customers leverage AWS Transform to reimagine and modernize their core systems on AWS. Today, Tim focuses on building repeatable patterns to deploy generative and agentic AI services that accelerate large-scale modernization programs in unprecedented timeframes.

Sunil Divvela Sunil Divvela is a Worldwide Specialist Solutions Architect for Mainframe Modernization at AWS. He works closely with customers and partners to innovate and accelerate their mainframe modernization journeys, leading initiatives from portfolio assessment through post-migration support using Generative AI and Agentic AI. Before joining AWS, Sunil served as a Senior Technology Architect for Infosys, where he led multiple mainframe transformation projects.

Yann Kindelberger Yann Kindelberger is a Principal Solution Architect at Amazon Web Services. Yann has worked with Mainframes for 23+, being a Mainframe Architect at IBM for more than 20 years. He is part of a WW team working on migration and modernization of Mainframe to the AWS Cloud. He joined AWS in 2021 and his role as a Solution Architect is to help, advise and support customers to migrate and modernize their Mainframe.

When your 20-year-old monolithic application needs new features and you can't afford downtime or risk breaking existing functionality, what do you do? This challenge faces countless organizations running mission-critical legacy systems that power their business but struggle to keep pace with modern requirements. At AWS, we often hear from customers who need to modernize their legacy applications while maintaining business continuity. In this blog post, we will explore how to use event-driven architecture (EDA) with the leave-and-layer pattern to modernize your applications incrementally. The "leave-and-layer" architectural pattern, combined with using an event bus like Amazon EventBridge, enables you to add new capabilities to existing applications without the complexity and risk of traditional modernization approaches. The Challenge: Innovation vs. Stability Consider a scenario: a developer with extensive experience in JavaScript and .NET joins a company running a Java-based e-commerce application. The application has been successful for two decades, but the company is losing market share because they can't innovate fast enough. The new developer's lack of domain knowledge of the application and limited Java skills create a challenge. The business needs to integrate with a third-party customer support system to notify teams whenever new customers sign up. The challenges are significant: the developer doesn't know Java, the application can't have downtime, and leadership mandates all new development must be cloud-native. This scenario illustrates a common dilemma: how do you extend existing applications, with new modern components, while maintaining business continuity and reducing risk? Two Paths: Strangler Fig vs. Leave-and-Layer The Strangler Fig Pattern The strangler fig pattern involves gradually replacing parts of a legacy application by extracting functionality from the monolith, implementing it as microservices, routing traffic to new services, and eventually "strangling" the old system. While effective, this approach presents several challenges. Teams need deep knowledge of the existing codebase to safely extract functionality. Database splitting becomes complex when multiple services share data stores. Converting synchronous transactions to asynchronous patterns requires careful orchestration. Application Programming Interface (API) design and versioning add another layer of complexity as you manage contracts between old and new systems. The Leave-and-Layer Pattern The leave-and-layer pattern takes a fundamentally different approach. You leave the existing application completely unchanged while layering new functionality alongside it. This approach uses loose coupling for integration and allows you to build new capabilities in your preferred technology stack. This pattern is ideal when you need to add functionality without affecting existing systems, work with unfamiliar legacy technology, deliver quickly with minimal risk, or build cloud-native extensions to traditional applications.

Building Asynchronous Integration with Amazon EventBridge The key to a successful leave-and-layer implementation lies in asynchronous communication. Modern systems benefit from asynchronous patterns where actions trigger responses without requiring immediate acknowledgment. Amazon EventBridge provides foundation for event-driven architecture through three core components. Event producers are systems that generate events, such as when a new customer signs up. Event consumers are systems that react to these events, like sending notifications to support teams. The event broker, which is EventBridge itself, acts as the intelligent routing layer between producers and consumers.

EventBridge offers three types of event buses to handle various integration scenarios. The default event bus works with events coming from AWS services. Custom event buses handle events from your applications. Software as a Service (SaaS) event buses integrate with upstream SaaS platforms that support EventBridge as a destination. Implementation: Minimal Code, Maximum Impact The integration challenge is solved with just a few lines of code added to the existing application. There are three steps: enabling event production in the legacy application, configuring EventBridge rules for routing and transformation, and setting up API Destinations for third-party integration. The Complete Solution Architecture The final architecture demonstrates the elegance of the leave-and-layer pattern. The legacy application continues running unchanged, serving existing customers without any disruption. A minimal event publishing layer publishes customer signup events to EventBridge. EventBridge rules provide the integration intelligence, matching event patterns and transforming data as needed. API Destinations handle connections to external systems, providing reliable delivery with built-in retry mechanisms. The entire integration happens asynchronously, ensuring that third-party system availability doesn't impact the core application.

Step 1: Enable Event Production To add event publishing to your legacy application, create an abstraction layer that hides the EventBridge implementation details. This approach lets you switch between event buses and publish events both synchronously and asynchronously. The following examples use the AWS SDK for Java v2 EventBridge client. Read, What is the AWS SDK for Java 2.x for complete SDK documentation. Create the Event Publishing Interface First, define the PostEvents interface that specifies how to publish events: import java.util.concurrent.CompletableFuture;

public interface PostEvents { void publishEvent(String source, String detailType, Object eventData); CompletableFuture publishEventAsync(String source, String detailType, Object eventData); }Java Implement the EventBridge Publisher Next, create the EventBridgeEventPublisher class that implements the PostEvents interface: import software.amazon.awssdk.services.eventbridge.EventBridgeClient; import software.amazon.awssdk.services.eventbridge.model.PutEventsRequest; import software.amazon.awssdk.services.eventbridge.model.PutEventsRequestEntry; import com.fasterxml.jackson.databind.ObjectMapper; import java.util.concurrent.CompletableFuture; import org.springframework.stereotype.Component;

@Component public class EventBridgeEventPublisher implements PostEvents { private final EventBridgeClient eventBridgeClient = EventBridgeClient.create(); private final ObjectMapper objectMapper = new ObjectMapper();

@Override public void publishEvent(String source, String detailType, Object eventData) { try { PutEventsRequestEntry event = PutEventsRequestEntry.builder() .source(source) .detailType(detailType) .detail(objectMapper.writeValueAsString(eventData)) .eventBusName("unicorn-shop-events") .build();

eventBridgeClient.putEvents(PutEventsRequest.builder() .entries(event) .build()); } catch (Exception e) { // Handle serialization/publishing errors throw new RuntimeException("Failed to publish event", e); } }

@Override public CompletableFuture publishEventAsync(String source, String detailType, Object eventData) { return CompletableFuture.runAsync(() -> publishEvent(source, detailType, eventData)); } }Java Update Your Controller Finally, modify your controller to use the new event publishing abstraction: import org.springframework.web.bind.annotation.RestController; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.http.ResponseEntity;

@PostMapping("/signup") public ResponseEntity createUser(@RequestBody User user) { // Existing user creation logic remains unchanged...

// Publish event using abstraction eventPublisher.publishEvent("unicorn.shop", "New Customer Signup", user);

return ResponseEntity.ok("User created successfully"); } }Java This approach maintains your existing user creation logic while adding event publishing capabilities. This abstraction provides several benefits: the EventBridge implementation details are hidden from business logic, and both synchronous and asynchronous publishing patterns are supported based on use case requirements. Step 2: Configure EventBridge Rules In Step 1, you configured your application to publish events with the source 'unicorn.shop' and detail type 'New Customer Signup'. Now you'll create rules to handle these specific events. EventBridge rules provide the intelligence layer that determines how events flow through your system. These rules match specific event patterns, transform data for downstream systems, and route events to appropriate targets, as shown in Figure 4. The rule includes pattern matching that ensures only events containing the required fields get processed. Any events that don't match this pattern are automatically rejected. You can create EventBridge rules using the AWS Command Line Interface (CLI), AWS CloudFormation, or the AWS Management Console. Here's how to set up the rule using AWS CLI: # First, create the custom event bus aws events create-event-bus --name customer-events

# Create the EventBridge rule aws events put-rule \ --event-bus-name customer-events \ --name customer-signup-rule \ --event-pattern '{ "source": ["ecommerce.application"], "detail-type": ["New Customer Signup"], "detail": { "firstName": [{"exists": true}], "lastName": [{"exists": true}], "email": [{"exists": true}] } }' \ --description "Route new customer signup events to external systems"Bash

Step 3: Use API Destinations EventBridge API Destinations eliminate the need for custom integration code when connecting to third-party systems. This feature provides built-in retry logic, error handling, and authentication support including API keys and OAuth. You configure an API Destination pointing to your third-party system's webhook URL. The rule includes an input transformer that converts the EventBridge event format into the JSON structure that the external system expects. This transformation happens entirely within EventBridge, requiring no additional compute resources or custom code.

Create the Authentication Connection First, set up authentication for your third-party system: # Create a connection for authentication aws events create-connection \ --name slack-connection \ --description "Connection to Slack webhook" \ --authorization-type API_KEY \ --auth-parameters '{ "ApiKeyAuthParameters": { "ApiKeyName": "Authorization", "ApiKeyValue": "Bearer xoxb-your-slack-bot-token" } }'Bash Create the API Destination Next, configure the destination endpoint: # Create the API destination aws events create-destination \ --name slack-customer-notifications \ --description "Send customer notifications to Slack" \ --connection-arn arn:aws:events:us-east-1:123456789012:connection/slack-connection \ --http-method POST \ --invocation-endpoint Add the Target to Your Rule Finally, connect the API destination to your EventBridge rule: # Add the API destination as a target to the rule aws events put-targets \ --event-bus-name customer-events \ --rule customer-signup-rule \ --targets '[ { "Id": "1", "Arn": "arn:aws:events:us-east-1:123456789012:destination/slack-customer-notifications", "HttpParameters": { "HeaderParameters": { "Content-Type": "application/json" } } } ]'Bash Benefits Realized This approach delivers immediate and measurable value to organizations. Existing applications experience limited downtime during implementation and continue to serve customers with no performance impact on the legacy application. Development teams can deliver new features within single sprints, demonstrating the speed advantage of the leave-and-layer approach. Developers can implement solutions using familiar AWS services rather than spending weeks learning legacy frameworks and patterns. The cloud-native foundation built with EventBridge provides a platform for future integrations without requiring additional changes to legacy applications. When used with the strangler fig pattern, this can accelerate the decomposition of the monolith by providing loose coupling. Perhaps most importantly, the solution proves its resilience in production. When external systems experience temporary outages, EventBridge automatically retries failed deliveries, ensuring no notifications are lost. Key Takeaways Pattern selection impacts project success. Choose leave-and-layer pattern when you need to extend rather than replace functionality, especially when working with unfamiliar technology stacks or tight delivery timelines. Asynchronous architecture provides inherent resilience. Event-driven systems handle third-party unavailability gracefully, automatically retrying failed operations and maintaining system stability even when dependencies experience issues. Minimal code changes can enable significant new capabilities. With a few lines of code to unlock integration possibilities that would require extensive custom development using traditional approaches. EventBridge simplifies complex integrations through managed services. API Destinations eliminate the need for custom integration code, while built-in transformation capabilities handle data format differences between systems. Call to Action Ready to implement the leave-and-layer pattern in your own legacy modernization project? Start by exploring the EventBridge documentation to learn more about event-driven architecture patterns. For hands on work, try out Building Event-Driven Applications With Amazon EventBridge on Skill Builder or Building event-driven architectures on AWS Workshop Studio.

Jeff Escott Jeff Escott is a Principal Architect at AWS with over 30 years of technology experience. He works with customers to define modernization strategies and adopt cloud-first approaches, focusing on helping customers transform across people, process, and technology. Additionally, Jeff has been a long time advocate for advancing the practice of architecture and growing future architects. Jeff holds a bachelor's degree in Actuarial Science from Worcester Polytechnic Institute and is based in Connecticut, USA.

Jorge Alvarez Jorge Alvarez is a Solutions Architect Specialist in Migration and Modernization at AWS. In his role, he provides guidance to customers in their Migration and Modernization journey in EMEA. Prior to becoming a solutions architect at AWS, Jorge was leading teams in the cloud space, helping customers in their journey and modernization efforts.

How Strands Agents brings geospatial generative AI to the public sector

Government agencies manage vast amounts of location-based information daily, tracking where incidents occur, coordinating nearby crews, and optimizing resource routing. Yet despite this spatial data representing roughly 80% of all government information, accessing these insights often requires specialized geographic information system (GIS) expertise and complex software that creates barriers for staff members. This post explores how AI agents powered by Strands Agents on Amazon Web Services (AWS) can transform geospatial workflows, making spatial data accessible to any government worker through natural language interactions. We'll examine the technical architecture, implementation details, and demonstrate the approach using Brussels' FixMyStreet open data API, an open data API available for the city of Brussels, which provides real-time nonemergency incident reports including graffiti, road maintenance, and public infrastructure repairs across the city. The challenge: Inaccessible location data Although approximately 80% of government data contains spatial components that could drive better decision-making across numerous areas, including infrastructure maintenance, emergency response, urban planning, and resource allocation, two critical barriers prevent broader access to this information. The first is bottlenecks due to GIS expertise. Traditional tools require training in spatial concepts, coordinate systems, and complex interfaces. Second, some interfaces are inaccessible. Without intuitive tools, valuable spatial insights remain locked away from the staff who could benefit most from location-based decision-making in their daily operations. The solution lies in intelligent agents that can bridge this gap, handling the technical complexity and allowing staff to interact with spatial data through simple, conversational requests. Solution overview This agent-based approach processes natural language queries and presents geospatial insights through an interactive map visualization. The AI agents are built with Strands Agents, a lightweight framework designed for creating task-oriented AI agents that can integrate multiple tools and data sources. Strands provides the structure needed to integrate natural language understanding—using models such as Anthropic's Claude Haiku 3.5 in Amazon Bedrock—with specialized geospatial tools while maintaining control over how these components work together. By combining these capabilities, the solution eliminates the technical barriers that prevent most government staff from accessing location-based insights while maintaining the accuracy and reliability required for public sector operations. The following real-world example demonstrates how this works in practice, using publicly available Brussels nonemergency incident data. Here's how a government employee might interact with the system using publicly available Brussels nonemergency data. User query: "Show me all graffiti reports in Grand Place that are ready to get processed"

Figure 1: User asking the agent "Show me all graffiti reports in Grand Place that are ready to get processed"

Agent response: "I've found 10 graffiti incidents in the Grand Place area that are currently in the PROCESSING status. Notably, 9 out of the 10 incidents have been open for more than 14 days, which might indicate a backlog in graffiti removal. Would you like me to help you create a route to address these incidents or provide more details about any specific incident?" The results appear immediately on an interactive map, while the agent seamlessly handles the technical complexity behind the scenes, as shown in the following screenshot.

Figure 2: The result of the query, showing the points from the search results for incident reports in "Grand Place" that are in "PROCESSING" status, as well as a short summary

Geocoding "Grand Place" to coordinates (4.35178, 50.84623; 4.35329, 50.84721) Filtering for "graffiti" category and "PROCESSING" status Executing a spatial query within the Grand Place bounding box Returning formatted results with summary statistics

The user can then ask a follow-up query for route optimization and assignment. For follow-up queries, the user can interact with the visualized results. When they select the bounding box from the previous response, it becomes highlighted and provides context for the next request, as shown in the following screenshot. User Query: "Create a route starting and ending at Avenue Des Arts 27 and assign it to the relevant team"

Figure 3: User asking the agent the follow-up query, "Create a route starting and ending at Avenue Des Arts 27 and assign it to the relevant team"

Agent Response: "I've created an optimized route starting and ending at Avenue Des Arts 27, which covers 10 graffiti incidents in the Grand Place area. The route has been assigned to the graffiti team with high priority, with special notes about the long-standing incidents. The team will receive full details about the route and incidents to address. Is there anything else you would like me to do?" The optimized route is displayed on the map with turn-by-turn directions, and team members receive instant notifications containing complete route details, the priority level and special notes, a direct link to the navigation interface, and a list of incidents to address, as shown in the following screenshot.

Figure 4: The result of the query, showing the route through the incident reports from the previous search result and assigning it to the relevant team for collection

This complete workflow, from natural language query to actionable route assignment, demonstrates how the solution transforms geospatial operations into straightforward, conversational interactions that still offer the precision and reliability that government operations require. Architecture overview The following architecture demonstrates how these concepts work together in practice, integrating AI processing, geospatial tools, and data management to deliver the solution's conversational geospatial capabilities. The solution architecture integrates several components to deliver seamless geospatial analysis:

User interface layer – A web-based interface that accepts natural language queries and displays interactive maps with search results. Tool layer – AWS Lambda functions handle geocoding, route creation, and route assignment. These are decoupled from the agent for modularity, whereas tools that generate SQL remain within the agent Lambda in the isolated subnet in the virtual private cloud (VPC) for secure PostGIS access. AI processing layer – Amazon Bedrock with Claude Haiku 3.5 interprets user intent and orchestrates the appropriate geospatial operations. Data layer – Amazon Relational Database Service (Amazon RDS) for PostgreSQL with PostGIS extensions provides spatial data storage and querying capabilities. Ingestion layer – A Lambda function updates nonemergency incidents from the Brussels FixMyStreet API every minute using Amazon EventBridge Scheduler.

Although the architecture provides the foundation for the solution, the real intelligence comes from how the agent processes and responds to user queries. Let's examine the systematic workflow, or Agent Loop, that enables natural language interactions with this infrastructure. Agent Loop The agent uses Strands Agents to process user queries through an iterative workflow that seamlessly orchestrates specialized geospatial tools. This Agent Loop operates through a systematic sequence:

Query analysis – The agent interprets natural language input to understand spatial intent and requirements. Tool selection – Based on current context and conversation history, the agent chooses the most appropriate geospatial tool (this decision can be revisited after each tool execution). Parameter extraction – The agent pulls the inputs it needs immediately—such as addresses, coordinates, and filters—and uses any new data gathered so far from previous tool execution and conversation history. Tool execution – The agent invokes the chosen tool, analyzes the output, and determines whether to continue the loop with additional tools or proceed to finalization. Result processing – After the iterative loop concludes, the agent formats spatial outputs for clear visualization. Response generation – The agent delivers natural language explanations alongside interactive map displays.

To be able to achieve its goals, the agent has some tools available:

Geocode Address – Converts addresses and place names to single coordinates or bounding-boxes Get Incidents In Area – Generates and executes PostGIS point-in-polygon queries using ST_Within() for spatial containment analysis Get Incidents Near Location – Generates and executes PostGIS proximity searches with ST_DWithin() to find incidents within specified radii Get Optimized Route – Calculates efficient routes for field teams and service vehicles Assign Route To Team – Assigns the optimized routes to relevant field teams with priority levels and contextual notes Get Current Time – Retrieves current date and time for temporal context

The following diagram illustrates the Agent Loop for the geospatial application, highlighting the available tools.

Figure 6: The agent loop of the geospatial agent, with the different tools available to the agent

This iterative approach brakes down complex geospatial queries into manageable steps, with each tool building upon previous results to deliver comprehensive, actionable insights. Tool-based query building Rather than allowing AI agents to generate arbitrary SQL queries (text-to-SQL), we've implemented a controlled, tool-based approach that balances flexibility with security and reliability. Direct text-to-SQL approaches let an LLM write raw SQL for every prompt. That works for quick prototypes, but in public sector environments, it introduces unpredictable queries, hard-to-trace errors, and wide security exposure. Our tool-based architecture addresses these concerns:

Controlled execution – The AI selects from preapproved tools rather than generating arbitrary SQL queries Parameterized queries – All database interactions use tested and secure code with user inputs is safely handled as parameters Consistent performance – Query patterns are optimized and predictable, enabling reliable system behavior Full auditability – The observability capabilities of Strands Agents provide complete logging of which tools were executed and with what parameters

Our approach begins by encoding existing field team workflows into dedicated tools, then expanding the catalog as new use cases emerge. Each new tool inherits the same security guardrails and operational standards. Because every query executes through owned, versioned tools, we can thoroughly test changes before deployment and reuse audited logic across teams—so that operations staff receive consistently trustworthy results. Radius search example The Strands framework enables reliable spatial analysis through validated tools. Here's an example of a proximity search tool that generates PostGIS queries for area-based incident searches:

base_query += " AND (" + " OR ".join(["LOWER(status) = LOWER(%s)"] * len(statuses)) + ")"

This tool definition uses the Strands Agents @tool decorator, which automatically generates a schema from the function signature and type constraints. This means that the agent's parameter choices are restricted to valid, domain-specific values. The status parameter demonstrates this constraint-based approach: IncidentStatus = Literal["CREATED", "PROCESSING", "CLOSED"] By restricting the agent to the three specific status values, "CREATED", "PROCESSING", and "CLOSED", the system prevents errors from invalid status strings but provides consistent data handling across all queries. This structured approach transforms potentially risky, open-ended SQL generation into a more secure and predictable system that maintains the flexibility government operations require while delivering the reliability they demand. Conclusion This solution shows how government agencies can democratize access to geospatial data by pairing the natural-language tooling of Strands Agents with purpose-built geospatial capabilities. The approach maintains the accuracy and reliability essential for public sector operations while dramatically expanding who can access spatial insights. The versatility of this agent-coordinated, tool-based approach extends well beyond incident response across numerous public sector applications such as urban planning, transportation, and environmental monitoring. Urban planning teams can surface permit backlogs, identify zoning conflicts, visualize hotspots, and drill into specific parcels through natural language queries displayed on interactive maps. Transportation departments can analyze traffic congestion patterns, optimize route planning, and seamlessly share navigation paths with field crews. Units can query air quality readings, overlay trend analysis, and proactively identify compliance risks across geographic regions. By using trusted AI models, validated tools, and shared map visualization to standardize queries, this architecture provides agencies with a practical, cost-effective alternative to traditional GIS software. The result is a scalable solution that puts sophisticated geospatial insights into the hands of far more government workers, enabling data-driven decision-making across departments without requiring specialized technical expertise.

TAGS: Artificial Intelligence, AWS Public Sector, geospatial data, government, technical how-to

Andreas Holt Andreas is a solutions architect intern at AWS. He is completing his master's in computer science at Aalborg University. His areas of interest are mainly in distributed systems and cloud computing.

Angel Zarramera Angel is a solutions architect at AWS working with public sector customers. His areas of interest are cloud governance, networking, and security.

We're excited to share that the Swift AWS Lambda Runtime project has officially moved to the AWS Labs organization. You can now find it here: 👉 This move marks a new chapter for the project, while maintaining full continuity with its roots. A thank you to the Swift community The Swift AWS Lambda Runtime was originally created seven years ago by the Swift community (the initial commit was made the 1st of November 2018), as part of a broader effort to make Swift a powerful choice for server-side development. Since then, the project has been an incubating project as part as the Swift Server Workgroup (SSWG). We want to thank the 58 contributors who have helped shape this project through their code, feedback, and support. Their collective work established a solid foundation that has allowed Swift developers to confidently run their code on AWS Lambda. Why choose Swift for your server-side workload Swift has evolved far beyond its origins as a language for Apple platforms — it's now a robust, high-performance choice for building modern server-side systems. Swift's combination of type safety, predictable performance, and developer-friendly syntax makes it an appealing option for backend services where reliability, maintainability, and scalability matter most. Over the past few years, several organizations have successfully deployed Swift in production on the server. According to the Swift.org ecosystem overview, companies migrating from other languages have achieved impressive results: one reported a 4× performance increase after switching to Swift, while another saw 40 % more throughput, 50 % less hardware usage, and 90 % lower memory consumption compared to their Java-based implementation. Web frameworks such as Vapor and Hummingbird have made Swift a first-class option for developing REST APIs, microservices, and event-driven systems. Whether you're building a traditional web service, a distributed system, or serverless functions, Swift provides the safety and speed developers expect — without compromising on readability or productivity. Why choose Swift for your Lambda functions The Swift AWS Lambda Runtime allows you to write Lambda functions using the Swift programming language, bringing the language's safety, performance, and expressiveness to the world of serverless computing. In an event-driven architecture like AWS Lambda — where functions start and stop on demand and billing is measured in milliseconds — every millisecond and megabyte matters. Swift's design makes it especially well-suited for this environment: it's fast to start, efficient in memory use, and helps developers produce safe, predictable code that performs consistently under varying loads. Here are a few of the advantages that make Swift a great fit for Lambda workloads:

Performance – Swift is fast, leveraging LLVM optimizations and native execution on Amazon Linux environments. Memory efficiency – Swift's small memory footprint and automatic memory management are crucial in Lambda's constrained runtime environment, directly impacting both performance and cost. Type safety – Swift's strong typing and compiler checks help catch errors at build time, reducing runtime issues. Concurrency safety – Swift's strict concurrency checking at compile time helps prevent data races and concurrency bugs before deployment. Modern developer experience – Swift's concise syntax, rich type system, and expressive APIs make it easy to write, test, and maintain Lambda functions efficiently.

Whether you're building APIs, event-driven microservices, or data processing functions, Swift offers a powerful, modern option for your serverless workloads. What changes — and what stays the same Going forward, governance of the project will transition to AWS, under the AWS Labs GitHub organization. There will be no major changes in how the project works today. The same codebase, APIs, and open source spirit remain intact. The project remains an incubation project as part of the Swift Server Workgroup. Our goal with this move is to increase visibility for the runtime and make it easier for Swift developers — and the broader serverless community — to discover, use, and contribute to it. By hosting it under AWS Labs, we hope to strengthen the collaboration between the Swift open source communities and AWS teams, and to encourage more experimentation and innovation in this space. Open source Lambda runtimes Swift joins a growing family of open source Lambda runtimes maintained by the community and AWS. If you're working with other languages, you might be interested in exploring all the open source Lambda runtimes in the official documentation. Get involved The Swift AWS Lambda Runtime remains an open source project, and community contributions are at its heart. We warmly welcome issues and pull requests from developers who want to help improve the runtime or extend its capabilities. Visit the repository, explore the documentation, and join us in building the next generation of Swift serverless applications: 👉

Sébastien Stormacq Seb has been writing code since he first touched a Commodore 64 in the mid-eighties. He inspires builders to unlock the value of the AWS cloud, using his secret blend of passion, enthusiasm, customer advocacy, curiosity and creativity. His interests are software architecture, developer tools and mobile computing. If you want to sell him something, be sure it has an API. Follow @sebsto on Bluesky, X, Mastodon, and others.

In today's competitive gaming landscape, machine learning (ML) has become essential for delivering personalized experiences, optimizing game mechanics, and driving business outcomes. However, traditional approaches to building and deploying ML systems often require extensive DevOps expertise, manual pipeline configuration, and complex infrastructure management that can slow down innovation and time-to-market. Game studios need agile, automated solutions that can rapidly iterate on ML models, while maintaining production reliability and scalability across diverse gaming use cases. Amazon SageMaker AI and MLOps Amazon SageMaker AI provides powerful MLOps capabilities. However, orchestrating the complete continuous integration and continuous delivery (CI/CD) pipeline—from model development to production deployment—typically involves navigating multiple Amazon Web Services (AWS) services. These include managing intricate dependencies and coordinating approval workflows. This complexity can create barriers for game studios, or game analytics teams, who want to focus on building great predictive models rather than wrestling with infrastructure. We will demonstrate how to leverage Amazon Bedrock Agents to create an intelligent MLOps assistant that streamlines the entire CI/CD pipeline construction and management process. We will combine the conversational capabilities of Amazon Bedrock with the robust MLOps features of Amazon SageMaker AI. With this solution game teams can create, manage, and deploy gaming prediction models using natural language commands. Our solution addresses common pain points in gaming machine learning model build, train, and deploy pipelines:

Rapid experimentation: Quickly spin up new prediction experiments without infrastructure overhead Automated workflows: Streamline the path from model training to production deployment Approval management: Handle model approvals through conversational interfaces Multi-project coordination: Manage multiple game titles and their respective models from a single interface

By the end of this walkthrough we will have created a fully functional MLOps agent, capable of managing complex machine learning workflows for gaming analytics. Your team can then deploy the gaming prediction solution with conversational commands such as, "Create a player churn CI/CD pipeline for my mobile puzzle game," or "Show status of build pipeline execution". Prerequisites Before starting you will need to make certain you have done or have the following:

A GitHub account with remote access The AWS Command Line Interface installed An Amazon SageMaker AI domain and user

Create and configure an MLOps management agent Set up the foundation infrastructure Before creating the Amazon Bedrock Agent, establish the core AWS infrastructure that will support the MLOps workflows. The infrastructure includes two AWS Identity and Access Management (IAM) roles:

Trust relationships and policies for each role have been provided and referenced in the create role steps. Roles First, create an mlops-agent-role with attached inline policies to enable the Amazon Bedrock Agent to access required AWS services that support an MLOps pipeline.

Create an IAM role, mlops-agent-role with Trusted entity type AWS service and use case Lambda Select the Trust relationships tab, Edit the trust policy and paste the trust relationship policy in the trusted entities editor Add permissions with Create inline policy and paste the mlops-agent-policy in the policy editor box Create a second inline policy for AWS Lambda invocation access, and paste the lambda-invoke-access policy in the policy editor Replace ACCOUNT_ID with your AWS account ID in the policy document

Next, create an IAM role that allows the AWS Lambda action invocation to access required AWS services.

Create an IAM role called lambda-agent-role with trusted entity type AWS service and use case Lambda Search for the AWSLambdaBasicExecutionRole managed policy and add Select the Trust relationships tab, Edit the trust policy and paste the trust relationship policy in the trusted entities editor Next, add permissions with Create inline policy and add a lambda-agent-policy

MLOps AWS Lambda function The AWS Lambda function serves as the backend engine for the Amazon Bedrock Agent, handling all MLOps operations through an API. The Amazon Bedrock action group invocation calls the AWS Lambda function using an action group schema that maps endpoints to actions. Use the following steps to create the AWS Lambda function:

In the console, select AWS Lambda Select Author from scratch Enter a Function name, we used: mlops-project-management Choose a Python 3.1x Runtime Select x86_64 Change the default execution role to use the existing lambda-agent-role previously created Select the Create function button

Download the AWS Lambda function file or clone the function from our GitHub AWS Samples repository Copy and paste the function code into the Lambda code window

Select the Configuration tab to access the Function and General configuration Choose the Edit button and update the function Timeout value to 15 minutes On the Configuration tab, select Permissions and Add permissions for Resource-based policy statements:

Choose AWS Service For Service, select other Statement ID: bedrock-agent-invoke Principal: amazonaws.com Source ARN: arn:aws:bedrock: : :agent/* Action: lambda:InvokeFunction and Save

The function is now ready to act as an Amazon Bedrock Agent and support the following actions:

/configure-code-connection – Set up AWS CodeConnections connection for GitHub integration /create-mlops-project – Create a new SageMaker MLOps project with GitHub integration /create-feature-store-group – Create SageMaker Feature Store Feature Group /create-model-group – Create SageMaker Model Package Group /create-mlflow-server – Create Amazon SageMaker AI MLflow Tracking Server /build-cicd-pipeline – Build CI/CD pipeline using seed code from GitHub /manage-model-approval – Manage model package approval in Amazon SageMaker AI Model Registry /manage-staging-approval – List models in staging ready for manual approval /manage-project-lifecycle – Handle project updates and lifecycle management /list-mlops-templates – List available MLOps AWS Service Catalog templates

Repository seed code population from GitHub Dynamic AWS CodeBuild build script generation with project-specific parameters Pipeline parameter injection and configuration Multi-stage approval workflow management Error handling and detailed logging for troubleshooting

Amazon Bedrock MLOps Agent Use the following steps to create the agent:

In the console, Navigate to Amazon Bedrock Select Agents from the left navigation panel under Build Choose Create Agent Configure the agent with these settings:

Agent Name: MLOpsOrchestrator Description: Intelligent assistant for gaming MLOps CI/CD pipeline management Foundation Model: US Anthropic Claude 3.7 Sonnet Use the existing service role, mlops-agent-role for the agent resource role

Configure the agent Instructions—provide instructions that establish the agent's identity and capabilities by using the following:

You are an expert MLOps engineer specializing in SageMaker pipeline orchestration. Help users create, manage, and deploy ML models through automated CI/CD pipelines. Always follow AWS best practices and provide clear status updates. Available actions include: - Creating CodeConnections for GitHub integration - Setting up MLOps projects and CI/CD pipelines - Managing feature stores and MLflow tracking - Handling model and deployment approvals

Amazon Bedrock Agent action groups Use the following steps to create action groups:

Enter the group name: ProjectManagement Enter the following description: Actions for managing SageMaker MLOps projects and GitHub integration Select the Action group type: Define with API schemas Under Action group invocation, make certain to select: Select an existing Lambda function Under Select Lambda function, select mlops-project-management, with Function version as $LATEST

Under Action group schema, select Define via in-line schema editor Download the MLOps agent OpenAPI schema from the GitHub AWS Samples repository Select JSON from the drop-down and paste the provided OpenAPI schema in the editor Choose Save and exit

Use the MLOps agent With the agent created and configured, it's ready to use for launching AWS resources to support an MLOps CI/CD pipeline. As a foundation of the pipeline, an AWS Service Catalog template defines AWS CodeBuild projects, AWS CodePipeline pipelines, and SageMaker AI inference endpoints for staging and production. Creating an Amazon SageMaker AI project launches these resources with configuration specified with the MLOps agent. Before creating an Amazon SageMaker AI project with the AWS Service Catalog template, you'll need to set up several prerequisites. These prerequisites include:

An AWS CodeConnection to access GitHub A managed MLflow tracking server A feature store with sample features for the MLOps template that handles model building, training, and deployment with third-party Git repositories

The Feature Store Group features are based on a synthetic player churn data set

For the AWS Service Catalog template, create two empty private GitHub repositories:

Select Test and Prepare in the Amazon Bedrock Agents console Enter a prompt to create resources using natural language

Start with what MLOps tasks can you perform? When using provided, example prompts, note created resources values and replace where appropriate

Use the agent to create a Feature Store group for gaming analytics by using the following prompt:

Create Feature Store group named "player-churn-features" with feature description "player_id as string identifier, player_lifetime as number, player_churn as integer, time_of_day features as floats, cohort_id features as binary flags, event_time as event time feature" and description "Feature group for player churn prediction model containing player behavior and engagement metrics"

Next, Create an Amazon SageMaker AI managed MLflow tracking server by entering the following prompt. Use your account ID where indicated:

Create MLflow tracking server named "player-churn-tracking-server" with artifact store "s3://game-ml-artifacts-ACCOUNT_ID/mlflow/" and size "Medium" and role_arn "arn:aws:iam::ACCOUNT_ID:role/mlops-agent-role"

To complete the AWS CodeConnection setup, select the created connection in the console and choose Update pending connection. Select Install a new app. You will be redirected to GitHub to authenticate and select repository access. Choose Connect and the connection status will change from Pending to Available.

With supporting MLOps infrastructure created, navigate to the MLOpsOrchestrator agent in the AWS console.

Create an Amazon SageMaker AI MLOps project by using the following prompt:

Create an MLOps project named "mlops-player-churn" with GitHub username "your Github username", build repository "player-churn-model-build", deploy repository "player-churn-model-deploy", using connection ARN "your connection arn"

The MLOps project creates and executes an Amazon SageMaker AI pipeline. Copy the model package group name from the prompt response. The pipeline execution adds a model to the Amazon SageMaker AI Model Registry in Pending manual approval Approve the model by using the following prompt:

Approve model in model package group "your model package group name"

Build a CI/CD pipeline for project "mlops-player-churn" with model build repository "gitUserName/player-churn-model-build", deploy repository "gitUserName/player-churn-model-deploy", connection ARN "your connection arn", feature group "player-churn-features", S3 bucket "game-ml-artifacts-ACCOUNT_ID", MLflow server "your-mlflow-arn", and pipeline "player-churn-training-pipeline"

To verify and visualize the pipeline, in the console, navigate to AWS CodePipeline.

There will be two pipelines, one for build and another for deploy.

Select the link of the build project to view pipeline steps.

To deploy a production inference endpoint, navigate to the AWS CodePipelines deploy pipeline.

To trigger CI/CD pipeline execution, push any changed code to the model-build repository.

Using an Amazon Bedrock Agent, a complete MLOps model build, and deployment of a CI/CD pipeline, has been created. Try out additional agent prompts to experiment with the flexibility and function of the agent. Amazon SageMaker Canvas can be used to connect to data sources (such as transactional databases, data warehouses, Amazon Simple Storage Service (Amazon S3)) or over 50 other data providers. SageMaker Canvas can be used to feature engineer data and as a data source for the MLOps model build and deploy pipeline. Cleanup To avoid ongoing charges, navigate to these AWS services in the console and terminate those resources. A command line automated cleanup script is available to delete resources as well. The script uses resource tags to safely identify and remove all MLOps deployed resources. The script automatically removes all MLOps resources tagged with CreatedBy=MLOpsAgent. Run cleanup-by-tags.sh to terminate resources. Conclusion Building an intelligent MLOps CI/CD pipeline management system using Amazon Bedrock Agents represents an advancement in how gaming teams can approach machine learning operations. Throughout this walkthrough, we've demonstrated how to transform complex, multi-service MLOps workflows into streamlined, conversational interactions that reduce the barrier to entry for gaming analytics. Contact an AWS Representative to find out how we can help accelerate your business. Further reading

Amazon SageMaker AI workflows Amazon SageMaker Pipelines Amazon SageMaker for MLOps MLOps Amazon SageMaker AI notebook samples Operationalize Machine Learning with Amazon SageMaker MLOps and MLFlow workshop

Steve Phillips Steve Phillips is a senior technical account manager at AWS in the North America region. Steve has worked with games customers for eight years and currently focuses on data warehouse architectural design, data lakes, data ingestion pipelines, and cloud distributed architectures.

Scott Selinger Scott Selinger is a Senior Solutions Architect in North America who works with Games customers. With his wealth of experience in the GameTech industry, he skillfully guides customers in leveraging AWS services to unlock the full potential of their endeavors.

Scott Flaster Scott Flaster is a Senior Technical Account Manager who works with Games customers. He is passionate about building large-scale distributed applications to solve business problems using his knowledge in AI/ML, Security, and Infrastucture.

Migrating Amazon DocumentDB Cluster across Regions: A step by step guide

As businesses expand globally and AWS continues to grow its regional footprint, some organizations need their database infrastructure to span multiple Regions. Organizations span Amazon DocumentDB (with MongoDB compatibility) clusters across AWS Regions to reduce latency by serving customers from the nearest AWS Regions, or to meet regulatory compliance requirements. You can also use Amazon DocumentDB Global Clusters to do cross-region migration with minimal downtime. On July 14, 2025, Amazon DocumentDB expanded Global Clusters support from 5 to 10 secondary Regions. This enhancement allows organizations to deploy their document compatible databases across more regions worldwide, improving performance for globally distributed applications and providing greater flexibility for compliance and Disaster Recovery (DR) strategies. Amazon DocumentDB is a serverless, fully managed, MongoDB API-compatible document database service that makes it easy and cost effective to operate critical document workloads at virtually any scale without managing infrastructure. Amazon DocumentDB serves tens of thousands of customers globally across all industries. You can enhance your applications with GenAI and machine learning (ML) capabilities using vector search for Amazon DocumentDB and integration with Amazon SageMaker Canvas. Solution overview In this post, you will learn to migrate a regional Amazon DocumentDB cluster from one AWS Region to another using the Global Cluster feature with low downtime and without performance impact. This solution will incur Global Cluster costs for replicated write I/O, compute instances, and storage across regions based on your configuration. You can use the AWS Pricing Calculator to estimate the cost based on your configuration. Below are the high-level steps to perform this migration:

Convert existing regional cluster into a global cluster by adding a Region Perform the switchover and update the application configuration Remove the global cluster setup

Prerequisites Let's consider an example scenario where we demonstrate how to migrate a regional Amazon DocumentDB cluster from ap-southeast-1 (Singapore) Region to ap-south-1 (Mumbai) using Global cluster setup through the AWS console

Amazon DocumentDB 5.0 instance-based cluster in the source Region ap-southeast-1 . You can use an existing cluster or create a new one. VPC/subnet setup in the target Region ap-south-1 Take a manual snapshot of cluster on source ap-southeast-1 Region Application is deployed in target Region ap-south-1

Convert the regional cluster into a Global Cluster We will create a global cluster by converting the source regional cluster docdb-sgp in ap-southeast-1 into a global cluster by adding ap-south-1 as secondary Region.

From the Amazon DocumentDB console in ap-southeast-1, select the docdb-sgp cluster and from the Actions menu, select Add Region. Choose the Secondary region as ap-south-1 and provide the global cluster identifier and secondary cluster identifier. For example, Global cluster identifier: docdb-migration-demo and Secondary cluster identifier: docdb-mum Choose the DB instance class and Number of replica instances for your secondary cluster. We recommend selecting the same number of instances and instance class for your secondary cluster to ensure you have the same capacity available for your application in the new Region. Enable Show advanced settings to select the network settings for your secondary Region cluster, including VPC details and subnet group. Ensure other settings match your primary cluster, such as encryption at rest settings. Note: You cannot change VPC, subnet groups and encryption settings after the cluster is created. Select secondary cluster port number and cluster parameter group. Make sure your cluster parameter group have similar settings as source Region cluster. Rest of the settings can be left to default and choose create cluster button

Wait for secondary Region to become active The secondary Region will need some time to become available. Once ready, the docdb-migration-demo will have its primary cluster in the ap-southeast-1 Region and a secondary cluster in the ap-south-1 Region.

Data Replication happens from primary Region cluster ap-south-east-1 to secondary Region cluster in ap-south-1. It is important to confirm the replication lag by monitoring the GlobalClusterReplicationLag CloudWatch metric on secondary cluster doc-mum. This metric should be minimum (in milliseconds)

Navigate Amazon DocumentDB console on the secondary ap-south-1 Region. On the doc-mum (secondary region cluster), select Monitoring tab to find GlobalClusterReplicationLag and choose view in metrics

Note: GlobalClusterReplicationLag metric is available on secondary clusters only Perform the switchover Once replication is synced (replica lag in millisecond range), we are ready to promote the secondary cluster in ap-south-1 to primary. As both the Regions are healthy, we will perform a Switchover to complete our migration.

Stop application writes to the source Region cluster. Using AWS DocumentDB console in any Region, select the global cluster docdb-migration-demo and from the Actions menu choose Switchover or Failover. Select the switchover and new primary Region cluster ap-south-1 Region cluster and hit the Confirm button. The switchover process takes few minutes to complete to prevent data loss. During this period, the application cannot perform any operation on the database (downtime).

Wait for status to become available for primary and secondary clusters. After switchover step completion, cluster is ap-south-1 Region becomes the new primary and the cluster in ap-southeast-1 Region becomes secondary.

Locate connection string for the ap-south-1 cluster docdb-mum, for more information see Finding a cluster's endpoints. Update the application connection configuration to use the docdb-mum cluster endpoint and start the application in ap-south-1 Region. Don't use global cluster endpoint. Make sure application is working fine and all the checkouts are good. Now you have successfully migrated to ap-south-1 Region. The secondary Region cluster in ap-southeast-1 is now unused. But, before deleting the secondary Region, we recommend keeping this setup running for a few days in case you need to switch back to the old region ap-southeast-1

To roll back to the previous primary Region ap-southeast-1, we can initiate a switchover from ap-south-1 to ap-southeast-1 and update the application endpoint, provided the ap-southeast-1 cluster or the global cluster configuration exists. Remove Global Cluster Setup We can remove the global cluster setup using the following steps

From the Amazon DocumentDB Console in ap-southeast-1 Region. Select the current secondary Region cluster docdb-sgp and from the Actions choose Remove from global and select Remove and Promote. This step removes docdb-sgp cluster from Global cluster setup and promotes it as standalone cluster.

Once docdb-sgp is removed from the Global cluster, only the docdb-mum cluster will remain in the global cluster with the role of primary cluster.

Remove the primary cluster in docdb-mum from Global cluster setup. Choose the current primary cluster docdb-mum and from Actions – Select Remove and Promote

Delete the Global Cluster docdb-migration-demo, which has 0 Regions Navigate to the ap-southeast-1 Region and delete the old cluster docdb-sgp

Note: You may need to disable the Deletion protection if it is enabled. Conclusion You can use Amazon DocumentDB Global Clusters to migrate your database between AWS Regions with minimum downtime and operational complexity. Whether you're moving for performance, compliance, or disaster recovery, this architecture supports scalable, cross-regional Amazon DocumentDB deployments with confidence. For more information about recent launches and blog posts, see Amazon DocumentDB resources.

Anshu Vajpayee Anshu is a Senior Amazon DocumentDB Specialist Solutions Architect at AWS, helping customers adopt NoSQL databases and modernize applications using Amazon DocumentDB. Before joining AWS, he worked extensively with relational and NoSQL databases.

Kaarthiik Thota Kaarthiik is a Senior NoSQL Specialist Solutions Architect at AWS. He is passionate about database technologies and enjoys helping customers solve problems and modernize applications using NoSQL databases. Before joining AWS, he worked extensively with Relational databases, NoSQL databases, and Analytics platforms for over 15 years.

This blog walks you through how to use the WorkSpaces applications Simple Pricing Tool for Always-On and On-Demand fleets with visualization of usage patterns, common pricing examples, and education licensing. To learn about Elastic fleets, read AWS announces Availability of Elastic fleets with Amazon WorkSpaces applications and refer to the WorkSpaces applications pricing page. You can also use the AWS Pricing Calculator with WorkSpaces applications to create an estimation of costs as part of a broader AWS service deployment. Amazon WorkSpaces applications is a fully managed application streaming service that enables you to centrally manage your desktop applications and securely deliver them to any computer. With WorkSpaces applications, you can easily scale to any number of users across the globe without acquiring, provisioning, and operating hardware or infrastructure. Software vendors are using WorkSpaces applications to stream GPU-intensive 3D design and engineering applications to users, while enterprises are replacing their application streaming technologies with WorkSpaces applications. Customers really like the WorkSpaces applications pricing model of paying only for the instances they provisioned to meet their users' streaming needs, but it is different than their existing application streaming environments that are based on provisioning for peak capacity, even during nights, weekends, and holidays when load would be lower. To help customers estimate their WorkSpaces applications price, we developed a simple, easy to use pricing tool. Simply provide your total number of users, actual concurrent usage per hour, instance type, and fleet utilization, and the pricing tool will estimate your per user price. It will also show you the estimated price savings when you use an On-Demand fleet instead of an Always-On fleet. Both fleet types use scaling policies to ensure there are enough streaming instances available when your users want to stream, while letting you (the admin) choose between application launch time, and cost. When your user launches their application on an Always-On streaming instance, the application starts loading almost immediately, while an On-Demand streaming instance has a brief wait (up to 90 seconds) while the streaming instance is powered on and made available. With Always-On streaming instances, you pay the hourly fee for the instance even when no user is streaming. With On-Demand streaming instances, you pay a lower stopped instance fee when the user isn't streaming, and the hourly fee when they are. The Amazon WorkSpaces applications Simple Pricing Tool provides only an estimate of your AWS fees related to your usage of WorkSpaces applications and doesn't include any taxes that might apply. Your actual fees depend on a variety of factors, including your actual usage of AWS services. Prerequisites

Download the WorkSpaces applications Pricing Tool: Microsoft Excel File Collect your actual or estimated concurrency usage by hour

How to use the WorkSpaces applications Pricing Tool The WorkSpaces applications Pricing Tool is provided as a Microsoft Excel or OpenOffice Calc spreadsheet that enables you to enter in basic information about your usage, then provides a cost estimate for the WorkSpaces applications environment. The spreadsheet has a Price Estimator worksheet, and a Usage Pattern worksheet. Within the spreadsheet, the cells that require your input are denoted by a light blue background. The cells with a gray background are informational or aggregations, while the cells with green backgrounds are calculated cost estimates. To get started, open the WorkSpaces applications Pricing Tool using Microsoft Office or Open Office. On the Price Estimator worksheet enter the following information:

Cell B8: The total number of unique users that will stream in a given month. You will fill out expected concurrency per hour later. Cell B9: Maximum number of user sessions to be provisioned per instance. This must be set > 1 if you are using a multi-session fleet. Cell B10: The Operating System to be used Cell B11: The AWS Region that will be used Cell B12: The instance type and size that will be used by your users Cell B13: The RDS SAL type – you can select between Commercial License Included, Bring Your Own License (BYOL) if you have RDS CALs that can be ported, or Academic License Included for qualified schools, universities, and public institutions. RDS SAL types are only applicable for Microsoft Windows operating system. For Amazon Linux 2, there is no RDS SAL type and you must set it to "Not Applicable". Cell B14: Enter in a percent of concurrency that you want to maintain as buffer capacity to ensure new users are able to start streaming without waiting for new instances to be provisioned. Note: Each cell has text that appears when you mouse over it providing additional details and instructions.

Once you have entered all of the information on the Price Estimator worksheet, switch to the Usage Pattern worksheet. On the Usage Pattern worksheet, enter the following:

Cell C2: Enter the number of weeks per month. A default of 4 weeks per month has been entered. Cell C5: Enter in the number of days per work week. A default of 5 days per work week has been entered. Cell C7-C30: Fill in your actual or estimated concurrency per hour during a week day. If you expect no usage in specific hours, enter 0. Column D will be automatically calculated based on the buffer capacity entered on the Price Estimator worksheet. Cell C35: Enter in the number of days per weekend. A default of 2 days per weekend has been entered. Cell C37-C61: Fill in your actual or estimated concurrency per hour during a weekend day. If you expect no usage in specific hours, enter 0. Column D will be automatically calculated based on the buffer capacity entered on the Price Estimator worksheet.

The WorkSpaces applications Pricing Tool will automatically calculate the number of instances and buffer instances based on what was entered in column C, Max number of sessions per instance, and the buffer capacity percent specified on the Price Estimator worksheet.

After you have filled in your per-hour usage patterns on the Usage Pattern worksheet, switch back to the Price Estimator worksheet. The WorkSpaces applications Pricing Tool calculates the effective monthly used and buffer hours, total monthly cost estimate, annualized cost estimate, and effective monthly cost per user estimate. The WorkSpaces applications Pricing Tool also calculates the cost savings by using On-Demand fleet type versus the Always-On fleet type, allowing you to choose between application launch speed and cost to deliver applications.

You can use the effectively monthly hours per user calculation to spot check whether the values entered in the usage pattern match your expectations of user usage. If they don't, switch to the Usage Pattern worksheet, and adjust the per-hour usage pattern. Now let's walk through some examples of using the WorkSpaces applications Pricing Tool. Example scenario streaming line of business applications Suppose you are an enterprise that uses an application streaming technology to stream the SAP GUI to your users, or you're a financial ISV that wants to stream your accounting software to your customers as a service and need to estimate the price of using WorkSpaces applications. Your infrastructure is built in the Oregon (us-west-2) AWS Region, and you have 2,000 total users that will stream during the month. The application runs perfectly using the stream.standard.medium instance type and size. Your usage is fairly predictable during the weekday, so you maintain a 20% buffer, with sporadic usage over the weekend. Your enterprise has Microsoft License Mobility, enabling you to port your RDS CALs to AWS for use with WorkSpaces applications. For users covered by your RDS CALs, you won't incur monthly user fees. Your inputs on the Price Estimator worksheet will look like this:

Based on this usage pattern, and the environment details, the WorkSpaces applications Pricing Tool estimates that the effective monthly cost per user would be $7.49 for Always-On, or $6.56 for On-Demand. You can find that at the bottom of the Price Estimator worksheet:

Example scenario streaming graphics apps Now suppose you are an industrial engineering company that wants to stream GPU-intensive applications like SOLIDWORKS or Siemens NX to your design engineers who are located in France, Germany, and Italy. Currently you are deploying engineering workstations, but your engineers want portability in how and where they work by using ultrabooks. You want to estimate the cost of using WorkSpaces applications GPU-backed instances to augment providing your design engineers with ultrabooks. Your product lifecycle management and product data management infrastructure are in the Frankfurt (eu-central-1) AWS Region, and you have 3,000 design engineers who will stream sporadically throughout the day. You are going to use the stream.graphics.g4dn.2xlarge for your design engineers to ensure they have sufficient CPU, memory, and GPU for them. You don't have existing RDS CALs to port, so you will be buying them from AWS. You're not too sure on your usage pattern, and so you want to maintain extra buffer capacity of 40% during the onboarding phase, then adjust after usage is identified. On weekends, you don't expect any usage, but want to maintain some capacity for the design engineers who may need it. On the Price Estimator worksheet, you would enter:

Based on this usage pattern and the environment details, the WorkSpaces applications Pricing Tool estimates that the effective monthly cost per user per month would be $109.63 for an Always-On fleet, or $80.06 for an On-Demand fleet. In this scenario, you can save 27.0% with your users waiting up to 90 seconds for their application to launch with an On-Demand fleet. You can find this at the bottom of the Price Estimator worksheet:

Example scenario streaming graphics apps using multi-session fleets Now let us revisit the previous scenario, you realized that when your users are streaming their graphics applications using WorkSpaces applications stream.graphics.g4dn.2xlarge instances the underlying resource utilization is not high and you would like to squeeze in more users on the same instance. With multi-session fleets you can achieve that. Let us assume we provision 3 user sessions on a single instance and take a more power instance stream.graphics.g4dn.4xlarge. On the Price Estimator worksheet, you would enter:

And for the usage pattern, you expect (Observe number of instances required are less than the number of users):

Based on this usage pattern and the multiple user session on a single instance, the WorkSpaces applications Pricing Tool estimates that the effective monthly cost per user per month would be $67.95 for an Always-On fleet, or $50.42 for an On-Demand fleet. In this scenario, using multi-fleet with same number of users and usage pattern you can save ~39% of your effective monthly cost/user.

Other costs to consider The WorkSpaces applications Pricing Tool helps you with estimating your streaming instances costs. Other costs that are common to WorkSpaces applications environments are storage. For example, you will pay Amazon S3 storage fees if you enable WorkSpaces applications's Home Folder or Application Settings Persistence features for your users. Or, if you create an Amazon FSx file share for your users to save their data to. Conclusion WorkSpaces applications enables you to migrate away from your designed-for-peak-capacity application streaming environment to a dynamic, scalable, managed service where you only pay for what you use, when you use it. And with the WorkSpaces applications Pricing Tool, you can quickly estimate the cost per user to compare against your alternative options.

TAGS: Amazon End User Computing, Amazon WorkSpaces, Cost Optimization, desktop and application streaming

Recently, Equinix introduced the Equinix Fabric Cloud Router (FCR), a virtual routing service that enhances multicloud networking while improving performance, security and cost-efficiency. Equinix FCR with AWS Direct Connect is available in AWS Marketplace, allowing organizations to establish private, dedicated connections across their cloud infrastructure. By procuring Equinix FCR in AWS Marketplace, organizations benefit from streamlined procurement processes, consolidated billing with their existing Amazon Web Services (AWS) account, and seamless integration that reduces deployment complexity while enabling faster time-to-value for multicloud connectivity solutions.

According to a TechTarget Enterprise Strategy Group survey, 94% of organizations work with multiple cloud providers—some choosing different providers for specific capabilities, while others make these choices based on merger and acquisition considerations. For organizations building their workloads across different cloud environments, secure, reliable connectivity is a major benefit. Equinix FCR, working with Direct Connect, is designed to provide low-latency connectivity between AWS and other cloud infrastructure at various speeds in a reliable manner. This post explains how FCR enables cloud connectivity, examines key benefits and use cases, and helps you to get started with deploying this solution. Key benefits: Rapid multicloud connectivity at scale FCR using Direct Connect in AWS Marketplace provides:

Third-party quick network deployment – Deploy private connectivity between clouds in under an hour without colocation, hardware, or licensing Proximity and connectivity – Access enterprise-grade service-level agreements (SLAs) and support for interconnections Security and reliability – Direct routing over a private network with built-in resiliency Flexible multicloud connectivity – Scale from 200 Mbps to 25 Gbps with pay-as-you-go options Unlimited connections – Create unlimited connections per cloud router

FCR provides Layer 3 connectivity at configurable speeds when merging systems across different clouds. This helps in scenarios involving mergers and acquisitions. Using Direct Connect, FCR deploys without hardware management overhead. For applications that need to access data in different clouds, FCR provides consistent performance and low-latency private connectivity across cloud providers. The service reduces data transfer costs while simplifying infrastructure management. When working with artificial intelligence and machine learning (AI/ML) models across clouds, FCR enables high-performance data transfer through Equinix's private network. This helps optimize costs while maintaining training schedules. For petabyte-scale migrations between clouds, Equinix FCR provides secure private connectivity through AWS Marketplace deployment. The service works with AWS DataSync to reduce transfer costs through compression and optimized data movement, benefiting from the lower data egress charges offered by Direct Connect. Prerequisites To set up and provision a new Fabric Cloud Router, you need to have the following prerequisites:

Solution walkthrough: Enhancing multicloud networking connectivity with AWS Direct Connect and Equinix This section walks through the steps to sign up for an AWS account, an Equinix free trial in AWS Marketplace, and to set up and provision a new Fabric Cloud Router through the Equinix Customer Portal. You can sign up for a public AWS Marketplace Free Trial, which allows you to try out a Fabric Cloud Router at the Standard service tier level at no cost during the 7-day trial period. You can provision up to 2 x 200 Mbps Fabric Virtual Circuits (VCs).

Sign in to AWS Marketplace. To create an AWS account, visit Start building on AWS today and choose Create an AWS Account. Enter Equinix in the AWS Marketplace search field or go to Equinix Fabric Cloud Router with AWS Direct Connect. In the list of search results, choose Fabric Cloud Router (Free Trial). Choose Try for free as shown in the following figure.

In the Offer type window, choose Subscribe to free trial as shown in the following figure.

After subscribing, complete the following to set up your account:

In the success window, choose Set up your account. In the registration form, enter your company, name, email address, and other information. Choose Complete Registration. You will receive two emails within 10–15 minutes after your registration is complete that will include

A welcome email with your subscription key. An email with the link for registering your account.

Check your junk or spam folder if you don't receive the emails after 15 minutes. In the second email, choose the link to Verify Email Address. Set your password and choose Complete Registration to set up your Customer Portal account. Using your subscription key from the first email, create and provision a Fabric Cloud Router or new VCs through the Customer Portal.

Creating a Fabric Cloud Router To create a new Cloud Router, follow the instructions at Creating a Fabric Cloud Router in the Equinix Fabric Cloud Router documentation. After you set up a Fabric Cloud Router, you can view and manage its connections as described in the Fabric Cloud Router documentation. To create a Fabric Cloud Router connection, use the subscription keys from your welcome email. Creating a Fabric Cloud Router connection using a subscription key The following steps show you how to establish connectivity between Direct Connect and your Fabric Cloud Router using a subscription key. Using a cloud router connection, you can connect to AWS services through a private network connection.

Sign in to the Equinix Customer Portal and navigate to Fabric. On the Connections tab, choose Create Connection, as shown in the following figure.

3. On the A Service Provider card, choose Connect to a Service Provider and search for Amazon Web Services.

4. On the AWS Amazon Web Services card, choose Select Service, then choose Create Connection. 5. In Enter Your AWS Information, choose a connection type as shown in the following figure:

Redundant – Create a redundant pair of connections. If you're using the free trial, don't set up redundant connections because this will use both Virtual Circuits you have access to on the connection to one cloud. For production networks, redundant links are recommended. Primary – Create a single, primary connection. Secondary – Create a single, secondary connection.

Figure 5: Enter your AWS information and choose a Connection Type

6. Enter your AWS Account ID and choose Next. 7. To retrieve your AWS account identifier, sign in to the AWS Management Console and choose your username in the top right corner. When you copy your account ID into the field, make sure to remove the hyphens, as shown in the following figure.

From the AWS Destination dropdown list, choose your metro. Make sure to use the same destination for both Connections selected and choose Next.

In Configure Your Connection on the Origin Asset Type drop-down list, choose Cloud Router. Choose +Select Cloud Router. In the form that displays on the right, choose a cloud router instance and choose Select Cloud Router. In the Primary Connection section

Enter the primary Connection Name. The name must contain 3–24 characters. Choose a Bandwidth from the dropdown list. The bandwidth value that you choose also applies to the secondary connection.

In the Secondary Connection section, enter your secondary Connection Name, and choose Next. In Review Order and Additional Information, review your connections. To view a summary of your order details, choose Show Order Summary in the lower right. To download a PDF summary, choose Download Order Summary as shown in Figure 7.

After completing these steps, the configuration establishes a Fabric Cloud Router instance with Direct Connect connectivity. The configuration provides private, dedicated connections that support flexible bandwidth options based on the selected tier. Monitor and manage your connections through the Equinix Customer Portal. Cleanup

If you don't plan to go ahead with the free trial for your AWS Marketplace subscription, please go to the Manage subscriptions page, choose Manage next to the software subscription and cancel the subscription. You can learn more about cancelling a free trial here. Similarly, on the Equinix console, please ensure you cleanup and delete any resources that were created as part of this getting started guide if you don't plan to use them.

Conclusion Equinix Fabric Cloud Router, a third-party solution available in AWS Marketplace with AWS Direct Connect, provides a streamlined approach to multicloud networking. In AWS Marketplace, organizations can establish secure, private connections between cloud environments without managing physical infrastructure. The service supports various use cases—from post-merger system integration to large-scale data migration—while helping optimize performance and costs. To explore how FCR can support your cloud connectivity requirements, try the 7-day trial in AWS Marketplace. Learn more about Equinix FCR with Direct Connect. Connect FCR to AWS.

Dhanil Parwani Dhanil is a Senior Partner Solutions Architect at AWS. He works closely with networking, security, and AI partners to build solutions and capabilities to enable and simplify their migrations and operations in the cloud. He holds an MS in telecommunications from the University of Colorado Boulder and has a passion for computer networking. Outside of work, Dhanil is an avid traveler and enjoys cheering Liverpool, FC.

Phil Griston Phil Griston is the AWS Alliance Director at Equinix. Prior to this, Phil has held partnership leadership roles at both AWS and Oracle Cloud.

Gene McColm Gene McColm is the Senior Principal Solutions Architect for the AWS Alliance at Equinix. For more than 10 years, he has worked with AWS, Equinix, and customers to ensure optimal design and deployment of hybrid cloud infrastructures

Transform your MCP architecture: Unite MCP servers through AgentCore Gateway

As AI agents are adopted at scale, developer teams can create dozens to hundreds of specialized Model Context Protocol (MCP) servers, tailored for specific agent use case and domain, organization functions or teams. Organizations also need to integrate their own existing MCP servers or open source MCP servers for their AI workflows. There is a need for a way to efficiently combine these existing MCP servers–whether custom-built, publicly available, or open source–into a unified interface that AI agents can readily consume and teams can seamlessly share across the organization. Earlier this year, we introduced Amazon Bedrock AgentCore Gateway, a fully managed service that serves as a centralized MCP tool server, providing a unified interface where agents can discover, access, and invoke tools. Today, we're extending support for existing MCP servers as a new target type in AgentCore Gateway. With this capability, you can group multiple task-specific MCP servers aligned to agent goals behind a single, manageable MCP gateway interface. This reduces the operational complexity of maintaining separate gateways, while providing the same centralized tool and authentication management that existed for REST APIs and AWS Lambda functions. Without a centralized approach, customers face significant challenges: discovering and sharing tools across organizations becomes fragmented, managing authentication across multiple MCP servers grows increasingly complex, and maintaining separate gateway instances for each server quickly becomes unmanageable. Amazon Bedrock AgentCore Gateway helps solves these challenges by treating existing MCP servers as native targets, giving customers a single point of control for routing, authentication, and tool management—making it as simple to integrate MCP servers as it is to add other targets to the gateway. Breaking down MCP silos: Why enterprise teams need a unified Gateway Let's explore this through a real-world example of an e-commerce ordering system, where different teams maintain specialized MCP servers for their specific domains. Consider an enterprise e-commerce system where different teams have developed specialized MCP servers:

The Shopping Cart team maintains an MCP server with cart management tools The Product Catalog team runs their MCP server for product browsing and search The Promotions team operates an MCP server handling promotional logic

Previously, an ordering agent would need to interact with each of these MCP servers separately, managing multiple connections and authentication contexts. With the new MCP server target support in AgentCore Gateway, these specialized servers can now be unified under a single gateway while maintaining their team-specific ownership and access controls. The power of this approach lies in its organizational flexibility. Teams can group their MCP servers based on multiple logical criteria:

Business unit alignment: Organize the MCP servers by business unit Product feature boundaries: Each product team owns their MCP server with domain-specific tools allowing them to maintain clear ownership while providing a unified interface for their agents Security and access control: Different MCP servers require different authentication mechanisms. The gateway handles the authentication complexity, making it simple for authorized agents to access the tools they need

The following diagram illustrates how an ordering agent interacts with multiple MCP servers through AgentCore Gateway. The agent connects to the gateway and discovers the available tools. Each team maintains control over their domain-specific tools while contributing to a cohesive agent experience. The gateway handles tool naming collisions, authentication, and provides unified semantic search across the tools.

The AgentCore Gateway serves as an integration hub in modern agentic architectures, offering a unified interface for connecting diverse agent implementations with a wide array of tool providers. The architecture, as illustrated in the diagram, demonstrates how the gateway bridges the gap between agent and tool implementation approaches, now enhanced with the ability to directly integrate MCP server targets. AgentCore Gateway integration architecture In AgentCore Gateway, a target defines the APIs, Lambda functions, or other MCP servers that a gateway will provide as tools to an agent. Targets can be Lambda functions, OpenAPI specifications, Smithy models, MCP servers, or other tool definitions. The target integration side of the architecture showcases the gateway's versatility in tool integration. With the new MCP server target support, the gateway can directly incorporate tools from public MCP servers, treating them as first-class citizens alongside other target types. This capability extends to federation scenarios where one AgentCore Gateway instance can serve as a target for another, for hierarchical tool organization across organizational boundaries. The gateway can seamlessly integrate with AgentCore Runtime instances that expose agents as tools, private MCP servers maintained by customers, traditional AWS Lambda functions, and both Smithy and AWS service APIs. Beyond target diversity, the gateway's authentication architecture provides additional operational benefits. The gateway decouples its inbound authentication from target systems, letting agents access tools that use multiple identity providers through a single interface. This centralized approach simplifies development, deployment, and maintenance of AI agents. Now, the same approach can be used for MCP server targets, where the gateway manages the complexity of interfacing with the server using the configured identity provider for the target. With this authentication foundation you get sophisticated tool management capabilities through a unified architecture. When an agent requests tool discovery, the gateway provides a consistent view across the integrated targets, with tools from MCP servers appearing alongside Lambda functions and traditional APIs. The semantic search capability operates uniformly across the tool types, so agents can discover relevant tools regardless of their implementation. During tool invocation, the gateway handles the necessary protocol translations, authentication flows, and data transformations, presenting a clean, consistent interface to agents while managing the complexity of different target systems behind the scenes. The addition of MCP server target support represents a significant evolution in the gateway's capabilities. Organizations can now directly integrate MCP-native tools while maintaining their investments in traditional APIs and Lambda functions. This flexibility allows for gradual migration strategies where teams can adopt MCP-native implementations at their own pace while facilitating continuous operation of existing integrations. The gateway's synchronization mechanisms make sure that tool definitions remain current across the different target types, while its authentication and authorization systems provide consistent security controls regardless of the underlying tool implementation. The gateway combines MCP servers, traditional APIs, and serverless functions into a coherent tool environment. This capability, along with enterprise-grade security and performance, makes it a beneficial infrastructure for agentic computing.

Solution Walkthrough In this post, we'll guide you through the steps to set up an MCP server target in AgentCore Gateway, which is as simple as adding a new MCP server type target to a new or existing MCP Gateway. Adding an MCP server to an AgentCore Gateway will allow you to centralize your tool management, security authentication, and operational best practices with managing MCP servers at scale.

Get started with adding MCP Server into AgentCore Gateway To get started, you will create an AgentCore Gateway and add your MCP Server as a target. Prerequisites Verify you have the following prerequisites:

AWS account with Amazon Bedrock AgentCore access. For more information review Permissions for AgentCore Runtime documentation. Python 3.12 or later Basic understanding of OAuth 2.0

You can create gateways and add targets through multiple interfaces:

AWS SDK for Python (Boto3) AWS Management Console AWS Command Line Interface (AWS CLI) AgentCore starter toolkit for fast and straightforward setup

The following practical examples and code snippets demonstrate how to set up and use Amazon Bedrock AgentCore Gateway. For an interactive walkthrough, you can use these Jupyter Notebook samples on GitHub. Create a gateway To create a gateway, you can use the AgentCore starter toolkit to create a default authorization configuration with Amazon Cognito for JWT-based inbound authorization. You can also use another OAuth 2.0-compliant authentication provider instead of Cognito.

# Create an authorization configuration, that specifies what client is authorized to access this Gateway auth_config = { "customJWTAuthorizer": { "allowedClients": [' '], # Client MUST match with the ClientId configured in Cognito. "discoveryUrl": ' ', } }

# Call the create_gateway API # This operation is asynchronous so may take time for Gateway creation # This Gateway will leverage a CUSTOM_JWT authorizer, the Cognito User Pool we reference in auth_config def deploy_gateway(poll_interval=5): create_response = gateway_client.create_gateway( name="DemoGateway", roleArn=" ", # The IAM Role must have permissions to create/list/get/delete Gateway protocolType="MCP", authorizerType="CUSTOM_JWT", authorizerConfiguration=auth_config, description="AgentCore Gateway with MCP Server Target", ) gatewayID = create_response["gatewayId"] gatewayURL = create_response["gatewayUrl"]

# Wait for deployment while True: status_response = gateway_client.get_gateway(gatewayIdentifier=gatewayID) status = status_response["status"] if status == "READY": print("✅ AgentCore Gateway is READY!") break elif status in ["FAILED"]: print(f"❌ Deployment failed: {status}") return None print(f"Status: {status} - waiting...") time.sleep(poll_interval)

# Values with needs to be replaced with real valuesPython

Create a sample MCP Server As an example, let's create a sample MCP server with three simple tools that return static responses. The server uses FastMCP with stateless_http=True which is required for AgentCore Runtime compatibility.

@mcp.tool() def getOrder() -> int: """Get an order""" return 123

@mcp.tool() def updateOrder(orderId: int) -> int: """Update existing order""" return 456

@mcp.tool() def cancelOrder(orderId: int) -> int: """cancel existing order""" return 789

Configure AgentCore Runtime deployment Next, we will use the starter toolkit to configure the AgentCore Runtime deployment. The toolkit can create the Amazon ECR repository on launch and generate a Dockerfile for deployment on AgentCore Runtime. You can use your own existing MCP server, we're using the following only as an example. In a real-world environment, the inbound authorization for your MCP server will likely differ from the gateway configuration. Refer to this GitHub code example to create an Amazon Cognito user pool for Runtime authorization.

boto_session = Session() region = boto_session.region_name print(f"Using AWS region: {region}")

required_files = ['mcp_server.py', 'requirements.txt'] for file in required_files: if not os.path.exists(file): raise FileNotFoundError(f"Required file {file} not found") print("All required files found ✓")

auth_config = { "customJWTAuthorizer": { "allowedClients": [ ' ' # Client MUST match with the ClientId configured in Cognito, and can be separate from the Gateway Cognito provider. ], "discoveryUrl": ' ', } }

print("Configuring AgentCore Runtime...") response = agentcore_runtime.configure( entrypoint="mcp_server.py", auto_create_execution_role=True, auto_create_ecr=True, requirements_file="requirements.txt", region=region, authorizer_configuration=auth_config, protocol="MCP", agent_name="mcp_server_agentcore" ) print("Configuration completed ✓")

Launch MCP server to AgentCore Runtime Now that we have the Dockerfile, let's launch the MCP server to AgentCore Runtime:

print("Launching MCP server to AgentCore Runtime...") print("This may take several minutes...") launch_result = agentcore_runtime.launch() agent_arn = launch_result.agent_arn agent_id = launch_result.agent_id print("Launch completed ✓")

Create MCP server as target for AgentCore Gateway Create an AgentCore Identity Resource Credential Provider for the AgentCore Gateway to use as outbound auth to the MCP server agent in AgentCore Runtime:

cognito_provider = identity_client.create_oauth2_credential_provider( name="gateway-mcp-server-identity", credentialProviderVendor="CustomOauth2", oauth2ProviderConfigInput={ 'customOauth2ProviderConfig': { 'oauthDiscovery': { 'discoveryUrl': ' ', }, 'clientId': ' ', # Client MUST match with the ClientId configured in Cognito for the Runtime authorizer 'clientSecret': ' ' } } ) cognito_provider_arn = cognito_provider['credentialProviderArn'] print(cognito_provider_arn)

gateway_client = boto3.client("bedrock-agentcore-control", region_name=region) create_gateway_target_response = gateway_client.create_gateway_target( name="mcp-server-target", gatewayIdentifier=gatewayID, targetConfiguration={"mcp": {"mcpServer": {"endpoint": mcp_url}}}, credentialProviderConfigurations=[ { "credentialProviderType": "OAUTH", "credentialProvider": { "oauthCredentialProvider": { "providerArn": cognito_provider_arn, "scopes": [" "], } }, }, ], ) # Asynchronously create gateway target gatewayTargetID = create_gateway_target_response["targetId"]

After creating a gateway target, implement a polling mechanism to check for the gateway target status using the get_gateway_target API call:

def poll_for_status(interval=5): # Poll for READY status while True: gateway_target_response = gateway_client.get_gateway_target(gatewayIdentifier=gatewayID, targetId=gatewayTargetID) status = gateway_target_response["status"] if status == 'READY': break elif status in ['FAILED', 'UPDATE_UNSUCCESSFUL', 'SYNCHRONIZE_UNSUCCESSFUL']: raise Exception(f"Gateway target failed with status: {status}") time.sleep(interval)

Test Gateway with Strands Agents framework Let's test the Gateway with the Strands Agents integration to list the tools from MCP server. You can also use other MCP-compatible agents built with different agentic frameworks.

from strands import Agent from mcp.client.streamable_http import streamablehttp_client from strands.tools.mcp.mcp_client import MCPClient

with client: # Call the listTools tools = client.list_tools_sync() # Create an Agent with the model and tools agent = Agent(model=yourmodel,tools=tools) ## you can replace with any model you like # Invoke the agent with the sample prompt. This will only invoke MCP listTools and retrieve the list of tools the LLM has access to. The below does not actually call any tool. agent("Hi , can you list all tools available to you") # Invoke the agent with sample prompt, invoke the tool and display the response agent("Get the Order id")Python

Refreshing tool definitions of your MCP servers in AgentCore Gateway The SynchronizeGatewayTargets API is a new asynchronous operation that enables on-demand synchronization of tools from MCP server targets. MCP servers host tools which agents can discover and invoke. With time, these tools might need to be updated, or new tools may be introduced in an existing MCP server target. You can connect with external MCP servers through the SynchronizeGatewayTargets API that performs protocol handshakes and indexes available tools. This API provides customers with explicit control over when to refresh their tool definitions, particularly useful after making changes to their MCP server's tool configurations. When a target is configured with OAuth authentication, the API first interacts with the AgentCore Identity service to retrieve the necessary credentials from the specified credential provider. These credentials are validated for freshness and availability before communication with the MCP server begins. If the credential retrieval fails or returns expired tokens, the synchronization operation fails immediately with appropriate error details, transitioning the target to a FAILED state. For targets configured without authentication, the API proceeds directly to tool synchronization. The tool processing workflow begins with an initialize call to the MCP server to establish a session. Following successful initialization, the API makes paginated calls to the MCP server's tools/list capability, processing tools in batches of 100 to optimize performance and resource utilization. Each batch of tools undergoes normalization where the API adds target-specific prefixes to help prevent naming collisions with tools from other targets. During processing, tool definitions are normalized to facilitate consistency across different target types, while preserving the essential metadata from the original MCP server definitions.

An Ops Admin initiates the SynchronizeGatewayTargets API, triggering AgentCore Gateway to refresh the configured MCP target. The gateway obtains an OAuth token from AgentCore Identity for secure access to the MCP target. The gateway then initializes a secure session with the MCP server to retrieve version capabilities. Finally, the gateway makes paginated calls to the MCP server tools/list endpoint to retrieve the tool definitions, making sure the gateway maintains a current and accurate list of tools.

The SynchronizeGatewayTargets API addresses a critical challenge in managing MCP targets within AgentCore Gateway: maintaining an accurate representation of available tools while optimizing system performance and resource utilization. Here's why this explicit synchronization approach is valuable: Schema consistency management: Without explicit synchronization, AgentCore Gateway would need to either make real-time calls to MCP servers during ListTools operations (impacting latency and reliability) or risk serving stale tool definitions. The SynchronizeGatewayTargets API provides a controlled mechanism where customers can refresh their tool schemas at strategic times, such as after deploying new tools or updating existing ones in their MCP server. This approach makes sure that tool definitions in the gateway accurately reflect the target MCP server's capabilities without compromising performance.

Performance impact trade-offs: The API implements optimistic locking during synchronization to help prevent concurrent modifications that could lead to inconsistent states. While this means multiple synchronization requests might need to retry if there's contention, this trade-off is acceptable because:

Tool schema changes are typically infrequent operational events rather than regular runtime occurrences The performance cost of synchronization is incurred only when explicitly requested, not during regular tool invocations The cached tool definitions facilitate consistent high performance for ListTools operations between synchronizations

Invoke the synchronize gateway API Use the following example to invoke the synchronize gateway operation:

def search_tools(gateway_url, access_token, query): headers = { "Content-Type": "application/json", "Authorization": f"Bearer {access_token}" }

payload = { "jsonrpc": "2.0", "id": "search-tools-request", "method": "tools/call", "params": { "name": "x_amz_bedrock_agentcore_search", "arguments": { "query": query } } }

# Example usage token_response = utils.get_token(user_pool_id, client_id, client_secret, scopeString, REGION) access_token = token_response['access_token'] results = search_tools(gatewayURL, access_token, "order operations") print(json.dumps(results, indent=2))Python

Implicit synchronization of tools schema During CreateGatewayTarget and UpdateGatewayTarget operations, AgentCore Gateway performs an implicit synchronization that differs from the explicit SynchronizeGatewayTargets API. This implicit synchronization makes sure that MCP targets are created or updated with valid, current tool definitions, aligning with the assurance from AgentCore Gateway that targets in READY state are immediately usable. While this might make create/update operations take longer than with other target types, it helps prevent the complexity and potential issues of having targets without validated tool definitions.

An Ops Admin creates or updates the MCP target using CreateGatewayTarget or UpdateGatewayTarget operations. AgentCore Gateway configures the new or updated MCP target. The gateway asynchronously triggers the synchronization process to update the tool definitions. The gateway obtains an OAuth token from AgentCore Identity for secure access. The gateway then initializes a secure session with the MCP server to retrieve version capabilities. Finally, the gateway makes paginated calls to the MCP server's tools/list endpoint to retrieve the tool definitions, making sure the gateway maintains a current and accurate list of tools.

ListTools behavior for MCP targets The ListTools operation in AgentCore Gateway provides access to tool definitions previously synchronized from MCP targets, following a cache-first approach that prioritizes performance and reliability. Unlike traditional OpenAPI or Lambda targets where tool definitions are statically defined, MCP target tools are discovered and cached through synchronization operations. When a client calls ListTools, the gateway retrieves tool definitions from its persistent storage rather than making real-time calls to the MCP server. These definitions were previously populated either through implicit synchronization during target creation/update or through explicit SynchronizeGatewayTargets API calls. The operation returns a paginated list of normalized tool definitions.

InvokeTool (tools/call) Behavior for MCP Targets The InvokeTool operation for MCP targets handles the actual execution of tools discovered through ListTools, managing real-time communication with the target MCP server. Unlike the cache-based ListTools operation, tools/call requires active communication with the MCP server, introducing specific authentication, session management, and error handling requirements. When a tools/call request arrives, AgentCore Gateway first validates the tool exists in its synchronized definitions. For MCP targets, AgentCore Gateway performs an initial initialize call to establish a session with the MCP server. If the target is configured with OAuth credentials, AgentCore Gateway retrieves fresh credentials from AgentCore Identity before making the initialize call. This makes sure that even if ListTools returned cached tools with expired credentials, the actual invocation uses valid authentication.

The MCP client initializes a request with MCP protocol version to AgentCore Gateway. The client then sends the tools/call request to the gateway. The gateway obtains an OAuth token from AgentCore Identity for secure access. The gateway initializes a secure session with the MCP server to invoke and handle the actual execution of the tool.

Search tool behavior for MCP targets The search capability in AgentCore Gateway enables semantic discovery of tools across the different target types, including MCP targets. For MCP targets, the search functionality operates on normalized tool definitions that were captured and indexed during synchronization operations, providing efficient semantic search without real-time MCP server communication. When tool definitions are synchronized from an MCP target, AgentCore Gateway automatically generates embeddings for each tool's name, description, and parameter descriptions. These embeddings are stored alongside the normalized tool definitions, enabling semantic search that understands the intent and context of search queries. Unlike traditional keyword matching, this allows agents to discover relevant tools even when exact terminology doesn't match.

Search for MCP server tools through the gateway Use the following example to search for tools through the gateway.

# Example usage token_response = utils.get_token(user_pool_id, client_id, client_secret, scopeString, REGION) access_token = token_response['access_token'] results = search_tools(gatewayURL, access_token, "math operations") print(json.dumps(results, indent=2))Python

Conclusion Today's announcement of MCP server support as a target type in Amazon Bedrock AgentCore Gateway is an advancement in enterprise AI agent development. This new capability addresses critical challenges in scaling MCP server implementations while maintaining security and operational efficiency. By integrating existing MCP servers alongside REST APIs and Lambda functions, AgentCore Gateway provides a more unified, secure, and manageable solution for tool integration at scale. Organizations can now manage their tools through a single, centralized interface while benefiting from unified authentication, simplified tool discovery and reduced maintenance overhead. For more detailed information and advanced configurations, refer to the code samples on GitHub, the Amazon Bedrock AgentCore Gateway Developer Guide and Amazon AgentCore Gateway pricing.

Frank Dallezotte is a Senior Solutions Architect at AWS and is passionate about working with independent software vendors to design and build scalable applications on AWS. He has experience creating software, implementing build pipelines, and deploying these solutions in the cloud. Ganesh Thiyagarajan is a Senior Solutions Architect at Amazon Web Services (AWS) with over 20 years of experience in software architecture, IT consulting, and solution delivery. He helps ISVs transform and modernize their applications on AWS. He is also part of the AI/ML Technical field community, helping customers build and scale Gen AI solutions. Dhawal Patel is a Principal Generative AI Tech lead at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to Agentic AI, Deep learning, distributed computing.

Optimizing TV and radio media buying using SmartSpot360 on AWS

This blog post was co-authored by Jhon Valencia (SVP of Technology & Data), Preston Porter (Vice President of Operations), and Claire McCue (Director of Marketing) at BMG360. When you depend on high-volume lead generation, media buying efficiency directly impacts revenue and scalability. However, as campaigns expand across channels, optimizing traditional media (like TV and radio) becomes increasingly complex. You need a reliable ability to adjust placements in real time and predict campaign performance using live data. To address this gap, BMG360 developed SmartSpot360™, an AI solution built on Amazon Web Services (AWS). SmartSpot360 transforms linear media buying by applying real-time analytics and predictive modeling to deliver optimized placements at scale. We will share how you can modernize legacy media operations, improve efficiency, and enhance campaign performance using an AWS-powered machine learning (ML) platform. Challenges and business context Linear TV and radio advertising remain valuable channels for reaching broad audiences, but they've historically relied on manual processes and fixed decision rules. These methods worked but didn't scale easily—particularly when trying to optimize spend in real time or predict performance across shifting audience behaviors and market conditions. You couldn't readily adjust campaigns in real time or effectively apply predictive insights to future placements at speed and at scale. BMG360 identified three core challenges in the traditional model:

Limited scalability of manual buying decisions Lack of real-time adaptability to changing performance data No predictive intelligence to guide future media investments

Manual optimization, based on historical data and planner expertise, often required significant time and introduced inefficiencies. Third-party media buying platforms, which offered a degree of automation, lacked the flexibility to integrate with proprietary datasets or adapt to unique campaign logic deployed by BMG360. Traditional business intelligence tools supported retrospective analysis but provided no mechanisms for real-time optimization or predictive modeling.

To address these constraints, BMG360 saw a clear opportunity: build an AI-driven system capable of learning from past performance, optimizing active campaigns in real time, and dynamically allocating spend across variables (such as daypart, rate, and creative mix). The result was SmartSpot360: a machine learning-powered platform designed to bring scale, speed, and precision to traditional media buying. How BMG360 engaged AWS The SmartSpot360 architecture is designed to be use case–dependent, meaning its configuration and service stack may vary depending on campaign scale, data sources, and optimization objectives. The following overview represents one reference implementation designed for high-volume TV and radio use cases. To support the development and scaling of SmartSpot360, BMG360 worked closely with AWS to evaluate architecture options for data processing, machine learning (ML) model training, and real-time API delivery. The AWS team provided guidance on selecting the right mix of serverless and ML services to balance performance, cost, and scalability. BMG360 was able to quickly scale and find efficiency by engaging with a dedicated AWS team, as well as through a referred AWS Partner, Usage.AI, to verify a stable and long-term solution was implemented from the ground up. As the application's potential was identified, several meetings to address resilience, efficiency, and Amazon SageMaker best practices culminated in a successful launch and expanded use case for the product. The business continues to engage with the AWS team, including through on-site discovery sessions with AWS thought leadership. These sessions confirm both the AWS-powered solutions and overall cloud application strategies of BMG360 remain in line with or ahead of industry best practices. Solution overview To bring scale and intelligence to traditional media, SmartSpot360 optimizes TV and radio placements in real time. Built to process and analyze over 3,500 ad spots each week, SmartSpot360 continuously learns from performance data to refine buying decisions across a range of variables. The system evaluates:

Spot cost, length, and frequency Time-of-day and day-of-week trends Creative variation and effectiveness Seasonal and holiday-driven audience behavior

Using advanced predictive modeling, SmartSpot360 dynamically adjusts placements based on historical outcomes and live campaign performance. Instead of relying on fixed schedules or manual rule sets, the system allocates spend where it's most likely to generate value. It automates what previously required extensive manual input and post-campaign analysis. This approach improves operational efficiency, while increasing the likelihood of higher-quality leads and stronger conversion outcomes. By shifting optimization from reactive to proactive, SmartSpot360 enables data-informed decision-making at scale across thousands of placements and changing market conditions.

The SmartSpot360 architecture is powered by AWS services that support high availability, scalability, and real-time performance:

Amazon SageMaker is used to train, validate, and deploy machine learning models that predict spot performance and optimize allocation logic. AWS Glue handles extract, transform, and load (ETL) operations, ingesting performance data from multiple sources and preparing it for modeling. AWS Lambda executes real-time logic for budget recommendations, creative rotation, and time-slot selection without managing infrastructure. Amazon API Gateway facilitates secure, API-driven interactions with SmartSpot360, enabling seamless integration with planning and reporting tools. Snowflake on AWS serves as the centralized data warehouse, supporting large-scale queries across historical and in-flight campaign data. Amazon Simple Notification Service (Amazon SNS) coordinates event-driven communication between services by publishing messages to topics that trigger downstream processes (such as model retraining, data processing, or API updates).

With this architecture, BMG360 can continuously improve the performance, reduce operational complexity, and rapidly scale SmartSpot360 as new use cases emerge. Business outcomes and metrics Adopting AI-driven media optimization produces measurable improvements across efficiency, spend, and revenue. Results achieved by BMG360, using SmartSpot360, during a health insurance TV campaign:

Media spend decreased by 32% Revenue generated increased by 52% Return on investment (ROI) for every TV dollar doubled

Operationally, automating spot selection, rate optimization, and daytime management reduced the manual workload, so teams could focus on strategic planning and creative execution. The broader takeaway When you integrate machine learning into traditional media channels, you can achieve real-time adaptability and predictive performance typically reserved for digital platforms. By building SmartSpot360 on AWS, BMG360 was able to scale media buying operations efficiently, leveraging cloud-based data processing, machine learning models, and real-time analytics. Key shifts enabled by AI and AWS infrastructure:

Dynamic budget optimization based on real-time campaign data Predictive modeling for spot performance and audience trends Reduced time spent on manual media buying processes Improved scalability and faster adaptation to market changes Stronger integration between traditional and digital media strategies

Machine learning improves media buying efficiency and reshapes the way you plan, execute, and scale traditional advertising in a digital-first world. Evolving SmartSpot360 with AWS As SmartSpot360 continues to evolve, BMG360 is expanding the platform's capabilities beyond media buying optimization to support broader planning and creative decision-making across channels. Future development is focused on:

Enhancing predictive models using reinforcement learning for deeper campaign intelligence Extending optimization to creative variants and message testing, allowing for more efficient creative to be identified and scaled faster than traditional methodologies Building a self-service planning interface for in-house teams and agency partners

Conclusion When you run high-volume campaigns across TV and radio, the ability to optimize in real time, predict outcomes, and reduce manual overhead becomes a competitive advantage for both clients and agencies alike. SmartSpot360, built on AWS, gives you that edge, helping you scale performance, not just media spend. As you continue to refine your media mix, the infrastructure you choose matters. Cloud-focused services, tools and machine learning capabilities give you the flexibility to evolve your planning, creative testing, and cross-channel coordination without sacrificing control or insight. Traditional media doesn't have to operate in a silo or fall behind digital channels. With a machine learning-powered solution you can bring precision, adaptability, and intelligence to every aspect of your media buying strategy. BMG360 continues to evolve SmartSpot360 by leveraging the power of AWS, so they don't just keep up with market shifts, they adapt in real time, align resources more effectively, and unlock greater value from every campaign. To learn more about SmartSpot360 contact BMG360. Or contact an AWS Representative to know how we can help accelerate your business. Further reading

AWS for Advertising and Marketing Get started with API Gateway Building Generative AI into Marketing Strategies: A Primer Best practices and design patterns for building machine learning workflows with Amazon SageMaker Pipelines Automate model retraining with Amazon SageMaker Pipelines when drift is detected

About BMG360 BMG360 is a full-service performance marketing and lead generation company that helps businesses grow through proprietary technology, deep media expertise, and high-converting creative.

Jake Bernstein Jake Bernstein is a Solutions Architect at Amazon Web Services with a passion for modernization and serverless first architecture. He focuses on helping customers optimize their architecture and accelerate their cloud journey.

Alexander Medina Alexander Medina is an AWS Solutions Architect with a background in Networking, Infrastructure, Security, and IT Operations. He is passionate about helping customers build Well-Architected systems on AWS.

Sonny Sharif Khan Sonny Sharif Khan is an Amazon Web Services Generative AI Platforms Adoption Lead AMER Strategic. Sonny has 25+ years of experience as a leader in data analytics, and artificial intelligence. He has spoken at the World Economic Forum (WEF) concerning AI impacting lost languages.

Simplify SQL Server Developer edition deployments with AWS Launch Wizard

Microsoft SQL Server Developer edition offers organizations a cost-effective solution for their non-production database workloads on AWS. While many enterprises rely on this edition for development and pre-production testing environments, the traditional deployment process can be cumbersome – requiring manual steps like logging into Amazon Elastic Compute Cloud (Amazon EC2) instances, downloading installation media, and performing individual instance configurations. AWS Launch Wizard now streamlines this entire process, offering an efficient way to deploy SQL Server Developer edition. In this blog post, we will demonstrate how you can leverage AWS Launch Wizard to simplify your SQL Server Developer edition deployment, reducing both the time and complexity traditionally associated with manual setup procedures. Whether you're a database administrator, developer, or solutions architect, you'll discover how to accelerate your deployment workflow while maintaining best practices for your non-production environments. Why use Launch Wizard for SQL Server Developer edition? AWS Launch Wizard deploys Microsoft SQL Server solutions on AWS through a guided deployment process. The service reduces deployment time from hours to minutes and automates infrastructure provisioning. By providing a few high-level details about the application, such as required performance characteristics, the wizard will then take care of identifying, provisioning, and configuring matching AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, and an Amazon Virtual Private Cloud (Amazon VPC). The wizard displays cost estimates for your deployment. Each resource selection update shows the new total cost, helping you align with your budget requirements. After your inputs are complete, Launch Wizard provisions these resources and configures them to create a fully functioning SQL Server deployment. The created resources are tagged, making it easy to identity and work with them, and the wizard also creates AWS CloudFormation templates, providing you with a baseline for repeatable and consistent application deployments. How does Bring Your Own Media (BYOM) work? For installation of SQL Server Developer edition, you must use your own media; setup files. In Launch Wizard, there are two installation mechanisms: 1. Bring Your Own Media (BYOM) – In this option, you can put SQL Server Developer setup files and place it in your S3 bucket. During the console or API experience, you provide AWS Launch Wizard with the location of the S3 object (setup file) for the deployment (for API, use the parameter SQLDeveloperEditionInstallMediaBucket). Launch Wizard places the S3 location of the S3 object (not the object itself) into the CloudFormation template in your account which executes the download and install of the software from the S3 bucket to the Windows instance already launched in your account. The S3 Bucket name must start with launchwizard prefix. 2. Bring Your Own URL (BYOURL) – In this option, you provide the public Microsoft URL of the downloadable setup file in the console or API that would be passed to the CloudFormation template like the BYOM approach (URL instead of S3 object). You can use SQLDeveloperEditionInstallMediaURL parameter to input URL for the API request. The URL below must start with " " prefix. This option eliminates the need to download setup files and provide access to an S3 bucket. The difference between the two methods is BYOM is an S3 bucket with the dev edition installation executable in it, whereas the BYOURL method downloads the executable to the instance. You can select the option that best fits your need. Here, Launch Wizard builds the orchestrations from your provided inputs. Since SQL Server Developer edition requires explicit acceptance of the End User License Agreement (EULA) for non-production workloads, Launch Wizard has integrated this step seamlessly into the deployment process. Instead of manually clicking through the EULA during installation, you can now accept these terms directly through the AWS console. Launch Wizard then automatically scripts this acceptance during the installation process, streamlining your deployment while ensuring compliance with Microsoft's licensing requirements. Supported configurations At the launch time, AWS Launch Wizard supports SQL Server 2022 Developer edition. SQL Server 2025 Developer edition will be supported once Microsoft releases general availability. Getting started with AWS Launch Wizard

Navigate to AWS Launch Wizard console and choose View workloads

Search for "sql" and choose MS SQL Server Choose Launch new deployment.

Under Choose deployment pattern dropdown, select SQL Server Developer Edition – Single Node – Windows. Choose Configure deployment.

In the SQL Server Launch Wizard, Step 1: Review Permissions displays information regarding the AWS Identity and Access Management (IAM) role used for the deployment. Launch Wizard creates this role on your behalf in your account so that it can provision the resources across AWS services for SQL Server deployment. This role includes permissions to get the object from S3 bucket starting with launchwizard prefix for BYOM deployments. If part of an organization, your account must have permissions to create or access AWS resources such as EC2 instances, an Amazon VPC, etc.

Under Step 2: Configure application settings, you can configure the application settings. In the General settings section, enter a Deployment name and specify a Simple Notification Service (SNS) topic ARN (Amazon Resource Name) to receive notifications and alerts about your application deployment status. You also have the option to select Enable rollback on failed deployment, which would immediately delete the resources created by the Launch Wizard deployment. This is useful if the deployment fails due to issues such as lack of permissions or incorrect Active Directory credentials. This option is disabled by default so that you have an opportunity to debug resources if a deployment fails. After diagnosing the issue, when you delete the failed Launch Wizard deployment, the related resources will be deleted. (Optional) You can enable CloudWatch Application Insights monitoring to monitor metrics and logs to detect and correlate anomalies and errors with your SQL application. (Optional) You can enable AWS Service Catalog product creation to export AWS CloudFormation template to create AWS Service Catalog for the deployment.

In the Connectivity section, you provide inputs for the networking infrastructure of your application resources. You can choose an existing Key pair name or create a new one. If you create a new key pair, hit the refresh button on the right to display the newly create key pair in the drop-down menu. The key pair will be used if you want to connect to the instances using RDP or obtain the administrator password. You have the option to select an existing Virtual Private Cloud (VPC) or create a new VPC.

For existing VPC, you need to select the Availability Zone (AZ) and private subnet, and select the Outbound Connectivity check box to confirm that public connectivity is established for the private subnets. If creating a new VPC, Launch Wizard will create and configure the subnets for you. Optionally, you can enable setup Remote Desktop Gateway preferences, which allows you to establish a secure, encrypted connection to your SQL instance via RDP over HTTPS. In Active Directory section, you can choose to enable or disable Active Directory. This gives flexibility to use the SQL instance with or without an Active Directory. The directory allows authorized users to authenticate with this SQL server instance using Windows authentication. Disable Active Directory option is only available for SQL Server Developer edition. If you choose to create a new VPC on the connectivity section, you must select to create a new Active Directory if it is enabled. For existing VPC, you can connect to existing Active Directory.

You can enter the domain administrator password, DNS name and optionally specify the Organization Unit (OU) within which you want to join the SQL instance to Active Directory. Next, under SQL Server Configuration, provide your SQL Server account details. For existing Active Directory, choose to use an existing service account or create a new account. For new Active Directory, you must create new SQL service account. If Active Directory is disabled, you can proceed and select SQL Server install type. For the SQL Server install type, choose License-included AWS provided AMI or use your own custom AMI. The License included AMI will include the Windows Licensing.

In Additional SQL Server settings – optional you can provide SQL node names. If you leave this fields blank, then Launch Wizard will assign unique name for you. Select Next to go to next step, Step 3: Configure infrastructure settings. Under SQL Server Developer Edition Version dropdown, you can choose SQL Server 2022 Developer edition Select the checkbox to accept the Microsoft End User License Agreement (EULA). This gives permissions to download and install SQL Developer edition on EC2 instance. Under installation method, choose one of the options: Bring Your Own Media (BYOM) or Bring your Own URL (BYOURL).

For BYOM, select Browse S3 and select the S3 bucket starting with launchwizard prefix containing the SQL Server installation media. For Bring your Own URL, enter the public Microsoft URL where SQL Server Developer edition setup could be found. The URL must start with Under Choose how you would like to define your infrastructure, you have two options. By default, you will see Launch Wizard recommends AWS infrastructure based on your needs. Here, under Compute needs, you can input your network, memory, and processor requirements for your instance, and Launch Wizard will recommend an instance type that can be seen under Recommended infrastructure. Similarly, under Storage and performance needs, you can input the type of storage drive and SQL Server throughput, and Launch Wizard will recommend the volume type.

Alternately, you can choose Specify your own instance and volume configuration to manually select the instance type and volume type. For volume type, you can consider to choose gp3 volume. Under Advanced settings, you can choose vCPU optimization options. By default, it will use all available vCPU for maximum performance.

Under Drive letters and volume size, you can optionally update the volume letters and volume size. At the end of the page, you can see the estimated monthly costs to deploy the resources, which gets updated as you modify your inputs, and can help you select what's best for your needs.

The final step is Step 4: Review and deploy, which shows the summary with all the configuration options selected on the previous steps. Once you hit Deploy, the Launch Wizard begins to spin up the resources and configure your application.

The deployment completes in 45 to 60 minutes, and you can go into the Amazon EC2 console to access your EC2 instances, AWS Systems Manager Application Manager to manage your application. In AWS CloudFormation, you can view the stack and template generated by Launch Wizard based on your selections, and this code can be re-used for future deployments or for integration with your CloudFormation-supported CI/CD tools. Cleaning up While there is no cost to use Launch Wizard, you will be charged for resources created as part of the application deployment. If you no longer need the application running, you can delete the deployment in Launch Wizard, which will delete all the AWS resources created as part of the deployment. Following are the steps to delete the deployment:

Navigate to AWS Launch Wizard console From the left menu, choose Deployments Select your SQL Server Deployment and choose Delete

Conclusion In this blog post, we have demonstrated how AWS Launch Wizard simplifies the deployment of SQL Server Developer edition, transforming a once manual process into a streamlined, wizard-driven experience. The service offers guided steps, flexible installation options, and automated resource provisioning while maintaining compliance with Microsoft's licensing requirements. AWS Launch Wizard's cost estimation features and reusable CloudFormation templates help teams plan and execute deployments more efficiently. Whether setting up development environments or test instances, this tool significantly reduces complexity and time, eliminating licensing costs for non-production workloads and offering an efficient path for organizations to leverage SQL Server in their development lifecycle. To further explore SQL Server deployments on AWS, we recommend checking out our related resources on Simplify SQL Server Always On deployments with AWS Launch Wizard and Amazon FSx, and SQL Server Always On Failover Cluster Instances with Amazon FSx for NetApp ONTAP.

Suhail Fouzan Suhail Fouzan is a Specialist Solutions Architect at Amazon Web Services (AWS) with over 15 years of experience in the IT industry. Specializing in Microsoft workloads, migration services, and operations management with AWS Systems Manager, Suhail helps customers successfully migrate their infrastructure to AWS. Outside of work, Suhail enjoys playing cricket and spending time with his family.

Yogi Barot Yogi Barot is WorldWide Microsoft Tech leader leading Microsoft Technical field community at AWS. Part of her role, she is leading community for technical enablement and supporting customers migrate and modernize Microsoft workload to AWS. Yogi has 26 years of experience working with different Microsoft technologies, her specialty is in SQL Server and different database technologies. Yogi has in depth AWS knowledge and expertise in running Microsoft workload on AWS.

Organizations are accelerating their global cloud footprints to deliver exceptional customer experiences and enhanced application performance worldwide. As teams scale across AWS Regions, there is opportunity to use hundreds of AWS services with consistent functionality across Regions. The AWS Knowledge Model Context Protocol (MCP) server helps provide programmatic access for deep understanding of AWS capabilities and their availability across AWS Regions. By combining the deep research of AI agents with MCP Servers, you can automate Regional expansion analysis and generate actionable migration strategies in hours instead of weeks. In this post, we show you how to build a multi-agent AI system using the new Regional availability tools in the AWS Knowledge MCP server to automate Regional expansion analysis and planning. These specialized agents run locally to analyze AWS CloudFormation and AWS CloudTrail logs within your environment. The agents then query AWS service, feature, API, and CloudFormation resource availability data through AWS Knowledge MCP tools to identify Regional capability gaps and generate actionable migration strategies. All customer data remains within your local environment and is not transmitted to external systems. By automating this analysis, teams can focus on implementation rather than spending time on manual research and compatibility validation. In this post you will learn how to implement AI agents that can:

Analyze your AWS infrastructure across Regions Identify service dependencies and feature requirements Perform deep research against target Regions when deploying workloads and functionalities Generate detailed migration strategies with implementation guidance Maintain continuous visibility into Regional capability changes

Solution overview Building production-ready AI agents for Regional expansion planning requires a systematic approach that combines infrastructure intelligence using deep research on Regional availability with automated guidance to help decision-making. We demonstrate how to combine the AWS Knowledge MCP server along with the AWS CloudFormation MCP and CloudTrail MCP servers to perform deep research with AI agents to automate Regional expansion planning. The solution implements a multi-agent orchestration pattern built on the Strands Agents SDK, where specialized agents collaborate to deliver comprehensive Regional expansion insights. Each agent serves a specific purpose in the analysis to discover your AWS infrastructure, analyze usage, check for Regional service availability and generate detailed reports with specific migration strategies and implementation timelines. MCP tools integration The multi-agent AI system uses several AWS MCP servers to access comprehensive service and Regional availability data. The following table shows the MCP tools used and their specific functions in the analysis:

CloudTrail Explorer CloudTrail MCP Server event_lookup Advanced CloudTrail event analysis

CFN Explorer AWS CloudFormation MCP Server resource_reading Deep inspection of AWS resource properties and attributes

CFN Explorer AWS CloudFormation MCP Server resource_listing Deep research to provide comprehensive resource enumeration

Capabilities Explorer AWS Knowledge MCP Server list_region Latest region metadata and alternative names

Capabilities Explorer AWS Knowledge MCP Server get_regional_availability API-level compatibility validation and CloudFormation resource support verification across Regions.

Multi Region Expansion Planner andPlanning Report Generator AWS Knowledge MCP Server search_documentation Deep search across AWS documentation

Multi Region Expansion Planner andPlanning Report Generator AWS Knowledge MCP Server read_documentation Convert AWS documentation to structured markdown

Multi Region Expansion Planner andPlanning Report Generator AWS Knowledge MCP Server recommend Get content recommendations for AWS documentation pages

Architecture The following architecture diagram shows how the multi-agent system, orchestrated through the Strands Agents SDK, interacts to analyze your infrastructure and generate expansion recommendations to reduce deployment risks while accelerating time-to-market for global applications.

The Orchestrator agent coordinates the analysis workflow using the Strands Agents SDK. Infrastructure Discovery analyzes CloudFormation stacks and CloudTrail logs using AWS CloudFormation MCP and CloudTrail MCP servers. AWS Knowledge MCP provides deep research on service availability and features. AI agents process the data to identify gaps and generate recommendations. The system produces three reports: Executive summary, Technical analysis, and Implementation plan

An AWS account with programmatic access configured through the AWS CLI (version 2.0 or later) AWS CloudTrail enabled in your source Region for usage analytics AWS CloudFormation Stacks deployed that represent your infrastructure Access to the Claude 4.0 model in Amazon Bedrock Strands Agents SDK (version 1.0 or later). For more information see the Strands Agent SDK User Guide. Python 3.11 or later environment AWS IAM permissions

{ "Version": "2012-10-17", "Statement": [{ "Sid": "CloudFormationAnalysis", "Effect": "Allow", "Action": ["cloudformation:ListStacks", "cloudformation:DescribeStacks", "cloudformation:ListStackResources", "cloudformation:DescribeStackResources"], "Resource": "*" }, { "Sid": "CloudTrailAnalysis", "Effect": "Allow", "Action": ["cloudtrail:LookupEvents"], "Resource": "*" }, { "Sid": "BedrockAccess", "Effect": "Allow", "Action": ["bedrock:InvokeModel", "bedrock:InvokeModelWithResponseStream"], "Resource": ["arn:aws:bedrock:*:*:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0"] }] }JSON

Solution implementation: Building your multi-Region expansion AI agent system We've architected this solution with an Orchestrator agent that coordinates five specialized agents through distinct analysis phases. This pattern ensures modularity, reliability, and extensibility as your expansion requirements evolve:

CFN Explorer – Analyzes all CloudFormation stacks and resources in a Region CloudTrail Explorer -Processes CloudTrail logs for API usage analysis with a default 7-days lookback Capabilities Explorer – Uses AWS Knowledge MCP tools for Regional availability intelligence Multi-Region Expansion Planner – Orchestrates comparative analysis across target Regions Planning Report Generator – Creates executive-ready expansion planning reports

Phase 1: Infrastructure discovery The CFN Explorer agent performs comprehensive infrastructure analysis:

Discovers and analyzes all CloudFormation stacks and their resources Identifies service dependencies and resource relationships Catalogs encryption keys, networking, and security configurations Orchestrator agent can pass any source Region and a list of target Regions as parameters

Implementation insight: The orchestrator accepts any source Region and multiple target Regions as parameters, to support flexible analysis scenarios. The following is an example of a successful connection to AWS Account with discovered CloudFormation stacks:

I'll analyze all CloudFormation stacks in the us-east-1 region using the xxxx AWS profile. Let me start by retrieving the list of stacks and then gather detailed information for each running stack.

Tool #1: use_aws ╭─ 🚀 List all CloudFormation stacks ─╮ │ │ │ Service: cloudformation │ │ Operation: list_stacks │ │ Region: us-east-1 │ │ Parameters: None │ │ │ ╰─────────────────────────────────────╯Code

Phase 2: Usage pattern analysis The CloudTrail Explorer agent analyzes actual service usage patterns:

CloudTrail Explorer processes 7 days of API usage logs by default. This can be configured to a different value as needed. Identifies actual service usage patterns beyond deployed resources. Maps API dependencies and integration patterns. Orchestrator agent can pass any number of days as lookback period for API analysis.

Production tip: Extend the lookback period for comprehensive analysis during peak usage periods. Phase 3: Regional availability assessment The Capabilities Explorer agent provides real-time Regional intelligence through deep research

Capabilities Explorer checks service availability across target Regions Validates API compatibility and CloudFormation resource support Identifies Regional limitations and feature gaps in target Regions

Phase 4: Gap analysis and planning The Multi-Region Planner agent delivers action insights:

Performs comprehensive gap analysis Generates specific workarounds and recommendations based on missing functionality Creates implementation roadmaps with effort estimates Suggests CloudFormation template changes to implement workarounds

Phase 5: Report generation The Planning Report Generator agent produces comprehensive documentation:

Executive summaries with business impact analysis Technical implementation guides with code examples Compliance requirements and risks matrices

Deploy the solution on your local machine To deploy the solution on your local machine, complete the following steps: Step 1: Clone the sample-multi-agent-region-expansion-planner repository:

Step 3: Configure the AWS profiles and target Regions in src/utils/config.py:

# AWS CLI profile to access account where workloads are deployed. PROFILE_NAME = 'your-profile-name'

# Source region where workloads are deployed. SOURCE_REGION = 'us-east-1'

# Target regions where workloads expansion need to be analyzed and planned. TARGET_REGIONS = ['eu-central-1', 'ca-central-1', 'ap-southeast-7'] Bash

Step 4: Configure the AWS profile and Amazon Bedrock model provider account in src/utils/config.py:

# Account ID to use Bedrock Models and create AI Agents INFRA_ACCOUNT = "123456789012"

# AWS CLI profile for auth BEDROCK_AWS_CLI_PROFILE = "bedrock-profile" Bash

Generated insights and deliverables Upon completion, the system generates three production-ready reports:

Executive summary: Business-focused insights with risk assessments Technical analysis: Deep dive into service dependencies and gaps Implementation plan: Step-by-step migration guide with code samples

Executive summary The executive summary is a business-focused report that provides key insights about Regional expansion readiness. It presents a clear view of service availability across target Regions, identifies critical dependencies, and outlines required architectural modifications. The report consolidates complex technical findings into actionable business intelligence that helps inform expansion decisions. The following is an example excerpt of the output for the executive summary:

This comprehensive expansion planning report provides detailed guidance for migrating AWS workloads from **us-east-1** to three target regions: **eu-central-1**, **ca-central-1**, and **ap-southeast-7**. The analysis reveals significant service availability challenges requiring extensive architectural modifications and a carefully orchestrated phased deployment approach.

Our analysis reveals a compelling business opportunity with 78% technical readiness across target regions, representing a $485,000 strategic investment that delivers $2.1M in additional annual revenue by Year 3. The expansion addresses critical market demands for data sovereignty, GDPR compliance, and reduced latency while establishing competitive differentiation through first-mover advantage in multi-region knowledge graph deployment. The technical foundation demonstrates strong compatibility with 94% API parity across regions, though strategic workarounds are required for specific service limitations in eu-south-1.Bash

Technical analysis The technical analysis provides engineers with detailed implementation guidance for Regional expansion. This report includes comprehensive service compatibility analysis, specific API modifications needed, and architectural recommendations for services with Regional variations. The following is an example excerpt of the output for the technical analysis:

Based on comprehensive analysis of current AWS workloads in **us-east-1** and assessment of four potential target regions, this document provides detailed migration feasibility analysis for:

- **ap-southeast-7** (Asia Pacific - Thailand) - **65% compatibility** - **eu-central-1** (Europe - Frankfurt) - **95% compatibility** - **ca-central-1** (Canada - Central) - **80% compatibility**

The source environment (us-east-1) contains **35 CloudFormation stacks** utilizing **21 unique AWS services** with the following critical components:

- **Neptune Global Database** with cross-region replication - **SageMaker notebook instances** for ML workloads - **Lambda functions** (15 total) with VPC configurations - **S3 buckets** (8 total) with cross-region dependencies - **IAM roles and policies** (25 roles, 12 policies) - **ElastiCache serverless** configurations - **CDK bootstrap resources** with region-specific naming

| Region | Migration Status | Recommended Action | |--------------------|------------------------|----------------------------------------------------------| | **eu-central-1** | ✅ **RECOMMENDED** | Proceed with migration | | **ca-central-1** | ⚠️ **CONDITIONAL** | Good foundation - missing CI/CD and development services | | **ap-southeast-7** | ❌ **NOT RECOMMENDED** | Significant service gaps - wait for service expansion |Bash

Implementation plan The implementation plan delivers a detailed migration roadmap. It provides step-by-step guidance for implementing the regional expansion, with clear phases and validation checkpoints. The following is an example excerpt of the output for the implementation plan:

# Multi-Region Expansion Planning Report ### Executive Report for KnowledgeForge Application Expansion

Source Region: us-east-1 Target Regions: eu-central-1, ca-central-1, ap-southeast-7 Analysis Date: Sep 3, 2025

Service Entity | Entity Type | Source Region | eu-central-1 | ca-central-1 | ap-southeast-7 | Status | Notes ---------------|-------------|---------------|--------------|--------------|----------------|-------------|------ dynamodb | AWS Service | Available | Available | Available | ? Unknown | Full Parity | All core APIs supported neptune | AWS Service | Available | Available | Available | ? Unknown | Partial | Global DB limitations in some regions sagemaker | AWS Service | Available | Limited | Available | ? Unknown | API Gaps | DescribeNotebookInstanceLifecycleConfig missing cloudwatch | AWS Service | Available | Available | Available | ? Unknown | Full Parity | All monitoring features available lambda | AWS Service | Available | Available | Available | ? Unknown | Full Parity | All runtimes supported s3 | AWS Service | Available | Available | Available | ? Unknown | Full Parity | Regional buckets required iam | AWS | Available | Available | Available | ? Unknown | Full Parity | GlobalBash

Environment clean up Maintain operational hygiene by running the cleanup command so that agents do not consume data from previous analyses.

Conclusion: Transforming Regional expansion with agent workflows and deep research In this post, we demonstrated how the combination of the AWS Knowledge MCP server and AI agents can transform Regional expansion planning from a manual process into an automated workflow. With this multi-agent system teams can:

Accelerate planning cycles with automated infrastructure analysis and deep research Make data-driven decisions using comprehensive Regional capability data Generate production-ready implementation strategies with specific code examples

For more information about multi-Region architectures, see AWS Capabilities by Region. For more information about building global applications on AWS, see AWS Global Infrastructure. We're excited to see how you use the AWS Knowledge MCP server to accelerate your global expansion. To get started, visit the AWS Knowledge MCP Getting Started page. Alternatively, you can also add the AWS Knowledge MCP server to your AI agentic IDEs and AI assistants such as Kiro, Cursor, Cline, or Amazon Q for command line to perform similar analysis without using the Strands Agents SDK.

About the authors Veera "Bhargav" Nunna is a Senior Data Engineer and Tech Lead at Amazon Web Services (AWS) pioneering Knowledge Graphs for Large Language Models and enterprise-scale data solutions. With over a decade of experience, he specializes in transforming enterprise AI from concept to production by delivering MVPs that demonstrate clear ROI while solving practical challenges like performance optimization and cost control. Veera is among the first to launch production-level MCP Servers, tools, and agentic solutions that solve critical problems at AWS and for customers globally. When not working on AI-driven solutions, he enjoys aviation and flies his family in small aircrafts, holding a private pilot's license. James Liu is a Senior Product Manager – Technical at Amazon Web Services (AWS), where he leads product development for cloud infrastructure and developer tools. With 10+ years of experience in product management, he specializes in driving product strategy, building cross-functional partnerships, and delivering scalable solutions from conception to launch. James focuses on AI-driven infrastructure optimization, dependency management systems that serve enterprise developers at scale and holds multiple patents in AI/ML. Outside of work, James is passionate about mentoring aspiring product managers and exploring the intersection of emerging AI technologies with practical business applications. Nikhil Jivan is a software engineer at AWS specializing in knowledge graphs and scalable data platforms. With 3+ years of experience in distributed systems and data engineering, he builds solutions that help customers unlock insights from their data. When not coding, you'll find him in the gym, at the piano, or exploring a new city. Sucharitha Boinapally is a Data leader at Amazon Web Services (AWS) with 13+ years of experience leading enterprise-scale data solutions. She pioneers AI-driven infrastructure optimization using knowledge graphs and MCP servers to transform complex AWS interdependencies into strategic intelligence, empowering teams to make data-driven decisions about AWS products. Her expertise in agentic AI approaches and advanced data orchestration enables insights that accelerate AWS's global infrastructure deployment and service innovation. Outside of work, Sucharitha enjoys growing her own food and spending quality time with her family, and friends.

Introducing AWS Capabilities by Region for easier Regional planning and faster global deployments

At AWS, a common question we hear is: "Which AWS capabilities are available in different Regions?" It's a critical question whether you're planning Regional expansion, ensuring compliance with data residency requirements, or architecting for disaster recovery. Today, I'm excited to introduce AWS Capabilities by Region, a new planning tool that helps you discover and compare AWS services, features, APIs, and AWS CloudFormation resources across Regions. You can explore service availability through an interactive interface, compare multiple Regions side-by-side, and view forward-looking roadmap information. This detailed visibility helps you make informed decisions about global deployments and avoid project delays and costly rework.

Getting started with Regional comparison To get started, go to AWS Builder Center and choose AWS Capabilities and Start Exploring. When you select Services and features, you can choose the AWS Regions you're most interested in from the dropdown list. You can use the search box to quickly find specific services or features. For example, I chose US (N. Virginia), Asia Pacific (Seoul), and Asia Pacific (Taipei) Regions to compare Amazon Simple Storage Service (Amazon S3) features. Now I can view the availability of services and features in my chosen Regions and also see when they're expected to be released. Select Show only common features to identify capabilities consistently available across all selected Regions, ensuring you design with services you can use everywhere.

The result will indicate availability using the following states: Available (live in the region); Planning (evaluating launch strategy); Not Expanding (will not launch in region); and 2026 Q1 (directional launch planning for the specified quarter). In addition to exploring services and features, AWS Capabilities by Region also helps you explore available APIs and CloudFormation resources. As an example, to explore API operations, I added Europe (Stockholm) and Middle East (UAE) Regions to compare Amazon DynamoDB features across different geographies. The tool lets you view and search the availability of API operations in each Region.

The CloudFormation resources tab helps you verify Regional support for specific resource types before writing your templates. You can search by Service, Type, Property, and Config.For instance, when planning an Amazon API Gateway deployment, you can check the availability of resource types like AWS::ApiGateway::Account.

You can also search detailed resources such as Amazon Elastic Compute Cloud (Amazon EC2) instance type availability, including specialized instances such as Graviton-based, GPU-enabled, and memory-optimized variants. For example, I searched 7th generation compute-optimized metal instances and could find c7i.metal-24xl and c7i.metal-48xl instances are available across all targeted Regions.

Beyond the interactive interface, the AWS Capabilities by Region data is also accessible through the AWS Knowledge MCP Server. This allows you to automate Region expansion planning, generate AI-powered recommendations for Region and service selection, and integrate Regional capability checks directly into your development workflows and CI/CD pipelines.

Now available You can begin exploring AWS Capabilities by Region in AWS Builder Center immediately. The Knowledge MCP server is also publicly accessible at no cost and does not require an AWS account. Usage is subject to rate limits. Follow the getting started guide for setup instructions. We would love to hear your feedback, so please send us any suggestions through the Builder Support page. — Channy

Organizations in regulated industries often mandate control over encryption keys when storing data in the cloud to meet compliance requirements. Although AWS Backup logically air-gapped vault provides secure, isolated backup storage, these customers have needed the ability to use their own AWS Key Management Service (AWS KMS) customer-managed keys (CMKs) to provide greater control of their backup data at rest. Today, we're announcing support for CMKs in AWS Backup logically air-gapped vault. You can use this new capability to maintain full control over your encryption keys while benefiting from the security and compliance advantages of air-gapped backup storage. In this post, we walk you through configuring CMKs with logically air-gapped vault and demonstrate how this feature helps you meet compliance requirements. Furthermore, we discuss additional controls needed to ensure that your setup is secure and protected. What's new? AWS Backup logically air-gapped vault now supports both AWS-owned keys (AOK) and AWS KMS CMKs. This gives you multiple ways to manage your backup encryption. Choosing the right key management option:

AOKs remain our recommended approach for most use cases due to their integrated functionality and robust security. CMKs are designed for organizations with specific governance requirements or regulatory compliance standards.

Key capabilities of CMK support: CMK support addresses three critical customer requirements:

Complete key lifecycle control: Manage key access, rotation, and retirement schedules according to your policies. Enhanced audit visibility: Track all key operations through comprehensive AWS CloudTrail. Streamlined compliance integration: Seamlessly incorporate backup encryption into existing monitoring and compliance workflows.

Architecture and components Core Architecture A central key vault account is a security strategy where encryption keys are centrally managed and shared across multiple accounts, enhancing security and compliance. This approach can be implemented either within the same AWS Organizations for streamlined governance or across different organizations for enhanced isolation. Unlike traditional KMS strategies that provision and manage keys locally within individual accounts, the central key vault model consolidates key management, monitoring, and permission boundaries in a dedicated account.

Figure 1: Using CMKs with AWS Backup logically air-gapped vault in a multi-account architecture This architecture maintains the fundamental security principles that make logically air-gapped vaults resilient against data loss and ransomware events. Your backups remain stored in service-owned accounts, physically isolated from your workloads, and protected by the Write-Once-Read-Many (WORM) model. This multi-layered approach ensures backup security and recoverability, even during account compromise scenarios. AWS Backup logically air-gapped vault integrates with Multi-party approval for AWS Organizations, enhancing security without compromising operational agility. This allows organizations to recover using the backup copies stored in the AWS Backup logically air-gapped vault. The key vault account managed CMKs can then be shared with either the forensics account or the recovery account for integrity validation and restore. Key Management Options When creating an AWS Backup logically air-gapped vault, you can now specify an AWS KMS CMK for encryption. This CMK can be sourced from the following options:

Your current account: For streamlined management within a single AWS account A designated key vault account: For centralized key management, which can be provisioned in the same AWS Organization or a different Organization (recommended for enhanced security isolation)

This integration enables you to maintain complete control over your backup encryption keys while preserving all the robust security benefits of logical air-gapping. Services in other accounts can reference and use the centrally stored keys to encrypt and decrypt their data, enabling secure sharing of customer-managed keys (CMKs) with workload accounts where backups are created and managed. The feature supports symmetric customer-managed keys and enables key reuse across multiple vaults. Although the encryption key cannot be changed after vault creation, AWS KMS automatic key rotation is fully supported, providing continuous updates to the key's backing material without service interruption. Combined with comprehensive audit capabilities and granular AWS Identity and Access Management (IAM) policies, organizations gain the flexibility to tailor their backup encryption strategy to meet specific security and compliance requirements. When using CMKs, customers should follow the best practices discussed in the following section to provision architecture constructs to cater to various aspects of a resilient recovery strategy. Best Practices & Operations Implementing secure backup encryption strategies with AWS Backup logically air-gapped vaults requires careful consideration of operational processes, security governance frameworks, and monitoring practices. The following sections cover security controls, monitoring and alerting, and operational procedures to help organizations establish robust, scalable, and compliant backup encryption strategies that align with enterprise security frameworks and regulatory requirements. Security Controls Comprehensive security controls for vault encryption go beyond basic key policies. Many enterprises adopt centralized encryption management approaches, creating dedicated key vault accounts to separate encryption operations from workload accounts and establish clear boundaries for sensitive key operations. Effective protection necessitates layering multiple security controls to create a robust defense-in-depth strategy. The following four security controls provide comprehensive protection through multi-layered access controls:

Service Control Policies (SCPs) enforce organization-wide restrictions, such as requiring MFA for sensitive operations like PutKeyPolicy, DisableKey, or ScheduleKeyDeletion across all accounts.

{ "Version":"2012-10-17", "Statement":[ { "Sid":"DenyKMSAdminOpsWithoutMFA", "Effect":"Deny", "Action":[ "kms:PutKeyPolicy", "kms:DisableKey", "kms:ScheduleKeyDeletion" ], "Resource":"*", "Condition":{ "BoolIfExists":{ "aws:MultiFactorAuthPresent":"false" } } } ] }JSON

Resource Control Policies (RCPs) apply guardrails directly at the resource level, preventing unauthorized cross-account access and mitigating the confused deputy problem where AWS services might be manipulated into using permissions inappropriately across accounts. Combined with AWS KMS grants, RCPs ensure that only authorized services gain temporary access to your keys. The same restrictions applied through SCPs can be implemented through RCPs, with the advantage that they apply to specific principals and are enforced directly on the CMKs.

{ "Version":"2012-10-17", "Statement":[ { "Sid":"DenyDestructiveKMSOperations", "Effect":"Deny", "Principal":"*", "Action":[ "kms:PutKeyPolicy", "kms:DisableKey", "kms:ScheduleKeyDeletion", "kms:DeleteAlias", "kms:UpdateKeyDescription", "kms:CancelKeyDeletion" ], "Resource":"*", "Condition":{ "BoolIfExists":{ "aws:MultiFactorAuthPresent":"false" } } } ] }JSON

Permission boundaries define maximum privileges for IAM roles, preventing administrators or automation from escalating access beyond approved limits. IAM and key policies provide granular access control at the identity and resource levels respectively.

This layered approach ensures that even if one control is misconfigured, others remain in place to protect your AWS KMS keys and logically air-gapped backups. Each layer addresses different risks: SCPs enforce global organizational rules, RCPs constrain resource usage, permission boundaries contain role privileges, and key policies define specific key access. This creates comprehensive protection for your backup encryption strategy. For detailed implementation guidance, refer to the AWS KMS access control glossary documentation for deeper insights into individual security control mechanisms. Monitoring and alerting Effective monitoring of AWS KMS key usage requires implementing comprehensive observability across multiple AWS services. AWS KMS integrates with Amazon CloudWatch, AWS CloudTrail, and Amazon EventBridge to provide real-time visibility into key operations, usage patterns, and potential security events. This multi-layered monitoring approach enables organizations to detect anomalies, track compliance metrics, and maintain detailed audit trails for regulatory requirements. Key monitoring strategies include:

CloudWatch Alerts for unusual key activity, such as failed operations or attempts to schedule key deletion CloudTrail Log Analysis for sensitive API calls including PutKeyPolicy, DisableKey or ScheduleKeyDeletion Automated Event Processing through EventBridge to trigger immediate responses to critical key management events SIEM Integration for centralized security monitoring and correlation with other enterprise security events

For example, you can create a CloudWatch metric filter to automatically detect and alert on key deletion attempts:

aws logs put-metric-filter \ --log-group-name "CloudTrail/Logs" \ --filter-name "KMSKeyDeletion" \ --metric-transformations \ metricName="KMSKeyPendingDeletion",metricNamespace="Custom/KMS",metricValue=1 \ --filter-pattern '{ ($.eventSource = "kms.amazonaws.com") && ($.eventName = "ScheduleKeyDeletion") }'TypeScript

Additionally, monitor sensitive operations like DisableKey, PutKeyPolicy, and RevokeGrant, which can significantly impact key availability and permissions. Route these events to EventBridge or your SIEM platform to enable automated alerting and remediation workflows. For comprehensive implementation guidance, refer to the AWS KMS monitoring documentation. Operational Procedures Effective key management extends beyond initial setup to encompass ongoing operational considerations that ensure long-term security and compliance. AWS KMS provides automatic annual key rotation, which updates the cryptographic material while retaining the same key ID. For most organizations, this standard rotation frequency is sufficient given that AWS KMS keys are protected by FIPS 140-3 Level 3 validated hardware security modules, and more frequent rotation provides minimal additional security benefits. Key operational considerations include:

Key Rotation Management: Organizations with compliance requirements mandating shorter rotation intervals can implement manual rotation processes in addition to automatic rotation Governance and Auditing: Tag keys with rotation requirements and compliance metadata to streamline audit processes and policy enforcement Lifecycle Management: Establish clear procedures for key creation, usage monitoring, and retirement aligned with data retention policies Access Review Processes: Implement regular reviews of key permissions and cross-account sharing arrangements to maintain least-privilege access Disaster Recovery Testing: Validate key availability and backup restoration procedures as part of regular DR exercises

These operational procedures ensure that your backup encryption strategy remains effective and compliant as your organization scales and regulatory requirements evolve. Implementation Guide Policy configurations Your AWS KMS key policy serves as the foundational security control for your backup encryption strategy. Following the principle of least privilege, we recommend structuring your key policies with distinct roles that separate administrative operations, usage permissions, and deletion capabilities. This role-based approach ensures that no single entity has excessive permissions while maintaining operational flexibility. The following policy statements must be applied to your CMK based on your specific use case and required operations with your logically air-gapped vault: Creating an AWS Backup logically air-gapped vault using CMK When creating a logically air-gapped vault, you must apply the AWS-managed policy AWSBackupFullAccess to your account role. This policy includes Allow actions that enable AWS Backup to interact with AWS KMS for grant creation on AWS KMS keys during backup, copy, and storage operations. Furthermore, you must ensure that the CMK key policy includes specific necessary permissions to share with the account where the logically air-gapped vault resides.

{ "Sid": "Allow use of the key to create a logically air-gapped vault", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::[account-id]:role/TheRoleToAccessAccount" }, "Action": [ "kms:CreateGrant", "kms:DescribeKey" ], "Resource": "*", "Condition": { "StringLike": { "kms:ViaService": "backup.*.amazonaws.com" } } }JSON

Copying or restoring within the same-account using a CMK During same-account copy or restore operations, AWS Backup assumes a customer-defined copy role such as AWSBackupDefaultServiceRole or a custom role that you specify. This role requires specific permissions to interact with your CMK for both encryption operations and grant management.

{ "Sid":"Allow use of the key for copy", "Effect":"Allow", "Principal":{ "AWS":"arn:aws:iam::[source-account-id]:role/service-role/AWSBackupDefaultServiceRole"[ "Source copy role" ] }, "Action":[ "kms:Encrypt", "kms:Decrypt", "kms:ReEncrypt*", "kms:GenerateDataKey*", "kms:DescribeKey" ], "Resource":"*", "Condition":{ "StringLike":{ "kms:ViaService":"backup.*.amazonaws.com" } } }, { "Sid":"Allow AWS Backup to create grant on the key for copy", "Effect":"Allow", "Principal":{ "AWS":"arn:aws:iam::[source-account-id]:role/service-role/AWSBackupDefaultServiceRole"[ "Source copy role" ] }, "Action":[ "kms:CreateGrant" ], "Resource":"*", "Condition":{ "Bool":{ "kms:GrantIsForAWSResource":"true" }, "StringLike":{ "kms:ViaService":"backup.*.amazonaws.com" } } }JSON

Copying or restoring cross-account using a CMK When copying or restoring between accounts, or from a recovery account through AWS Resource Access Manager (AWS RAM) or Multi-party approval, the CMK must trust both the recovery account's service role and the destination account's service-linked role (SLR). This cross-account trust enables secure key operations across organizational boundaries.

{ "Sid":"Allow use of the key for copy/restore from a recovery account", "Effect":"Allow", "Principal":{ "AWS":[ "arn:aws:iam::[recovery-account-id]:role/service-role/AWSBackupDefaultServiceRole", [ "Recovery account copy/restore role" ]"arn:aws:iam::[destination-account-id]:role/aws-service-role/backup.amazonaws.com/AWSServiceRoleForBackup" ][ "Destination SLR" ] }, "Action":[ "kms:Encrypt", "kms:Decrypt", "kms:ReEncrypt*", "kms:GenerateDataKey*", "kms:DescribeKey" ], "Resource":"*" }, { "Sid":"Allow AWS Backup to create grant on the key for copy", "Effect":"Allow", "Principal":{ "AWS":[ "arn:aws:iam::[recovery-account-id]:role/service-role/AWSBackupDefaultServiceRole"[ "Recovery account copy/restore role" ]"arn:aws:iam::[destination-account-id]:role/aws-service-role/backup.amazonaws.com/AWSServiceRoleForBackup" ], [ "Destination SLR" ] }, "Action":[ "kms:CreateGrant" ], "Resource":"*", "Condition":{ "Bool":{ "kms:GrantIsForAWSResource":"true" } } }JSON

These policy statements implement least-privilege service access by restricting principals per operation and ensuring that all grants are explicitly for AWS resources through kms:GrantIsForAWSResource. The recovery-account-id refers to the source account in normal cross-account and cross-Region restore scenarios, while it represents the restore account when using AWS RAM or Multi-party approval shares. Depending on your specific use case, you can apply a combined policy that accommodates multiple scenarios. API & Console Implementation Having established the necessary policy configurations for secure key management, we now shift our focus to practical implementation. These enhancements provide organizations with greater control over their backup encryption strategy through:

Enhanced Vault Creation: The ability to specify a CMK using EncryptionKeyArn when creating AWS Backup logically air-gapped vaults. Encryption Visibility: The addition ofEncryptionKeyType in API responses to indicate whether a vault uses an AWS Backup service-owned key by default or a customer-managed key.

This section focuses on the specific APIs that have been modified to support these CMK capabilities. For comprehensive coverage of all AWS Backup APIs, refer to the AWS Backup API Reference documentation. Create an AWS Backup logically air-gapped vault with CMK To create a logically air-gapped vault with a CMK, you can use the 'CreateLogicallyAirGappedBackupVault' API. This API necessitates specifying both 'MinRetentionDays' and 'MaxRetentionDays' parameters, because AWS Backup logically air-gapped vault is a vault locked in compliance mode. To support CMK, we've added an optional EncryptionKeyArn field to the API input. If you don't provide an EncryptionKeyArn, then AWS Backup creates the AWS Backup logically air-gapped vault using an AWS Backup service owned key by default. Vault creation is asynchronous, so the vault will likely be in a CREATING state initially. The following is an example using the AWS Command Line Interface (AWS CLI):

aws backup create-logically-air-gapped-backup-vault --region us-east-1 --backup-vault-name ExampleLAGVault --encryption-key-arn arn:aws:kms:us-east-1:123456789012:key/1234abcd-12ab-34cd-56ef-1234567890ab --min-retention-days 7 --max-retention-days 35 --creator-request-id 12312873218-13213-1321TypeScript

Similarly, you can create an AWS Backup logically air-gapped vault with CMK in console using the example in the following figure.

Figure 2: Creating an AWS Backup logically air-gapped vault with CMK encryption Describe an AWS Backup logically air-gapped vault You can use the 'DescribeBackupVault' API to view the attributes and state of your AWS Backup logically air-gapped vault. The response includes an EncryptionKeyType field that indicates whether the vault is encrypted using an AOK or a CMK. The following shows how to describe an AWS Backup logically air-gapped vault using the AWS CLI:

The command returns detailed information about the vault, including its encryption configuration:

{ BackupVaultName:"ExampleLAGVault", BackupVaultArn:"arn:aws:backup:us-east-1:123456789012:backup-vault:ExampleLAGVault", VaultType:"LOGICALLY_AIR_GAPPED_BACKUP_VAULT", VaultState:"AVAILABLE", EncryptionKeyArn:"arn:aws:kms:us-east-1:123456789012:key/1234abcd-12ab-34cd-56ef-1234567890ab", CreationDate:"2025-09-25T14:03:49.412000-07:00", CreatorRequestId:"86eacd7f-6f0e-4aed-8be6-20e1f8a28c1f", NumberOfRecoveryPoints:0, Locked:true, MinRetentionDays:7, MaxRetentionDays:35, LockDate:"2025-09-25T14:03:49.412000-07:00", EncryptionKeyType:"CUSTOMER_MANAGED_KMS_KEY" }TypeScript

Similarly, in console you can go to Vaults and search your AWS Backup logically air-gapped vault by name and observe the CMK encryption key.

A logically air-gapped vault with an CMK encryption key List AWS Backup vaults You can use the 'ListBackupVaults' API to view all vaults in your account. The response includes the EncryptionKeyType field for each vault, indicating whether it uses an AOK or a CMK. The following shows how to list all backup vaults using the AWS CLI:

The command returns a list of vaults with their configurations. The EncryptionKeyType field in the response shows either CUSTOMER_MANAGED_KMS_KEY or AWS_OWNED_KMS_KEY for each vault.

{ BackupVaultList:[ { BackupVaultName:"Default", BackupVaultArn:"arn:aws:backup:us-east-1:123456789012:backup-vault:Default", VaultType:"BACKUP_VAULT", CreationDate:"2020-11-05T14:15:22.329000-08:00", EncryptionKeyArn:"arn:aws:kms:us-east-1:123456789012:key/9493c201-1d02-4d2a-943a-650fee1c949e", CreatorRequestId:"Default", NumberOfRecoveryPoints:41358, Locked:false }, { BackupVaultName:"ExampleLAGVault", BackupVaultArn:"arn:aws:backup:us-east-1:974288443796:backup-vault:ExampleLAGVault", VaultType:"LOGICALLY_AIR_GAPPED_BACKUP_VAULT", VaultState:"AVAILABLE", CreationDate:"2025-09-25T14:21:01.166000-07:00", EncryptionKeyArn:"arn:aws:kms:us-east-1:974288443796:key/96b4d755-8228-44c5-8e23-7bf29017653a", CreatorRequestId:"8f5528ff-4ed8-408c-855c-5f89388330f7", NumberOfRecoveryPoints:2, Locked:true, MinRetentionDays:7, MaxRetentionDays:35, LockDate:"2025-09-25T14:21:01.166000-07:00", EncryptionKeyType:"CUSTOMER_MANAGED_KMS_KEY" } ] }TypeScript

Describe a recovery point You can use the 'DescribeRecoveryPoint' API to view details of a specific recovery point. To describe a recovery point using the AWS CLI:

Replace with the Amazon Resource Name (ARN) of the recovery point you want to describe, and with the name of the vault containing the recovery point. List recovery points by backup vault You can use the 'ListRecoveryPointsByBackupVault' API to view all recovery points within a specific backup vault. To list recovery points in a vault using the AWS CLI:

Replace with the name of the vault containing the recovery point. List recovery points by resource You can also use the 'ListRecoveryPointsByResource' API to view all recovery points for a specific resource across backup vaults. To list recovery points for a resource using the AWS CLI:

Replace with the ARN of the resource want to describe. The EncryptionKeyType field in the response of all of the preceding commands return either CUSTOMER_MANAGED_KMS_KEY or AWS_OWNED_KMS_KEY, indicating the type of encryption key used for the vault containing this recovery point. Cleaning up After you've created your logically air-gapped vault, you can clean up any resources to avoid unnecessary charges by following the steps in the Cleaning up backups section of the AWS Prescriptive Guidance. Summary In this post, we introduced CMK support for AWS Backup logically air-gapped vault. This enhancement builds upon the AWS Backup logically air-gapped vault's core security features: immutable backup storage, automatic vault locking, and cross-account sharing capabilities. This is done while enabling organizations to maintain complete control over their encryption keys. This add-on is particularly valuable for regulated industries that must meet specific key management requirements while maintaining robust data protection. Getting started with CMKs for AWS Backup logically air-gapped vault is direct through the AWS Backup console, API, or AWS CLI. Organizations can create new vaults with CMKs while preserving all security benefits of logical air-gapping. To learn more about using CMKs with AWS Backup logically air-gapped vault, visit the AWS Backup product page and documentation.

TAGS: AWS Backup, AWS Cloud Storage, AWS Key Management Service (AWS KMS), data protection, data resiliency

Desiree Brunner Desiree is a Security Solutions Architect at AWS specializing in enterprise security transformations. Her background is in data protection and security architecture, focused on a variety of security domains, including applied cryptography and regulatory compliance. She partners with enterprise customers to design and implement comprehensive security strategies that protect critical workloads while enabling business agility and innovation.

Mani Manasa Mylavarapu Mani Manasa Mylavarapu is an Engineering Manager at Amazon Web Services specializing in cloud security and data protection. She has played a key role in building enterprise-grade backup and encryption systems that safeguard customer data globally. Passionate about leadership and innovation, she combines deep technical expertise with a people-first approach to scaling secure, reliable cloud platforms.

Sabith Venkitachalapathy Sabith Venkitachalapathy is an expert in designing AWS recovery resilience solutions, ensuring disaster recovery and high availability for critical workloads. Focused on Financial Services (FSI) and Healthcare and Life Sciences (HCLS), Sabith leverages AWS to tackle industry challenges and drive innovation. He shares practical insights to help organizations build resilient, secure cloud architectures.

MSP Incentive for Customer Management: recognizes MSP Partners who help customers adopt and use cloud solutions. MSP Incentive for Strategic Services: rewards MSP Partners who help customers innovate and enhance their cloud capabilities through specific strategic AWS services. MSP Government Practice Benefit: supports MSP Partners that serve government customers through their digital transformation journey.

Get started We're excited to roll out these benefits to help you build and scale your MSP practice.

Review the FAQs in AWS Partner Central (login required). Review the updated MSP Program Guide with detailed information on benefits, incentive structures, and eligibility requirements. Available in AWS Partner Central by the end of December 2025. Attend enablement sessions in November and December 2025 (invites to follow), continuing throughout 2026.

For questions or to learn how these new benefits can support your MSP business strategy, contact your AWS PDM. Not yet a validated AWS MSP Partner? Join the AWS MSP Program to unlock revenue opportunities, technical enablement, incentives, and workshops with AWS experts. ¹ IDC, Worldwide Managed Cloud Services Forecast, 2025–2029: A Competitive Services View of Technology Outsourcing Services Markets, doc #US52883625, August 2025

TAGS: APN Launches, APN Programs, AWS MSP Partner Program, AWS Partner Network, AWS Public Sector Partners, Government, Managed Service Provider

Empowering personalized learning at scale: Loyola Marymount University's AI course companion

The traditional model of academic support—which includes office hours, tutoring centers, and teaching assistants—can't always keep pace with student needs in today's universities. Studies have shown that 85% of students have already adapted to the changing landscape of academic support by using generative AI tools, but these generic solutions often fall short of providing content tailored to their classes and personalized guidance—and their use can raise concerns about data privacy and academic integrity. Loyola Marymount University (LMU) envisioned something different: an AI companion that could speak in each professor's voice, reference the exact materials from their courses, and be available around the clock. Working with Amazon Web Services (AWS), the university developed a secure, course-specific tool called the AI Study Companion, which has earned strong faculty approval while providing students with the support they need at substantial cost savings compared to commercial alternatives. Addressing student needs with 24/7 course-specific support LMU's mission emphasizes personal connections in learning through a high-touch, individualized approach. With most students turning to generic, off-the-shelf AI tools, the university saw an opportunity. "One of the things that sparked this is, 'How do we make a better version of what's currently available?'" said Matt Frank, director of teaching, learning, and research technology at LMU. Brian Drawert, manager of research computing at LMU and the AI Study Companion's developer, explained the core issue: "AI was already trying to help students with their coursework, but doing it poorly. The challenge was giving them a chat interface that actually answered questions for their class." Modern learners also juggle complex schedules, including jobs, family commitments, and study abroad programs, making traditional faculty office hours inaccessible to many students. Building a 24/7 solution was particularly important. Building on AWS for data control and student privacy LMU's existing relationship with AWS made it a natural foundation for the project. Through ongoing data center migrations to AWS, the university had already established trust in the platform's security capabilities—which was a crucial factor given the sensitive nature of educational data. "We didn't want to use a tool where their data would be going to some outside vendor, and we had no idea what was happening with that data," Frank explained. "So, we really wanted to build an environment where LMU would be in control of the data and the security." This control was essential for meeting the Family Educational Rights and Privacy Act (FERPA) compliance requirements and protecting faculty intellectual property (IP). FERPA protects students' educational records and personal information, while faculty course content—including lectures, syllabi, and teaching materials—represents significant scholarly work and IP that required robust protection. Since the system would ingest classroom recordings containing both professor and student voices, maintaining strict access controls was non-negotiable. Building within LMU's controlled AWS environment helped keep both student data and faculty IP secure and inaccessible to external vendors or AI training models. The technical foundation of the AI Study Companion leverages multiple AWS services: Amazon Bedrock for core generative AI capabilities, Amazon Transcribe for processing classroom recordings, Amazon Simple Storage Service (Amazon S3) for data storage, Amazon Elastic Container Service (Amazon ECS) for scalability, Amazon OpenSearch for search functionality, AWS Lambda for automated processes, and AWS Web Application Firewall (WAF) for security. "We wanted to start with a robust toolkit," Frank explained. "Amazon Bedrock presented itself as that correct toolkit. We were able to build our specific needs and use cases on top of an already robust and flexible platform." Rapid development through AWS specialist collaboration The project timeline proved remarkably fast. LMU hired Drawert in late 2024 and held its first AWS meeting on January 27, 2025. Drawert made his first code commit on March 28, 2025 and by early May, LMU demonstrated a working solution at the Association of Jesuit Colleges & Universities IT Management Conference (AJCU-CITM). The timeline was particularly compressed because LMU wanted to launch the AI Study Companion in classrooms by August 2025 to start helping students at the beginning of the fall semester. AWS solutions architect Lorin Miller worked closely with Drawert throughout the development process. The collaboration began with discovery sessions to understand LMU's vision and requirements, then moved quickly into proof-of-concept development. AWS brought in specialists for AI, containers, and networking to make sure each component was properly optimized. LMU's existing cloud team also provided essential support in getting Drawert trained on the platform. Echo360, LMU's classroom capture vendor, also supported the project with an innovation grant that helped offset hosting costs. Weekly content ingestion mirrors faculty teaching approach LMU's AI Study Companion works differently from generic AI tools. Instead of providing all course information at once, the system mirrors how faculty structure learning throughout a semester. "We don't give the tool all of the information right at the start of the semester," Frank said. "We will give it the actual information that the students are going to learn in week one in week one, and then week two in week two." This approach respects faculty instructional choices while providing students with appropriately leveled guidance. In practice, this means that Amazon Transcribe processes classroom recordings and transcripts are reviewed for accuracy before being uploaded to the knowledge base—maintaining human oversight. From the students' perspective, the experience is straightforward. Students log in with LMU credentials, select their course, and can interact conversationally with the AI Study Companion on any device. They can request study guides, clarify missed material, or ask questions about course content. Beyond providing accurate answers about actual course content, the AI Study Companion has one unexpected benefit that emerged during testing. "We discovered that it not only provided class-specific information, but also captured the professor's unique style, such as their affectations, inflections, and even their jokes," Drawert explained. Early results show faculty satisfaction and cost savings The project pilot launched in August 2025 with approximately 125 students. Faculty feedback has been consistently positive, with professors noting the tool's accuracy and alignment with course material. While formal research data remains under Institutional Review Board (IRB) review, early impressions suggest strong potential. "The AI Study Companion has been an exciting and innovative project," said Kat Weaver, interim executive vice president and provost at LMU. "Faculty using the tool have reported their satisfaction with the ease of integration as well as student engagement and comprehension." Based on these encouraging results, the university plans aggressive expansion. The solution's cost-effectiveness makes this kind of expansion feasible. Compared to enterprise AI tool licenses costing $30 per student monthly, LMU's custom solution provides substantial savings while delivering more tailored functionality. This combination of lower costs and superior personalization has attracted attention from other institutions, leading to discussions about potential partnerships to adopt similar solutions. Because of this, LMU is exploring how to package the solution for broader adoption. "This could help a lot of students, potentially thousands of students within the next 12 months," Drawert noted. Supporting responsible AI innovation in higher education LMU's AI Study Companion demonstrates how institutions can leverage cloud technology and generative AI to enhance learning without sacrificing core educational values. By building on AWS infrastructure, the university developed a solution with the desired functionality of commercial alternatives while maintaining complete ownership of the educational experience. For higher education leaders considering AI adoption, LMU's approach offers a blueprint for responsible innovation that puts student success and faculty expertise at the center while leveraging cloud-native AI services. Learn how AWS helps education institutions use generative AI to deliver customized, secure student experiences. Read related stories on the AWS Public Sector Blog:

How Northwestern University built a multilingual generative AI search tool with AWS 6 EdTech AI trends: How artificial intelligence is reshaping education Benchmark Education accelerates grading and boosts student feedback with generative AI on AWS

TAGS: Artificial Intelligence, AWS Public Sector, customer story, higher education

Lorin Miller Lorin is a solutions architect at AWS who works with higher education institutions in California. She helps customers build secure, scalable solutions and develop forward-looking cloud and IT strategies. Lorin's background in artificial intelligence, IT process optimization, and cloud migration patterns enables organizations to accelerate digital innovation.

Matt Frank Matt is the director of teaching, learning, and research technology at Loyola Marymount University, where he oversees instructional technology, research computing, learning space design, and the creation of multimedia-based academic resources that enhance student learning and promote faculty outreach.

Brian Drawert Brian is the manager of research computing at Loyola Marymount University, supporting high-performance computing and data-driven research. With a background in software development and expertise in designing high-scale distributed systems, he has contributed to projects spanning computational science, virtual reality, and biochemical modeling. An avid sailor, he also applies his technical skills to marine navigation and communications.

By: Dhanya Kurup, Partner Solutions Architect – AWS By: Nirmal Mukhi, VP & Head of Engineering, GenerativeAgent – ASAPP By: Rachel Corbat, Partner Solutions Architect – AWS

The airline industry stands at a crossroads in customer service evolution. Airlines are seeing increased customer service demands as global air travel returns to pre-pandemic levels. Customers want immediate, personalized service across all channels, including mobile applications and social media platforms, airport self-service kiosks, and traditional channels like call centers and service counters. In turn, airlines need to manage operational costs and variable service demands. This transformation isn't just about incremental improvements – it's about reimagining customer experience through the lens of artificial intelligence and cloud technology. ASAPP, an AWS Advanced Partner, provides AI-powered solutions for customer service. Founded in 2014, ASAPP helps transform contact centers by automating tasks, assisting representatives, and improving customer experience. Their GenerativeAgent platform handles complex conversations while maintaining enterprise-level performance and safety standards. This blog explains how ASAPP's GenerativeAgent platform, built on AWS infrastructure, combines cloud capabilities with specialized AI to transform customer service operations. The solution uses Amazon Bedrock with Claude Sonnet and Claude Haiku foundation models, integrating with services like Amazon Connect for comprehensive capabilities. These include intelligent conversation management, human-AI collaboration, and enterprise integration. The Evolution of Airline Customer Service Research shows that existing phone-based customer service centers that rely on manual call routing and scripted responses are struggling to meet modern demands. These systems typically include basic Interactive Voice Response (IVR) menus, queue-based call distribution, and isolated customer relationship management (CRM) databases that don't integrate with reservation systems. Customers expect faster and more convenient service while interactions grow increasingly complex. Airlines must navigate sudden contact surges from weather events, time-sensitive customer needs, and international regulatory requirements. The complexity increases with the integration of multiple legacy aviation systems and processes. These include Passenger Service System (PSS) for reservations, Global Distribution Systems (GDS) for inventory, Departure Control Systems (DCS) for boarding, and Revenue Management Systems (RMS) for calculations. This fragmentation means customer service representatives often need to manually access and navigate multiple platforms to resolve even a single customer request. Current self-service solutions have limitations. First-generation IVR systems and rule-based chatbots operate on pre-defined decision trees and fixed response templates. For example, when a customer calls about a delayed flight, these systems can only provide basic status updates but cannot handle complex requests such as multi-carrier rebooking or refund processing. Industry reports confirm these limitations lead to low containment rates, resulting in increased operational costs, decreased customer satisfaction, and added complexity to interactions, ultimately frustrating both customers and service representatives. Transforming Self-Service Through AI As per this Forbes report, customer experience improvements deliver measurable business value, driving revenue 4-8% higher than industry competitors. Companies with superior customer experience reduce service costs by 15-20% and generate 5 times more revenue than competitors. ASAPP's GenerativeAgent platform helps enterprises capture these opportunities through seamless integration with AWS services, maintaining existing technology investments while adding powerful generative AI capabilities.

Figure 1 – Workflow diagram showing ASAPP's Multi Agent Architecture Key Platform Capabilities The GenerativeAgent platform transforms customer service with intelligent conversation management, human-AI collaboration, and enterprise integration. This diagram illustrates the step-by-step process of how customer interactions flow through the platform using AWS services: Customer Interaction Initiation

Customer interaction begins when contacts arrive via voice using Amazon Connect or through digital channels The system enables conversational AI-powered end-to-end interactions with seamless topic identification and switching through advanced natural language processing and contextual memory management The platform maintains interaction history, manages backend API access, and leverages document context to enable complex problem-solving capabilities

The platform uses different foundational models for different tasks: Claude Haiku for input safety and scope validation, Claude Sonnet for reasoning, tool calling, and output verification The precise choice of which models are used in the overall orchestration is influenced by the use case and customer preferences Amazon Bedrock manages model deployment, model output, and scaling

Deep integration with existing airline systems through an adaptation layer that transforms legacy API formats and data structures into standardized formats that can be efficiently processed by AI models Real-time access via APIs to operational data such as booking, flight status, and baggage enables GenerativeAgent to reason and solve customer problems Secure retrieval and handling of customer information: API transforms redact sensitive data, and any exposed sensitive data is redacted prior to storage and encrypted in transit and at rest

The system intelligently routes tasks between AI and customer service representatives Most requests including complex workflows can be handled with full automation Human-In-the-Loop Agent (HILA™) functionality for complex scenarios

Interaction outcomes feed back into the system Continuous monitoring of system performance such as which conversations may identify opportunities for improvement, summaries, structured data (including sentiment analysis, resolution rates, and use case-specific metrics) Automated scenario testing for chat and voice to simulate and evaluate system behaviors prior to production launch or to test changes

Leveraging Amazon Bedrock for Flexibility and Performance The GenerativeAgent platform uses Amazon Bedrock foundation models with a built-in orchestrator that coordinates task execution. This flexibility optimizes responses by enabling specialized model selection: one model may handle empathetic dialogue, while another model focuses on transactional tasks like flight changes. The orchestrator coordinates tasks including input safety validation, reasoning, RAG processing, and output verification, while models used for each can be selected based on specific use case requirements. This approach ensures contact centers receive tailored AI solutions without performance trade-offs or additional security complexity. Enterprise-Grade Security and Compliance ASAPP's platform maintains strong security practices and compliance frameworks including SOC 2, GDPR, and HIPAA requirements. The platform implements industry-standard encryption, data protection, and comprehensive access controls with audit trails. ASAPP provides privacy controls such as data redaction, access governance, monitoring, and incident response. This approach allows airlines to maintain robust data protection and demonstrate compliance while focusing on delivering exceptional customer experiences. Real-World Impact: A Leading Airline's Transformation A major U.S. airline's implementation of the GenerativeAgent platform demonstrates the solution's transformative potential. Completed in 39 days, the implementation delivered impressive results:

49% increase in customer self-service adoption within this timeframe 91% first-contact resolution rate, representing a 21% improvement from baseline 79% reduction in average handling time: customer issues resolved significantly faster than with representatives only 92% customer satisfaction scores, reflecting improved service quality and experience

These results showcase how AI-driven customer service can deliver tangible benefits while improving experiences for both customers and representatives. Looking Ahead ASAPP's GenerativeAgent platform will continue to evolve with innovative AI/ML technologies and foundation models. Future enhancements will enable more predictive capabilities, cross-channel integration, and personalized customer interactions. Through this evolution, airlines can expect AI to anticipate and address customer needs while maintaining the balance between automation and human expertise. Conclusion ASAPP's GenerativeAgent platform, built on Amazon Bedrock and AWS infrastructure, sets a new standard in customer service automation. For airlines and other industries, it offers enhanced customer experiences through AI that works with customer service representatives while maintaining enterprise security and scalability. This work between ASAPP and AWS shows how technology can transform challenges into opportunities for improved customer engagement and operational efficiency. To learn more about ASAPP's GenerativeAgent platform or to get started, contact ASAPP through the AWS Partner Network or check out their listing on AWS Marketplace. . .

ASAPP – AWS Partner Spotlight ASAPP is an AWS Advanced Technology Partner and AWS Generative AI Competency Partner that provides AI-powered contact center software designed to transform customer service operations through advanced artificial intelligence and automation technologies that help organizations expand contact center capacity while reducing costs and improving customer experiences. Contact ASAPP | Partner Overview | AWS Marketplace

TAGS: Amazon Bedrock, Amazon Connect, Artificial Intelligence, AWS Marketplace, Contact Center, Intermediate (200), Partner solutions, Thought Leadership, Travel and Hospitality

Introducing structured output for Custom Model Import in Amazon Bedrock

With Amazon Bedrock Custom Model Import, you can deploy and scale fine-tuned or proprietary foundation models in a fully managed, serverless environment. You can bring your own models into Amazon Bedrock, scale them securely without managing infrastructure, and integrate them with other Amazon Bedrock capabilities. Today, we are excited to announce the addition of structured output to Custom Model Import. Structured output constrains a model's generation process in real time so that every token it produces conforms to a schema you define. Rather than relying on prompt-engineering tricks or brittle post-processing scripts, you can now generate structured outputs directly at inference time. For certain production applications, the predictability of model outputs is more important than their creative flexibility. A customer service chatbot might benefit from varied, natural-sounding responses, but an order processing system needs exact, structured data that conforms to predefined schemas. Structured output bridges this gap by maintaining the intelligence of foundation models while verifying their outputs meet strict formatting requirements. This represents a shift from free-form text generation to outputs that are consistent, machine-readable, and designed for seamless integration with enterprise systems. While free-form text excels for human consumption, production applications require more precision. Businesses can't afford the ambiguity of natural language variations when their systems depend on structured outputs to reliably interface with APIs, databases, and automated workflows. In this post, you will learn how to implement structured output for Custom Model Import in Amazon Bedrock. We will cover what structured output is, how to enable it in your API calls, and how to apply it to real-world scenarios that require structured, predictable outputs. Understanding structured output Structured output, also known as constrained decoding, is a method that directs LLM outputs to conform to a predefined schema, such as valid JSON. Rather than allowing the model to freely select tokens based on probability distributions, it introduces constraints during generation that limit choices to only those that maintain structural validity. If a particular token would violate the schema by producing invalid JSON, inserting stray characters, or using an unexpected field name the structured output rejects it and requires the model to select another allowed option. This real-time validation helps keep the final output consistent, machine readable, and immediately usable by downstream applications without the need for additional post-processing. Without structured output, developers often attempt to enforce structure through prompt instructions like "Respond only in JSON." While this approach sometimes works, it remains unreliable due to the inherently probabilistic nature of LLMs. These models generate text by sampling from probability distributions, introducing natural variability that makes responses feel human but creates significant challenges for automated systems. Consider a customer support application that classifies tickets: if responses vary between "This seems like a billing issue," "I'd classify this as: Billing," and "Category = BILLING," downstream code cannot reliably interpret the results. What production systems require instead is predictable, structured output. For example:

With a response like this, your application can automatically route tickets, trigger workflows, or update databases without human intervention. By providing predictable, schema-aligned responses, structured output transforms LLMs from conversational tools into reliable system components that can be integrated with databases, APIs, and business logic. This capability opens new possibilities for automation while maintaining the intelligent reasoning that underpin the value of these models. Beyond improving reliability and simplifying post-processing, structured output offers additional benefits that strengthens performance, security and safety in production environments.

Lower token usage and faster responses: By constraining generation to a defined schema, structured output removes unnecessary verbose, free-form text, resulting in reduced token count. Because token generation is sequential, shorter outputs directly translate to faster responses and lower latency, improving overall performance and cost efficiency. Enhanced security against prompt injection: Structured output narrows the model's expression space and helps prevent it from producing arbitrary or unsafe content. Bad actors cannot inject instructions, code or unexpected text outside the defined structure. Each field must match its expected type and format, making sure outputs remain within safe boundaries. Safety and policy controls: Structured output enables you to design schemas that inherently help prevent harmful, toxic, or policy-violating content. By limiting fields to approved values, enforcing patterns, and restricting free-form text, schemas make sure outputs align with regulatory requirements.

In the next section, we will explore how structured output works with Custom Model Import in Amazon Bedrock and walks through an example of enabling it in your API calls. Using structured output with Custom Model Import in Amazon Bedrock Let's start by assuming you have already imported a Hugging Face model into Amazon Bedrock using the Custom Model Import feature. Prerequisites Before proceeding, make sure you have:

An active AWS account with access to Amazon Bedrock A custom model created in Amazon Bedrock using the Custom Model Import feature Appropriate AWS Identity and Access Management (IAM) permissions to invoke models through the Amazon Bedrock Runtime

With these prerequisites in place, let's explore how to implement structured output with your imported model. To start using structured output with a Custom Model Import in Amazon Bedrock, begin by configuring your environment. In Python, this involves creating a Bedrock Runtime client and initializing a tokenizer from your imported Hugging Face model. The Bedrock Runtime client provides access to your imported model using the Bedrock InvokeModel API. The tokenizer applies the correct chat template that aligns with the imported model, which defines how user, system, and assistant messages are combined into a single prompt, how the role markers (for example, , ) are inserted, and where the model's response should begin. By calling tokenizer.apply_chat_template(messages, tokenize=False) you can generate a prompt that matches the exact input format your model expects, which is essential for consistent and reliable inference, especially when structured encoding is enabled.

import boto3 from transformers import AutoTokenizer from botocore.config import Config

# HF model identifier imported into Bedrock hf_model_id = " >" # Example: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B model_arn = "arn:aws:bedrock: >: >:imported-model/your-model-id" region = " >"

# Initialize tokenizer aligned with your imported model tokenizer = AutoTokenizer.from_pretrained(hf_model_id)

Implementing structured output When you invoke a custom model on Amazon Bedrock, you have the option to enable structured output by adding a response_format block to the request payload. This block accepts a JSON schema that defines the structured of the model's response. During inference, the model enforces this schema in real-time, making sure that each generated token conforms to the defined structure. Below is a walkthrough demonstrating how to implement structured output using a simple address extraction task. Step 1: Define the data structure You can define your expected output using a Pydantic model, which serves as a typed contract for the data you want to extract.

class Address(BaseModel): street_number: str = Field(description="Street number") street_name: str = Field(description="Street name including type (Ave, St, Rd, etc.)") city: str = Field(description="City name") state: str = Field(description="Two-letter state abbreviation") zip_code: str = Field(description="5-digit ZIP code")Python

Step 2: Generate the JSON schema Pydantic can automatically convert your data model into a JSON schema:

schema = Address.model_json_schema() address_schema = { "name": "Address", "schema": schema }Python

This schema defines each field's type, description, and requirement, creating a blueprint that the model will follow during generation. Step 3: Prepare your input messages Format your input using the chat format expected by your model:

messages = [{ "role": "user", "content": "Extract the address: 456 Tech Boulevard, San Francisco, CA 94105" }]Python

Step 4: Apply the chat template Use your model's tokenizer to generate the formatted prompt:

Step 5: Build the request payload Create your request body, including the response_format that references your schema:

request_body = { 'prompt': prompt, 'temperature': 0.1, 'max_gen_len': 1000, 'top_p': 0.9, 'response_format': { "type": "json_schema", "json_schema": address_schema } }Python

Step 6: Invoke the model Send the request using the InvokeModel API:

Step 7: Parse the response Extract the generated text from the response:

Because the schema defines required fields, the model's response will contain them:

{ "street_number": "456", "street_name": "Tech Boulevard", "city": "San Francisco", "state": "CA", "zip_code": "94105" }Python

The output is clean, valid JSON that can be consumed directly by your application with no extra parsing, filtering, or cleanup required. Conclusion Structured output with Custom Model Import in Amazon Bedrock provides an effective way to generate structures, schema-aligned outputs from your models. By shifting validation into the model inference itself, structured output reduce the need for complex post-processing workflows and error handling code. Structured output generates outputs that are predictable and straightforward to integrate into your systems and supports a variety of use cases, for example, building financial applications that require precise data extraction, healthcare systems that need structured clinical documentation, or customer service systems that demand consistent ticket classification. Start experimenting with structured output with your Custom Model Import today and transform how your AI applications deliver consistent, production-ready results.

About the authors Manoj Selvakumar is a Generative AI Specialist Solutions Architect at AWS, where he helps organizations design, prototype, and scale AI-powered solutions in the cloud. With expertise in deep learning, scalable cloud-native systems, and multi-agent orchestration, he focuses on turning emerging innovations into production-ready architectures that drive measurable business value. He is passionate about making complex AI concepts practical and enabling customers to innovate responsibly at scale—from early experimentation to enterprise deployment. Before joining AWS, Manoj worked in consulting, delivering data science and AI solutions for enterprise clients, building end-to-end machine learning systems supported by strong MLOps practices for training, deployment, and monitoring in production. Yanyan Zhang is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. Lokeshwaran Ravi is a Senior Deep Learning Compiler Engineer at AWS, specializing in ML optimization, model acceleration, and AI security. He focuses on enhancing efficiency, reducing costs, and building secure ecosystems to democratize AI technologies, making cutting-edge ML accessible and impactful across industries. Revendra Kumar is a Senior Software Development Engineer at Amazon Web Services. In his current role, he focuses on model hosting and inference MLOps on Amazon Bedrock. Prior to this, he worked as an engineer on hosting Quantum computers on the cloud and developing infrastructure solutions for on-premises cloud environments. Outside of his professional pursuits, Revendra enjoys staying active by playing tennis and hiking. Muzart Tuman is a software engineer utilizing his experience in fields like deep learning, machine learning optimization, and AI-driven applications to help solve real-world problems in a scalable, efficient, and accessible manner. His goal is to create impactful tools that not only advance technical capabilities but also inspire meaningful change across industries and communities.

Amazon VPC Lattice is a fully managed application networking service that lets you connect, secure, and monitor communication between your services and resources within and across VPCs and accounts. Today, we announce custom domain names for Lattice resources. In this post, we will examine this feature in detail. Resource owners can now specify an Fully Qualified Domain Name (FQDN) as the custom domain name for a Lattice resource before sharing it with consumers in other VPCs and accounts. Consumers can then access the resource using this FQDN. VPC Lattice manages a private hosted zone in the consumer's VPC that enables clients to connect to the resource using the custom domain name. Resource owners have the flexibility to either use domains they own or domains they don't own such as Amazon-provided or third-party domains. Consumers can choose which types of domains they want to allow VPC Lattice to manage private hosted zones for in their VPCs. In the rest of this post, we will first go over the use cases that this feature and describe how the feature works with examples. Prerequisites This guide assumes you understand VPC Lattice fundamentals. If you need background information, review the VPC Lattice User Guide before proceeding. Key Use Cases Custom domain names for resources extend VPC Lattice's capabilities by addressing several important new use cases:

Cluster-based resources: Customers can now use resource configurations to share and access cluster-based resources across VPC and account boundaries, such as cache clusters, Kafka clusters, etc. To access cluster-based resources, a client needs to know the cluster topology and be able to query specific nodes in the cluster. With this feature, cluster owners can specify a custom domain name for each node in the cluster, thereby enabling clients to identify and query individual nodes directly. TLS-based applications: Customers can now use resource configurations to share and access TLS-based applications. In Transport Layer Security (TLS), clients must connect using a domain for which the server can present a valid certificate. With this feature, resource owners can specify the required domain name as the custom domain name in the resource configuration, ensuring seamless TLS connectivity for clients. Amazon-provided or third-party owned domains: Customers can now share resources whose domains are provided by Amazon or third parties. AWS-native resources often have endpoints of the form example.amazonaws.com or example.aws. With this feature, a resource owner can share such resources by creating a resource configuration and specifying the Amazon-provided endpoint as custom domain name. For example, an Amazon MSK cluster might have multiple broker endpoints such as b-1.example-cluster.abc123.c2.kafka.us-east-1.amazonaws.com and b-2.example-cluster.abc123.c2.kafka.us-east-1.amazonaws.com. Resource owners can create resource configurations for each broker endpoint, specifying the MSK-provided domains as custom domain names, enabling consumers to connect to specific Kafka brokers using the AWS-provided endpoints. Enhanced Security posture: Resource owners can use both verified and unverified custom domains. Verified domains are those whose ownership is verified by AWS, ensuring that a given domain is used only by its legitimate owner. They provide enhanced security posture to consumers. Unverified domains offer flexibility for development scenarios or when using Amazon/third-party domains. This dual approach maintains strong security controls for production workloads while providing flexibility for development and testing environments.

Now, let's examine the important VPC Lattice components that enable this functionality. Introducing Domain Verification Domain verification is the process of proving ownership of a domain (e.g., example.com) that you want to use as a custom domain name for resource configurations. For instance, if you are part of Example Organization, your resource and application endpoints follow patterns such as abc.example.com or xyz.example.com. You may therefore want to use FQDNs (Fully Qualified Domain Names)—complete domain names that specify the exact location in the DNS hierarchy—ending in 'example.com' as custom domain names for your resource configurations. Domain Verification streamlines this process. A Domain Verification object contains two key properties: the domain and its verification status. Verification Process: When you initiate verification for a domain, VPC Lattice automatically creates a Domain Verification object in 'pending' status and returns a TXT record configuration comprising of a (Name, Value) pair. To prove your ownership of the domain, you must create a TXT record using this pair in your domain's DNS zone. VPC Lattice checks whether the TXT record matches what was returned. Upon successful validation, the status of the Domain Verification changes to 'verified'. Integration with Resource Configuration: A resource configuration reflects its custom domain name's verification status. When you describe a resource configuration using a verified domain such as example.com, you will see the verification status as a property of the resource configuration. Subdomain Inheritance: Once you verify a domain, all its subdomains are automatically considered verified. For instance, after verifying example.com, you can immediately use FQDNs such as abc.example.com and def.xyz.example.com as custom domain names for resource configurations in your account with no additional verification required. Configuration Steps The example below captures the verification process. Step 1: Start domain verification To start domain verification from the AWS Management Console, navigate to the VPC menu, select Domain Verifications and click Start domain verification.

Figure 1 shows how to start the domain verification process using the AWS Management Console

To check the verification status, navigate to Domain Verification under the Lattice and PrivateLink menu, where you will see the TXT record name and value that need to be added to your DNS zone

Step 2: Create TXT record Add this name-value pair as a TXT record in your domain's DNS zone to prove ownership. VPC Lattice will then update the Domain Verification to 'verified'. The status change may take a few hours to complete.

Figure 3: Shows how to create TXT records for Route53 hosted zones

How It Works Now that we understand Domain Verification, let us walk through an end-to-end workflow covering three personas: resource owner, consumer (VPC owner), and service network owner. Resource Owner Experience Step 1 (Optional): Verify your domain As a resource owner, you have the flexibility to use a verified or unverified domain. The verification status is visible to consumers with whom you share the resource configuration. For verified domains, you will need a Domain Verification object with 'verified' status. Keep the Domain Verification ID ready for the next step. Step 2: Create Resource Configuration When creating a resource configuration of type 'Single' with a custom domain name:

For verified domains: Provide the Domain Verification ID in the Resource Configuration. For unverified domains: Leave the ID field blank if you are using an Amazon-provided domain, a third-party domain that you do not own, or a domain you have not verified. In this case, the resource configuration will reflect a status of 'unverified'.

Figure 4: Shows how to specify a custom domain name and verification ID for a resource configuration

Resource Configuration of type 'Group' and 'Child' A resource configuration of type 'group' does not have a custom domain name, but it dictates the custom domain names of its children. You first define a 'Group Domain' on the group. Subsequently, for each child you can specify a custom domain name that is a subdomain of the Group Domain. When using verified domains, you only need to provide the Domain Verification ID corresponding to the Group Domain. This Domain Verification automatically applies to each child. Step 1: Create the Resource Configuration Group Begin by creating the group that will contain your child configurations:

Open the Amazon VPC console Navigate to Resource configurations under PrivateLink and Lattice Click Create Resource configuration Enter a unique name for your group Select Resource group as the configuration type Enter Group Domain Name for the group

Figure 5: Shows how to create a group resource configuration group and specify a Group Domain

Step 2: Specify a custom domain name for the child resource configuration

Figure 6: Creating a child resource configuration and specifying a custom domain name

VPC Owners As a consumer, you can access resource configurations shared with you from your VPC and/or on-premises network. To do so, you must first connect the resource configuration to your VPC either directly using a Resource Endpoint (RE), or by placing the resource configuration in a service network and connecting the service network to your VPC using a Service Network Endpoint (SN-E) or Service Network Association (SN-A). Refer to the Understanding VPC Lattice Components section to learn more about RE, SN-E, and SN-A. Regardless of which connection method you choose, you have three controls to configure the types of domains for which you want VPC Lattice to manage private hosted zones in your VPC. The first control in the console is 'Enable DNS Names', which tells VPC Lattice to manage private hosted zones in your VPC. If you do not enable DNS names, no zones will be created. In the CLI and APIs, this control is called 'private-dns-enabled'. Once you have enabled DNS names, you can use the second control called 'Private DNS preferences' to configure the types of domains for which you want to allow zone creation. You can choose among several options:

'All domains' (VPC Lattice manages private hosted zones for all custom domain names), 'Verified domains only' (VPC Lattice manages private hosted zones only for custom domain names that are verified), 'Specified domains only' (VPC Lattice manages private hosted zones only if the custom domain name is in the list of domains you have allowed), and 'Verified domains and specified domains' (VPC Lattice manages private hosted zones if the custom domain name is either verified or is in the list of domains you have allowed). The default is 'Verified domains only'.

If you select either "Specified domains only" or "Verified domain and allowed domains", you can use the third control called 'Specified domains' to provide the list of domains you want to allow. You can use wildcards in your specified domains list. Service Network Owners As a Service Network Owner, you have an 'Enable DNS Name' control that overrides the Enable DNS Names control configured by a VPC owner for their SN-E or SN-A. When creating a service-network-resource-association to add a resource configuration to a service network, you must enable DNS name for the resource configuration. If you do not enable DNS name, VPC Lattice will not manage private hosted zones for that resource configuration in any VPC to which the service network is connected, even if the SN-E or SN-A has enabled DNS names. This creates a hierarchical control structure where the service network setting acts as the top-level control that overrides VPC-level settings. Step 1: From the VPC console:

Navigate to Lattice Service networks Select your service network Go to the Resource associations tab Click Create resource association Select your resource configuration Enable host names Create the association.

Navigate to Endpoints Click Create endpoint Select VPC Lattice service networks as the service category Choose your service network and VPC Configure the networking settings Create the endpoint.

Figure 8: Shows the three controls when creating a Resource endpoint

Considerations Domain Verification: Multiple AWS accounts can verify ownership of the same domain. VPC Lattice periodically checks your DNS zone for the TXT record. If the TXT record is deleted, the Domain Verification corresponding to that domain will switch to a status of 'unverified'. The 'unverified' status will be inherited by all its subdomains and resource configurations where the Domain Verification is being used. Resource Configuration Requirements: Custom domain names and Domain Verification ID must be specified at the time of creation of the resource configuration. A resource configuration cannot be modified after creation to add a custom domain name or Domain Verification ID. Group Domain: A group domain is required on a group resource configuration if you want VPC Lattice to create private hosted zones in consumer VPCs. If you do not specify a Group Domain, you can still specify custom domain names for each child. The custom domain names will be discoverable by the consumers who whom you share the group resource configuration, but VPC Lattice will not manage private hosted zones for them in the consumer VPC. Domain Conflicts: Multiple resource configurations can have the same custom domain name. However, two resource configurations with the same custom domain name cannot be added to the same service network or as child resource configurations of the same group. If a VPC already has a hosted zone for given domain, then VPC Lattice skips private hosted zone creation in that VPC for other resources with the same custom domain name. Conclusion Custom domain names for VPC Lattice resource configurations allow customers to share and access resources in VPC Lattice, such as clusters and TLS applications. By eliminating manual DNS configuration steps, this feature enables you to focus on building and deploying applications rather than managing DNS infrastructure. Domain Verification provides a scalable approach to prove ownership for domains. Automatic private hosted zone creation simplifies accessing resources. This enhancement reinforces VPC Lattice's core value proposition of simplifying application networking so that developers can focus on building and deploying applications. Whether you are running databases, clusters, or other resources, custom domain names reduce infrastructure management overhead. The feature is available in all AWS Regions where VPC Lattice is offered. For more information, visit the VPC Lattice documentation or explore the VPC Lattice Getting Started guide.

Ashish Kumar Ashish Kumar is a Senior Product Manager Tech at AWS, based out of the San Francisco Bay Area. He manages advanced cloud services in the virtual private cloud and application networking areas, such as AWS PrivateLink, VPC Lattice, and Elastic Load Balancing. Ashish has a Bachelor's from the Indian Institute of Technology Guwahati and an MBA + Master's in Business Analytics from the University of Notre Dame, USA.

Natti Swaminathan Natti is a Principal solutions architect on the US federal civilian team at AWS. He works closely with customers to build and architect mission critical solutions. Natti has extensive experience leading, architecting, and implementing high-impact technology solutions that address diverse business needs. He has a master's degree in electrical and computer engineering from Wichita State University and an MBA from North Carolina State.

Enhance retail operations using Concierto Insights & Agentic AI Platform with AWS

Retail enterprises are under increasing pressure to drive intelligent decisions, personalize experiences, and operate efficiently while managing fragmented data across ecommerce, Point of Sale (POS), warehouse, and supplier systems. Unifying and governing this data facilitates advanced analytics, AI adoption, and agile execution. In this post, we describe how the Concierto Insights & Agentic AI Platform, a third-party solution available in AWS Marketplace, helps retail and consumer goods organizations build high-quality, governed data products on AWS through domain-driven modeling and built-in quality controls. Organizations can use Concierto Insights to standardize data management processes and support their forecasting, planning, and innovation initiatives. This post demonstrates how to subscribe to the Concierto Insights & Agentic AI in AWS Marketplace and provides step-by-step guidance on how to use AWS Marketplace streamlined procurement processes to deploy a solution that helps teams design, publish, and consume trusted data products, standardizing data pipelines and powering enterprise-wide AI and decision-making. Retail persistent data challenges Retailers often face the following data challenges:

Retail search – Inadequate tagging reduces product discovery, driving search abandonment and revenue loss Inventory visibility – System gaps create stockouts, diverting customers to competitors Product planning – Data fragmentation impairs mix decisions, impacting revenue Fulfilment and returns – Data inconsistencies delay shipping and complicates return processing Supplier performance – Limited integration causes inventory and quality challenges Demand forecasting – Disconnected data creates inventory imbalances and margin loss

Solution overview Concierto Insights by Trianz is a modern analytics platform on AWS that transforms fragmented retail data into business-ready data products without coding.oncierto's federation engine queries data across multiple data sources in real time and leverages its Data Build Tool (dbt) based transformation engine to enrich data at the source instead of relying on ETL. Concierto Insights addresses critical retail challenges by enabling real-time inventory visibility, unifying planning data for better product mix decisions, streamlining fulfilment, integrating supplier performance, and supporting accurate demand forecasting. Concierto's approach consists of the following key tenets:

Federation over migration – Connect to diverse data sources without relocating data Semantic modeling – Define a common business language across data silos Data quality and governance – Enforce accuracy and compliance upfront Self-service data marketplace – Allow stakeholders to search, access, and act on data directly AI and BI assistant – Deliver insights conversationally with Concierto's Conversa and Live Intel

Figure 1: Concierto Insights high-level architecture In this post, we set up Concierto Insights using Amazon Elastic Container Service (ECS), Amazon Relational Database Service (Amazon RDS), AWS Network Firewall, Amazon Simple Storage Service (Amazon S3), and supporting network components. The following diagram depicts the overall solution architecture.

Figure 2: Concierto Insights deployment architecture The Concierto Insights solution consists of the following key components:

DNS and load balancing – A private hosted zone resolves DNS for internal and external load balancers and Amazon RDS, with traffic flowing from the Application Load Balancer (ALB) to the Network Load Balancer (NLB), then to the internal ALB for security. Private subnets –The application core runs in a virtual private cloud's (VPC's) private subnets, with traffic controlled through security groups and routed through an external ALB to a Multi-AZ Amazon ECS Fargate cluster. Internal service communications – Internal ECS services communicate securely through an internal ALB, providing service-to-service interaction within the private network. Data persistence and encryption – Amazon RDS for PostgreSQL provides highly available, Multi-AZ data persistence, with AWS Key Management Service (AWS KMS) encryption securing data at rest. Monitoring and security – Application logs and metrics are stored in Amazon CloudWatch, integrated with Amazon Simple Notification Service (Amazon SNS) for alerts. ALBs are protected by AWS WAF, and TLS provides secure internal and external communication.

Prerequisites To implement this solution, you must have the following prerequisites:

Active AWS account AWS Identity and Access Management (IAM) role with administrative permissions

Launch Concierto Insights & Agentic AI Platform in AWS Marketplace Complete the following steps to launch the Concierto Insights & Agentic AI Platform:

Go to the Concierto Insights & Agentic AI Platform in AWS Marketplace. Choose View purchase options. The Subscribe to Concierto Insights & Agentic AI screen is displayed. Choose Subscribe. Once complete the Launch Concierto Insights & Agentic AI screen is displayed. From the Launch Concierto-Enterprise Agentic AI screen, refer to the downloadable deployment guide PDF in the Follow the vendor's instructions In the Deployment template section, choose the CFT_Path link to download the AWS CloudFormation Navigate to AWS CloudFormation console. Create a new stack by uploading the template file (yaml). Specify stack details. Configure stack options. Review and create the stack. CloudFormation automatically executes the stack.

After the CloudFormation stack is complete, you will receive a follow-up email. This email will inform the user that their CloudFormation stack has been provisioned and will provide essential details such as the username, password, and the application URL to access their environment. Log in to Concierto Complete the following steps to log into Concierto Insights:

Visit the URL received in the email after you deployed the CloudFormation stack. Log in using the username and password provided in the email.

Create unstructured data source Complete the following steps to create an unstructured data source:

Choose Configure in the navigation pane and choose Data Sources.

Choose Amazon S3 or SharePoint as your data source (for this post, we use Amazon S3). Enter a data source name, description, and domain. Enter a database name and S3 location. Choose On Demand or Scheduled trigger.

Figure 4: Unstructured data source creation Create structured data source Complete the following steps to create a structured data source:

Choose Configure in the navigation pane and choose Data Sources. Choose your database type, such as PostgreSQL, MySQL, or Snowflake Enter a data source name, description, and domain. Provide connection details, including host, port, database name, schema, user, password, and optional Amazon Resource Name (ARN). Choose On Demand or Scheduled trigger.

Figure 5: Structured data source creation Create data product Complete the following steps to create a data product:

On the Discovery tab, run a SQL query and choose Publish. Review and optionally edit the auto-filled name and description. For your columns, assign business names, toggle cache, and Conversa options. Review the summary of your settings and choose Submit.

Figure 6: Data product creation The data product appears on the Published tab with all attributes. Using the Concierto Insights & Agentic AI Platform In this section, we explore the features of Concierto Insights and provide some examples of how to use it. Published tab The Published tab shows only data products created by the logged-in user.

Figure 7: Published data products Marketplace tab The Marketplace tab displays all data products across Concierto; they are marked as Accessible or Restricted based on IAM role permissions.

Figure 8: Published Marketplace data products Concierto Conversa With Conversa, users can query a data product:

Choose Load DP and choose the data product to load. The "DP Loaded Successfully" message appears, and the product is now visible in the left pane. Pick from query suggestions or enter a custom query. Press Enter or choose Go and turn on Ad-hoc mode as needed.

The results appear with a visualization. You can save or publish the results as insights.

Figure 9: Concierto Insights Conversa SQL notebook The SQL notebook feature supports SQL operations like SELECT, INSERT, UPDATE, and DELETE on data products.

Figure 10: SQL Notebook Python notebook Users can use the Python notebook feature to write and execute Python code for data exploration and visualization.

Figure 11: Python Notebook AI assistant Users can also generate queries using the AI assistant:

Choose the AI assistant icon to open the AI Assistant Choose your domain, subdomain, and data product. Enter a natural language prompt to generate a query. Choose Run to run the query.

You can publish the query results as a data product.

Figure 12: AI Assistant Live Intel: Charts With the Live Intel feature, users can choose a data product and configure chart type, axes, filters, and settings, and then save the chart with a name and description.

Figure 13: Live Intel – Charts Live Intel: Applications The Live Intel feature also provides pre-built applications with multiple charts to monitor and explore key metrics in real time.

Figure 14: Live Intel – Applications Conclusion This post demonstrates how Concierto Insights, available in AWS Marketplace, enhances AWS's comprehensive data services by creating an integrated, business-user-friendly platform. Concierto Insights amplifies AWS capabilities through enterprise governance, conversational AI, and Agentic orchestration that accelerates business outcomes. Data teams can focus on deriving insights rather than tool orchestration, while business users can naturally interact with federated data without deep technical expertise. For retail organizations, Concierto Insights streamlines federated data implementation by connecting diverse data sources like POS, inventory, and eCommerce systems through its federation-first approach and semantic modeling layer. The platform helps retailers drive customer-focused decisions across search, inventory, fulfillment, and planning while benefiting from AWS Marketplace's security, scalability, and cost optimizations. To find out more, visit Concierto Insights & Agentic AI Platform or request a demo. About the Authors

Hamdy Eed Hamdy Eed is a Sr. Partner Solution Architect with AWS and is a member of the Serverless Technical Field Community. With over 20 years of industry experience, he is well-recognized as a Subject Matter Expert in serverless and enterprise integration. Prior to AWS, he worked at NIC Inc. and held several positions at IBM. In his spare time, he coaches soccer to elementary school students and enjoys international travel. You can reach him on LinkedIn.

Karthik Rajendran Karthik Rajendran is a Senior Solutions Architect at AWS with over 25 years of extensive experience in enterprise architecture across diverse sectors including Finance, Retail, Insurance, and Energy. He specializes in architecting and building distributed, hybrid, and cloud-native applications, passionately supporting enterprise customers through their cloud migration journeys. Beyond his professional expertise, Karthik enjoys Milky Way photography and riding motorcycles.

Gaurav Mittal Gaurav Mittal is the Product Head for Concierto at Trianz, overseeing strategy and innovation for the company's cloud modernization platform. With more than 20 years of experience in data, AI, and enterprise platforms, he focuses on architecting scalable, automation-driven solutions for clients worldwide. Prior to Trianz, he led product initiatives across enterprise SaaS organizations. In his spare time, Gaurav enjoys technology blogging and mentoring startup teams. You can reach him on LinkedIn

Ramandeep Singh Ramandeep Singh is a Data & AI Solutions Lead at Trianz, helping enterprises accelerate transformation through cloud, analytics, and AI-driven innovation. With 14 years of experience in IT services and consulting, he leads Presales initiatives, GTM Offerings strategy, and Client workshops. He has co-authored whitepapers on AI and modernization. In his spare time, he follows technology trends and travels to explore new perspectives. You can reach him on LinkedIn

Vendor Insight Report: AWS recognized for AI investment, operational expertise, and customer commitment

Gartner has recently published its "Vendor Insight Report: Amazon Web Services." This comprehensive analysis authored by industry experts Jim Hare, Lydia Leong, Ed Anderson, and Dolores Ianni helps Chief Information Officers (CIOs) evaluate strategic investments in complex IT vendors. The Vendor Insight Report complements other Gartner market and product research with a high-level overview of select vendors and their top strengths and challenges. We are pleased that AWS is recognized in this year's report for our artificial intelligence investments, operational expertise, and customer commitment. Let's explore the key areas that shaped this recognition. Strategic investment in AI infrastructure and services At AWS, we believe continued strategic investment in AI is critical for enabling our customers to pioneer new products and services, transform customer experiences, and scale their businesses. This commitment is exemplified by Amazon's decision to increase capital expenditures to $100 billion in FY2025, the vast majority of which will be spent on building new state-of-the-art AI data centers. In addition, we've continued to innovate with our custom server processors and chips, including our specialized in-house AI accelerators AWS Trainium and AWS Inferentia, which provide customers with more cost effective, higher performing, and more energy-efficient AI infrastructure. Alongside this infrastructure and hardware innovation, we're continuing to expand our core AI service portfolio to offer customers the most comprehensive set of AI services, tools, and resources available in the cloud. Over the past year, we have made more than 100 AI-related product announcements. Among the most notable are Amazon Bedrock AgentCore, a platform that enables customers to build, deploy, and operate highly capable AI agents securely at scale, and Kiro, an agentic IDE that streamlines the developer journey from prototype to production through spec-driven development. We've also launched a dedicated AI agent and tool category in the AWS Marketplace, giving customers access to software with embedded agents, pre-built agents, development solutions, and professional services from AWS Partners. The Amazon Nova family of foundation models deliver frontier intelligence with industry-leading price performance, and Amazon Bedrock Guardrails provide organizations with integrated safety and privacy safeguards that work across multiple foundation models while maintaining compliance and responsible AI policies. As Amazon CEO Andy Jassy said in his 2024 Letter to Shareholders, "we continue to believe AI is a once-in-a-lifetime reinvention of everything we know, the demand is unlike anything we've seen before, and our customers, shareholders, and business will be well-served by our investing aggressively now." Additional funding for customer AI initiatives We believe in investing in our customers' success as well. When we launched the AWS Generative AI Innovation Center in 2023, we had one clear goal: help customers turn AI potential into real business value. Through this initiative, we provide dedicated teams of AWS developers, solution architects, and vertical industry experts to work side-by-side with AWS customers to build new AI applications customized for their businesses. The results are noteworthy: thousands of customers have already deployed solutions that have driven millions of dollars in productivity gains and transformed customer experiences. Now, as AI evolves toward more autonomous, agentic systems, we're strengthening our commitment with an additional $100 million investment in the program to help customers pioneer this next wave of AI innovation. Sovereign-by-design When we speak to public sector and regulated industry customers in Europe, they consistently highlight a common challenge: balancing cloud adoption with stringent regulatory requirements for data residency and operational independence. Many feel forced to choose between AWS's full capabilities and feature-limited sovereign cloud solutions. To better serve these customers, we've built the AWS European Sovereign Cloud, a new, independent cloud infrastructure that will operate with an independent governance structure, a dedicated Security Operations Center, and under a newly established parent company, and will be locally controlled in the EU, led by EU citizens, and subject to local laws. Customers will benefit from the full power of AWS—including our familiar architecture, expansive service portfolio, and APIs—while maintaining strict operational independence and complete control over data residency. Launching in 2025, the AWS European Sovereign Cloud will be the only fully-featured, independently operated sovereign cloud in the industry, backed by strong technical controls, sovereign assurances, and legal protections. Choice and flexibility Customers need the freedom to innovate and don't want to be restricted by limited technology options. AWS provides this freedom through the industry's broadest and deepest set of cloud capabilities, enabling customers to build optimized solutions that balance performance and cost-effectiveness. Our comprehensive portfolio of 240+ services spans core infrastructure services (compute, storage, and databases) to emerging technologies (artificial intelligence, machine learning, quantum computing, and IoT), complemented by 40+ purpose-built industry services and over 450 industry-specific solutions in the AWS Solutions Library. We understand that not every customer requires the same level of flexibility in their solutions. For those seeking simplicity, Amazon Lightsail provides a streamlined way to host and manage cloud-based applications, offering pre-configured Linux and Windows application and development stacks (including WordPress, Windows Server, Plesk, LAMP, and Nginx). These come with essential features like automated database management, simplified load balancers, and easy-to-use firewalls, with the option to integrate over 90 AWS services as needs evolve. Additionally, our robust suite of low-code and no-code solutions, including tools like Amazon SageMaker Canvas for AI/ML tasks, AWS Amplify for web development, and AWS Infrastructure Composer for visual application development, enable users of all technical levels to leverage sophisticated cloud services effectively. Performance, scalability, and reliability A common pain point we hear from customers revolves around infrastructure provisioning and management, particularly when it comes to scaling applications internationally. Luckily, our vast global network of data centers and our diverse infrastructure solutions provide customers with best-in-class infrastructure, designed to deliver the highest performance, scalability, and reliability for applications whether they are running in the cloud, on-premises, or at the edge. A key differentiator is our Region architecture: Every AWS Region consists of at least three independent, physically distinct Availability Zones (AZs) that are separated by meaningful distance, each with isolated and redundant power infrastructure and connectivity, ensuring that mission critical applications and workloads continue operating even during equipment failures, natural disasters, or human-caused disruptions. This resilient infrastructure is further strengthened by the AWS Global Network, which spans over 9 million kilometers of terrestrial and subsea fiber-optic cabling, delivering ultra-low latency and superior data throughput while maintaining industry-leading reliability. End-to-end security Lastly, customers are looking for ways to move security from something that is tacked on at the end of a process, slowing innovation, to something that is automated and integral to everything they do. At AWS, security is our top priority, embedded in our organizational structure, and built into every product roadmap, engineering plan, and weekly stand-up meeting. This commitment has resulted in what we believe is the most secure global cloud infrastructure on which to build, migrate, and manage applications and workloads. Our comprehensive security approach is backed by over 300+ cloud security tools and validated by millions of customers, including those in the most security-sensitive industries like government, healthcare, and financial services. Looking forward A new era of IT is unfolding before us, characterized by the intelligent automation of routine, day-to-day tasks enabling teams to focus on innovation, revenue generation, and customer satisfaction, and new agentic systems that can act autonomously across digital boundaries while maintaining the security, reliability, and governance standards required for enterprise deployment. AWS remains at the forefront of this revolution, investing heavily in technologies and services that enable customers to capitalize on this paradigm shift. Our commitment remains unwavering: to deliver the most comprehensive and innovative cloud platform in the industry while helping customers transform their businesses. To learn more about how AWS can help your organization innovate with confidence, contact your AWS account team or visit our website. For detailed insights, access the complete 2025 Gartner AWS Vendor Insights report. Gartner does not endorse any vendor, product or service depicted in its research publications and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose. GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and is used herein with permission. All rights reserved.

Christopher Swenson Christopher Swenson is a Principal Product Marketing and Messaging Manager at Amazon Web Services based in Seattle, WA, where he helps drive messaging strategy and execution.

By: David MacDonald, Solutions Architect – AWS By: Justin Soong, Founder & CEO – Authsignal By: Ashutosh Bhadauriya, Developer Experience Engineer – Authsignal

Authentication needs to be both secure and user-friendly. Users expect smooth, seamless experiences while businesses require strong security controls to protect accounts and transactions from fraud. In this post, we'll explore how Amazon Cognito and Authsignal work together to create great authentication experiences that adapt to user context and business needs. Learn how non-technical business users can configure and optimize authentication flows throughout the user journey to manage both fraud and user experience. Choosing the right authentication approach Authentication can rely on three fundamental factors:

Knowledge: Something you know (such as a password) Possession: Something you have (such as a security token) Inherence: Something you are (such as a fingerprint)

Combining these factors enables multi-factor authentication (MFA), which helps mitigate threats such as credential stuffing. Common implementations pair knowledge factors (passwords) with possession factors (one-time passwords (OTPs)). When selecting authentication methods, consider:

Assurance: how difficult it is for someone other than the legitimate user to use or copy the authentication method Friction: how much effort users need to spend to complete the authentication process Cost: initial setup costs, ongoing operational expenses (such as SMS message fees), and maintenance requirements Availability: whether users have reliable access to the authentication method across different devices and locations

Your business needs to balance authentication methods against both risk tolerance and business needs. For example, high-value financial transactions or sensitive data access might warrant high-assurance authentication methods such as hardware security keys despite their increased friction and cost. However, lower-risk activities like viewing public account information might only require basic authentication methods to maintain user convenience. Businesses also need to adapt as authentication method effectiveness evolves with technology and threats. For example, passkeys use public-key cryptography to combine possession (device bound) with inherence (biometrics), enabling low-friction passwordless authentication. Meanwhile, traditional methods like Short Message Service (SMS) OTP have become less secure as interception techniques advance. Businesses can maintain strong security while minimizing friction through adaptive authentication. This approach adjusts authentication requirements based on risk signals such as:

Device characteristics: new or unfamiliar devices, browser fingerprints, and operating system integrity Location patterns: unexpected countries, unusual locations for the user, and IP reputation Behavioral patterns: sign-in velocity, unusual times of day, and abnormal navigation patterns Transaction patterns: amount thresholds, new recipients, and unusual merchant categories Session attributes: connection type, virtual private network (VPN) or proxy usage, and session duration and activity Account history: previous suspicious activity, account age, and historical transaction patterns

For example, sign-ins from new devices in unexpected locations might trigger additional steps such as a push notification for authentication through your mobile application, while routine access patterns follow a streamlined flow. However, authentication can extend beyond the initial sign-in step. Applications can benefit from a more dynamic approach to security. The right authentication method at the right time Single sign-in authentication requires both high-assurance authentication methods and robust session management to protect against threat actors who might attempt to intercept authenticated sessions. However, forcing users to complete high-assurance authentication steps every time they sign in creates unnecessary friction—imagine requiring a hardware security key just to check your account balance. A more effective approach implements contextual authentication throughout the user journey, balancing security and user experience based on the sensitivity of each action. This lets users start with streamlined authentication for basic access, then complete additional authentication only when attempting higher-risk actions such as large payments or changing security settings. You can enhance this approach by incorporating risk signals and transaction information to determine the most appropriate authentication method for each user action. High-value actions combined with suspicious signals might require stronger authentication methods. For example, when a user attempts a bank transfer over $1,000 from a new country, the application requires hardware token authentication before proceeding. By combining adaptive and continuous authentication, businesses can implement low-friction sign-in processes when risk signals permit, while enforcing additional authentication requirements for subsequent high-value actions based on both signals and business risk tolerance. But how can you implement adaptive and continuous authentication in your application to create great authentication experiences? Let's explore how to architect a solution to this challenge. Architecting for adaptive and continuous authentication A robust adaptive authentication system combines user actions, risk signals, and business rules to determine appropriate authentication requirements at the right time in the user journey. Figure 1 illustrates this architecture, where an authentication rules engine processes user actions (like sign-in attempts and transactions), risk signals, and transaction data to determine the necessary authentication methods. Key authentication decision points might include:

Initial sign-in attempts High-value transactions Security setting changes Access to sensitive data Account recovery processes

Figure 1: User actions, including sign-in, are combined with signals to trigger a rules engine to decide on the right authentication method before allowing the user to proceed For this architecture to effectively serve your business needs, it should include the following characteristics:

Business-friendly configuration: The rules engine must be configurable by business users who understand risk tolerance and user impact, not only developers. While developers implement the initial integration, business users should be able to adjust authentication rules without code changes. For example, a fraud analyst should be able to lower transaction thresholds or strengthen authentication requirements for specific regions in response to emerging threats. Extensibility: The system must evolve alongside both emerging threats and authentication technologies. This means building a flexible architecture that can incorporate new signal sources ranging from basic IP address validation to sophisticated device fingerprinting and behavioral analysis. As authentication technology advances, your system should readily adopt new methods like WhatsApp OTP without major architectural changes. Finally, it must support custom business logic and risk models that reflect your organization's unique security requirements and risk tolerance. Observability: Effective authentication management requires clear visibility into system behavior and outcomes. Business users need detailed insights into authentication patterns across different methods, helping them understand how users interact with security measures. They should be able to analyze which rules are triggering and how effectively they mitigate risks. Understanding user friction points and drop-off rates helps optimize the balance between security and usability. Additionally, comprehensive monitoring helps identify emerging fraud patterns and anomalies, enabling proactive security adjustments before they impact your business significantly.

Now that you know the required characteristics, how can you build an adaptive and continuous authentication system on AWS? Better together: Amazon Cognito and Authsignal Authsignal builds on the Amazon Cognito customer identity and access management (CIAM) user pool service by providing its own suite of authentication methods. Amazon Cognito sends authentication requests to Authsignal's rules engine during sign-in, where the engine selects which Authsignal authentication method to present. As shown in Figure 2, your application also sends ongoing user actions, transaction data, and user signals to Authsignal's rules engine. The engine makes authentication decisions based on where the user is in their journey, enabling adaptive and continuous authentication. This integration creates a system that balances security with user experience.

Figure 2: Authsignal integrates with Amazon Cognito to drive authentication at all stages of the user journey The suite of authentication methods offered by Authsignal gives additional options for adjusting the balance between assurance, friction, cost, and availability. Methods like WhatsApp OTP offer a cost-effective alternative to SMS authentication while also reducing interception threats such as SIM swapping. High assurance biometric verification incorporating liveness detection gives businesses options to mitigate evolving deepfake threats. You can integrate Authsignal with Amazon Cognito at any point — whether you're starting with a new Amazon Cognito user pool or extending your existing one. For adaptive authentication during sign-in, you can continue to use Amazon Cognito Adaptive Authentication, so that you can configure your user pool to block suspicious sign-ins or add second-factor authentication based on risk levels. Alternatively, you can use Authsignal's Amazon Cognito integration to select from Authsignal's suite of authentication methods based on user signals and your own business rules. You can then integrate with Authsignal's pre-built UI to progressively enhance security by adding continuous authentication for specific high-risk actions after sign-in. This flexible approach preserves your existing Amazon Cognito implementation while strengthening authentication controls where they matter most. After being integrated, Authsignal's no-code rules and policy engine can be used to define conditional logic based on signals and transaction data that drives the right level of authentication for a given action. This gives your non-technical business users the ability to update thresholds and mitigate risks without requiring developers to update code. And with a unified user timeline that displays all authentication activities and outcomes, your users can analyze authentication patterns and optimize policies based on real-world usage data. Conclusion Creating great authentication experiences requires balancing security and user experience throughout the user journey. By combining the robust CIAM capabilities provided by Amazon Cognito with Authsignal's authentication service, businesses can implement continuous and adaptive authentication that evolves with their needs. Ready to enhance your authentication experience? Get started with Authsignal in AWS Marketplace today. .

Authsignal – AWS Partner Spotlight Authsignal provides drop-in passwordless authentication, passkeys, biometrics, and adaptive MFA without the complexity of migrations, development overhead, and operational costs of building from scratch. Contact Authsignal | Partner Overview | AWS Marketplace

TAGS: Amazon Cognito, AWS Partner Network, Best Practices, Foundational (100), Partner solutions, Thought Leadership

Democratizing AI: How Thomson Reuters Open Arena supports no-code AI for every professional with Amazon Bedrock

This post is cowritten by Laura Skylaki, Vaibhav Goswami, Ramdev Wudali and Sahar El Khoury from Thomson Reuters. Thomson Reuters (TR) is a leading AI and technology company dedicated to delivering trusted content and workflow automation solutions. With over 150 years of expertise, TR provides essential solutions across legal, tax, accounting, risk, trade, and media sectors in a fast-evolving world. TR recognized early that AI adoption would fundamentally transform professional work. According to TR's 2025 Future of Professionals Report, 80% of professionals anticipate AI significantly impacting their work within five years, with projected productivity gains of up to 12 hours per week by 2029. To unlock this immense potential, TR needed a solution to democratize AI creation across its organization. In this blog post, we explore how TR addressed key business use cases with Open Arena, a highly scalable and flexible no-code AI solution powered by Amazon Bedrock and other AWS services such as Amazon OpenSearch Service, Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB, and AWS Lambda. We'll explain how TR used AWS services to build this solution, including how the architecture was designed, the use cases it solves, and the business profiles that use it. The system demonstrates TR's successful approach of using existing TR services for rapid launches while supporting thousands of users, showcasing how organizations can democratize AI access and support business profiles (for example, AI explorers and SMEs) to create applications without coding expertise. Introducing Open Arena: No-code AI for all TR introduced Open Arena to non-technical professionals to create their own customized AI solutions. With Open Arena users can use cutting-edge AI powered by Amazon Bedrock in a no-code environment, exemplifying TR's commitment to democratizing AI access. Today, Open Arena supports:

High adoption: ~70% employee adoption, with 19,000 monthly active users. Custom solutions: Thousands of customized AI solutions created without coding, used for internal workflows or integrated into TR products for customers. Self-served functionality: 100% self-served functionality, so that users, irrespective of technical background, can develop, evaluate, and deploy generative AI solutions.

The Open Arena journey: From prototype to enterprise solution Conceived as a rapid prototype, Open Arena was developed in under six weeks at the onset of the generative AI boom in early 2023 by TR Labs – TR's dedicated applied research division focused on the research, development, and application of AI and emerging trends in technologies. The goal was to support internal team exploration of large language models (LLMs) and discover unique use cases by merging LLM capabilities with TR company data. Open Arena's introduction significantly increased AI awareness, fostered developer-SME collaboration for groundbreaking concepts, and accelerated AI capability development for TR products. The rapid success and demand for new features quickly highlighted Open Arena's potential for AI democratization, so TR developed an enterprise version of Open Arena. Built on the TR AI Platform, Open Arena enterprise version offers secure, scalable, and standardized services covering the entire AI development lifecycle, significantly accelerating time to production. The Open Arena enterprise version uses existing system capabilities for enhanced data access controls, standardized service access, and compliance with TR's governance and ethical standards. This version introduced self-served capabilities so that every user, irrespective of their technical ability, can create, evaluate, and deploy customized AI solutions in a no-code environment.

"The foundation of the AI Platform has always been about empowerment; in the early days it was about empowering Data Scientists but with the rise of Gen AI, the platform adapted and evolved on empowering users of any background to leverage and create AI Solutions." – Maria Apazoglou, Head of AI Engineering, CoCounsel

As of July 2025, the TR Enterprise AI Platform consists of 15 services spanning the entire AI development lifecycle and user personas. Open Arena remains one of its most popular, serving 19,000 users each month, with increasing monthly usage. Addressing key enterprise AI challenges across user types Using the TR Enterprise AI Platform, Open Arena helped thousands of professionals transition into using generative AI. AI-powered innovation is now readily in the hands of everyone, not just AI scientists. Open Arena successfully addresses four critical enterprise AI challenges:

Enablement: Delivers AI solution building with consistent LLM and service provider experience and support for various user personas, including non-technical. Security and quality: Streamlines AI solution quality tracking using evaluation and monitoring services, whilst complying with data governance and ethics policies. Speed and reusability: Automates workflows and uses existing AI solutions and prompts. Resources and cost management: Tracks and displays generative AI solution resource consumption, supporting transparency and efficiency.

The solution currently supports several AI experiences, including tech support, content creation, coding assistance, data extraction and analysis, proof reading, project management, content summarization, personal development, translation, and problem solving, catering to different user needs across the organization.

Figure 1. Examples of Open Arena use cases. AI explorers use Open Arena to speed up day-to-day tasks, such as summarizing documents, engaging in LLM chat, building custom workflows, and comparing AI models. AI creators and Subject Matter Experts (SMEs) use Open Arena to build custom AI workflows and experiences and to evaluate solutions without requiring coding knowledge. Meanwhile, developers can develop and deploy new AI solutions at speed, training models, creating new AI skills, and deploying AI capabilities. Why Thomson Reuters selected AWS for Open Arena TR strategically chose AWS as a primary cloud provider for Open Arena based on several critical factors:

Comprehensive AI/ML capabilities: Amazon Bedrock offers easy access to a choice of high-performing foundation models from leading AI companies like AI21 Labs, Anthropic, Cohere, DeepSeek, Luma AI, Meta, Mistral AI, OpenAI, Qwen, Stability AI, TwelveLabs, Writer, and Amazon. It supports simple chat and complex RAG workflows, and integrates seamlessly with TR's existing Enterprise AI Platform. Enterprise-grade security and governance: Advanced security controls, model access using RBAC, data handling with enhanced security features, single sign-on (SSO) enabled, and clear operational and user data separation across AWS accounts. Scalable infrastructure: Serverless architecture for automatic scaling, pay-per-use pricing for cost optimization, and global availability with low latency. Existing relationship and expertise: Strong, established relationship between TR and AWS, existing Enterprise AI Platform on AWS, and deep AWS expertise within TR's technical teams.

"Our long-standing partnership with AWS and their robust, flexible and innovative services made them the natural choice to power Open Arena and accelerate our AI initiatives." – Maria Apazoglou, Head of AI Engineering, CoCounsel

Open Arena architecture: Scalability, extensibility, and security Designed for a broad enterprise audience, Open Arena prioritizes scalability, extensibility and security while maintaining simplicity for non-technical users to create and deploy AI solutions. The following diagram illustrates the architecture of Open Arena.

Figure 2. Architecture design of Open Arena. The architecture design facilitates enterprise-grade performance with clear separation between capability and usage, aligning with TR's enterprise cost and usage tracking requirements. The following are key components of the solution architecture:

No-code interface: Intuitive UI, visual workflow builder, pre-built templates, drag-and-drop functionality. Enterprise integration: Seamless integration with TR's Enterprise AI Platform, SSO enabled, data handling with enhanced security, clear data separation. Solution management: Searchable repository, public/private sharing, version control, usage analytics.

TR developed Open Arena using AWS services such as Amazon Bedrock, Amazon OpenSearch, Amazon DynamoDB, Amazon API Gateway, AWS Lambda, and AWS Step Functions. It uses Amazon Bedrock for foundational model interactions, supporting simple chat and complex Retrieval-Augmented Generation (RAG) tasks. Open Arena uses Amazon Bedrock Flows as the custom workflow builder where users can drag-and-drop components like prompts, agents, knowledge bases and Lambda functions to create sophisticated AI workflows without coding. The system also integrates with AWS OpenSearch for knowledge bases and external APIs for advanced agent capabilities. For data separation, orchestration is managed using the Enterprise AI Platform AWS account, capturing operational data. Flow instances and user-specific data reside in the user's dedicated AWS account, stored in a database. Each user's data and workflow executions are isolated within their respective AWS accounts, which is required for complying with Thomson Reuters data sovereignty and enterprise security policies with strict regional controls. The system integrates with Thomson Reuters SSO solution to automatically identify users and grant secure, private access to foundational models. The orchestration layer, centrally hosted within the Enterprise AI Platform AWS account, manages AI workflow activities, including scheduling, deployment, resource provisioning, and governance across user environments. The system features fully automated provisioning of Amazon Bedrock Flows directly within each user's AWS account, avoiding manual setup and accelerating time to value. Using AWS Lambda for serverless compute and DynamoDB for scalable, low-latency storage, the system dynamically allocates resources based on real-time demand. This architecture makes sure prompt flows and supporting infrastructure are deployed and scaled to match workload fluctuations, optimizing performance, cost, and user experience.

"Our decision to adopt a cross-account architecture was driven by a commitment to enterprise security and operational excellence. By isolating orchestration from execution, we make sure that each user's data remains private and secure within their own AWS account, while still delivering a seamless, centrally-managed experience. This design empowers organizations to innovate rapidly without compromising compliance or control." – Thomson Reuters' architecture team

Evolution of Open Arena: From classic to Amazon Bedrock Flows-powered chain builder Open Arena has evolved to cater to varying levels of user sophistication:

Open Arena v1 (Classic): Features a form-based interface for simple prompt customization and basic AI workflow deployment within a single AWS account. Its simplicity appeals to novice users for straightforward use cases, though with limited advanced capabilities. Open Arena v2 (Chain Builder): Introduces a robust, visual workflow builder interface, enabling users to design complex, multi-step AI workflows using drag-and-drop components. With support for advanced node types, parallel execution, and seamless cross-account deployment, Chain Builder dramatically expands the system's capabilities and accessibility for non-technical users.

Thomson Reuters uses Amazon Bedrock Flows as a core feature of Chain Builder. Users can define, customize, and deploy AI-driven workflows using Amazon Bedrock models. Bedrock Flows supports advanced workflows combining multiple prompt nodes, incorporating AWS Lambda functions, and supporting sophisticated RAG pipelines. Operating seamlessly across user AWS accounts, Bedrock Flows facilitates secure, scalable execution of personalized AI solutions, serving as the fundamental engine for the Chain Builder workflows and driving TR's ability to deliver robust, enterprise-grade automation and innovation. What's next? TR continues to expand Open Arena's capabilities through the strategic partnership with AWS, focusing on:

Driving further adoption of Open Arena's DIY capabilities. Enhancing flexibility for workflow creation in Chain Builder with custom components, such as inline scripts. Developing new templates to represent common tasks and workflows. Enhancing collaboration features within Open Arena. Extending multimodal capabilities and model integration. Expanding into new use cases across the enterprise.

"From innovating new product ideas to reimagining daily tasks for Thomson Reuters employees, we continue to push the boundaries of what's possible with Open Arena." – Maria Apazoglou, Head of AI Engineering, CoCounsel

Conclusion In this blog post, we explored how Thomson Reuters' Open Arena demonstrates the successful democratization of AI across an enterprise by using AWS services, particularly Amazon Bedrock and Bedrock Flows. With 19,000 monthly active users and 70% employee adoption, the system proves that no-code AI solutions can deliver enterprise-scale impact while maintaining security and governance standards. By combining the robust infrastructure of AWS with innovative architecture design, TR has created a blueprint for AI democratization that empowers professionals across technical skill levels to harness generative AI for their daily work. As Open Arena continues to evolve, it exemplifies how strategic cloud partnerships can accelerate AI adoption and transform how organizations approach innovation with generative AI.

About the authors Laura Skylaki, PhD, leads the Enterprise AI Platform at Thomson Reuters, driving the development of GenAI services that accelerate the creation, testing and deployment of AI solutions, enhancing product value. A recognized expert with a doctorate in stem cell bioinformatics, her extensive experience in AI research and practical application spans legal, tax, and biotech domains. Her machine learning work is published in leading academic journals, and she is a frequent speaker on AI and machine learning Vaibhav Goswami is a Lead Software Engineer on the AI Platform team at Thomson Reuters, where he leads the development of the Generative AI Platform that empowers users to build and deploy generative AI solutions at scale. With expertise in building production-grade AI systems, he focuses on creating tools and infrastructure that democratize access to cutting-edge AI capabilities across the enterprise. Ramdev Wudali is a Distinguished Engineer, helping architect and build the AI/ML Platform to enable the Enterprise user, data scientists and researchers to develop Generative AI and machine learning solutions by democratizing access to tools and LLMs. In his spare time, he loves to fold paper to create origami tessellations, and wearing irreverent T-shirts As the director of AI Platform Adoption and Training, Sahar El Khoury guides users to seamlessly onboard and successfully use the platform services, drawing on her experience in AI and data analysis across robotics (PhD), financial markets, and media. Vu San Ha Huynh is a Solutions Architect at AWS with a PhD in Computer Science. He helps large Enterprise customers drive innovation across different domains with a focus on AI/ML and Generative AI solutions. Paul Wright is a Senior Technical Account Manager, with over 20 years experience in the IT industry and over 7 years of dedicated cloud focus. Paul has helped some of the largest enterprise customers grow their business and improve their operational excellence. In his spare time Paul is a huge football and NFL fan. Mike Bezak is a Senior Technical Account Manager in AWS Enterprise Support. He has over 20 years of experience in information technology, primarily disaster recovery and systems administration. Mike's current focus is helping customers streamline and optimize their AWS Cloud journey. Outside of AWS, Mike enjoys spending time with family & friends.

IPv4 address exhaustion is a challenge in modern networking, as most IPv4 addresses have been depleted with the growth of the internet. Previously, AWS Lambda only supported inbound and outbound connectivity over IPv4, but it has since introduced support for dual-stack endpoints, so that you can transition from IPv4 to IPv6. AWS continues to add support for IPv6, recently announcing support for inbound IPv6 connectivity over AWS PrivateLink, and dual-stack endpoint support for Amazon API Gateway. With these IPv6 capabilities now available in Lambda, you should understand how to use them effectively. This post examines the benefits of transitioning Lambda functions to IPv6, provides practical guidance for implementing dual-stack support in your Lambda environment, and considerations for maintaining compatibility with existing systems during migration. Benefits of transitioning You can transition to IPv6 to future-proof your overall architecture by preparing ahead of the broader transition to IPv6, and establish compatibility with IPv6 clients or services. IPv6 also eliminates the need for a NAT gateway when the Lambda functions need internet connectivity from a private subnet in your Amazon Virtual Private Cloud (Amazon VPC). Lambda functions can direct traffic to the egress-only internet gateway, potentially eliminating the NAT gateway and its associated charges and streamlining network design. This transition provides cost savings, as egress-only internet gateways are free to use, as opposed to NAT gateways that incurs an hourly charge. Furthermore, IPv6 offers improved network efficiency by eliminating NAT translation overhead, so that Lambda functions can establish direct connections with clients. IPv6 also has more advantages such as native Quality of Service (QoS), which streamlines header structure and reduces packet fragmentations. Architectural implications Lambda functions are often deployed inside of a VPC to access VPC resources. For VPC Lambda functions to access the internet, routing traffic through an NAT gateway is a common approach. For Lambda functions with IPv6 support, Lambda functions can now route traffic directly through the egress-only internet gateway, which eliminates the need for a NAT gateway and the extra hop, as shown in the following figures.

Figure 1. Lambda internet connectivity through a NAT Gateway (IPv4) and Lambda internet connectivity through an egress-only internet gateway (IPv6).

Once the egress-only internet gateway is in place, you need to update the route table to reflect this. If you have used 0.0.0.0/0 as the default route for IPv4 traffic, you should add ::/0 as the default route for IPv6 traffic. The following image shows the updated route table.

Figure 2. Lambda private subnet routing tables for an NAT Gateway (IPv4) as opposed to a dual-stack including an egress-only internet gateway (IPv6)

If you are using Lambda function URLs, no transition is needed. Lambda function URLs are inherently IPv6-capable and can be accessed by IPv6 clients without needing architectural changes or modifications. This IPv6 compatibility for function URLs operates independently of your Lambda function's VPC configuration, and clients can reach your Lambda function URLs over IPv6 even when dual-stack is not enabled in your VPC. For Lambda functions that interact exclusively with AWS services through internal traffic, IPv6 offers limited benefits. For example, in an architecture where a Lambda function processes requests from Amazon API Gateway and queries a database hosted on Amazon Relational Database Service (Amazon RDS), no architectural change is expected. Internal traffic routes using the RDS cluster endpoint and Lambda Amazon Resource Name (ARN), not IP addresses, as shown in the following figure.

Figure 3. A common architecture pattern where Lambda processes events from API Gateway and reads/writes to Amazon RDS. You reference the Lambda function ARN and RDS cluster endpoint instead of IPv4/IPv6 addresses.

Transitioning from IPv4 to IPv6 By default, Lambda functions communicate over IPv4 to their destinations. For Lambda functions to communicate with IPv6 destinations, dual-stack VPC configuration is needed. This allows Lambda functions to communicate over both IPv4 and IPv6. If your VPC does not have IPv6 support, then you need to first add IPv6 support for your VPC. You need to follow these steps to enable IPv6 traffic for a Lambda function:

Assign IPv6 block to VPC: You need to edit the existing VPC CIDRs to add an IPv6 CIDR block. If you select the option of Amazon-provided IPv6 CIDR block, then you are assigned a /56 IPv6 CIDR block from the Amazon pool of IPv6 addresses. You also have the option to assign an Amazon VPC IP Address Manager allocated or your own IPv6 CIDR block. Assign IPv6 block to Subnets: After assigning an IPv6 CIDR block to the VPC, you must manually configure IPv6 CIDR blocks for each existing subnet, with each subnet receiving a portion of the VPC's IPv6 address space. Update route tables: For your Lambda function's IPv6 traffic to reach the internet, you need to add a route (::/0) to the egress-only internet gateway. Update security groups: By default, security groups allow all outbound traffic. To restrict outbound IPv6 traffic from your Lambda function, you must remove the default egress rule and add specific restrictive outbound rules. For inbound traffic, security group rules are needed when your Lambda function receives direct network connections, such as traffic through AWS PrivateLink connections. Enable IPv6 dual-stack on the Lambda function: When you assign IPv6 addresses for your Lambda function's subnet, you can enable IPv6 dual-stack for the Lambda function. Then, Lambda creates new Elastic network interfaces (ENI) with IPv4 and IPv6 protocols with both IPv4 and IPv6 addresses. Although most updates to the Lambda function have zero downtime, enabling dual-stack may cause disruption in connectivity. To prevent downtime during the transition, we recommend using Lambda versions and aliases to implement a blue/green deployment strategy. You can publish your IPv6-enabled Lambda function as a new version while keeping the current version active and serve traffic through the alias. After testing the new IPv6 version, you can update the alias to switch the traffic. This approach provides a rollback capability, and you can revert the alias to point back to the previous version if needed.

When you have completed these steps, your Lambda function can support dual-stack networking and communicate over both IPv4 and IPv6. Conclusion In this post, we covered the benefits of transitioning your AWS Lambda functions from IPv4 to IPv6, the architectural implications, and steps for how you could make the transition.We recommend transitioning your Lambda functions to support both IPv4 and IPv6 traffic to gain its benefits. The Lambda IPv6 support helps address IPv4 exhaustion while providing cost savings and network clarification. Once organizations transition to supporting only IPv6 traffic, they can eliminate NAT gateways for Lambda functions needing internet access, thus reducing both costs and architectural complexity. As AWS expands IPv6 support across services, transitioning Lambda functions to dual-stack networking positions organizations for long-term compatibility while delivering immediate operational benefits. For more information on how to enable IPv6 access for Lambda functions in dual-stack VPC, see the Lambda documentation. For more serverless learning resources, visit Serverless Land.

AWS Partners Demonstrate Enterprise AI Agent Solutions with Amazon Bedrock AgentCore

By: Veena Vasudevan, Principal Partner Solutions Architect, Data & AI – AWS by: Salman Taherian, Senior Partner Development Specialist, Data & AI – AWS Following the general availability of Amazon Bedrock AgentCore on October 13, 2025, AWS Partners are seizing first-mover advantage in the enterprise AI agent market, rapidly turning proof-of-concept innovations into production solutions. AgentCore provides the foundation to build, deploy, and operate agents using any framework or model without infrastructure management by offering services for Runtime, Memory, Gateway, Identity, and Observability.

Amazon Bedrock AgentCore is driving unprecedented enterprise transformation. These select success stories showcase how early-adopter partners are delivering immense value for their customers today. Caylent Optimizes CloudZero Advisor CloudZero provides a FinOps solution that enables enterprises to understand and optimize cloud costs in real time by connecting engineering decisions directly to business outcomes. As more companies onboarded to their platform, CloudZero faced growing challenges scaling their AI-driven cloud cost optimization and infrastructure analysis capabilities. Operations were increasingly slowed by complexity and high developer cognitive load required to manage and tune custom agentic AI workflows processing petabytes of cost and usage data.

Caylent implemented CloudZero Advisor, an agentic platform built on AgentCore Runtime that orchestrates five specialized agents—Cost/Billing, Cloud Pricing, Benchmarking, CostFormation, and Knowledge Base—to deliver comprehensive cloud cost analysis through natural language interactions. AgentCore Memory manages context across cost analysis sessions, while AgentCore Gateway services enable seamless integration with cloud resources. By migrating from Amazon Bedrock Agents to AgentCore, CloudZero removed the complexity of maintaining custom memory systems and achieved 5x faster response times, delivering 2–4s Time to First Token (TTFT) per step versus 30+s previously.

"AgentCore enabled us to build CloudZero Advisor into a production-ready, agentic platform that delivers real speed and efficiency. The performance gains we're seeing turn agentic AI from a proof of concept into a core part of CloudZero's data platform, unlocking use cases that were previously too slow or costly and scaling seamlessly to meet 50x growth," said Randall Hunt, CTO, Caylent.

The implementation reduced developer cognitive load by 75% through automated memory management and standardized CI/CD pipelines that simplified code maintenance. DeepEval validation and built-in failover mechanisms strengthened system resilience, enabling seamless multi-agent collaboration with secure multi-tenant isolation for complex cloud optimization workloads. Cisco Brings Zero Trust Security to AI Agents for Enterprise Customers Traditional identity and security systems weren't designed for AI agents, creating governance challenges as enterprises adopt agentic AI. Organizations need visibility over agent-tool communications and data exfiltration prevention.

Cisco partners with AWS, integrating Cisco Duo and Cisco Secure Access with AgentCore to build comprehensive zero trust architectures for agentic AI. The solution combines Cisco Duo with AgentCore Identity to create an identity fabric with granular access policies and just in time access for human and non-human identities, including machine and agentic identities. Cisco Secure Access's MCP semantic inspection proxy, integrated with AgentCore Gateway, provides a unique capability—the ability to intercept and inspect all MCP messages between AI agents and tools, detecting threats such as tool poisoning and prompt injections while preventing data exfiltration through DLP controls.

The integration extends zero trust principles to AI agents with the same rigorous policies as humans. AgentCore Gateway facilitates secure discovery and access to MCP servers and tools with zero trust policies and user consent. Cisco Duo integration provides strong authentication and adaptive access controls for both human users and AI agents. Organizations monitor MCP communications, view threats detected and blocked and enforce policies through Cisco Secure Access's dashboard. AgentCore Observability tracks agent behavior and detects anomalies.

"Cisco is teaming up with AWS AgentCore and creating secure identities for AI agents", said Pradeep Musugu, Sr. Director, IT Security, Cisco. "We need clear rules to establish and manage access from human and non-human identities, and constant inspection of the meaning/intent of each MCP message. The combined approach of extending Duo's identity provider capabilities and Secure Access' MCP message security into AWS AgentCore will allow us to build and use AI confidently, keeping both our AI agents, team members, and data secure as we grow with AI."

Customers can leverage Cisco Duo IAM to integrate with AWS AgentCore for inbound and outbound authentication today. Cisco's MCP semantic inspection proxy is in production with selected customers, with broader integration planned for early 2026. This enables organizations to build and scale AI agent deployments with strict security and compliance. Genpact Enhances Financial Crime Detection at Scale for Apex Fintech Solutions Apex Fintech Solutions needed to transform financial crime investigation workflows across their digital clearing and custody platform. Their existing process struggled with siloed reporting structures, making it difficult to analyze patterns across accounts, entities, and transactions—limiting their ability to detect and prevent financial crimes while maintaining regulatory compliance.

Genpact implemented their riskCanvas® Data Explorer solution with a sophisticated agent-to-agent communication system built on AgentCore Runtime and leveraging AgentCore Observability. The implementation features a Supervisor Agent pattern coordinating specialized agents for accounts, entities, items, and transactions analysis. This agent intelligently translates domain-specific questions into data query and analysis tasks routed to the appropriate specialist agent. AgentCore Memory maintains context across complex investigations, transforming previously disconnected reporting structures into an integrated intelligence platform that delivers insights through natural language queries.

"Moving from efficiency gains to enterprise value requires a holistic approach to agent orchestration and lifecycle management – and that's exactly what we did for Apex leveraging Genpact's Agentic Development Lifecycle with Amazon Bedrock AgentCore to deliver a riskCanvas® Data Explorer roll-out that helped them improve financial crime detection and compliance", said Matt Arellano, AWS Practice Lead at Genpact.

The solution enables financial crime analysts to process daily trades across millions of brokerage accounts through natural language interactions. Following successful deployment, Apex plans to expand AgentCore across all financial crime detection workflows including AML monitoring, fraud detection, and compliance reporting. PwC Australia Accelerates Digital Modernization for Major Australian Bank PwC Australia is leading an Agentic transformation for a major Australian bank's modernization, streamlining architecture functions to accelerate projects, improve budgets, and prevent rework. By leveraging AgentCore Runtime and Gateway services, PwC is integrating strategic architecture AI agents within the bank's existing business processes to automate complex architectural reviews and ensure alignment with the bank's governance framework and pre-defined implementation patterns. The agents replace manual review processes with AI-driven pattern compliance checks and human-in-the-loop oversight for critical decision points.

"PwC is excited to be the transformation partner for pivotal Agentic transformation initiatives in the banking industry. These strategic initiatives will streamline technology development and modernisation programs – unlocking consistent improvements, returns on investment, along with business outcomes such as speed to market, competitive positioning, and the ability to launch innovative products at scale. PwC is leveraging AgentCore runtime and gateway services, to build an initial architecture agent, within a bank's existing processes, to target an estimated productivity improvement of over 30%, streamlining the bank's architecture functions, to accelerate projects, reduce budgets, and prevent the late emergence of critical issues," said Noel Williams, Banking and Capital Markets Lead Partner, PwC Australia.

AgentCore's serverless runtime enables PwC to deploy agents without managing infrastructure complexity, while Gateway services provide seamless integration with the bank's enterprise systems. Reply Transforms Totemia's Vacation Discovery with Conversational AI Totemia, France's leading childhood vacation camp platform, faced a critical challenge. Parents were overwhelmed with over 60 generic search results, couldn't handle multi-child searches, and saw no real-time availability, leading to customer abandonment and conversion losses.

Reply partnered with Totemia to develop an Offer Agent built on Strands SDK and hosted on AgentCore Runtime. The solution leverages AgentCore Gateway to invoke AWS Lambda functions generating dynamic SQL queries via Amazon Bedrock foundation models (Anthropic Claude) based on client criteria. These functions connect to Amazon RDS to fetch real-time offers and return the top 5 matches. AgentCore Memory enables multi-turn conversations, refining recommendations based on previous interactions. When no exact match exists, the agent proactively suggests adjusting criteria through iterative refinement. AgentCore Identity via Amazon Cognito provides authentication, while AgentCore Observability tracks performance and reliability.

"With AgentCore, we can easily develop and test our solutions using different Python frameworks, giving us flexibility and faster experimentation," said Xihui Wang, Reply Data Scientist. "The built-in gateway enables us to securely integrate Lambda functions as tools while also making them reusable across multiple agents. In addition, AgentCore provides built-in memory management, observability, and identity services, which reduce development overhead and simplify scaling to production."

AgentCore's ready-to-use building blocks eliminated custom infrastructure development, enabling Reply to move from prototyping to production rapidly. The agent narrows results from 60 to 5-10 tailored recommendations, achieving 65% reduced search time, 40% increased bookings, 24/7 availability, 25% conversion growth, and supporting 30,000 users monthly—projecting 200-300% ROI within year one. Conclusion These early-adopter success stories demonstrate how AWS Partners are capitalizing on AgentCore to deliver transformative value across industries – from financial services to travel, security to banking. As enterprise demand for AI agents accelerates, AgentCore enables partners to rapidly scale production-ready solutions while maintaining enterprise-grade security and performance.

The opportunity is immediate. Visit Amazon Bedrock AgentCore to start building today. Partners can access exclusive resources and latest capabilities through AWS Partner Central's Agentic AI Hub, and showcase their expertise through the AWS Generative AI Competency program.

TAGS: AI Agents, Amazon Bedrock, Amazon Bedrock Agents, Announcements, AWS Partner Network, Customer Solutions, Partner solutions

Introduction In this blog we outline options for online and offline data transfer scenarios to AWS. Customers' migration goals, available network bandwidth speed and reliability, and size of data to be imported, can then guide the choice on which service to use. AWS DataSync is a secure, online service that automates and accelerates moving data between on premises and AWS storage services and is the recommended service for data migration when network bandwidth is not a limiting factor. To mitigate network bandwidth limitations while using AWS DataSync, the general recommendation is to leverage an on-demand AWS Direct Connect hosted connection, for the duration of the data transfer, through AWS Direct Connect Delivery Partners. When your data is in a location where network bandwidth is limited or network connectivity is intermittent, we recommend AWS Data Transfer Terminal along with bring-your-own storage device. This blog post focuses on data transfer scenarios and provides guidance in choosing the right hardware, software, and upload suite reservation durations for your migration needs. We walk through possible factors preventing transfers from achieving optimal results and conclude with an overview of fully managed offerings taking care of the entire transfer experience. What is AWS Data Transfer Terminal AWS Data Transfer Terminal provides access to a network-ready, physical location ("Terminal") for customers to bring their storage devices for accelerated high-volume data transfer to or from the cloud. The service allows high-bandwidth connections of up to 100 Gbps to AWS public endpoints with a single fiber connection, available in secure, reservable AWS locations globally. Each AWS Data Transfer Terminal provides at least two fibers, allowing up to 200 Gbps by means of Equal-Cost Multi-Path (ECMP) routing or by connecting two devices in parallel, while select locations present four fibers (400 Gbps aggregate). While Amazon S3 buckets are a common destination for Data Transfer Terminal uploads, Amazon EC2 and other publicly accessible AWS services can be targets. Upload suite reservations can last up to 24 hours, and you must bring all necessary equipment, as AWS staff can't handle your data or assist with transfers. You'll be charged for on-demand use per hour for each location. There is a port hour charge, with lower costs if you upload data into AWS Regions in the same continent as your location. To learn more, visit the AWS Data Transfer Terminal pricing page. An overview of the service and the underlying infrastructure can be found in the following blog post. For an updated list of Data Transfer Terminal locations, see the FAQs. As prerequisites, customers should be familiar with data transfer methods, including loading on-premises data to the hardware storage device of choice, before bringing it to a Data Transfer Terminal location. To get started with data transfers using AWS Data Transfer Terminal, follow these steps:

Determine the appropriate location first. You can find an updated list of locations in the FAQs, under the section "Where are Data Transfer Terminals located?". Schedule the AWS Data Transfer Terminal session at your preferred site through the AWS Management Console at least 24 hours in advance. Prepare your hardware, ensuring compatibility with 100G QSFP-LR4 fiber connections and adequate system configurations for maximum throughput and optimal data upload experience. Refer to the documentation for more information. Make sure to jot down serial numbers of each server and storage device that will be brought in. They will have to be communicated to the security staff at check-in, and verified again upon leaving the location, as per standard datacenter security procedures. On the day of your visit, present government-issued ID for escorted access to a private transfer suite. Once in the upload suite, connect your pre-configured hardware to the provided high-speed ports and initiate your data transfer. When done, clear the room without leaving anything behind, and allow staff to cross-check serial numbers before leaving.

The Challenge of Uploading at 100 Gbps and Beyond For high-rate data transfers, the hardware and software you choose determines overall performance. Transferring at 10 Gbps is mainstream at the time of writing and poses little to no challenge for most existing devices with default configurations and no specific tuning. But as you get closer to target 100 Gbps and beyond, optimal results can only be obtained by using dedicated transfer software running on an OS with proper drivers and kernel tuning, installed on a balanced system with appropriate storage throughput, interface bandwidth, and network capacity to avoid bottlenecks. Fall short of any of these aspects, and you won't achieve the optimal speeds being targeted. Choosing a solution that fits your business and technical requirements can be daunting, especially considering the wealth of options on the market. To help navigate the landscape, we present the results of tests performed on select solutions in conjunction with AWS Data Transfer Terminal. Each solution offers different transfer speed, storage, weight, form and cost factors to suit different data transfer use cases. While a 100 Gbps bandwidth connection might seem straightforward to achieve, the effective throughput (that is, the rate of actual data transferred end-to-end) is lower because of various overheads. For instance, when transferring data through AWS Data Transfer Terminal, the maximum bandwidth of 100 Gbps (or 12.5 GB/s) can be witnessed on the fiber connection at the networking infrastructure level. However, the throughput (actual data transferred) over the same is impacted by factors such as TCP/IP overhead, storage performance, and the characteristics of the dataset being transferred. An Ethernet frame requires extra bits for preambles, headers, and spacing, which consumes bandwidth that is not used for data. Depending on packet size, the effective throughput can end up being 85% of the total bandwidth. Large files typically transfer faster than multiple small files because of reduced protocol overhead, making it essential to consider these nuances when planning data transfer operations. For these reasons, to estimate the effective throughput, we recommend accounting for a 15% overhead with respect to the connection bandwidth speeds provided in this article. Properly estimating transfer rates is crucial to reduce the risk of running out of time, or unnecessary costs because of over-booking. Choosing the Right Solution Customers have asked us what systems to bring along to the AWS Data Transfer Terminal location, as they all have different data upload needs. Some customers need an affordable way to upload small amounts of data on a regular basis, while others need to upload large amounts of data one time. Customers might have TV productions that need to upload several terabytes of footage daily to their geo-distributed post-production contractors. On the higher end, organizations may need to perform complete infrastructure migrations to AWS, moving petabyte-scale amounts of data within tight timeframes. This blog post navigates the reader through the aspects involved in choosing the most appropriate transfer solutions for your needs. Once the right solution is determined, customers can take advantage of AWS Data Transfer Terminal on-demand high bandwidth connectivity to get data into AWS quickly. For example, automotive companies such as Rivian have reduced their data upload times by 3x, as narrated in this blog post. Understanding Hardware and Software Components for Optimal Data Transfer Performance The following sections walk through the three key aspects of uploading data to an S3 bucket with AWS Data Transfer Terminal: the software stack, storage options, and the hardware solutions. Several options are discussed, considering budget, form factor, and time constraints. Transfer Software Agents To complete your data transfer onsite, we recommend the aws s3 cp and aws s3 sync commands that are part of our AWS Command Line Interface. For best transfer rates, review the documentation to enable optimizations such as leveraging the CRT (Common Runtime) base library. The CRT is a modular family of independent packages, written in C, more performant than the default Python interpreter. As an alternative, customers with demanding transfer goals are encouraged to explore their own implementation of S3 transfer agents by using AWS SDKs, available for different languages. The highest performing tested configurations leverage proprietary custom code implementing AWS SDK for C++. Parallelization and batching can help maximize hardware utilization, especially when dealing with multiple smaller files. Encryption, while essential for security and confidentiality, can introduce overhead and reduce transfer speeds, particularly on systems without hardware encryption acceleration. It's important to note that smaller files reduce overall transfer efficiency and increase the resulting transfer times, as each file requires its own overhead in terms of protocol headers, connection setup, and system processing, Storage Considerations The storage subsystem is generally the most common bottleneck hindering the maximum achievable throughput by a solution. Transferring at 100 Gbps requires reading from storage at a speed of 12.5 GB/s. Modern storage solutions offer various performance tiers. NVMe (Non-Volatile Memory Express) SSDs offer the highest performance. Gen 5 NVMe drives deliver around 14 GB/s, while SATA SSDs are limited to about 550 MB/s. Traditional spinning hard drives (HDDs) lag, with speeds ranging from 80-160 MB/s, making them suitable for archival storage but less ideal for high-performance data transfer operations. Creating RAID (Redundant Array of Independent Disks) with lower performing drives by combining multiple HDDs helps improve performance by aggregating capacity, performance, and redundancy. Only high-performing NVMe Gen5 can theoretically support 100 Gbps speeds without the need to be part of a RAID volume for improved aggregate read speeds. An overview of SSD vs HDD can be found here.

Figure 1: Storage devices, from Hard Disk Drives (HDD) with spinning disks, to modern compact Solid State Drives (SSD)

For most transfer solutions on the market, storage capacity is configurable. Hardware vendors allow configuring specific amounts of onboard storage at the time of order. Alternatively, mass storage can be directly attached and mounted as external storage. Extra storage can be externally mounted using Thunderbolt or SAS (Serial Attached SCSI) interfaces. For example, Thunderbolt 4, offers a theoretical bandwidth up to 40 Gbps (5 GB/s). SAS interfaces, generally used in enterprise environments, can achieve up to 22.5 Gbps per lane with SAS-4, making them excellent options for high-performance storage systems. Network Interfaces When considering network interfaces, modern Network Interface Cards (NIC) support speeds from 1 Gbps to 100 Gbps, though real-world performance is lower because of TCP/IP overhead and other factors. Ethernet ports with the standard RJ-45 copper connection and speed of 1 Gbps are nowadays mainstream, whereas 10 Gbps are gaining adoption. The 2×100 Gbps fiber connections provided by AWS Data Transfer Terminal require 100GBASE-LR4 optical QSFP28 transceivers, which most of the solutions reviewed in this blog post support. Jumbo frames are currently not supported.

Multiple Devices via a 100G Switch Another option is to leverage the onboard 10GbE Ethernet port commonly found in mainstream servers and devices nowadays, connected via a CAT7+ cable to a 10GbE port on a switch supporting 100 Gbps uplinks. The QSFP28 on the switch is then connected to the AWS Data Transfer Terminal fiber port. The approach removes the need for multiple expensive PCIe cards and/or external enclosures, using just a single transceiver per switch. At the time of writing, each AWS Data Transfer Terminal fiber connection provides up to a maximum of five IPs via Dynamic Host Configuration Protocol (DHCP) leases, therefore capping the maximum speed to 50 Gbps in aggregate when using Small Form-factor Pluggable (SFP) to combine multiple 10 Gbps copper connections to the storage devices. As every AWS Data Transfer Terminal has at least two fibers available, another setup in parallel can be used to provide another 50Gbps, bringing this up to 100 Gbps in aggregate.

Figure 3: Expected aggregated bandwidth up to 50 Gbps when connecting 5x 10GbE devices

Leveraging Multiple Fiber Connections Concurrently Transfer solutions supporting multiple fiber connections concurrently can go beyond 100 Gbps. A common approach leverages Equal-Cost Multi-Path (ECMP) for increased bandwidth, and it is generally available out of the box on most operating systems by default. Currently ECMP is the only option available to AWS Data Transfer Terminal customers to aggregate multiple connections from a single device. Other approaches requiring Border Gateway Protocol (BGP) or Link Aggregation Control Protocol (LACP) are not available at the time of writing. White-Glove End-to-End Transfer Services For customers looking for a fully managed experience, contracting a comprehensive AWS Partner service that includes support, physical transit and end-to-end storage transfer via AWS Data Transfer Terminal is a compelling solution. These partners can provide a fully managed experience that includes:

Initial delivery of storage device(s) to customer sites System set-up support loading data onto the devices on site Scheduling of the upload suite reservation Transporting the device to the AWS Data Transfer Terminal facility Data upload to AWS and associated transfer monitoring Clearing of customer data upon upload validation Returning storage devices to conclude the engagement

Conclusions Successful high-speed data transfer depends on the right hardware, storage, and software choices. This blog post covered various aspects involved to perform optimal data transfers via AWS Data Transfer Terminal, spanning from basic systems to advanced solutions hitting triple-digit Gbps. The solutions vary significantly in terms of throughput, storage capacity, form factor, and cost, allowing customers to choose based on their specific requirements. Correctly estimating the time to transfer data is paramount to support accurate planning of reservation hours, accounting for transfer protocols overheads and other real-world factors. For customers looking for a fully managed service, white-glove services are available through AWS Partners who can manage the entire transfer process from pickup to upload. As the AWS Data Transfer Terminal service continues to evolve, we encourage customers to share their experiences and testing results to help expand our knowledge base of solutions available for our customers. Submissions of new test results to be evaluated by our team can be done via Command Center, creating a case as Service: Data Transfer Terminal, Category: Other, Severity: General guidance.

Marcello Golfieri Marcello Golfieri is a Principal Solutions Architect at AWS in Strategic Accounts. With extensive experience in enterprise infrastructure engineering, resilient architectures, and DevOps, Marcello advises organizations operating at the most massive scale. Beyond work, loves playing Contract Bridge, learning foreign languages, and retrocomputing.

Sofia Guevara Sofia Guevara is part of the Customer Solutions team in Amazon Web Services, where she brings her enthusiasm for sustainable technology and solving enterprise challenges. Working backwards from customer needs, she helped shape AWS Data Transfer Terminal from beta to its launch at re:Invent 2024. As a strategic advisor to Amazon Devices, she champions sustainable architecture and cloud implementations. She believes in challenging customers to push boundaries and actively practice innovation. Prior to AWS, she drove technical consulting projects for business process management as a product owner of cloud modernizations.

Bhavya Paliwal Bhavya Paliwal is a Senior Product Manager at AWS focused on Data Transfer Terminal. She is also focused on product management for AWS Networking services where she drives the development of remote connectivity products. Bhavya has led product management for 6+ years and her goal is to deliver secure, user-friendly solutions that enable customers to seamlessly access their applications and workloads.

Camden Forgia Camden Forgia is a Principal Product Manager for Amazon Project Kuiper focused on delivering the best connectivity experience using Amazon Project Kuiper's constellation of low-earth orbit satellites. Previously, he was a product manager in Amazon Web Services (AWS) focused on building and launching products that deliver the AWS global network infrastructure in novel ways for AWS customers. He was the product manager for external services such as AWS Data Transfer Services and AWS Data Transfer Terminal. Camden has led operations and product management over 10 years for technology companies ranging from semiconductor to cloud industries.

Subbu Mahadevan Subbu Mahadevan is a Principal Product Manager at AWS focused on AWS Outposts and Snowball Edge services. Previously, he led product management for AWS network monitoring and data transfer services. Subbu has led product management for over two decades across branch office networking products, tools for infrastructure automation, and services for hybrid cloud environments.

Organizations need seamless access to their structured data repositories to power intelligent AI agents. However, when these resources span multiple AWS accounts integration challenges can arise. This post explores a practical solution for connecting Amazon Bedrock agents to knowledge bases in Amazon Redshift clusters residing in different AWS accounts. The challenge Organizations that build AI agents using Amazon Bedrock can maintain their structured data in Amazon Redshift clusters. When these data repositories exist in separate AWS accounts from their AI agents, they face a significant limitation: Amazon Bedrock Knowledge Bases doesn't natively support cross-account Redshift integration. This creates a challenge for enterprises with multi-account architectures who want to:

Leverage existing structured data in Redshift for their AI agents. Maintain separation of concerns across different AWS accounts. Avoid duplicating data across accounts. Ensure proper security and access controls.

Solution overview Our solution enables cross-account knowledge base integration through a secure, serverless architecture that maintains secure access controls while allowing AI agents to query structured data. The approach uses AWS Lambda as an intermediary to facilitate secure cross-account data access.

Users enter their natural language question in Amazon Bedrock Agents which is configured in the agent account. Amazon Bedrock Agents invokes a Lambda function through action groups which provides access to the Amazon Bedrock knowledge base configured in the agent-kb account above. Action group Lambda function running in agent account assumes an IAM role created in agent-kb account above to connect to the knowledge base in the agent-kb account. Amazon Bedrock Knowledge Base in the agent-kb account uses an IAM role created in the same account to access Amazon Redshift data warehouse and query data in the data warehouse.

Amazon Bedrock agent in the agent account that handles user interactions. Amazon Redshift serverless workgroup in VPC and private subnet in the agent-kb account containing structured data. Amazon Bedrock Knowledge base using the Amazon Redshift serverless workgroup as structured data source. Lambda function in the agent account. Action group configuration to connect the agent in the agent account to the Lambda function. IAM roles and policies that enable secure cross-account access.

Two AWS accounts. Create an AWS account if you do not have one. Specific permissions required for both account which will be set up in subsequent steps. Install the AWS CLI (2.24.22 – current version) Set up authentication using IAM user credentials for the AWS CLI for each account Make sure you have jq installed, jq is lightweight command-line JSON processor. For example, in Mac you can use the command brew install jq (jq-1.7.1-apple – current version) to install it. Navigate to the Amazon Bedrock console and make sure you enable access to the meta.llama3-1-70b-instruct-v1:0 model for the agent-kb account and access for us.amazon.nova-pro-v1:0 model in the agent account in the us-west-2, US West (Oregon) AWS Region.

Assumption Let's call the AWS account profile, agent profile that has the Amazon Bedrock agent. Similarly, the AWS account profile be called agent-kb that has the Amazon Bedrock knowledge base with Amazon Redshift Serverless and the structured data source. We will use the us-west-2 US West (Oregon) AWS Region but feel free to choose another AWS Region as necessary (the prerequisites will be applicable to the AWS Region you choose to deploy this solution in). We will use the meta.llama3-1-70b-instruct-v1:0 model for the agent-kb. This is an available on-demand model in us-west-2. You are free to choose other models with cross-Region inference but that would mean changing the roles and polices accordingly and enable model access in all Regions they are available in. Based on our model choice for this solution the AWS Region must be us-west-2. For the agent we will be using an Amazon Bedrock agent optimized model like us.amazon.nova-pro-v1:0. Implementation walkthrough The following is a step-by-step implementation guide. Make sure to perform all steps in the same AWS Region in both accounts. These steps are to deploy and test an end-to-end solution from scratch and if you are already running some of these components, you may skip over those steps.

Make a note of the AWS account numbers in the agent and agent-kb account. In the implementation steps we will refer them as follows:

Note: These steps use example profile names and account numbers, please replace with actuals before running. Create the Amazon Redshift Serverless workgroup in the agent-kb account:

Log on to the agent-kb account Follow the workshop link to create the Amazon Redshift Serverless workgroup in private subnet Make a note of the namespace, workgroup, and other details and follow the rest of the hands-on workshop instructions.

Set up your data warehouse in the agent-kb account. Create your AI knowledge base in the agent-kb account. Make a note of the knowledge base ID. Train your AI Assistant in the agent-kb account. Test natural language queries in the agent-kb account. You can find the code in aws-samples git repository: sample-for-amazon-bedrock-agent-connect-cross-account-kb. Create necessary roles and policies in both the accounts. Run the script create_bedrock_agent_kb_roles_policies.sh with the following input parameters.

–agent-kb-profile agent-kb The agent knowledgebase profile that you set up with the AWS CLI with aws_access_key_id, aws_secret_access_key as mentioned in the prerequisites.

–lambda-role lambda_bedrock_kb_query_role This is the IAM role the agent account Bedrock agent action group lambda will assume to connect to the Redshift cross account

–kb-access-role bedrock_kb_access_role This is the IAM role the agent-kb account which the lambda_bedrock_kb_query_role in agent account assumes to connect to the Redshift cross account

–kb-access-policy bedrock_kb_access_policy IAM policy attached to the IAM role bedrock_kb_access_role

–lambda-policy lambda_bedrock_kb_query_policy IAM policy attached to the IAM role lambda_bedrock_kb_query_role

–knowledge-base-id XXXXXXXXXX Replace with the actual knowledge base ID created in Step 4

–agent-account 111122223333 Replace with the 12-digit AWS account number where the Bedrock agent is running. (agent account)

–agent-kb-account 999999999999 Replace with the 12-digit AWS account number where the Bedrock knowledge base is running. (agent-kb acccount)

Download the script (create_bedrock_agent_kb_roles_policies.sh) from the aws-samples GitHub repository. Open Terminal in Mac or similar bash shell for other platforms. Locate and change the directory to the downloaded location, provide executable permissions:

If you are still not clear on the script usage or inputs, then you can run the script with the –help option and the script will display the usage: ./create_bedrock_agent_kb_roles_policies.sh –help Run the script with the right input parameters as described in the previous table.

./create_bedrock_agent_kb_roles_policies.sh --agent-profile agent \ --agent-kb-profile agent-kb \ --lambda-role lambda_bedrock_kb_query_role \ --kb-access-role bedrock_kb_access_role \ --kb-access-policy bedrock_kb_access_policy \ --lambda-policy lambda_bedrock_kb_query_policy \ --knowledge-base-id XXXXXXXXXX \ --agent-account 111122223333 \ --agent-kb-account 999999999999PowerShell

The script on successful execution shows the summary of the IAM, roles and policies created in both accounts.

Log on to both the agent and agent-kb account to verify the IAM roles and policies are created.

For the agent account: Make a note of the ARN of the lambda_bedrock_kb_query_role as that will be the value of CloudFormation stack parameter AgentLambdaExecutionRoleArn in the next step.

For the agent-kb account: Make a note of the ARN of the bedrock_kb_access_role as that will be the value of CloudFormation stack parameter TargetRoleArn in the next step.

Run the AWS CloudFormation script to create a Bedrock agent:

Download the CloudFormation script: cloudformation_bedrock_agent_kb_query_cross_account.yaml from the aws-samples GitHub repository. Log on to the agent account and navigate to the CloudFormation console, and verify you are in the us-west-2 (Oregon) Region, choose Create stack and choose With new resources (standard).

In the Specify template section choose Upload a template file and then Choose file and select the file from (1). Then, choose Next. Enter the following stack details and choose Next.

BedrockAgentDescription Agent to query inventory data from Redshift Serverless database Keep this as default

BedrockAgentInstructions You are an assistant that helps users query inventory data from our Redshift Serverless database using the action group. Do not change

Complete the acknowledgement and choose Next. Scroll down through the page and choose Submit. You will see the CloudFormation stack is getting created as shown by the status CREATE_IN_PROGRESS. It will take a few minutes, and you will see the status change to CREATE_COMPLETE indicating creation of all resources. Choose the Outputs tab to make a note of the resources that were created. In summary, the CloudFormation script does the following in the agent account.

Creates a Bedrock agent Creates an action group Also creates a Lambda function which is invoked by the Bedrock action group Defines the OpenAPI schema Creates necessary roles and permissions for the Bedrock agent Finally, it prepares the Bedrock agent so that it is ready to test.

Verify Nova Pro (us.amazon.nova-pro-v1:0) model access in the agent account. Navigate to the Amazon Bedrock console and choose Model access under Configure and learn. Search for Model name : Nova Pro to verify access. If not, then enable model access.

Verify access to the meta.llama3-1-70b-instruct-v1:0 model in the agent-kb account. This should already be enabled as we set up the knowledge base earlier.

Run the agent. Log on to agent account. Navigate to Amazon Bedrock console and choose Agents under Build. Choose the name of the agent and choose Test. You can test the following questions as mentioned the workshop's Stage 4: Test Natural Language Queries page. For example:

Who are the top 5 customers in Saudi Arabia? Who are the top parts supplier in the United States by volume? What is the total revenue by region for the year 1998? Which products have the highest profit margins? Show me orders with the highest priority from the last quarter of 1997.

Phrase your question to be more specific Use terminology that matches your table descriptions Try questions similar to your curated examples Verify your question relates to data that exists in the TPCH dataset Use Amazon Bedrock Guardrails to add configurable safeguards to questions and responses.

Clean up resources It is recommended that you clean up any resources you do not need anymore to avoid any unnecessary charges:

Navigate to the CloudFormation console for the agent and agent-kb account, search for the stack and and choose Delete. S3 buckets need to be deleted separately.

For deleting the roles and policies created in both accounts, download the script delete-bedrock-agent-kb-roles-policies.sh from the aws-samples GitHub repository.

Open Terminal in Mac or similar bash shell on other platforms. Locate and change the directory to the downloaded location, provide executable permissions:

If you are still not clear on the script usage or inputs, then you can run the script with the –help option then the script will display the usage: ./ delete-bedrock-agent-kb-roles-policies.sh –help Run the script: delete-bedrock-agent-kb-roles-policies.sh with the same values for the same input parameters as in Step7 when running the create_bedrock_agent_kb_roles_policies.sh script. Note: Enter the correct account numbers for agent-account and agent-kb-account before running.

./delete-bedrock-agent-kb-roles-policies.sh --agent-profile agent \ --agent-kb-profile agent-kb \ --lambda-role lambda_bedrock_kb_query_role \ --kb-access-role bedrock_kb_access_role \ --kb-access-policy bedrock_kb_access_policy \ --lambda-policy lambda_bedrock_kb_query_policy \ --agent-account 111122223333 \ --agent-kb-account 999999999999PowerShell The script will ask for a confirmation, say yes and press enter.

Summary This solution demonstrates how the Amazon Bedrock agent in the agent account can query the Amazon Bedrock knowledge base in the agent-kb account. Conclusion This solution uses Amazon Bedrock Knowledge Bases for structured data to create a more integrated approach to cross-account data access. The knowledge base in agent-kb account connects directly to Amazon Redshift Serverless in a private VPC. The Amazon Bedrock agent in the agent account invokes an AWS Lambda function as part of its action group to make a cross-account connection to retrieve response from the structured knowledge base. This architecture offers several advantages:

Uses Amazon Bedrock Knowledge Bases capabilities for structured data Provides a more seamless integration between the agent and the data source Maintains proper security boundaries between accounts Reduces the complexity of direct database access codes

As Amazon Bedrock continues to evolve, you can take advantage of future enhancements to knowledge base functionality while maintaining your multi-account architecture.

About the Authors Kunal Ghosh is an expert in AWS technologies. He passionate about building efficient and effective solutions on AWS, especially involving generative AI, analytics, data science, and machine learning. Besides family time, he likes reading, swimming, biking, and watching movies, and he is a foodie. Arghya Banerjee is a Sr. Solutions Architect at AWS in the San Francisco Bay Area, focused on helping customers adopt and use the AWS Cloud. He is focused on big data, data lakes, streaming and batch analytics services, and generative AI technologies. Indranil Banerjee is a Sr. Solutions Architect at AWS in the San Francisco Bay Area, focused on helping customers in the hi-tech and semi-conductor sectors solve complex business problems using the AWS Cloud. His special interests are in the areas of legacy modernization and migration, building analytics platforms and helping customers adopt cutting edge technologies such as generative AI. Vinayak Datar is Sr. Solutions Manager based in Bay Area, helping enterprise customers accelerate their AWS Cloud journey. He's focusing on helping customers to convert ideas from concepts to working prototypes to production using AWS generative AI services.

This post was contributed by Jiaqi Leng, Joseph Li, Xiaodi Wu Scientists and engineers face numerous computational challenges in fields like fluid dynamics [1], modeling heat and sound propagation [2], and aircraft design [3]. Simulating partial differential equations (PDEs) in high dimensions offers a powerful approach to addressing these challenges. However, solving these high-dimensional differential equations is challenging for classical computers, as the computational complexity increases exponentially with the problem dimension. Quantum computers, capable of efficiently manipulating high-dimensional data in a non-classical way, offer potential for addressing these complex problems. Over the past decades, there has been progress in developing quantum algorithms for PDEs, including both linear and nonlinear equations [4,5,6]. However, most existing quantum algorithms rely on sophisticated input models of the problem data, such as block-encoded matrices and QRAM, which require large, fault-tolerant quantum computers and are thus unlikely to be implemented with near-term quantum hardware. We, a group of researchers in the University of Maryland and the University of California, Berkeley, introduce a novel technique named Hamiltonian embedding [7], which aims to be a leap toward harnessing near-term quantum technology for simulating qudit Hamiltonians (e.g., high-dimensional PDEs with spatial discretization, bosonic systems, etc.) This technique allows us to map the desired simulation to a quantum evolutionary process that can be efficiently implemented with near-term quantum hardware. Upon measurement and simple post-selection, the dynamical properties of the desired high-dimensional PDE can be recovered. Notably, realizations of Hamiltonian embedding are not restricted to gate-based quantum computers. In this post, we demonstrate the use cases of Hamiltonian embedding in both the IonQ and QuEra devices, both of which are accessible through Amazon Braket.

Hamiltonian embedding: mapping differential operators to local spin operators To simulate differential equations on qubit-based quantum computers, it is necessary to discretize the differential equation in some way. Here, we consider the finite difference method applied to first and second order differential operators. Upon discretization, we can map the differential equation to a quantum dynamics problem via a finite-dimensional Hamiltonian called the problem Hamiltonian. For example, the evolution of a free particle is governed by the Schrödinger equation.

To discretize the (1D) Laplace operator, we can use the stencil method:

The resulting problem Hamiltonian (up to a minus sign) is a tridiagonal matrix with main diagonal elements Hj,j = -2h-2 and sub/super diagonal elements Hj,j+1=Hj+1,j=h-2. In practice, the problem Hamiltonians arising from many differential equations have the form of sparse, banded matrices (such as tridiagonal matrices) [8]. Ideally, one would like to have a quantum computer that directly simulates the time evolution of the problem Hamiltonian, without requiring any additional overhead. However, existing quantum computers are using local spin operators (i.e., Pauli matrices), and their native operators allow for only local interactions between qubits. Meanwhile, the qubit operator representation of sparse matrices often requires highly non-local interaction terms. How do we address this mismatch between the problem Hamiltonian and the device Hamiltonian? To enable the simulation of differential equations on quantum hardware, we develop a technique known as Hamiltonian embedding. The main idea is to map the problem Hamiltonian to an embedding Hamiltonian (i.e., local spin operators) which are more easily simulated by the physical hardware. More concretely, given a target problem Hamiltonian A, we aim to design a larger operator H (called an embedding Hamiltonian) composed of local spin operators that admits a block-diagonal decomposition: H = diag(A,*), where A is embedded in the upper left corner of H. Therefore, by simulating the time evolution of H on a quantum computer —presumably easier than simulating A itself — we effectively implement the evolution of A in the upper left corner e-iHt=diag(e-iAt,*). In what follows, we present an example to illustrate the idea. Let us consider an 8-by-8 binary-valued matrix A with all zeros except for A1,8=A8,1=1 and Aj,j+1=Aj+1,j=1 for j = 1,…,7.

This matrix is circulant and is a simple version of a Laplace operator with periodic boundary conditions. In the language of quantum computing, A corresponds to a three-qubit Hamiltonian. Although A has a simple structure, representing it as a quantum circuit (for example, using block-encoding) is challenging and may require multiple extra qubits to assist in the process [9]. If we break down A into combinations of basic Pauli strings, we end up with many component operators; some of them, like XXX, involve interactions between 3 qubits at once. This makes simulating such a seemingly simple Hamiltonian a challenge for today's quantum computers. Alternatively, Hamiltonian embedding allows us to represent A using only very simple quantum operations. The idea is to consider a 4-qubit Hamiltonian Sx=X1+X2+X3+X4, where each Xj is a Pauli-X operator acting on the site j. The Hilbert space of this Hamiltonian has basis. Then, we consider an 8-dimensional subspace spanned by the basis shown in Table 1 (called circulant unary code, see Sec. B.2.1 in [7]):

Table 1: Example of the circulant unary code for representing banded circulant matrices. We can readily verify that projecting Sx onto this subspace yields exactly our target matrix A, as the Hamming distance between two adjacent codewords (including between 1 and 8) is always 1. By preparing an initial state within this subspace and penalizing any leakage outside of it, researchers can achieve the quantum simulation of A using a small number of elementary gates (or analog evolution time) without ancilla qubits. We post-select measurement outcomes confined to the relevant subspace, disregarding those outside of it. For this problem, alternative embedding schemes — such as those based on antiferromagnetic or one-hot codes — can achieve the same goal. For more details of the Hamiltonian embedding technique, we encourage interested readers to read the original paper [7].

Simulating 2D Schrödinger equation using QuEra The time evolution of a quantum system in real space is described by the Schrödinger equation:

where u(t,x) is the wave function, f(x) is a potential function, and the spatial variable x is in -dimensional real space. In practice, the dimension d may be very high, corresponding to the number of electrons in quantum chemistry [10] or the dimension of complex nonlinear optimization problems [11]. In this blog, we showcase how to leverage Hamiltonian embedding to simulate the dynamics generated by a two-dimensional Schrödinger equation using real quantum computing devices. By performing spatial discretization of the system Hamiltonian

Where D is the discretized 1D Laplace operator (as discussed above), and U is a diagonal matrix corresponding to the potential field f(x,y). The size of the problem Hamiltonian is N2, where N is the discretization number per dimension. On the QuEra device, the machine Hamiltonian is given by

Where A(t) and B(t) denote the Rabi frequency and detuning, respectively; Xj's are the Pauli-X matrices, nj counts if the j-th qubit is in the excited state (i.e., number operator); Vij=C6/|ri – rj|6 denotes the Rydberg interaction between atoms i and j. Since the Rydberg interaction coefficient Vij must be positive, the Rydberg Hamiltonian HRyd requires a ingenious design of the embedding scheme, as the unary and one-hot embeddings cannot be straightforwardly applied. In order to match the device Hamiltonian, we devise an embedding scheme which we call the antiferromagnetic embedding. Intuitively, this encoding allows us to label the states using domain walls in an antiferromagnet (i.e., two neighboring qubits measured in the opposite states), as given in Table 2.

Table 2: Example of antiferromagnetic code for representing the real-space Schrödinger equation. As it turns out, this embedding scheme enables us to map the problem Hamiltonian Hprob to the Rydberg Hamiltonian HRyd via a deliberate choice of the parameters A(t), B(t), and the atom locations. In our experiment, we choose a discretization number N=7, which amounts to a total of 12 qubits, where the qubits are grouped into two chains of 6 qubits each. Each chain of qubits is used to represent a single spatial variable (x and y). With the Hamiltonian embedding, the (discretized) kinetic operator is mapped to the Pauli-X operators:

and the (discretized) potential operator is mapped to the diagonal part in the QuEra machine Hamiltonian:

Although encoding arbitrary continuous potential U requires more complex programmability than what is currently available, the native Rydberg interactions give rise to an effective potential field with checkerboard-like pattern. In Figure 1, we demonstrate the quantum simulation results on QuEra at various times. The 3D bar plot shows the quantum wave packets at T = 0.5, 1, 1.5, where the QuEra results are obtained by performing computational basis measurement. Despite some hardware-induced noise, our experimental results show a qualitative match between the QuEra device and the numerical simulation.

Figure 1 – Setup and results for simulating 2D Schrödinger equation using QuEra. Rydberg atom positions in the experiment¬ (left), measurement resu¬lts from numerical simulation (top right) and experiment results obtained from QuEra Aquila (bottom right). Taken from [7].

Simulating 1D Schrödinger equation using IonQ In the previous experiment, the engineering of the potential field U is heavily restricted by the neutral atom configuration. We now demonstrate the simulation of Schrödinger equations with more structured potentials. For demonstration purposes, we illustrate how to simulate a one-dimensional Schrödinger equation on IonQ's 25-qubit trapped ion quantum c­omputer. It is worth noting that our method can be readily generalized to arbitrary spatial dimensions with only polynomial overheads in the dimension of the Schrödinger equations. We consider a quantum Hamiltonian for a single bosonic mode:

The corresponding Schrödinger equation is defined over the real line. Due to the unbounded physical space, we map the physical Hamiltonian Hbs to a finite-dimensional tridiagonal matrix using a method known as Fock space truncation (a different method than the previously used spatial discretization). To map this matrix (i.e., the problem Hamiltonian) to one that is more easily implemented on IonQ, we use the one-hot embedding. Intuitively, the one-hot embedding makes use of the location of a single excitation to determine which state the system is in. Importantly, the use of Hamiltonian embedding gives us an embedding Hamiltonian which involves at most two-qubit operators (such as XX interactions). While one-hot embedding is known to be less compact and does not lead to speedups in one spatial dimension, our Hamiltonian embedding framework allows us to leverage the natural tensor structure in the Schrödinger equation. As a result, to simulate a Schrödinger equation in d dimensions, we only require O(d) qubits and polynomial-in-d interactions terms, yielding an exponential quantum advantage over naïve mesh-based approaches to PDE simulation. The resulting embedding Hamiltonian contains up to 2-body interaction terms, which is suitable for near-term devices. More importantly, the embedding Hamiltonian can be split into a sum of an ''off-diagonal'' part and a ''diagonal'' part: the ''off-diagonal'' part can be directly implemented using the Mølmer–Sørensen gate (native to IonQ), and the ''diagonal part" can be realized by parameterized single-qubit Z rotations. Since both Hamiltonian components preserve the number of excitations, by Trotterizing the dynamics, we effectively implement flip-flop dynamics that approximately simulate the 1D bosonic Hamiltonian Hbs. For details, we refer the readers to Sec. 3.4 in [7]. In our experiment, we truncate the Fock space to 5 levels and simulate a one-dimensional example. The initial state is a Gaussian distribution centered at 0 and the potential field is a quadratic function centered at 0.25. The wave function oscillates left and right over time, and we can simulate the dynamics for various time points to observe its behavior. The position and momentum operators are embedded using both XX and Z operators, so we perform observable measurements in both bases to compute their expectation values (Figure 2). Although there is substantial error due to noisy gates, we still observe the same oscillating behavior in our experiments as predicted by analytical closed-form solutions.

Figure 2 – Problem setup and results for real-space quantum simulation on IonQ Aria-1. As shown in the leftmost figure, the initial state is a Gaussian distribution centered at 0, and the potential field is a parabola centered at 0.25. Experiment results for the expected position (middle) and kinetic energy (right) are shown in blue, with the closed form solutions shown in pink. Taken from [4].

Discussion and outlook We have shown how our Hamiltonian embedding technique expands the realm of quantum applications on near-term hardware. Our experiments demonstrate the first steps towards simulating high-dimensional differential equations, one of the most anticipated practical applications of quantum computers. Since the technique is broadly applicable to both analog and digital quantum computers, we expect that Hamiltonian embedding can enable quantum applications both on near-term devices and in the future with the rapid advancement of quantum technologies. Beyond quantum dynamics problems, we are excited to see how Hamiltonian embedding can accelerate practical implementations of quantum computing applications in other domains. References [1] Kundu P. K., Cohen I.M., Dowling D.R., & Capecelatro J. (2024). Fluid mechanics. Elsevier. [2] Evans, L. C. (2022). Partial differential equations. American Mathematical Society. [3] Anderson, J. (2011). Fundamentals of Aerodynamics (SI units). McGraw hill. [4] Childs, A. M., Liu, J. P., & Ostrander, A. (2021). High-precision quantum algorithms for partial differential equations. Quantum, 5, 574. [5] Liu, J. P., Kolden, H. Ø., Krovi, H. K., Loureiro, N. F., Trivisa, K., & Childs, A. M. (2021). Efficient quantum algorithm for dissipative nonlinear differential equations. Proceedings of the National Academy of Sciences, 118(35). [6] Jin, S., & Liu, N. (2022). Quantum algorithms for computing observables of nonlinear partial differential equations. arXiv preprint arXiv:2202.07834. [7] Leng, J., Li, J., Peng, Y., & Wu, X. (2024). Expanding Hardware-Efficiently Manipulable Hilbert Space via Hamiltonian Embedding. Quantum, 9, 1857. [8] Morton, K. W., & Mayers, D. F. (2005). Numerical solution of partial differential equations: an introduction. Cambridge university press. [9] Camps, D., Lin, L., Van Beeumen, R., & Yang, C. (2024). Explicit quantum circuits for block encodings of certain sparse matrices. SIAM Journal on Matrix Analysis and Applications, 45(1), 801-827. [10] Babbush, R., Berry, D. W., McClean, J. R., & Neven, H. (2019). Quantum simulation of chemistry with sublinear scaling in basis size. npj Quantum Information, 5(1), 92. [11] Leng, J., Hickman, E., Li, J., & Wu, X. (2023). Quantum Hamiltonian Descent. arXiv preprint arXiv:2303.01471.

TAGS: Amazon Braket, Hamiltonian, ionQ, quantum algorithms, quantum computing, quantum research, Quantum Technologies, QuEra

Jiaqi Leng Jiaqi Leng is a Postdoctoral Researcher at UC Berkeley. His research focuses on quantum algorithms for mathematical optimization and scientific computing.

Joseph Li Joseph Li is a PhD student the University of Maryland, advised by Prof. Xiaodi Wu. He is interested in near-term implementable quantum algorithms for scientific computing and practical approaches for Hamiltonian simulation.

Xiaodi Wu Xiaodi Wu is an Associate Professor in the Department of Computer Science at the University of Maryland and a Fellow at the Joint Center for Quantum Information and Computer Science (QuICS). He is also an Amazon Visiting Academic working for Amazon Braket. He aims to bridge the gap between the theoretical foundation of quantum computing and the limitation of realistic quantum machines.

Mastering Multi-Region Resilience and Scalability: Active-Active Design with Amazon ElastiCache Redis

This post is co-written by Jayadev Sirimamilla from Citibank, along with Sayan Deb Ghosh and Dibyarup Basu from TCS. Introduction: The Active-Active Redis Challenge in Cloud Migration In today's digital-first world, milliseconds (ms) matter, especially for global financial institutions processing millions of transactions. Customers need their financial transactions to be instantly recorded, reflected, and accessible across geographical regions in real-time. Translating the business promise of delivering sensitive data like financial transactions, reflecting latest state reliably, in real-time, over a geographically distributed landscape, for millions of transactions per hour, necessitates solutions like Distributed Cache e.g. Redis Cache. While Amazon Web Services (AWS) drives digital transformation, migrating Active-Active Redis deployments presents challenges. Enterprises use Active-Active Redis across global data centers through Redis Enterprise, providing cross-regional reliability and seamless failover. However, Amazon ElastiCache for Redis, while offering superior elasticity, operational simplicity, cost efficiency and cross-region replication, doesn't natively support Active-Active configuration across regions—a feature many organizations rely upon in their on-premise Enterprise Redis deployments. This perceived gap often raises concerns among technical decision-makers looking to escape the constraints of traditional infrastructure:

Capital-intensive capacity planning requiring upfront investments Burdensome annual licensing compliance and audits Inflexible multi-year contractual commitments Over-provisioning to accommodate peak loads

This blog demonstrates how Tata Consultancy Services (TCS) developed an innovative approach to achieve Active-Active functionality for Amazon ElastiCache across multiple AWS regions, bridging the gap between on-premise capabilities and cloud-native services—all while eliminating the need for Enterprise Redis licensing costs. TCS is an AWS Premier Tier Services Partner and Managed Cloud Services Provider (MSP) with Migration Competency. Citigroup's Existing Architecture and Migration Challenge: Citigroup, one of the world's leading financial institutions digital banking systems relied on a dual-region infrastructure in Singapore and Tokyo, using Active-Active Enterprise Redis Cluster for caching user sessions and business rules. The system processed 3 million cache hits hourly (60% read, 40% write), with 70% of reads accessing local data and 30% requiring cross-region access. During their AWS migration, a critical challenge emerged: while their existing Redis Enterprise supported Active-Active configuration across regions, Amazon ElastiCache (mandated by their Enterprise Architecture board) only supported Active-Passive setups. This limitation threatened to increase latency for write operations from the secondary region, potentially impacting their global digital banking services' performance and customer experience. The situation required an innovative solution to maintain global write capabilities within AWS's architecture. Pilot Scope and requirements: The customer's Technology Pilot requirements focus on validating the core Active-Active Redis architecture across regions while incorporating essential production features such as at-rest and in-transit encryption, multi-AZ deployment, cluster mode sharding, and VPC peering configurations as a minimum viable product. While these production-grade features add complexity to baseline performance measurements and introduce some performance overhead, their inclusion provides a realistic assessment of the production-ready solution. The Pilot should be able to prove that it can replicate the active-active geo distribution capability in Amazon ElastiCache Redis without a need to performing any application code refactor. It should achieve the same by deploying Multiple Clusters of Amazon Global ElastiCache Redis ensuring latest copy of data always being stored locally using its native Cross-Region replication feature. Pilot Success Criteria:

Regional Writes: Achieve single digit ms write latency in each geographic region without adding the complexity of multi-AZ, encryption etc. Regional Reads without conflict resolution: All read operations accessing data from the local region's read/write node must complete in less than 10ms, which addresses the 70% of workload that occurs within the same region. Regional Reads with conflict resolution: When data is replicated across regions, multiple versions of the same record may exist simultaneously, accounting for 30% of the total workload. For these cross-region operations, read requests requiring conflict resolution must complete in less than 20ms to maintain application performance. Seamless Failover: In case of ElastiCache failure in any region, the replicated node in the surviving region should be able accept applications Write traffic from another region along with Reads.

Figure1: Cross-Region Active-Active Amazon ElastiCache Architecture Active-Active Amazon ElastiCache Architecture Walk through:

Setup two Amazon ElastiCache for Redis clusters in Active/ Passive mode:

Cluster One (C1): In the Singapore region, C1:Node1 serves as the primary Read/Write node, handling local write operations for Singapore traffic while also providing read access to all data directly written to this region. To ensure data availability across regions, all data written to C1:Node1 in Singapore is asynchronously replicated to C1:Node2 in Tokyo. This replication performed within a range of 5-10 ms enables local read access in the Tokyo region, allowing applications in Tokyo to retrieve Singapore-originated data without the need for cross-region queries, thus optimizing read performance and reducing latency. Cluster Two (C2): In the Tokyo region, C2:Node1 functions as the primary Read/Write node, processing local write operations for Tokyo traffic while providing read access to all data written directly to this region. To maintain data consistency, all data written to C2:Node1 in Tokyo is asynchronously replicated to C2:Node2 in Singapore, enabling local read access in the Singapore region. This replication strategy allows Singapore-based applications to access Tokyo-originated data through local reads, optimizing performance and minimizing cross-region latency.

Write Operations: Write operations are processed by the local Node1 in each region, with data being asynchronously replicated to Node2 of the same cluster in the other region. For example, when a write occurs in Singapore, C1:Node1 (Singapore) processes it and then replicates the data to C1:Node2 (Tokyo). Each write operation includes timestamp metadata, which is crucial for the system's conflict resolution mechanism during read operations. Read Operations and Conflict Resolution: The architecture employs a concurrent dual-node query strategy for read operations, enhancing data consistency across regions. Applications simultaneously query both local nodes in their region (e.g., in Tokyo, C2.Node1 and C1.Node2). To manage conflicts, each data record includes a timestamp field ("ts"). When responses are received, the system compares timestamps, considering the record with the most recent timestamp as authoritative. By automatically selecting the most up-to-date version, the system ensures users receive consistent information regardless of which regional node they query. This approach effectively handles asynchronous replication between regions, providing a coherent global data view. Seamless Failover: In case, of any Amazon ElastiCache failure in any region, the Node2 in other region can be manually promoted to ACTIVE status. The promoted Node2 will then handle both READ and WRITE operations. This failover mechanism ensures business continuity with minimal disruption.

Testing and Validation: Write Propagation Scenarios: When a new record is written in Region-1:

Cluster1:Node1 (Read/ Write) {"id":1, "name":"John", "ts":"20240730″} Cluster1:Node2 (Read) {"id":1, "name":"John", "ts":"20240730″}

Reading from either region returns: {"name": "John", "ts": "20240730"} Update Conflict Resolution: When an update occurs in Region-2:

The system fetches all records with ID:1, compares timestamps, and returns the most recent: {"id":1, "name": "John Doe", "ts": "20240731"} Delete Operations and Data Consistency: When records expire in one or more regions:

The system returns the only surviving record: {"id":1, "name": "John Doe", "ts": "20240731"} Pilot Results: Citigroup's Expectation: Amazon ElastiCache must match or exceed the performance metrics offered by current on-premise Active-Active Enterprise Redis Cluster for successful migration approval. Performance Requirements Comparison:

Regional Read Latency without conflict resolution < 10 ms Met

Regional Read Latency with conflict resolution < 20 ms Met

Business Benefits: 1. Time to Market: The solution built using Amazon ElastiCache Redis enables fast re-platform migration without major code refactoring, improving operational efficiency and market responsiveness. 2. Resilience: Architecture ensures business continuity during regional disruptions through seamless failover capabilities, maintaining enterprise-grade reliability for global banking operations. 3. Architecture Alignment: The architecture complies with Citigroup's Enterprise Architecture standards, enabling smooth migration of critical workloads to AWS without exceptions. 4. Performance: It matches previous Redis Enterprise system's performance, maintaining single-digit ms latency and cross-region data consistency. 5. Cost Efficiency: Eliminates Redis Enterprise licensing fees and related compliance costs. Optimizes resources through auto-scaling, following pay-as-you-go model instead of over-provisioning Conclusion: This innovative approach to implementing Active-Active functionality with Amazon ElastiCache demonstrates how organizations can maintain enterprise-grade capabilities while embracing cloud-native services in lieu of procuring expensive licensed enterprise grade products. For financial institutions and other enterprises with similar requirements, this architecture provides a blueprint for successful migration of mission-critical Redis workloads to AWS. TCS has a proven record of migrating mission-critical applications to AWS with associates who are trained and certified in AWS services implementation. For more information about migrating Enterprise Redis workloads and implementing Active-Active functionality with Amazon ElastiCache on AWS, please contact the TCS team.

Jayadev Sirimamilla Jayadev Sirimamilla is a Digital Lead Principal Engineer & Emerging Tech at Citibank. He brings extensive experience in software engineering, platform architecture, and guiding strategic technology initiatives

Sayan Deb Ghosh Sayan Deb Ghosh is a Director of Solutions Architecture at TCS, specializing in Application and Data modernization on Cloud. He has 20+ years of experience in designing and delivering solutions for complex application transformations. He is focused on enabling AI-led platforms to accelerate cloud modernization. Outside of work, Sayan is interested in hiking, playing Table Tennis and demystifying data

Dibyarup Basu Dibyarup Basu is a Solution Architect at TCS, specializing in Cloud Modernization. With over 20 years of experience, Dibyarup has led solution architecture and design for cloud-native backend microservices and integration stacks across diverse geographies and domains. Outside of consulting, he enjoys Indian classical music, playing cricket, and spending quality time with family and friends.

Sanjay Gupta Sanjay Gupta is a Principal Solution Architect managing TCS partnerships worldwide. With over 25 years of experience in technology and industry, he brings a unique blend of domain knowledge and technical leadership. Sanjay specializes in migrations, modernization, and large-scale transformation initiatives across multiple industries, including financial services and the travel sector. Outside of work, Sanjay enjoys hiking, watching cricket, and exploring new places.